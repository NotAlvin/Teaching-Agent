topic_id,title,latex_content,text_content,content_type
1,What Is Linear Algebra,"\chapter{\whatIsTitle?}\label{warmup}


Many difficult problems can be handled easily 
once  relevant information is organized in a certain way. 
This text aims to teach you how to organize information in cases where certain mathematical structures are present. 
Linear algebra is, in general, the study of those structures. Namely

\vspace{3mm}
\shabox{Linear algebra is the study of vectors and linear functions.} 
\vspace{3mm}

\noindent In broad terms, vectors are things you can add and  linear functions are %very special 
functions of vectors that respect vector addition. 
The goal of this text is to teach you to organize information about vector spaces in a way that makes problems involving linear functions of many variables easy. 
(Or at least tractable.) 

To get a feel for the general idea of organizing information, of vectors, and of linear functions this chapter has brief sections on each. 
We start here in hopes of putting students in the right mindset for the 
odyssey that follows; the latter chapters cover the same material at a slower pace.  
% a little better lets try some examples. 
Please be prepared to change the way you think about some familiar mathematical objects
and keep a pencil and piece of paper handy!

\section{Organizing Information}
\label{organize}
Functions of several variables are often presented in one line such as 
$$f(x,y)=3x+5y\,.$$
But lets think carefully; what is the left hand side of this equation doing? 
Functions and equations are different mathematical objects so why is the equal sign necessary?

\begin{center}
\href{https://www.youtube.com/watch?v=dvoG5P9Uczg}{\raisebox{-.4cm}{\includegraphics[scale=.075]{take1.jpg}}} \hspace{1cm}\scalebox{1.2}{\tt A Sophisticated Review of Functions }
\end{center}
If someone says 
\vspace{-.01cm}
\begin{center}
``Consider the function of two variables $7\beta-13 b$."" 
\end{center}
we do not quite have all the information we need to determine the relationship between inputs and outputs. 
%This is the foundation of the powerful yet easy to use  mathematics known as linear algebra. 
%%%%an alternative wording
%Difficult %science 
%problems involving multiple variables can be handled easily 
%once  relevant information is organized in a certain way. 
%You are about to learn powerful methods of organizing information 
%applicable when a certain kind of  object is involved in a problem; a linear function. It takes quite a bit of work to understand what a linear function is, but it does not take much to see what it looks like to organize information.   
%%This is the foundation of the powerful yet easy to use  mathematics known as linear algebra. 
\begin{example} (Of organizing and reorganizing information)\\ \label{OrderInfo}
You own stock in 3 companies: $Google$, $Netflix$, and $Apple$. 
The value $V$ of your stock portfolio as a function of the number of shares you own $s_{N}, s_{G},s_{A}$ of these companies 
%with logos 
 is %given by
$$
%%%%%%%%%%%%%%%%
%%%% HEY 
%%%% Do not type V\colvec{ s_a\\s_b\\s_c }= here!
%%%%That already involves choice of  an order! 
24s_{G}+80s_{A}+35s_{N}\,.$$
Here is an ill posed question: what is $V\colvec{1\\2\\3} $?\\

The column of three numbers is ambiguous! 
%We can't determine if 
Is it is meant to denote % the value of your portfolio 
\begin{itemize}
\item 1 share of ${G}$, 2 shares of ${N}$ and 3 shares of ${A}$? 
\item 1 share of ${N}$, 2 shares of ${G}$ and 3 shares of ${A}$?  
\end{itemize}
Do we multiply the first number of the input by 24 or by 35? 
No one has specified an order for the variables, 
so we do not know how to calculate an output associated with a particular input.\!\footnote{Of course we would know how to calculate an output if the input is described in the tedious form such as ``1 share of ${G}$, 2 shares of ${N}$ and 3 shares of ${A}$"", but that is unacceptably tedious! We want to use ordered triples of numbers to concisely describe inputs.}% that input is written as a triple of numbers. 

A different notation for $V$ can clear this up; 
%since specifying an oder for the variables amounts to instructions of which number from the input to multiply by which of thee numbers 80, 35, 24, 
we can denote $V$ itself as an ordered triple of numbers that reminds 
us what to do to each number from the input.
\begin{center}
\shabox{Denote $V$ by  $\rowvec{ 24 &80& 35 } $ and thus write 
$V\colvec{ 1\\2\\3 }_{\!\!\!\!B }
= 
\begin{pmatrix} 24 &80& 35 \end{pmatrix}
\colvec{ 1\\2\\3 }%_{\!\!\! \!\alpha }.
$
\qquad \qquad
\phantom{ $\colvec{1\\2} $}%for space
to remind us to calculate 
$24(1)+80(2)+35(3)=334$ \qquad\qquad
\phantom{ $\colvec{1\\2} $}%for space
because we chose the order 
%$\colvec{s_{G}\\s_ {A}\\s_{N}}$ 
$\colvec{{G} & {A} & {N}}$ 
and named that order $B$ \qquad
so that inputs are interpreted as 
$\colvec{s_{G}\\s_ {A}\\s_{N}}$ 
.} 
\end{center}
%as a concise reminder of how to calculate the output assigned to any particular input ... {\it but only if you know what the input means}. 
%The input is a column of three numbers. 
%You need to know to interpret the 
%top number as stock in company $a$, 
%second from the top in company $b$, etc.. 
%Thus, the choice of notation for $V$ involved a choice of order of the companies. 
If we change the order for the variables we should change the notation for $V$.
\begin{center}
\shabox{
Denote $V$ by  $\rowvec{  35 & 80& 24 } $ and thus write 
$V\colvec{ 1\\2\\3 }_{\!\!\!\!B'}
= 
\rowvec{  35 & 80& 24 } 
\colvec{ 1\\2\\3 }$
\qquad \qquad
\phantom{ $\colvec{1\\2} $}%for space
to remind us to calculate 
$35(1)+80(2)+24(3) =264.$ \qquad\qquad
\phantom{ $\colvec{1\\2} $}%for space
because we chose the order 
$\colvec{{N} & {A} & {G} }$ 
and named that order $B'$\qquad
so that inputs are interpreted as 
$\colvec{s_{N}\\ s_ {A}\\ s_{G}}$ 
.
} 
\end{center}
The  subscripts $B$ and $B'$ on the columns of numbers are just symbols\footnote{We were free to choose any symbol to denote these orders. We chose $B$ and $B'$ because we are hinting at a central idea in the course: choosing a basis.} reminding us of how to interpret the column of numbers.
But the distinction is critical; as shown above
$V$ assigns completely different numbers to the same  columns of numbers with different subscripts. 

There are six different ways to order the three companies. 
Each way will give different notation for the same function $V$, and a different way of assigning numbers to columns of three numbers. 
Thus, it is critical to make clear which ordering is used if the reader is to understand what is written. 
Doing so is a way of organizing information. 

This example is a hint at a much bigger idea central to the text; 
our choice of order is an example of choosing a {\it basis}\footnote{
Please note that this is an {\it example} of choosing a basis, 
not a statement of the definition of the technical term ``basis"". 
You can no more learn the definition of ``basis""  from this example 
than learn the definition of  ``bird"" by seeing a penguin.}\!\!. 
\end{example}

The main lesson of an introductory linear algebra course is this: 
you have considerable freedom in how you organize information about certain functions, 
and you can use that  freedom to 
\begin{enumerate}
\item uncover aspects of functions that don't change with the choice (Ch \ref{eigenvalseigenvects})
\item make calculations maximally easy (Ch \ref{sec:diagonalization}  and Ch \ref{sec:leastsquaresSVD}) 
\item approximate functions of several variables (Ch \ref{sec:leastsquaresSVD}).
\end{enumerate}
Unfortunately, because the subject (at least for those learning it) requires seemingly arcane and tedious computations
involving large arrays of numbers known as matrices, the key concepts and the wide applicability of linear algebra are
easily missed. So we reiterate, 

\vspace{3mm}
\shabox{Linear algebra is the study of vectors and linear functions.} 
\vspace{3mm}

\noindent In broad terms, vectors are things you can add and  linear functions are 
functions of vectors that respect vector addition. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
\noindent 
\section{What are Vectors?}
Here are some examples of things that can be added:

\begin{example} (Vector Addition)
\begin{enumerate}[(A)]
\item Numbers: Both $3$ and $5$ are numbers and so is $3+5$.\\[-2mm]
\item 3-vectors: $\colvec{1 \\ 1\\ 0} + \colvec{0\\1\\1}=\colvec{1 \\ 2\\ 1}$.\\[-1mm]
\item Polynomials: If $p(x)=1+x-2x^2+3x^3$ and $q(x)=x+3x^2-3x^3+x^4$ then\\[1mm] their sum $p(x)+q(x)$ is the new polynomial $1+2x+x^2+x^4$.\\
\item Power series: If $f(x)=1+x+\frac1{2!} x^2 + \frac1{3!} x^3 +\cdots$ and $g(x)=1-x+\frac1{2!} x^2 - \frac1{3!} x^3 +\cdots$ \\[1mm]
 then $f(x)+g(x)=2+ x^2 +\frac2 {4!} x^4+\cdots$ is also a power series.\\
\item Functions: If $f(x)=e^x$ and $g(x)=e^{-x}$ then their sum $f(x)+g(x)$ is the new function $2\cosh x$.
\end{enumerate}
\end{example}

\noindent
There are clearly different kinds of vectors. 
Stacks of numbers are not the only things that are vectors, as examples C, D, and E show. 
Vectors of different kinds can not be added; What possible meaning could the following have? 
$$\colvec{9\\3} + e^x$$

In fact, you should think of all five kinds of vectors above as different kinds, and that you should not add vectors that are not of the same kind. 
On the other hand, any two things of the same kind ``can be added''. 
This is the reason you should now start thinking of all the above objects as vectors! 

In Chapter~\ref{vectorSpaces} we will give the precise rules that  vector addition must obey. 
In the above examples, however, notice that the vector addition rule stems from the rules for adding numbers. 
 

When adding the same vector over and over, for example
$$
x+x\, ,\: \  x+x+x\, ,\:\    x+x+x+x\,  ,\, \ldots\, ,
$$
we will write
$$
2x\, ,\: \, 3x\, , \:\,  4x\, ,\, \ldots\, ,
$$
respectively. For example
$$
4\colvec{1\\1\\0}=\colvec{1\\1\\0}+\colvec{1\\1\\0}+\colvec{1\\1\\0}+\colvec{1\\1\\0}=\colvec{4\\4\\0}\, .
$$
Defining $4x=x+x+x+x$ is fine for integer multiples, but does not help us make sense of $\frac13 x$. For the different types of vectors 
above, you can probably guess how to multiply a vector by a scalar. For example
$$
\frac 13 \colvec{1\\[1mm]1\\[1mm]0} = \colvec{\frac13\\[1mm] \frac13\\[1mm]0}\, .
$$

 A very special vector can be produced from any vector of any kind by scalar multiplying any vector by the number~$0$. 
This is called the {\it zero vector}\index{Zero vector} and is usually denoted simply $0$. This gives five very different kinds of zero from the 5 different kinds of vectors in   examples A-E above.
\begin{enumerate}[(A)]
\item $0(3)=0$ (The zero number)
\item $0\colvec{1\\1\\0}=\colvec{0\\0\\0} $ (The zero 3-vector)
\item $0\left(1+x-2x^2+3x^3\right)=0$ (The zero polynomial)
\item 
$0\!\left(  1+x\!-\!\frac1{2!}x^2\!+\!\frac1{3!}x^3\!+\cdots \!\right) \!
 =  0+0x+0x^2\!+0x^3\!+\cdots $(The zero power~series)
 \item $0\left(   e^x \right) =0$ (The zero function)
 \end{enumerate}

In any given situation that you plan to describe using vectors, you need to decide on a way to add and scalar multiply vectors.
In summary:
\vspace{3mm}
\begin{center}
\shabox{Vectors are things you can add and scalar multiply.} 
\end{center}
\vspace{3mm}

\noindent 
Examples of kinds of vectors:\\
\begin{itemize}
\item numbers
\item n-vectors
\item 2nd order polynomials
\item polynomials
\item power series
\item functions with a certain domain
\end{itemize}



\section{What are Linear Functions?}
\label{LTs}

In calculus classes, the main subject of investigation was the rates of change of functions. 
In linear algebra, functions
will again be the focus of your attention, 
but functions of a very special type. 
In precalculus you 
were perhaps encouraged to think of a function as a machine~$f$
into which one may feed a real number. 
For each input $x$ this machine outputs a single real number~$f(x)$. 

\begin{center}
\includegraphics[scale=.3]{\whatIsPath//machine.jpg}
\end{center}

In linear algebra, the functions we study will have vectors (of some type) as both inputs and outputs. 
We just saw that vectors are objects that can be added or scalar multiplied---a very general notion---so the functions we are going to study will look novel at first. 
So things don't get too abstract, here are five questions that can be rephrased in terms of functions of vectors.

\begin{example}  \label{EX}
(Questions involving Functions of Vectors in Disguise)\\
\begin{enumerate}[(A)]
\item\label{FVA}What number $x$ satisfies $10x=3$?
\item\label{FVB} What 3-vector~$u\!\, $  satisfies\footnote{ The \hyperlink{crossprod}{cross product} appears in this equation.}~$\!\colvec{1 \\ 1\\ 0} \times u = \colvec{0\\1\\1}$?\\[-.4cm]
%$$\!\!\!\!\!\!\!\!\!\colvec{x\\ y\\ z} \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!\colvec{\phantom{-}z\\-z\\y-x}$$
\item \label{FVC}What polynomial~$p$ satisfies~$\int_{-1}^1  p(y) dy = 0$ and $\int_{-1}^1 y p(y) dy=1$?\\
%$$p(x) \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!\colvec{\int_{-1}^1  p(y) dy\\[2mm]
%x\int_{-1}^1 y p(y) dy}$$
\item \label{FVD}What power series~$f(x)$ satisfies~$x\frac{d}{dx} f(x) -2f(x)=0$?
%$$f(x) \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!x\frac{d}{dx} f(x) -2f(x)$$
\item What number~$x$~satisfies~$4 x^2=1$?
%$$f(x) \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!f''(x)+f^3(x) -\sqrt{x}$$
\end{enumerate}
\vspace{.3cm}
All of these are of the form 
\begin{enumerate}[($\star$)]
\item What vector $X$ satisfies $f(X)=B$?
\end{enumerate}
with a function\footnote{In math terminology, each question is asking for the level set of $f$ corresponding to $B$.} $f$ known, a vector $B$ known, and a vector $X$ unknown.
\end{example}
The machine needed for part~(\ref{FVA}) is as in the picture below. 
$$x \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!10 x\, \hspace{8mm}$$
This is just like a function $f$ from calculus that takes in a number $x$ and spits out the number $10x$. (You might write $f(x)=10x$ to indicate this).
For part~(\ref{FVB}), we need something more sophisticated. 
$$\!\!\!\!\!\!\!\!\!\colvec{x\\ y\\ z} \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!\ccolvec{\phantom{-}z\\-z\\\!y-x\!\!}\, ,$$
The inputs and outputs are both 3-vectors. The output is the cross product of the input with... how about you complete this sentence to make sure you understand.

The machine needed for example~(\ref{FVC}) looks like it has just one input and two outputs; we input a polynomial and get a 2-vector as output.
$$\quad p \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!\colvec{\int_{-1}^1  p(y) dy\\[3mm]
\int_{-1}^1 y p(y) dy}\, .$$
This example is important because it displays an important feature; 
{\it the inputs for this function are functions}.

While this sounds complicated, %Rest assured that 
linear algebra is the
%involves the 
study of %only a very 
simple %(yet very important) class of 
functions of vectors; its time to describe the essential characteristics of linear functions. 

Let's use the letter $L$ to denote an arbitrary linear function and think again about vector addition and scalar multiplication. 
Also,  suppose that $v$ and $u$ are vectors and $c$ is a number. 
Since $L$ is a function from vectors to vectors, if we input $u$ into $L$, the output $L(u)$ will also be some sort of vector. 
The same goes for 
$L(v)$.
(And remember, our input and output vectors might be something other than stacks of numbers!) 
Because vectors are things that can be added and scalar multiplied, 
$u+v$ and $cu$ are also vectors, and so they can be used as inputs.
The essential characteristic of linear functions is what can be said about 
%the outputs 
$L(u+v)$ and $L(cu)$ in terms of 
%the outputs 
$L(u)$ and $L(v)$. 

Before we tell you this essential characteristic, 
ruminate on this picture. 

\begin{center}
\includegraphics[scale=.4]{\whatIsPath/L.jpg}
\end{center}

The ``blob'' on the left represents all the vectors that you are allowed to input into the function $L$,  the blob on the right denotes the
possible outputs, and the lines tell you which inputs are turned into which outputs.\footnote{The domain, codomain, and rule of correspondence of the function are represented by the left blog, right blob, and arrows, respectively.} A full pictorial description of the functions would require all inputs and outputs and lines to be explicitly drawn, but we are being diagrammatic; we only drew four of each.  




Now think about adding $L(u)$ and $L(v)$ to get yet another vector $L(u)+L(v)$ or of multiplying $L(u)$ by $c$ to obtain the vector $cL(u)$, and placing both on the right blob of the picture above. 
But wait! Are you certain that these are possible outputs!?
%Hopefully you noticed that there are two vectors  apparently {\it not shown} on the blob of outputs:
%$$
%L(u)+L(v)\quad \&\quad cL(u)\, .
%$$ 
%You might already be able to guess the values we would like these to take. If not, 

Here's the answer 
\begin{center}
\shabox{\scalebox{1.1}{
The key to the whole class, from which everything else \hypertarget{twopart}{follows}: 
%\newline The essential features of a linear function $L$ are
}}
\end{center}
\begin{enumerate}
\item Additivity:
$$L(u+v)=L(u)+L(v)\, .$$
\item Homogeneity:
$$L(cu)=cL(u)\, .$$
\end{enumerate}

\noindent Most functions of vectors do not obey this requirement.\!\footnote{{\it E.g.:} If $f(x)=x^2$ then $f(1+1)=4 \neq f(1)+f(1)=2$. Try any other function you can think of!}  At its heart, linear algebra is the study of functions that do. 

Notice that the additivity 
requirement says that the function $L$ respects vector addition: {\it it does not matter if you first add $u$ and~$v$ and then input their sum into
$L$, or first input $u$ and $v$ into $L$ separately and then add the outputs.} The same holds for scalar multiplication--try writing out the scalar multiplication version of the italicized sentence. When a function of vectors obeys the additivity and homogeneity properties we say that it is {\it linear} (this is the ``linear'' of linear algebra). Together, additivity and homogeneity are called {\it linearity}. 
Are there other, equivalent, names for linear functions? Yes:
%\begin{enumerate}[]
%\item Linear transformation 
%\item Linear map
%\item Linear operator
%\item Homomorphism
%\end{enumerate}
\begin{center}
\includegraphics[scale=.37]{\whatIsPath/Names.jpg}
\\
\shabox{Function~=~Transformation~=~Operator}\\
\end{center}

And now for a hint at the power of linear algebra. 
The questions in examples~(\ref{FVA}-\ref{FVD}) can all be restated as %a single equation:
\begin{center}
\shabox{
$
L v = w
$}
\end{center}
where $v$ is an unknown, $w$ a known vector, and $L$ is  a known linear transformation.
To check that this is true, one needs to know the rules for adding vectors (both inputs and outputs)
and then check linearity of $L$. Solving the equation $Lv=w$ often amounts  to solving systems of linear equations,
the skill you will learn in  Chapter~\ref{systems}.



A great example is the derivative operator.
\begin{example} (The derivative operator is linear)\\
For any two functions~$f(x)$,~$g(x)$ and any number~$c$, in calculus you probably learnt that the derivative operator satisfies
\begin{enumerate}
\item ~$\frac{d}{dx} (cf)=c\frac{d}{dx} f$, \\[-.5cm]
\item~$\frac{d}{dx}(f+g)=\frac{d}{dx}f+\frac{d}{dx}g$.\\[-.5cm]
\end{enumerate}
If we view functions  as vectors with addition given by addition of functions and with scalar multiplication given by multiplication of functions by constants, then these familiar properties of derivatives are just the linearity property of linear maps.
\end{example}

Before introducing matrices, notice that for linear maps~$L$ we will often write simply $L u$ instead of $L(u)$. This is because the linearity
property of a linear transformation $L$ means that $L(u)$ can be thought of as multiplying the vector $u$ by the linear operator $L$.
For example, the linearity of $L$ implies that if $u,v$ are vectors and $c,d$ are numbers, then
\begin{center}
\shabox{\scalebox{1.1}{
$
L(c u + d v) = c L u + d L v\, ,
$}}
\end{center}
which feels a lot like the regular rules of algebra for numbers. Notice though, that ``$u L$'' makes no sense here.

\begin{remark}
A sum of multiples of vectors $c u + dv$ is called a {\it linear combination}\index{Linear combination} of $u$~and~$v$.
\end{remark}

\section{So, What is a Matrix?}
Matrices are linear functions of a certain kind. 
They appear almost ubiquitously in linear algebra because---and this is the central lesson of introductory linear algebra courses---
\begin{center}
\shabox{Matrices are the result of organizing information related to linear functions. }
\end{center}
This idea will take some time to develop, but we provided an elementary example in 
Section \ref{organize}. A good starting place to  learn about matrices is by studying {\it systems of  linear equations}. 

\begin{example} 
A room contains~$x$ bags and~$y$ boxes of fruit.
\begin{center}
\includegraphics[scale=.25]{\whatIsPath/boxesbags.jpg}
\end{center}
Each bag contains 2 apples and 4 bananas and each box contains 6 apples and 8 bananas. 
There are 20 apples and 28 bananas in the room. Find~$x$ and~$y$.
\\

%<pic please>\\

\noindent
The values are the numbers~$x$ and~$y$ that simultaneously make both of the following equations true:
\begin{eqnarray*}
	2\, x + 6\, y & =  & 20 \\
	4\, x + 8\, y & = & 28\, .
\end{eqnarray*}
\end{example}
Here we have an example of a \emph{System of Linear Equations}\index{Linear System!concept of}.\footnote{Perhaps you can see that both lines are of the form $Lu=v$ with $u=\colvec{x\\y}$ an unknown, $v=20$ in the first line, $v=28$ in the second line, and $L$ different functions in each line? We give the typical less sophisticated description in the text above.}   
It's a collection of equations in which variables are multiplied by constants and summed, and no variables are multiplied together:  There are no powers of variables 
(like~$x^2$ or~$y^5$), non-integer or negative powers of variables (like~$y^{1/7}$ or~$x^{-3}$), and no places where variables are multiplied together (like~$xy$).
%\begin{center}\href{\webworkurl ReadingHomework1/1/}{Reading homework: problem 1.1}\end{center}
%\reading{1}{1}
\Reading{WhatIsLinearAlgebra}{1}

%The system of equations above has the feel of multiplication of contents per package by number of packages. 
%What we have is a function that takes in two numbers (number of bags and number of boxes) and gives out two numbers (number of apples and number of bananas.) 
%This function is linear: double the number of bags and the number of boxes and you will double the number of apples and number of bananas. Same for tripling, quadrupling, etc...
%
%An important idea underlies the following observation. 
\noindent
Information about the fruity contents of the room can be stored two ways: 
\begin{enumerate}[(i)]
\item In terms of the number of apples and bananas. 
\item In terms of the number of bags and boxes. 
\end{enumerate}
Intuitively, knowing the information in one form allows you to figure out the information in the other form. 
Going from~(ii) to~(i) is easy: 
If you knew there were~3 bags and~2 boxes it would be easy to calculate the number of apples and bananas, and doing so would have the feel of multiplication (containers times fruit per container). 
In the example above we are required to go the other direction, from~(i) to~(ii). This  feels like the opposite of multiplication, {\it i.e.}, division. Matrix notation will 
make clear what we are ``multiplying"" and ``dividing'' by. 

The goal of Chapter~\ref{systems} is to efficiently solve systems of linear equations. 
%This is going to be a generalization of dividing both sides of the system of equations by...something. 
%To show you what we mean, we will now give you an example of an inefficient method of solving the system from example 3:\\
%
%\noindent
%{\bf Inefficient Method}:\\
%Rearrange the first equation into~$x=10-3y$ and substitute the result into the second equation to obtain~$28=4(10-3y)+8y$. This equation has only one unknown; the jargon is ``$x$ has been eliminated"". The equation implies that~$y=3$. That result can be ``back substituted"" into either of the original equations, for example the first, to obtain 
%$20=2x+6\cdot 3 \Leftrightarrow x=1$.\\
%
%It is easy to get lost when using this method. Especially when dealing with large systems of linear equations, such as 256 equations in 256 variables. It would be nice if we could lay out our work in a way that resonates with the intuition we have built from solving simpler algebra problems, like the familiar procedure
%\begin{eqnarray*}
%3x+4 &=&7\\
%{\rm Subtract}~&4&\\
%3x&=&3\\
%{\rm divide~by}~&3&\\
%x&=&1 \,.
%\end{eqnarray*}
%That is, lets keep the variables on the left hand side and rewrite our {\it system} of equations after each step. \\
%
%
%\noindent
%{\bf More Efficient Method:}\\
%Divide (both sides of) the second equation by 2 to obtain the equivalent system
%\begin{eqnarray*}
%	2\cdot x + 6\cdot y & = & 20 \\
%	2\cdot x + 4\cdot y  & = & 14 \,.
%\end{eqnarray*}
%Subtract the first equation from the second, to get the equivalent system 
%\begin{eqnarray*}
%	2\cdot x + 6\cdot y & =  & 20  \\
%	 0\cdot x - 2\cdot y & = & -6\, .
%\end{eqnarray*}
%Now add three times the second equation to the first
%\begin{eqnarray*}
%	2\cdot x + 0\cdot y & = & 2 \\
%	0\cdot x - 2\cdot y & = & -6\, .
%\end{eqnarray*}
%At this point the result~$x=1,y=3$ is obvious. Further, the elimination of~$y$ from the first equation and elimination of~$x$ from the second is clear as can be. 
%%This is elimination, a key idea in this course. 
%%The idea here is to follow some algorithm to have just one variable with non-zero coefficient in each equation, and to rewrite the {\it system} of linear equations at each step. 
%
%There is a clear shortcoming to this ``efficient method"": we need to rewrite too much at each step! The~$x$'s are rewritten in the same place over and over, and similarly for the~$y$'s and the equal signs. 
%Lets work toward reducing the amount of things we rewrite by combining the pair of equations in example 3 into a singe equation using vectors from 2-space. 
Partly, this is just a matter of finding a better notation, but one that hints at a deeper underlying mathematical structure.
For that, we need rules for adding and scalar multiplying 2-vectors; 
$$
c\colvec{x\\y}:=\colvec{cx\\cy}\, \mbox{ and } \colvec{x\\y}+\colvec{x'\\y'}:=\colvec{x+x'\\y+y'}\, .
$$
Writing our fruity equations as an equality between 2-vectors and then using these rules we have:
%First we reduce the number of appearances of the equal sign:
\begin{equation*}
   \left.
\begin{array}{lr}
   	2\, x + 6\, y  =  20 \\
	4\, x + 8\, y  =  28
     \end{array}
   \right\} 
\ \Longleftrightarrow  \  \colvec{ 2x+6y \\ 4x+8y}  =\colvec{ 20\\ 28}
%\, .
%\end{eqnarray*}
%Now we can reduce the number of appearances of the symbols~$x$ and~$y$ using vector addition and scalar multiplication:
%\begin{eqnarray*}
%    \colvec{ 2x+6y \\ 4x+8y}  %=\colvec{ 20\\ 28}
\ \Longleftrightarrow \ 
   x \colvec{ 2\\ 4} + y \colvec{ 6\\ 8} =\colvec{ 20\\ 28} \, .
\end{equation*}
Now we  introduce a function which takes in 2-vectors\footnote{To be clear, we will use the term 2-vector to refer to stacks of two numbers such as~$\colvec{7\\11}$. If we wanted to refer to the vectors $x^2+1$ and $x^3-1$ (recall that polynomials are vectors) we would say ``consider the two vectors $x^3-1$ and $x^2+1$''. We apologize through giggles for the possibility of the phrase ``two 2-vectors.""} and gives out 2-vectors. We denote it by  an array of numbers  called a {\it matrix\,\! .}
\begin{equation*}
 {\rm {\bf The~function}}   \begin{pmatrix}
      2     & 6 \\
      4     & 8
    \end{pmatrix}~{\rm {\bf is~defined~by}}
    \begin{pmatrix}
      2     & 6 \\
      4     & 8
    \end{pmatrix}
  \colvec{x \\ y}
  := x \colvec{ 2\\ 4} + y \colvec{ 6\\ 8} \, .
%  \colvec{20 \\ 28}
\end{equation*}
A similar definition applies to matrices with different numbers and sizes.

\begin{example}(A bigger matrix)
$$
\begin{pmatrix}1&0&3&4\\
5&0&3&4\\
-1&6&2&5
\end{pmatrix}
\ccolvec{x\\y\\z\\w}
:=x
\begin{pmatrix}1\\5\\-1
\end{pmatrix}
+y
\begin{pmatrix}0\\0\\6
\end{pmatrix}
+z
\begin{pmatrix}3\\3\\2
\end{pmatrix}
+w\begin{pmatrix}4\\4\\5
\end{pmatrix}\, .
$$
\end{example}


Viewed as a machine that inputs and outputs 2-vectors, our $2\times2$ matrix does the following:
$$
\colvec{x\\y}\raisebox{-11mm}{\includegraphics[scale=.15]{\whatIsPath//machine_matrix.jpg} }\!\!\!\!\!\!\colvec{2x+6y\\4x+8y}\, .
$$
Our fruity problem is now rather concise.
\begin{example}  (This time in purely mathematical language): \\[.2cm]
What vector~$  \colvec{x \\ y}$ satisfies 
$
    \begin{pmatrix}
      2     & 6 \\
      4     & 8
    \end{pmatrix}
  \colvec{x \\ y}
  =   \colvec{20 \\ 28}
$?
\end{example}
%\\
This is of the same~$Lv=w$ form as our opening examples. 
The matrix encodes fruit per container. The equation is roughly fruit per container times number of containers equals fruit. To solve for number of containers we want to \hypertarget{ch1divide}{somehow ``divide''} by the matrix. 


Another way to think about the above example is to remember the rule for multiplying a matrix times a vector.
\hypertarget{system2matrix}{If} you have forgotten this, you can actually  guess a good rule by making sure the matrix equation is the same as the system of linear equations.
This would require that
$$
 \begin{pmatrix}
      2     & 6 \\
      4     & 8
    \end{pmatrix}
  \colvec{x \\ y}
  :=   \colvec{2x+6y \\ 4x+8y}
$$
Indeed this is an example of
\hypertarget{ch1vecmult}{the general rule} that you have probably seen before
%\begin{center}
%``Turn the vector sideways, and combine it with the rows of the matrix"" \end{center}
\begin{equation*}\label{2x2multiplication}
    \begin{pmatrix}
      p     & q  \\
      r      & s
    \end{pmatrix}
  \colvec{x \\ y}
  :=
  \colvec{px+qy \\ rx+sy}=x\colvec{p\\r} + y\colvec{q\\s}\, .
\end{equation*}\\%[.2cm]
Notice, that the second way of writing the output on the right hand side of this equation is very useful because it
tells us what all possible outputs a matrix times a vector look like -- they are just sums of the columns of the matrix
multiplied by scalars. The set of all possible outputs of a matrix times a vector is called 
the {\bf column space}\index{Column Space!concept of} (it is also the image of the linear function defined by the matrix).
%\reading{1}{2}
\Reading{WhatIsLinearAlgebra}{2}


 


\noindent
Multiplication by 
\hypertarget{earlier}{a matrix} is an example of a \emph{Linear Function}\index{Linear Transformation!concept of}, because it takes one vector and turns it into another in a ``linear'' way.
Of course, we can have much larger matrices if our system has more variables.

\videoscriptlink{what_is_linear_algebra_Possibilities.mp4}{Matrices in Space!}{script_what_is_linear_algebra_hint}

\noindent
Thus
\hypertarget{Matrices are linear operators}{matrices can be viewed as linear functions.} 
The statement of this for the matrix in our fruity example is as follows.
\begin{enumerate}
\item\label{ITEM1}~$\begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
   \lambda \colvec{x \\ y} 
   =\lambda  \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
   \colvec{x \\ y} ~$ and
\item\label{ITEM2}
$     \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
   \left[ \colvec{x \\ y} +\colvec{x' \\ y'} \right] 
   = \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
\colvec{x \\ y}
   +
    \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
    \colvec{x' \\ y'}
.$
\end{enumerate}
These equalities can be verified using the rules we introduced so far.
\begin{example} Verify that 
$\begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}$
is a linear operator.

\noindent
The matrix-function is homogeneous if the expressions on the left hand side and right hand side of the equation in part~\ref{ITEM1} above are indeed equal. 
\begin{equation*}
\begin{split}
\begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
    \left[
   \lambda \colvec{a \\ b} \right]
 =
\begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
   \colvec{\lambda a \\ \lambda b} 
 &=
  \lambda a \colvec{2 \\ 4} 
+   
     \lambda b \colvec{6 \\ 8} \\[2mm]&
 = \colvec{2\lambda a \\ 4\lambda a} 
+   
      \colvec{6\lambda b \\ 8\lambda b}=  \underline{\colvec{2\lambda a+6\lambda b\\4\lambda a+8\lambda b}} \end{split}\end{equation*}
    \\[.1cm]
while
 \begin{equation*}\begin{split} \lambda  \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
   \colvec{a \\ b} ~
   =
     \lambda\left[ a \colvec{2 \\ 4} 
+   
     b \colvec{6 \\ 8} \right]
   &=\lambda\left[\colvec{2a\\4a}+\colvec{6b\\8b}\right]\\[2mm]&=\lambda\colvec{2a+6b\\4a+8b} =  \underline{\colvec{2\lambda a+6\lambda b\\4\lambda a+8\lambda b} .}
     \end{split}\end{equation*}
\vspace{3mm}
\noindent
The underlined expressions are  identical, so the matrix is homogeneous. \\

The matrix-function is additive if the left and right side of the equation  in part~\ref{ITEM2} above are indeed equal. 

\begin{equation*}
\begin{split}
     \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
   \left[ \colvec{a \\ b} +\colvec{c \\ d} \right] 
   &= 
        \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
    \colvec{a +c\\ b+d}  
   =
        (a+c) \colvec{2 \\ 4} 
        +
         (b+d) \colvec{6 \\ 8}\\[2mm]&
         =
        \colvec{2(a+c) \\ 4(a+c)} 
        +
        \colvec{6(b+d) \\ 8(b+d)}
             =
       \underline{ \colvec{2a+2c +6b+6d\\ 4a+4c+8b+8d} }
       \end{split}\end{equation*}
 which we need to compare to  
\begin{equation*}
\begin{split}
  \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
\colvec{a \\ b}
 &  +
    \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
    \colvec{c \\ d}
=
a\colvec{2\\4} + b\colvec{6\\ 8} + c\colvec{2\\4} +d\colvec{6\\8}\\[2mm]&
=\colvec{2a\\4a} + \colvec{6b\\ 8b} + \colvec{2c\\4c} +\colvec{6d\\8d}
=\underline{\colvec{2a+2c +6b+6d\\ 4a+4c+8b+8d} }\, .
\end{split}\end{equation*}
Thus multiplication by a  matrix is additive and homogeneous, and so it is, by definition, linear. 
\end{example}

%\noindent

We have come full circle; matrices are just examples of the kinds of linear operators that appear in algebra problems like those in 
section~\ref{LTs}. 
Any equation of the form~$Mv=w$ with~$M$ a matrix, and~$v,w$ $n$-vectors is called  a {\it matrix equation}\index{Matrix equation}. 
Chapter~\ref{systems} is about efficiently solving systems of linear equations, or equivalently matrix equations.

\subsection{Matrix Multiplication is Composition of Functions}

What would happen if we placed two of our expensive machines end to end?
$$
\includegraphics[scale=.15]{\whatIsPath//machine_matrix.jpg} \raisebox{1mm}{\includegraphics[scale=.15]{\whatIsPath//machine_matrix1.jpg}} \raisebox{12mm}{\Huge ?}
$$
The output of the first machine would be fed into the second. 
$$
\scalebox{.7}{$\colvec{x\\y}$}\hspace{-.6mm}\raisebox{-9mm}{\includegraphics[scale=.12]{\whatIsPath//machine_matrix.jpg} }\!\!\!\!\!\!\scalebox{.7}{$\colvec{2x+6y\\4x+8y}$}
\raisebox{-9mm}{\includegraphics[scale=.12]{\whatIsPath//machine_matrix1.jpg} }\!\!\!\!\!\!\!\!\!\!\!\!\!
  \raisebox{-3mm}{ \scalebox{.7}{$\begin{array}{l}\ \ \ \ \colvec{1.(2x+6y)+2.(4x+8y)\\0.(2x+6y)+1.(4x+8y)}\\[4mm] \ =\colvec{10x+22y\\4x+8y}\end{array}$}}
$$
Notice that the same final result could be achieved with a single machine:
$$
\colvec{x\\y}\raisebox{-11mm}{\includegraphics[scale=.15]{\whatIsPath//machine_matrix2.jpg} }\!\!\!\!\!\!\colvec{10x+22y\\4x+8y}\, .
$$
There is a simple matrix notation for this called {\it matrix multiplication}\index{Matrix multiplication}
$$
\begin{pmatrix}1&2\\0&1\end{pmatrix}
\begin{pmatrix}2&6\\4&8\end{pmatrix}
=\begin{pmatrix}10&22\\4&8\end{pmatrix}\, .
$$ 
Try review problem~\ref{matmult} to learn more about matrix multiplication. 

In the language\footnote{The notation $h:A\to B$ means that $h$ is a function with domain $A$ and codomain $B$. See the webwork  background  set\hwref{Background}{3} if you are unfamiliar with this notation or these terms.} 
of functions, if $$f:U\longrightarrow V\quad \mbox{and}\quad g:V\longrightarrow W$$
the new function obtained by plugging the outputs if $f$ into $g$ is called $g\circ f$,
$$
g\circ f:
U\longrightarrow  W$$
where
$$
(g\circ f)(u)=g(f(u))\, .
$$
This is called the {\it composition of functions}\index{Composition of functions}.
Matrix multiplication is the tool required for computing the composition of linear functions.


\subsection{The Matrix Detour}
Linear algebra is about linear functions, not matrices. 
%This lesson is hard to learn after a full term of working with matrices. 
%Start thinking about this on day one of the course. 
The following presentation is meant to get you thinking about this idea constantly throughout the course.
\begin{center}
\shabox{Matrices only get involved in linear algebra when certain notational choices are made.}
\end{center}
To exemplify, lets look at the derivative operator again.

\begin{example}{of how matrices come into linear algebra.}\\
Consider the equation
%question ``which quadratic function $f$ satisfies
$$\left( \frac{d}{dx}+2\right) f= x+1$$
where $f$ is unknown (the place where solutions should go) and  the linear differential operator $\frac{d}{dx}+2$ is understood to take in quadratic functions (of the form $ax^2+bx+c$) and give out other quadratic functions. 

Let's simplify the way we denote the quadratic functions; 
we will  
$${\rm denote~~}ax^2+bx+c \text{~~as~~} \colvec{a\\b\\c}_B.$$
The subscript $B$ serves to remind us of our particular notational convention; we will compare to another notational convention later. With the convention $B$ we can say
$$  \left( \frac{d}{dx}+2\right) \colvec{a\\b\\c}_B
=\left( \frac{d}{dx}+2\right)(ax^2+bx+c)$$
$$
=(2ax+b)+(2ax^2+2bx+2c) = 2a x^2+(2a+2b)x+(b+2c)
$$
$$
=\colvec{\mc{2a}\\2a+2b\\\mc{b+2c}}_B
= 
\left[ 
\begin{pmatrix}   
2&0&0\\
2&2&0\\
0&1&2
\end{pmatrix}
\colvec{a\\b\\c}\right]_B.
$$
That is, our notational convention for quadratic functions has induced a notation for the differential operator $\frac{d}{dx}+2$ as a matrix. 
We can use this notation to change the way that 
the following two equations say exactly the same thing.
$$
 \left( \frac{d}{dx}+2\right) f=x+1
 \Leftrightarrow
\left[ 
\begin{pmatrix}   
2&0&0\\
2&2&0\\
0&1&2
\end{pmatrix}
\colvec{a\\b\\c}\right]_B = \colvec{0\\1\\1}_B.$$
Our notational convention has served as an organizing principle to yield the system of equations 
$$
\begin{array}{cc}
2a&=0\\
2a+2b&=1\\
b+2c&=1
\end{array}
$$
with solution $\colvec{0\\ \frac12\\[1mm] \frac14}_{\!B}  $, where the subscript $B$ is used to remind us that this stack of numbers  encodes the vector $\frac12x+\frac14$, which is indeed the  solution to our equation since, substituting for $f$ yields the true statement $\left(\frac{d}{dx}+2\right)(\frac12x+\frac14)=x+1$. 
\end{example}

It would be nice  to have a systematic way
to rewrite any linear equation as an equivalent matrix equation. 
It will be a little while before we can learn to  organize information in a way generalizable to all linear equations, 
but keep this example in mind throughout the course. 

The general idea is presented in the picture below; sometimes a linear equation is too hard to solve as is, but by organizing information and reformulating the equation as a matrix equation the process of finding solutions becomes tractable. 
\begin{center}
{\includegraphics[width=.8\textwidth]{\whatIsPath/detourAbs} }\\
\end{center}
A simple example with the knowns ($L$ and $V$ are $\frac{d}{dx}$ and $3$, respectively) is shown below, although the detour is unnecessary in this case since you know how to anti-differentiate.
\begin{center}
{\includegraphics[width=.8\textwidth]{\whatIsPath/detourEg} }\\
\end{center}


To drive home the point that we are not studying matrices but rather linear functions, and that those linear functions can be represented as matrices under certain notational conventions, 
consider how changeable the notational conventions are. 


\begin{example}{of how a different matrix comes into the same linear algebra problem.}\\

\noindent Another possible notational convention  is to
$$\text{denote~~}a+bx+cx^2 \text{~~as~~} \colvec{a\\b\\c}_{\!\!\!B'}.$$
With this alternative notation
$$  \left( \frac{d}{dx}+2\right) \colvec{a\\b\\c}_{B'}
=\left( \frac{d}{dx}+2\right)(a+bx+cx^2)$$
$$
=(b+2cx)+(2a +2bx+2cx^2 ) = (2a+b) + (2b+2c)x+2cx^2
$$
$$
=\ccolvec{2a+b\\2b+2c\\2c}_{B'}
= 
\left[ 
\begin{pmatrix}   
2&1&0\\
0&2&2\\
0&0&2
\end{pmatrix}
\colvec{a\\b\\c}\right]_{B'}.$$
Notice that we have obtained {\it a different matrix for the same linear function}. 
The  equation we started with 
$$
 \left( \frac{d}{dx}+2\right) f=x+1
 \Leftrightarrow
\left[ 
\begin{pmatrix}   
2&1&0\\
0&2&2\\
0&0&2
\end{pmatrix}
\colvec{a\\b\\c}\right]_{B'} = \colvec{1\\1\\0}_{\!B'}$$
$$
\Leftrightarrow
\begin{array}{r}
2a+b=1\\
2b+2c=1\\
2c=0
\end{array}
$$
has the solution 
$\colvec{ \frac14\\[1mm] \frac12\\[1mm]0}$. Notice that we have obtained {\it a different 3-vector for the same vector},  since in the notational convention $B'$ this 3-vector represents $\frac14+\frac12x$. 
\end{example}

One linear function can be represented (denoted) by a huge variety of matrices. %with the same effects for any choice of representation. 
The representation only depends on how vectors are denoted as n-vectors.


%We hope this helps you understand why we will be looking at matrices so much even though they will not be our primary objects of study. But, unfortunately we can not start at the climax of our story; 
%we must cover how to use matrices per se before we cover how to change to matrix notation and then use matrices! Thus, the next chapter is devoted to the study of matrices in and of them selves. 



%\section{Where are we going?}
%
%Let us reiterate, linear algebra is about more than just matrices. It is about linear operators. 
%While matrices are among the simplest linear operators, they are the most powerful; as hinted at above, complicated and confusing operators can be re-written as matrices as long as they are linear. Before one can see that, one must have a deep understanding of matrices and their structure. This course is mostly about learning that structure.
%
%To hint at what we meant by ``structure of matrices"" lets look back on some math you already know.  
%The fundamental theorem of arithmetic says that any integer can be factored into a product of prime numbers (and bio further). 
%The fundamental theorem of algebra says that polynomials over the complex numbers can be factored into first order polynomials (and no further, e.g.~$x^4-1=(x+1)(x-1)(x+1)(x-1)$). 
%The prime numbers are the building blocks of integers and the first order polynomials are the building blocks of polynomials. We will first work toward answering the question, what are the building blocks of matrices? We will discover in chapter 2 that the building blocks are objects called elementary row operations. In Chapter 3 will categorize these objects in detail.

%This example uses augmented matrices which have not been introduced yet!
%\videoscriptlink{what_is_linear_algebra_3_3_matrix.mp4}{A~$3 \times 3$ matrix example}{scripts_what_is_linear_algebra_3_3_matrix}

%This vid uses the old first page of the text and  might be a bit confusing. 
%\videoscriptlink{what_is_linear_algebra_overview.mp4}{Video Overview}{video_what_is_overview}

%I've opted to use the word operator instead of transformation-David
%The matrix is an example of a \emph{Linear Transformation}\index{Linear Transformation!concept of}, because it takes one vector and turns it into another in a linear way:


%\References{
%Hefferon, Chapter One, Section 1\\
%Beezer, Chapter SLE, Sections WILA and SSLE\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/System_of_linear_equations}{Systems of Linear Equations}
%}


\section{Review Problems}

You probably have already noticed that understanding sets, functions and basic logical operations 
is a must to do well in linear algebra. Brush up on these skills by trying these background webwork
problems:

\begin{center}
\begin{tabular}{|c|c|}
\hline
Logic &
\hwref{Background}{1}\\ 
Sets &
\hwref{Background}{2}\\ 
Functions &
\hwref{Background}{3}\\ 
Equivalence Relations &
\hwref{Background}{4}\\ 
Proofs &
\hwref{Background}{5}\\ 
\hline
\end{tabular}
\end{center}

\noindent
Each chapter also has reading and skills WeBWorK problems:
\vspace{2mm}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading problems &
\hwrref{WhatIsLinearAlgebra}{1}, \hwrref{WhatIsLinearAlgebra}{2}\\
\hline
\end{tabular}
\vspace{4mm}

\noindent
Probably you will spend most of your time on the following review questions:

\input{\whatIsPath/problems}

%\newpage

%\begin{comment}
%There are two things to notice about  this list: First,  the range of operations in algebra seems to go beyond addition, subtraction, multiplication, and division while the range of possible objects goes beyond numbers. [In fact, its best to think of the objects (numbers) being generalized while trying to maintain
%the basic properties of addition, subtraction, {\it etc}..., as far as possible.]
%%; in fact, we mean only to hint at the variety of objects and operations found in algebra.   
%Second, these very different questions all have the same form. To emphasize this we restate them:
%\begin{example} (Algebra \hypertarget{AandD}{Questions} Restated)
%% Note to the Editor:
%% putting all in Ax=b is BAD! They emotionally want x to stand for a number, not a function,
%Solve
%$$
%{\cal A}\, {\bf x} = {\bf b}\, ,
%$$
%where
%\begin{enumerate}[(A)]
%\item \label{A}
%${\bf x}$ and ${\bf b}$ are numbers, $${\cal A}\, {\bf x}:=10 \, {\bf x}\, ,$$
%and ${\bf b}=3$.
%%What 
%%number~$x$ satisfies~$Ax=10$ where~$Ax:=5x?$\\[-.6cm]
%\item 
%${\bf x}$ and ${\bf b}$ are 3-vectors,  $${\cal A}\, {\bf x}:=\colvec{1 \\ 1\\ 0} \times {\bf x}\, ,$$\\
%\vspace{-15mm}
%and ${\bf b}=\colvec{1\\1\\0}$.
%%What vector~$v$ satisfies~$Bv = \colvec{0\\1\\1}$% \\
%%where ~$Bv:=\colvec{1 \\ 1\\ 0} \times v$?\\[-.4cm]
%\item 
%${\bf x}$ and ${\bf b}$ are polynomials in the variable $x$, $${\cal A}\, p(x):=x\int_{-1}^1 yp(y)dy\, ,$$ 
%and ${\bf b}=x$.
%%What polynomial~$g$ satisfies~$Cg=h$ where~$Cg$ is the function defined by \\[.3cm]
%%$Cg(x)=\int_{-1}^1 f(x,y)g(y) dy = h(x)$ where~$f(x,y):=xy,~h(x):=x$?\\[-.5cm]
%\item ${\bf x}$ and ${\bf b}$ are polynomials, 
%$$
%{\cal A}\,  p(x):=\frac{dp(x)}{dx}-2p(x)\, ,
%$$
%and ${\bf b}=0$
%%What polynomial~$f$ satisfies~$Df=0$ 
%%where~$Df$ is the function \\[.3cm]
%%$Df(x):=x\frac{d}{dx}f(x)-2f(x)$?\\[-5mm]
%\item  \label{E}
%${\bf x}$ and ${\bf b}$ are functions, 
%$$
%{\cal A}  \big(f(x) \big):= f''+f^3\, ,
%$$
%and ${\bf b}=\sqrt{x}$.
%%What function~$f$ satisfies~$Ef=g$ where~$Ef:=f''(x)+\big(f(x)\big)^3$ and~$g(x):=\sqrt{x}$?
%\end{enumerate}
%\end{example}
%\noindent
%\reading{1}{1}
%You probably sense already that one of our main subjects will be the  study of equations 
%that can be written in the form
%\begin{center}
%\shabox{$Ax=b$}
%\end{center}
%%\begin{eqnarray*}
%%Ax=b
%%\end{eqnarray*}
%but where $x$ and $b$ can be much more general creatures than just numbers. The technical answer to the question ``how general?'',
%is that $x$ and $b$ will be {\it vectors}. We will say much more about what this means later, but for now all you need
%to know is that vectors are objects that you can {\it add} and {\it scalar multiply}, so that the algebraic operations
%$$
%x+x'\, ,\mbox{ and } c\, x\, ,
%$$
%where $c$ is any number, make sense.
%
%You might also be wondering why we wrote ${\cal A}\big(f(x)\big)$ rather than ${\cal A} f(x)$ in part~(\ref{E}) of our example,
%or perhaps why we did not write ${\cal A}({\bf x})$ in part~(\ref{A}). In fact all the the above problems involve functions (denoted ${\cal A}$)
%of vectors. However, in the first four examples the function~${\cal A}$ (that takes in a some sort of vector and returns a vector as its output)
%is a {\it linear operator} which means that it obeys a
%very special  property called {\it linearity}. This is the ``Linear'' in Linear Algebras and we will explain it next. 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%%\newpage
%\section{Operators: Functions of Functions}
%A few comments on jargon and notation are in order before we move on. ~$D$ takes in functions like~$x^4$ and gives out functions like~$4x^4-2x^4=2x^4$. You could say that~$D$ is a function that takes in functions and gives out functions.  
%Alternatively, that awful combination of words can be avoided by using the word 
%``{\it operator}"" instead of ``function"";
%~$D$ is an operator that takes in functions and gives out functions. 
% Similarly,~$C$ is an operator that takes in, for example,~$x^3$ and gives out ~$\frac12 x$. 
%We hope you can understand why we don't always use the 
%parentheses notation for arguments of functions when we think of the function as an operator;~$Dg(x)$ looks much better~$D(g)(x)$.
%
%%Video of operators? 
%
%If we are going to think of~$B$ as an operator too we need to understand the objects it takes in and gives out in a new way.
%It takes vectors from 3-space to vectors in 3-space. 
%Such vectors  
%\hypertarget{vecs as fun}{are really functions;} 
%a vector from 3-space~$v$ is a function whose domain is just (the set containing only)~$1,2,$ and~$3$. 
%
%
%\begin{example}
%\noindent
%The vector~$v=\colvec{v_1\\v_2\\v_3}=\colvec{7\\14\\21}$ is the function that: \\[.4cm]
%\begin{center}
%\begin{tabular}{l}
%Takes in~$1$ and gives out~$v_1=7$\\[1mm]
%Takes in~$2$ and gives out~$v_2=14$\\[1mm]
%Takes in~$3$ and gives out~$v_3=21$.\\[1mm]
%\end{tabular}
%\end{center}
%\end{example}
%
%\noindent
%This is an explicit definition of a function as opposed to of the more familiar algebraic definition. In analogy to defining a function~$f$ with domain all real numbers by the algebraic statement~$f(x)=7x$, we can define the function~$v$ by~$v_i=7i$. 
%The operator~$B$ takes in such a  function and gives out another. For example~$$Bv=\colvec{1\\1\\0}\times \colvec{7\\14\\21}=\colvec{21\\-21\\7}.$$
%
%
%You can also think of vectors from two space as functions with domain (the set containing only)~$1$ and~$2$. Similarly vectors from n-space are functions of (the set containing only)~$1,2,...,n$. This includes the case~$n=1$; we can even think of~$A$ as an operator. 
%
%\reading{1}{2}
%
%
%
%Digest what you can of these ideas now, but don't worry if you feel that some of this new stuff is not sinking in immediately. We will revisit all of these ideas. The point here is not to learn about any of the particular operators~$A,B,C,D,E$ but rather to show you that the world of algebra is much bigger than the study of questions involving just exponentiation, multiplication, division, addition and subtraction of numbers. Let your imagination go wild with our examples as inspiration. Imagine all the possible integral operators like~$C$ and differential operators like~$D$. We hope you will conclude that your adventures in algebra are just beginning, and that there are many very difficult problems out there. 
%The tools you will learn in linear algebra will help you answer all four of these example questions in an efficient way with the same method. That is the long story we wish to tell. 
%\end{comment}
%
%%<picture please: flow chart with algebra problem-> linear or not linear, if linear then-> matrix equation
%
%%\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%\section{What is Linear?} 
%
%\begin{example} (Vectors) \\[-.5cm]
%\begin{enumerate}[(A)]
%\item Numbers. If
%\item What vector~$v$ from 3 space satisfies the cross product equation~$ \colvec{1 \\ 1\\ 0} \times v = \colvec{0\\1\\1}$?\\[-.4cm]
%\item What polynomial~$g$ satisfies~$\int_{-1}^1 f(x,y)g(y) dy = h(x)$, \\[.2cm]
% where~$f(x,y):=xy$ and~$h(x):=x$?\\[-.5cm]
%\item What power series~$f$ satisfies~$x\frac{d}{dx} f(x) -2f(x)=0$?
%%\item What power series~$f$ satisfies~$x\frac{d}{dx} f(x) -2f(x)=0$?
%\item What what function~$f$ satisfies~$f''(x)+f^3(x) =\sqrt{x}~$?
%\end{enumerate}
%\end{example}
%
%
%
%
%Students often ask questions like ``how do I know when to use a particular method""? A question like ``how do I know when to use integration by parts?"" is hard to  answer, but in contrast, ``when can I use  linear algebra?"" is easy to answer: %for the kinds of questions given above; 
%{\it when the operator involved is linear.} \\
%
%\noindent
%An operator~$E$ is \hypertarget{asked in chapter 1}{said to be linear} if it has two properties:\\[-.5cm]
%\begin{enumerate}
%\item {\it Homogeneity:} ``Constants can be pulled out""
%$$E(cv)=cEv\, .$$ \\[-.5cm]
%\item {\it Additivity:} ``The operator can be distributed""
%$$E(v+u)=Ev+Eu\, .$$ 
%\end{enumerate}
%
%
%
%
%\noindent
%A great example is the derivative operator: for any two functions~$f$,~$g$ and any number~$c$ the derivative operator satisfies\\[-.5cm]
%\begin{example} (The derivative operator is linear)\\
%\begin{enumerate}
%\item ~$\frac{d}{dx} (cf)=c\frac{d}{dx} f$, \\[-.5cm]
%\item~$\frac{d}{dx}(f+g)=\frac{d}{dx}f+\frac{d}{dx}g$.\\[-.5cm]
%\end{enumerate}
%\end{example}
%\noindent
%We now explicitly verify that~$B$ and~$C$ are linear. You will verify that~$A$ and~$D$ are linear in the \hyperlink{ch1probset}{problem set} that ends this chapter.  \\
%
%\begin{example} Verifying that~$B$ is linear:
%
%We need to check that $B$ is linear acting on any vector $v=\colvec{x\\ y\\ z}$ so we compute~$Bv$
%using the cross product
%$$
%Bv=\colvec{1 \\ 1\\ 0} \times \colvec{x \\ y\\ z}=\colvec{z\\ -z\\y-x}\, .
%$$
%Now, until we know what a scalar multiple $cv$ of the vector $v$ means, the problem makes no sense. 
%Here we make (the logical) choice
%$$
%c\colvec{x \\ y\\ z} := \colvec{cx\\ cy\\ cz}\, .
%$$
%Now, using this definition and our result for how $B$ acts on an arbitrary vector we find:
%$$
%B(cv)=B\colvec{cx\\ cy\\ cz}=\colvec{cz\\-cz\\cy-cx}\, .
%$$
%We hope that this coincides with $cBv$ which also compute using our result for $Bv$ and the definition
%for multiplying a vector by a number:
%$$
%cBv=c\colvec{z\\ -z\\y-x}=\colvec{cz\\ c(-z)\\c(y-x)}=\colvec{cz\\-cz\\cy-cx}\, .
%$$
%Thankfully, we got the same result for both $B(cv)$ and $cBv$, so the homogeneity property holds.
%
%We still have to check additivity. Again, this makes no sense until we define what the sum $v+u$ of two arbitrary vectors $v=\colvec{x\\y\\z}$ 
%and $u=\colvec{x'\\y'\\z'}$. For that we again make a logical choice
%$$
%\colvec{x\\y\\z}+\colvec{x'\\y'\\z'}:=\colvec{x+x'\\y+y'\\z+z'}\, .
%$$
%Again, we have to make two computations: $B(u+v)$ (first add then act with $B$) and $Bu+Bv$ (first act with $B$, then add):
%$$
%B(u+v)=B\colvec{x+x'\\y+y'\\z+z'}=\colvec{z+z'\\-(z+z')\\(y+y')-(x+x')}=\colvec{z+z'\\-z-z'\\y+y'-x-x'}\, ,
%$$
%$$
%Bu+Bv=B\colvec{x\\y\\z}+B\colvec{x'\\y'\\z'}=\colvec{z\\-z\\y-x}+\colvec{z'\\-z'\\y'-x'}=\colvec{z+z'\\-z-z'\\y+y'-x-x'}\, .
%$$
%Both computations give the same answer so now we know that $B$ also obeys additivity and can conclude that $B$ is linear.
%%AI)~$A(cx)=5cx= c5x=cAx$, \\
%%AII) A(x+y)=5(x+y)=5x+5y=Ax+Ay\\
%%\item~$B(cv)=\colvec{1 \\ 1\\ 0} \times cv
%%\!\!\!\!\!\!\!\!\!\!\!\!\!\!\stackrel{\rm \begin{array}{c}\rm\scriptstyle Using\  the\ rule\\[-2mm] \rm\scriptstyle for\ cross\ products\ of\\[-2mm]\rm
%%\scriptstyle scalar\ multiples\\\downarrow\\[1mm]\end{array}}= 
%%\!\!\!\!\!\!\!\!\!\!\!\!\!\!c\colvec{1 \\ 1\\ 0} \times v=c\, Bv$\, .\\
%%\item~$B(u+v)=\colvec{1 \\ 1\\ 0} \times (u+v)
%%\!\!\!\!\!\!\!\!\!\!\!\!\!\!\stackrel{\rm \begin{array}{c}\rm\scriptstyle Using\  the\ rule\\[-2mm] \rm\scriptstyle for\ cross\ products\\[-2mm]\rm
%%\scriptstyle of\ sums\\\downarrow\\[1mm]\end{array}}
%%=\!\!\!\!\!\!\!\!\!\!\!\!\!\!\colvec{1 \\ 1\\ 0} \times u+\colvec{1 \\ 1\\ 0} \times v =Bu+Bv\, .$\\[.5cm]
%%\end{enumerate} 
%\end{example} 
%The above example looks a little silly, this because it is really just checking something you probably already know about cross products of vectors. Namely, if $u$, $v$ and $w$ are vectors and $c$ is a number then
%$$
%w\times(cv)=c(w\times v)\quad\mbox{and}\quad  w\times(v+u)=w\times v+w\times u\, .
%$$
%You might also have noticed how important it was to know what $cu$ and $u+v$ actually meant to even solve the problem.
%In fact, one of the reasons linear algebra is so powerful, is that it can be applied to any choice for what $cu$ and $u+v$ mean
%that happens to obey a few basic rules. You will learn about these in Chapter~\ref{\vectorSpacesPath}.
%
%Here is another example involving linear operators made from integrals where we assume all the usual rules for integrals. Notice how much easier checking linearity is.
%\begin{example} Verifying that~$C$ is linear:\
%\begin{enumerate}
%\item~$C(cg)(x)= \int_{-1}^1f(x,y) cg(y)dy =c\int_{-1}^1f(x,y) g(y)dy =c\,Cg(x)$\\[.2cm]
%\item~$C(g_1+g_2)(x) =  \int_{-1}^1f(x,y) [g_1(y)+g_2(y)]dy\\[.2cm]
%\phantom{a}~~~~~
%=
%\int_{-1}^1f(x,y) g_1(y)dy
%+\int_{-1}^1f(x,y)g_2(y)dy
%=(Cg_1 +Cg_2)(x)$\\%[.4cm]
%\end{enumerate}
%\end{example}
%%DI)~$D(cf)(x)=(x\frac{d}{dx}-2)cf(x)=c(x\frac{d}{dx}-2)f(x)=cDf(x)$\\[.1cm]
%%DII)~$D(f_1+f_2)(x)=xf_1'(x)-2f_1(x) +xf_2'(x)-2f_2(x) =(Df_1+Df_2)(x)$.\\
%%You now know how to test if an operator is linear, and have some examples.  
%\noindent 
%One moral of our story is that if~$A$ is a {\it linear} operator while~$b$ and~$x$ are vectors, then~$Ax=b$ is equivalent to a matrix{\it -type} equation. 
%That is why we will begin by studying matrices.\\
%% We have only hinted at the wide range of possible examples for linear operators.
%This moral does {\it not} apply to non-linear operators:
%\begin{example} Show that~$E$ is {\it not} linear.\\[-5mm]
%\begin{enumerate}  
%\item~$E(5f)=5f''+5^3f^3 \neq 5f''+5f^3=5Ef~$\\[-5mm]
%\item Not needed.%\\[-5mm]
%\end{enumerate}  
%The operator~$E$ is not linear because it is not homogeneous.\
%\end{example}
%
%Actually,~$E$ is not additive either, but we did not need to show this. Once either one of the linearity properties is broken, the operator is not linear. (Unfortunately, you are a criminal the moment that you break just one law, no matter if you are a law abiding citizen in every other way!)
%
%\videoscriptlink{Homnotadd.mp4}{A Homogeneous Nonlinear Operator}{scripts_H}
%
%\begin{example} Show that operator~$Y$ that acts on numbers by~$Yx=5x+3$ is not linear. \\ 
%Lets check additivity first:
%$$
%Y(x+y)= 5(x+y)+3=(5x+3)+(5x)\neq (5x+3)+(5x+3)=Yx+Yy\, .
%$$
%This proves linearity, unfortunately homogeneity fails:
%$$Y(cx)=5cx+3=c (5x+3) +3 -3c= c\, Yx +3-3c\neq cYx \mbox{ if } c\neq1\, .$$
%Notice that the special case $c=1$ holds (and just says $Y(1.x)=1.Yx$), but the homogeneity
%requirement must work for {\it any} $c$. 
%
%The operator~$Y$ is neither homogeneous nor additive, and therefore not linear.
%\end{example}
%
%This example might surprise you because the graph of the equation~$y=5x+3$ is a line, and what else could linear mean other than ``line-like""? In fact, there is a linear operator associated with this equation: 
%the operator~$L$ that acts on two-vectors as 
%$$L\colvec{x\\y}=y-5x.$$ 
%The line is then the collection of solutions to 
%$$L\colvec{x\\y}=10 \Leftrightarrow y-5x=3 \,.$$ 
%Similarly, the  operator~$M$ on 3-space with 
%$$M\colvec{x\\y\\z}=2x-3y+5z$$ 
%is linear. The set of solutions to 
%$$M\colvec{x\\y\\z}=10 \Leftrightarrow 2x-3y+5z=10$$ 
%form a plane. Lines and planes are {\it level sets} of linear functions. 
%
%\videoscriptlink{Linear_Operator_on_Lines.mp4}{Linear Operators Preserve Lines}{scripts_H}
%
%
%
%%It is one thing to be able to tell which operators are linear and which are not, and it is another to have a feel for what linear operators do. Lets look at three simple example problems to get a feel for this. On the way we will introduce the main tool of linear algebra, the idea of a matrix. \\
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\noindent
%
","\chapter{\whatIsTitle?}\label{warmup}


Many difficult problems can be handled easily 
once  relevant information is organized in a certain way. 
This text aims to teach you how to organize information in cases where certain mathematical structures are present. 
Linear algebra is, in general, the study of those structures. Namely

\vspace{3mm}
\shabox{Linear algebra is the study of vectors and linear functions.} 
\vspace{3mm}

\noindent In broad terms, vectors are things you can add and  linear functions are %very special 
functions of vectors that respect vector addition. 
The goal of this text is to teach you to organize information about vector spaces in a way that makes problems involving linear functions of many variables easy. 
(Or at least tractable.) 

To get a feel for the general idea of organizing information, of vectors, and of linear functions this chapter has brief sections on each. 
We start here in hopes of putting students in the right mindset for the 
odyssey that follows; the latter chapters cover the same material at a slower pace.  
% a little better lets try some examples. 
Please be prepared to change the way you think about some familiar mathematical objects
and keep a pencil and piece of paper handy!

\section{Organizing Information}
\label{organize}
Functions of several variables are often presented in one line such as 
$$f(x,y)=3x+5y\,.$$
But lets think carefully; what is the left hand side of this equation doing? 
Functions and equations are different mathematical objects so why is the equal sign necessary?

\begin{center}
\href{https://www.youtube.com/watch?v=dvoG5P9Uczg}{\raisebox{-.4cm}{\includegraphics[scale=.075]{take1.jpg}}} \hspace{1cm}\scalebox{1.2}{\tt A Sophisticated Review of Functions }
\end{center}
If someone says 
\vspace{-.01cm}
\begin{center}
``Consider the function of two variables $7\beta-13 b$."" 
\end{center}
we do not quite have all the information we need to determine the relationship between inputs and outputs. 
%This is the foundation of the powerful yet easy to use  mathematics known as linear algebra. 
%%%%an alternative wording
%Difficult %science 
%problems involving multiple variables can be handled easily 
%once  relevant information is organized in a certain way. 
%You are about to learn powerful methods of organizing information 
%applicable when a certain kind of  object is involved in a problem; a linear function. It takes quite a bit of work to understand what a linear function is, but it does not take much to see what it looks like to organize information.   
%%This is the foundation of the powerful yet easy to use  mathematics known as linear algebra. 
\begin{example} (Of organizing and reorganizing information)\\ \label{OrderInfo}
You own stock in 3 companies: $Google$, $Netflix$, and $Apple$. 
The value $V$ of your stock portfolio as a function of the number of shares you own $s_{N}, s_{G},s_{A}$ of these companies 
%with logos 
 is %given by
$$
%%%%%%%%%%%%%%%%
%%%% HEY 
%%%% Do not type V\colvec{ s_a\\s_b\\s_c }= here!
%%%%That already involves choice of  an order! 
24s_{G}+80s_{A}+35s_{N}\,.$$
Here is an ill posed question: what is $V\colvec{1\\2\\3} $?\\

The column of three numbers is ambiguous! 
%We can't determine if 
Is it is meant to denote % the value of your portfolio 
\begin{itemize}
\item 1 share of ${G}$, 2 shares of ${N}$ and 3 shares of ${A}$? 
\item 1 share of ${N}$, 2 shares of ${G}$ and 3 shares of ${A}$?  
\end{itemize}
Do we multiply the first number of the input by 24 or by 35? 
No one has specified an order for the variables, 
so we do not know how to calculate an output associated with a particular input.\!\footnote{Of course we would know how to calculate an output if the input is described in the tedious form such as ``1 share of ${G}$, 2 shares of ${N}$ and 3 shares of ${A}$"", but that is unacceptably tedious! We want to use ordered triples of numbers to concisely describe inputs.}% that input is written as a triple of numbers. 

A different notation for $V$ can clear this up; 
%since specifying an oder for the variables amounts to instructions of which number from the input to multiply by which of thee numbers 80, 35, 24, 
we can denote $V$ itself as an ordered triple of numbers that reminds 
us what to do to each number from the input.
\begin{center}
\shabox{Denote $V$ by  $\rowvec{ 24 &80& 35 } $ and thus write 
$V\colvec{ 1\\2\\3 }_{\!\!\!\!B }
= 
\begin{pmatrix} 24 &80& 35 \end{pmatrix}
\colvec{ 1\\2\\3 }%_{\!\!\! \!\alpha }.
$
\qquad \qquad
\phantom{ $\colvec{1\\2} $}%for space
to remind us to calculate 
$24(1)+80(2)+35(3)=334$ \qquad\qquad
\phantom{ $\colvec{1\\2} $}%for space
because we chose the order 
%$\colvec{s_{G}\\s_ {A}\\s_{N}}$ 
$\colvec{{G} & {A} & {N}}$ 
and named that order $B$ \qquad
so that inputs are interpreted as 
$\colvec{s_{G}\\s_ {A}\\s_{N}}$ 
.} 
\end{center}
%as a concise reminder of how to calculate the output assigned to any particular input ... {\it but only if you know what the input means}. 
%The input is a column of three numbers. 
%You need to know to interpret the 
%top number as stock in company $a$, 
%second from the top in company $b$, etc.. 
%Thus, the choice of notation for $V$ involved a choice of order of the companies. 
If we change the order for the variables we should change the notation for $V$.
\begin{center}
\shabox{
Denote $V$ by  $\rowvec{  35 & 80& 24 } $ and thus write 
$V\colvec{ 1\\2\\3 }_{\!\!\!\!B'}
= 
\rowvec{  35 & 80& 24 } 
\colvec{ 1\\2\\3 }$
\qquad \qquad
\phantom{ $\colvec{1\\2} $}%for space
to remind us to calculate 
$35(1)+80(2)+24(3) =264.$ \qquad\qquad
\phantom{ $\colvec{1\\2} $}%for space
because we chose the order 
$\colvec{{N} & {A} & {G} }$ 
and named that order $B'$\qquad
so that inputs are interpreted as 
$\colvec{s_{N}\\ s_ {A}\\ s_{G}}$ 
.
} 
\end{center}
The  subscripts $B$ and $B'$ on the columns of numbers are just symbols\footnote{We were free to choose any symbol to denote these orders. We chose $B$ and $B'$ because we are hinting at a central idea in the course: choosing a basis.} reminding us of how to interpret the column of numbers.
But the distinction is critical; as shown above
$V$ assigns completely different numbers to the same  columns of numbers with different subscripts. 

There are six different ways to order the three companies. 
Each way will give different notation for the same function $V$, and a different way of assigning numbers to columns of three numbers. 
Thus, it is critical to make clear which ordering is used if the reader is to understand what is written. 
Doing so is a way of organizing information. 

This example is a hint at a much bigger idea central to the text; 
our choice of order is an example of choosing a {\it basis}\footnote{
Please note that this is an {\it example} of choosing a basis, 
not a statement of the definition of the technical term ``basis"". 
You can no more learn the definition of ``basis""  from this example 
than learn the definition of  ``bird"" by seeing a penguin.}\!\!. 
\end{example}

The main lesson of an introductory linear algebra course is this: 
you have considerable freedom in how you organize information about certain functions, 
and you can use that  freedom to 
\begin{enumerate}
\item uncover aspects of functions that don't change with the choice (Ch \ref{eigenvalseigenvects})
\item make calculations maximally easy (Ch \ref{sec:diagonalization}  and Ch \ref{sec:leastsquaresSVD}) 
\item approximate functions of several variables (Ch \ref{sec:leastsquaresSVD}).
\end{enumerate}
Unfortunately, because the subject (at least for those learning it) requires seemingly arcane and tedious computations
involving large arrays of numbers known as matrices, the key concepts and the wide applicability of linear algebra are
easily missed. So we reiterate, 

\vspace{3mm}
\shabox{Linear algebra is the study of vectors and linear functions.} 
\vspace{3mm}

\noindent In broad terms, vectors are things you can add and  linear functions are 
functions of vectors that respect vector addition. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
\noindent 
\section{What are Vectors?}
Here are some examples of things that can be added:

\begin{example} (Vector Addition)
\begin{enumerate}[(A)]
\item Numbers: Both $3$ and $5$ are numbers and so is $3+5$.\\[-2mm]
\item 3-vectors: $\colvec{1 \\ 1\\ 0} + \colvec{0\\1\\1}=\colvec{1 \\ 2\\ 1}$.\\[-1mm]
\item Polynomials: If $p(x)=1+x-2x^2+3x^3$ and $q(x)=x+3x^2-3x^3+x^4$ then\\[1mm] their sum $p(x)+q(x)$ is the new polynomial $1+2x+x^2+x^4$.\\
\item Power series: If $f(x)=1+x+\frac1{2!} x^2 + \frac1{3!} x^3 +\cdots$ and $g(x)=1-x+\frac1{2!} x^2 - \frac1{3!} x^3 +\cdots$ \\[1mm]
 then $f(x)+g(x)=2+ x^2 +\frac2 {4!} x^4+\cdots$ is also a power series.\\
\item Functions: If $f(x)=e^x$ and $g(x)=e^{-x}$ then their sum $f(x)+g(x)$ is the new function $2\cosh x$.
\end{enumerate}
\end{example}

\noindent
There are clearly different kinds of vectors. 
Stacks of numbers are not the only things that are vectors, as examples C, D, and E show. 
Vectors of different kinds can not be added; What possible meaning could the following have? 
$$\colvec{9\\3} + e^x$$

In fact, you should think of all five kinds of vectors above as different kinds, and that you should not add vectors that are not of the same kind. 
On the other hand, any two things of the same kind ``can be added''. 
This is the reason you should now start thinking of all the above objects as vectors! 

In Chapter~\ref{vectorSpaces} we will give the precise rules that  vector addition must obey. 
In the above examples, however, notice that the vector addition rule stems from the rules for adding numbers. 
 

When adding the same vector over and over, for example
$$
x+x\, ,\: \  x+x+x\, ,\:\    x+x+x+x\,  ,\, \ldots\, ,
$$
we will write
$$
2x\, ,\: \, 3x\, , \:\,  4x\, ,\, \ldots\, ,
$$
respectively. For example
$$
4\colvec{1\\1\\0}=\colvec{1\\1\\0}+\colvec{1\\1\\0}+\colvec{1\\1\\0}+\colvec{1\\1\\0}=\colvec{4\\4\\0}\, .
$$
Defining $4x=x+x+x+x$ is fine for integer multiples, but does not help us make sense of $\frac13 x$. For the different types of vectors 
above, you can probably guess how to multiply a vector by a scalar. For example
$$
\frac 13 \colvec{1\\[1mm]1\\[1mm]0} = \colvec{\frac13\\[1mm] \frac13\\[1mm]0}\, .
$$

 A very special vector can be produced from any vector of any kind by scalar multiplying any vector by the number~$0$. 
This is called the {\it zero vector}\index{Zero vector} and is usually denoted simply $0$. This gives five very different kinds of zero from the 5 different kinds of vectors in   examples A-E above.
\begin{enumerate}[(A)]
\item $0(3)=0$ (The zero number)
\item $0\colvec{1\\1\\0}=\colvec{0\\0\\0} $ (The zero 3-vector)
\item $0\left(1+x-2x^2+3x^3\right)=0$ (The zero polynomial)
\item 
$0\!\left(  1+x\!-\!\frac1{2!}x^2\!+\!\frac1{3!}x^3\!+\cdots \!\right) \!
 =  0+0x+0x^2\!+0x^3\!+\cdots $(The zero power~series)
 \item $0\left(   e^x \right) =0$ (The zero function)
 \end{enumerate}

In any given situation that you plan to describe using vectors, you need to decide on a way to add and scalar multiply vectors.
In summary:
\vspace{3mm}
\begin{center}
\shabox{Vectors are things you can add and scalar multiply.} 
\end{center}
\vspace{3mm}

\noindent 
Examples of kinds of vectors:\\
\begin{itemize}
\item numbers
\item n-vectors
\item 2nd order polynomials
\item polynomials
\item power series
\item functions with a certain domain
\end{itemize}



\section{What are Linear Functions?}
\label{LTs}

In calculus classes, the main subject of investigation was the rates of change of functions. 
In linear algebra, functions
will again be the focus of your attention, 
but functions of a very special type. 
In precalculus you 
were perhaps encouraged to think of a function as a machine~$f$
into which one may feed a real number. 
For each input $x$ this machine outputs a single real number~$f(x)$. 

\begin{center}
\includegraphics[scale=.3]{\whatIsPath//machine.jpg}
\end{center}

In linear algebra, the functions we study will have vectors (of some type) as both inputs and outputs. 
We just saw that vectors are objects that can be added or scalar multiplied---a very general notion---so the functions we are going to study will look novel at first. 
So things don't get too abstract, here are five questions that can be rephrased in terms of functions of vectors.

\begin{example}  \label{EX}
(Questions involving Functions of Vectors in Disguise)\\
\begin{enumerate}[(A)]
\item\label{FVA}What number $x$ satisfies $10x=3$?
\item\label{FVB} What 3-vector~$u\!\, $  satisfies\footnote{ The \hyperlink{crossprod}{cross product} appears in this equation.}~$\!\colvec{1 \\ 1\\ 0} \times u = \colvec{0\\1\\1}$?\\[-.4cm]
%$$\!\!\!\!\!\!\!\!\!\colvec{x\\ y\\ z} \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!\colvec{\phantom{-}z\\-z\\y-x}$$
\item \label{FVC}What polynomial~$p$ satisfies~$\int_{-1}^1  p(y) dy = 0$ and $\int_{-1}^1 y p(y) dy=1$?\\
%$$p(x) \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!\colvec{\int_{-1}^1  p(y) dy\\[2mm]
%x\int_{-1}^1 y p(y) dy}$$
\item \label{FVD}What power series~$f(x)$ satisfies~$x\frac{d}{dx} f(x) -2f(x)=0$?
%$$f(x) \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!x\frac{d}{dx} f(x) -2f(x)$$
\item What number~$x$~satisfies~$4 x^2=1$?
%$$f(x) \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!f''(x)+f^3(x) -\sqrt{x}$$
\end{enumerate}
\vspace{.3cm}
All of these are of the form 
\begin{enumerate}[($\star$)]
\item What vector $X$ satisfies $f(X)=B$?
\end{enumerate}
with a function\footnote{In math terminology, each question is asking for the level set of $f$ corresponding to $B$.} $f$ known, a vector $B$ known, and a vector $X$ unknown.
\end{example}
The machine needed for part~(\ref{FVA}) is as in the picture below. 
$$x \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!10 x\, \hspace{8mm}$$
This is just like a function $f$ from calculus that takes in a number $x$ and spits out the number $10x$. (You might write $f(x)=10x$ to indicate this).
For part~(\ref{FVB}), we need something more sophisticated. 
$$\!\!\!\!\!\!\!\!\!\colvec{x\\ y\\ z} \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!\ccolvec{\phantom{-}z\\-z\\\!y-x\!\!}\, ,$$
The inputs and outputs are both 3-vectors. The output is the cross product of the input with... how about you complete this sentence to make sure you understand.

The machine needed for example~(\ref{FVC}) looks like it has just one input and two outputs; we input a polynomial and get a 2-vector as output.
$$\quad p \raisebox{-11mm}{\includegraphics[scale=.14]{\whatIsPath//machine_blank.jpg} }\! \! \!\! \! \!\colvec{\int_{-1}^1  p(y) dy\\[3mm]
\int_{-1}^1 y p(y) dy}\, .$$
This example is important because it displays an important feature; 
{\it the inputs for this function are functions}.

While this sounds complicated, %Rest assured that 
linear algebra is the
%involves the 
study of %only a very 
simple %(yet very important) class of 
functions of vectors; its time to describe the essential characteristics of linear functions. 

Let's use the letter $L$ to denote an arbitrary linear function and think again about vector addition and scalar multiplication. 
Also,  suppose that $v$ and $u$ are vectors and $c$ is a number. 
Since $L$ is a function from vectors to vectors, if we input $u$ into $L$, the output $L(u)$ will also be some sort of vector. 
The same goes for 
$L(v)$.
(And remember, our input and output vectors might be something other than stacks of numbers!) 
Because vectors are things that can be added and scalar multiplied, 
$u+v$ and $cu$ are also vectors, and so they can be used as inputs.
The essential characteristic of linear functions is what can be said about 
%the outputs 
$L(u+v)$ and $L(cu)$ in terms of 
%the outputs 
$L(u)$ and $L(v)$. 

Before we tell you this essential characteristic, 
ruminate on this picture. 

\begin{center}
\includegraphics[scale=.4]{\whatIsPath/L.jpg}
\end{center}

The ``blob'' on the left represents all the vectors that you are allowed to input into the function $L$,  the blob on the right denotes the
possible outputs, and the lines tell you which inputs are turned into which outputs.\footnote{The domain, codomain, and rule of correspondence of the function are represented by the left blog, right blob, and arrows, respectively.} A full pictorial description of the functions would require all inputs and outputs and lines to be explicitly drawn, but we are being diagrammatic; we only drew four of each.  




Now think about adding $L(u)$ and $L(v)$ to get yet another vector $L(u)+L(v)$ or of multiplying $L(u)$ by $c$ to obtain the vector $cL(u)$, and placing both on the right blob of the picture above. 
But wait! Are you certain that these are possible outputs!?
%Hopefully you noticed that there are two vectors  apparently {\it not shown} on the blob of outputs:
%$$
%L(u)+L(v)\quad \&\quad cL(u)\, .
%$$ 
%You might already be able to guess the values we would like these to take. If not, 

Here's the answer 
\begin{center}
\shabox{\scalebox{1.1}{
The key to the whole class, from which everything else \hypertarget{twopart}{follows}: 
%\newline The essential features of a linear function $L$ are
}}
\end{center}
\begin{enumerate}
\item Additivity:
$$L(u+v)=L(u)+L(v)\, .$$
\item Homogeneity:
$$L(cu)=cL(u)\, .$$
\end{enumerate}

\noindent Most functions of vectors do not obey this requirement.\!\footnote{{\it E.g.:} If $f(x)=x^2$ then $f(1+1)=4 \neq f(1)+f(1)=2$. Try any other function you can think of!}  At its heart, linear algebra is the study of functions that do. 

Notice that the additivity 
requirement says that the function $L$ respects vector addition: {\it it does not matter if you first add $u$ and~$v$ and then input their sum into
$L$, or first input $u$ and $v$ into $L$ separately and then add the outputs.} The same holds for scalar multiplication--try writing out the scalar multiplication version of the italicized sentence. When a function of vectors obeys the additivity and homogeneity properties we say that it is {\it linear} (this is the ``linear'' of linear algebra). Together, additivity and homogeneity are called {\it linearity}. 
Are there other, equivalent, names for linear functions? Yes:
%\begin{enumerate}[]
%\item Linear transformation 
%\item Linear map
%\item Linear operator
%\item Homomorphism
%\end{enumerate}
\begin{center}
\includegraphics[scale=.37]{\whatIsPath/Names.jpg}
\\
\shabox{Function~=~Transformation~=~Operator}\\
\end{center}

And now for a hint at the power of linear algebra. 
The questions in examples~(\ref{FVA}-\ref{FVD}) can all be restated as %a single equation:
\begin{center}
\shabox{
$
L v = w
$}
\end{center}
where $v$ is an unknown, $w$ a known vector, and $L$ is  a known linear transformation.
To check that this is true, one needs to know the rules for adding vectors (both inputs and outputs)
and then check linearity of $L$. Solving the equation $Lv=w$ often amounts  to solving systems of linear equations,
the skill you will learn in  Chapter~\ref{systems}.



A great example is the derivative operator.
\begin{example} (The derivative operator is linear)\\
For any two functions~$f(x)$,~$g(x)$ and any number~$c$, in calculus you probably learnt that the derivative operator satisfies
\begin{enumerate}
\item ~$\frac{d}{dx} (cf)=c\frac{d}{dx} f$, \\[-.5cm]
\item~$\frac{d}{dx}(f+g)=\frac{d}{dx}f+\frac{d}{dx}g$.\\[-.5cm]
\end{enumerate}
If we view functions  as vectors with addition given by addition of functions and with scalar multiplication given by multiplication of functions by constants, then these familiar properties of derivatives are just the linearity property of linear maps.
\end{example}

Before introducing matrices, notice that for linear maps~$L$ we will often write simply $L u$ instead of $L(u)$. This is because the linearity
property of a linear transformation $L$ means that $L(u)$ can be thought of as multiplying the vector $u$ by the linear operator $L$.
For example, the linearity of $L$ implies that if $u,v$ are vectors and $c,d$ are numbers, then
\begin{center}
\shabox{\scalebox{1.1}{
$
L(c u + d v) = c L u + d L v\, ,
$}}
\end{center}
which feels a lot like the regular rules of algebra for numbers. Notice though, that ``$u L$'' makes no sense here.

\begin{remark}
A sum of multiples of vectors $c u + dv$ is called a {\it linear combination}\index{Linear combination} of $u$~and~$v$.
\end{remark}

\section{So, What is a Matrix?}
Matrices are linear functions of a certain kind. 
They appear almost ubiquitously in linear algebra because---and this is the central lesson of introductory linear algebra courses---
\begin{center}
\shabox{Matrices are the result of organizing information related to linear functions. }
\end{center}
This idea will take some time to develop, but we provided an elementary example in 
Section \ref{organize}. A good starting place to  learn about matrices is by studying {\it systems of  linear equations}. 

\begin{example} 
A room contains~$x$ bags and~$y$ boxes of fruit.
\begin{center}
\includegraphics[scale=.25]{\whatIsPath/boxesbags.jpg}
\end{center}
Each bag contains 2 apples and 4 bananas and each box contains 6 apples and 8 bananas. 
There are 20 apples and 28 bananas in the room. Find~$x$ and~$y$.
\\

%<pic please>\\

\noindent
The values are the numbers~$x$ and~$y$ that simultaneously make both of the following equations true:
\begin{eqnarray*}
	2\, x + 6\, y & =  & 20 \\
	4\, x + 8\, y & = & 28\, .
\end{eqnarray*}
\end{example}
Here we have an example of a \emph{System of Linear Equations}\index{Linear System!concept of}.\footnote{Perhaps you can see that both lines are of the form $Lu=v$ with $u=\colvec{x\\y}$ an unknown, $v=20$ in the first line, $v=28$ in the second line, and $L$ different functions in each line? We give the typical less sophisticated description in the text above.}   
It's a collection of equations in which variables are multiplied by constants and summed, and no variables are multiplied together:  There are no powers of variables 
(like~$x^2$ or~$y^5$), non-integer or negative powers of variables (like~$y^{1/7}$ or~$x^{-3}$), and no places where variables are multiplied together (like~$xy$).
%\begin{center}\href{\webworkurl ReadingHomework1/1/}{Reading homework: problem 1.1}\end{center}
%\reading{1}{1}
\Reading{WhatIsLinearAlgebra}{1}

%The system of equations above has the feel of multiplication of contents per package by number of packages. 
%What we have is a function that takes in two numbers (number of bags and number of boxes) and gives out two numbers (number of apples and number of bananas.) 
%This function is linear: double the number of bags and the number of boxes and you will double the number of apples and number of bananas. Same for tripling, quadrupling, etc...
%
%An important idea underlies the following observation. 
\noindent
Information about the fruity contents of the room can be stored two ways: 
\begin{enumerate}[(i)]
\item In terms of the number of apples and bananas. 
\item In terms of the number of bags and boxes. 
\end{enumerate}
Intuitively, knowing the information in one form allows you to figure out the information in the other form. 
Going from~(ii) to~(i) is easy: 
If you knew there were~3 bags and~2 boxes it would be easy to calculate the number of apples and bananas, and doing so would have the feel of multiplication (containers times fruit per container). 
In the example above we are required to go the other direction, from~(i) to~(ii). This  feels like the opposite of multiplication, {\it i.e.}, division. Matrix notation will 
make clear what we are ``multiplying"" and ``dividing'' by. 

The goal of Chapter~\ref{systems} is to efficiently solve systems of linear equations. 
%This is going to be a generalization of dividing both sides of the system of equations by...something. 
%To show you what we mean, we will now give you an example of an inefficient method of solving the system from example 3:\\
%
%\noindent
%{\bf Inefficient Method}:\\
%Rearrange the first equation into~$x=10-3y$ and substitute the result into the second equation to obtain~$28=4(10-3y)+8y$. This equation has only one unknown; the jargon is ``$x$ has been eliminated"". The equation implies that~$y=3$. That result can be ``back substituted"" into either of the original equations, for example the first, to obtain 
%$20=2x+6\cdot 3 \Leftrightarrow x=1$.\\
%
%It is easy to get lost when using this method. Especially when dealing with large systems of linear equations, such as 256 equations in 256 variables. It would be nice if we could lay out our work in a way that resonates with the intuition we have built from solving simpler algebra problems, like the familiar procedure
%\begin{eqnarray*}
%3x+4 &=&7\\
%{\rm Subtract}~&4&\\
%3x&=&3\\
%{\rm divide~by}~&3&\\
%x&=&1 \,.
%\end{eqnarray*}
%That is, lets keep the variables on the left hand side and rewrite our {\it system} of equations after each step. \\
%
%
%\noindent
%{\bf More Efficient Method:}\\
%Divide (both sides of) the second equation by 2 to obtain the equivalent system
%\begin{eqnarray*}
%	2\cdot x + 6\cdot y & = & 20 \\
%	2\cdot x + 4\cdot y  & = & 14 \,.
%\end{eqnarray*}
%Subtract the first equation from the second, to get the equivalent system 
%\begin{eqnarray*}
%	2\cdot x + 6\cdot y & =  & 20  \\
%	 0\cdot x - 2\cdot y & = & -6\, .
%\end{eqnarray*}
%Now add three times the second equation to the first
%\begin{eqnarray*}
%	2\cdot x + 0\cdot y & = & 2 \\
%	0\cdot x - 2\cdot y & = & -6\, .
%\end{eqnarray*}
%At this point the result~$x=1,y=3$ is obvious. Further, the elimination of~$y$ from the first equation and elimination of~$x$ from the second is clear as can be. 
%%This is elimination, a key idea in this course. 
%%The idea here is to follow some algorithm to have just one variable with non-zero coefficient in each equation, and to rewrite the {\it system} of linear equations at each step. 
%
%There is a clear shortcoming to this ``efficient method"": we need to rewrite too much at each step! The~$x$'s are rewritten in the same place over and over, and similarly for the~$y$'s and the equal signs. 
%Lets work toward reducing the amount of things we rewrite by combining the pair of equations in example 3 into a singe equation using vectors from 2-space. 
Partly, this is just a matter of finding a better notation, but one that hints at a deeper underlying mathematical structure.
For that, we need rules for adding and scalar multiplying 2-vectors; 
$$
c\colvec{x\\y}:=\colvec{cx\\cy}\, \mbox{ and } \colvec{x\\y}+\colvec{x'\\y'}:=\colvec{x+x'\\y+y'}\, .
$$
Writing our fruity equations as an equality between 2-vectors and then using these rules we have:
%First we reduce the number of appearances of the equal sign:
\begin{equation*}
   \left.
\begin{array}{lr}
   	2\, x + 6\, y  =  20 \\
	4\, x + 8\, y  =  28
     \end{array}
   \right\} 
\ \Longleftrightarrow  \  \colvec{ 2x+6y \\ 4x+8y}  =\colvec{ 20\\ 28}
%\, .
%\end{eqnarray*}
%Now we can reduce the number of appearances of the symbols~$x$ and~$y$ using vector addition and scalar multiplication:
%\begin{eqnarray*}
%    \colvec{ 2x+6y \\ 4x+8y}  %=\colvec{ 20\\ 28}
\ \Longleftrightarrow \ 
   x \colvec{ 2\\ 4} + y \colvec{ 6\\ 8} =\colvec{ 20\\ 28} \, .
\end{equation*}
Now we  introduce a function which takes in 2-vectors\footnote{To be clear, we will use the term 2-vector to refer to stacks of two numbers such as~$\colvec{7\\11}$. If we wanted to refer to the vectors $x^2+1$ and $x^3-1$ (recall that polynomials are vectors) we would say ``consider the two vectors $x^3-1$ and $x^2+1$''. We apologize through giggles for the possibility of the phrase ``two 2-vectors.""} and gives out 2-vectors. We denote it by  an array of numbers  called a {\it matrix\,\! .}
\begin{equation*}
 {\rm {\bf The~function}}   \begin{pmatrix}
      2     & 6 \\
      4     & 8
    \end{pmatrix}~{\rm {\bf is~defined~by}}
    \begin{pmatrix}
      2     & 6 \\
      4     & 8
    \end{pmatrix}
  \colvec{x \\ y}
  := x \colvec{ 2\\ 4} + y \colvec{ 6\\ 8} \, .
%  \colvec{20 \\ 28}
\end{equation*}
A similar definition applies to matrices with different numbers and sizes.

\begin{example}(A bigger matrix)
$$
\begin{pmatrix}1&0&3&4\\
5&0&3&4\\
-1&6&2&5
\end{pmatrix}
\ccolvec{x\\y\\z\\w}
:=x
\begin{pmatrix}1\\5\\-1
\end{pmatrix}
+y
\begin{pmatrix}0\\0\\6
\end{pmatrix}
+z
\begin{pmatrix}3\\3\\2
\end{pmatrix}
+w\begin{pmatrix}4\\4\\5
\end{pmatrix}\, .
$$
\end{example}


Viewed as a machine that inputs and outputs 2-vectors, our $2\times2$ matrix does the following:
$$
\colvec{x\\y}\raisebox{-11mm}{\includegraphics[scale=.15]{\whatIsPath//machine_matrix.jpg} }\!\!\!\!\!\!\colvec{2x+6y\\4x+8y}\, .
$$
Our fruity problem is now rather concise.
\begin{example}  (This time in purely mathematical language): \\[.2cm]
What vector~$  \colvec{x \\ y}$ satisfies 
$
    \begin{pmatrix}
      2     & 6 \\
      4     & 8
    \end{pmatrix}
  \colvec{x \\ y}
  =   \colvec{20 \\ 28}
$?
\end{example}
%\\
This is of the same~$Lv=w$ form as our opening examples. 
The matrix encodes fruit per container. The equation is roughly fruit per container times number of containers equals fruit. To solve for number of containers we want to \hypertarget{ch1divide}{somehow ``divide''} by the matrix. 


Another way to think about the above example is to remember the rule for multiplying a matrix times a vector.
\hypertarget{system2matrix}{If} you have forgotten this, you can actually  guess a good rule by making sure the matrix equation is the same as the system of linear equations.
This would require that
$$
 \begin{pmatrix}
      2     & 6 \\
      4     & 8
    \end{pmatrix}
  \colvec{x \\ y}
  :=   \colvec{2x+6y \\ 4x+8y}
$$
Indeed this is an example of
\hypertarget{ch1vecmult}{the general rule} that you have probably seen before
%\begin{center}
%``Turn the vector sideways, and combine it with the rows of the matrix"" \end{center}
\begin{equation*}\label{2x2multiplication}
    \begin{pmatrix}
      p     & q  \\
      r      & s
    \end{pmatrix}
  \colvec{x \\ y}
  :=
  \colvec{px+qy \\ rx+sy}=x\colvec{p\\r} + y\colvec{q\\s}\, .
\end{equation*}\\%[.2cm]
Notice, that the second way of writing the output on the right hand side of this equation is very useful because it
tells us what all possible outputs a matrix times a vector look like -- they are just sums of the columns of the matrix
multiplied by scalars. The set of all possible outputs of a matrix times a vector is called 
the {\bf column space}\index{Column Space!concept of} (it is also the image of the linear function defined by the matrix).
%\reading{1}{2}
\Reading{WhatIsLinearAlgebra}{2}


 


\noindent
Multiplication by 
\hypertarget{earlier}{a matrix} is an example of a \emph{Linear Function}\index{Linear Transformation!concept of}, because it takes one vector and turns it into another in a ``linear'' way.
Of course, we can have much larger matrices if our system has more variables.

\videoscriptlink{what_is_linear_algebra_Possibilities.mp4}{Matrices in Space!}{script_what_is_linear_algebra_hint}

\noindent
Thus
\hypertarget{Matrices are linear operators}{matrices can be viewed as linear functions.} 
The statement of this for the matrix in our fruity example is as follows.
\begin{enumerate}
\item\label{ITEM1}~$\begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
   \lambda \colvec{x \\ y} 
   =\lambda  \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
   \colvec{x \\ y} ~$ and
\item\label{ITEM2}
$     \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
   \left[ \colvec{x \\ y} +\colvec{x' \\ y'} \right] 
   = \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
\colvec{x \\ y}
   +
    \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
    \colvec{x' \\ y'}
.$
\end{enumerate}
These equalities can be verified using the rules we introduced so far.
\begin{example} Verify that 
$\begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}$
is a linear operator.

\noindent
The matrix-function is homogeneous if the expressions on the left hand side and right hand side of the equation in part~\ref{ITEM1} above are indeed equal. 
\begin{equation*}
\begin{split}
\begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
    \left[
   \lambda \colvec{a \\ b} \right]
 =
\begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
   \colvec{\lambda a \\ \lambda b} 
 &=
  \lambda a \colvec{2 \\ 4} 
+   
     \lambda b \colvec{6 \\ 8} \\[2mm]&
 = \colvec{2\lambda a \\ 4\lambda a} 
+   
      \colvec{6\lambda b \\ 8\lambda b}=  \underline{\colvec{2\lambda a+6\lambda b\\4\lambda a+8\lambda b}} \end{split}\end{equation*}
    \\[.1cm]
while
 \begin{equation*}\begin{split} \lambda  \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
   \colvec{a \\ b} ~
   =
     \lambda\left[ a \colvec{2 \\ 4} 
+   
     b \colvec{6 \\ 8} \right]
   &=\lambda\left[\colvec{2a\\4a}+\colvec{6b\\8b}\right]\\[2mm]&=\lambda\colvec{2a+6b\\4a+8b} =  \underline{\colvec{2\lambda a+6\lambda b\\4\lambda a+8\lambda b} .}
     \end{split}\end{equation*}
\vspace{3mm}
\noindent
The underlined expressions are  identical, so the matrix is homogeneous. \\

The matrix-function is additive if the left and right side of the equation  in part~\ref{ITEM2} above are indeed equal. 

\begin{equation*}
\begin{split}
     \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
   \left[ \colvec{a \\ b} +\colvec{c \\ d} \right] 
   &= 
        \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
    \colvec{a +c\\ b+d}  
   =
        (a+c) \colvec{2 \\ 4} 
        +
         (b+d) \colvec{6 \\ 8}\\[2mm]&
         =
        \colvec{2(a+c) \\ 4(a+c)} 
        +
        \colvec{6(b+d) \\ 8(b+d)}
             =
       \underline{ \colvec{2a+2c +6b+6d\\ 4a+4c+8b+8d} }
       \end{split}\end{equation*}
 which we need to compare to  
\begin{equation*}
\begin{split}
  \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
\colvec{a \\ b}
 &  +
    \begin{pmatrix}
      2             &6 \\
      4            &8
    \end{pmatrix}
    \colvec{c \\ d}
=
a\colvec{2\\4} + b\colvec{6\\ 8} + c\colvec{2\\4} +d\colvec{6\\8}\\[2mm]&
=\colvec{2a\\4a} + \colvec{6b\\ 8b} + \colvec{2c\\4c} +\colvec{6d\\8d}
=\underline{\colvec{2a+2c +6b+6d\\ 4a+4c+8b+8d} }\, .
\end{split}\end{equation*}
Thus multiplication by a  matrix is additive and homogeneous, and so it is, by definition, linear. 
\end{example}

%\noindent

We have come full circle; matrices are just examples of the kinds of linear operators that appear in algebra problems like those in 
section~\ref{LTs}. 
Any equation of the form~$Mv=w$ with~$M$ a matrix, and~$v,w$ $n$-vectors is called  a {\it matrix equation}\index{Matrix equation}. 
Chapter~\ref{systems} is about efficiently solving systems of linear equations, or equivalently matrix equations.

\subsection{Matrix Multiplication is Composition of Functions}

What would happen if we placed two of our expensive machines end to end?
$$
\includegraphics[scale=.15]{\whatIsPath//machine_matrix.jpg} \raisebox{1mm}{\includegraphics[scale=.15]{\whatIsPath//machine_matrix1.jpg}} \raisebox{12mm}{\Huge ?}
$$
The output of the first machine would be fed into the second. 
$$
\scalebox{.7}{$\colvec{x\\y}$}\hspace{-.6mm}\raisebox{-9mm}{\includegraphics[scale=.12]{\whatIsPath//machine_matrix.jpg} }\!\!\!\!\!\!\scalebox{.7}{$\colvec{2x+6y\\4x+8y}$}
\raisebox{-9mm}{\includegraphics[scale=.12]{\whatIsPath//machine_matrix1.jpg} }\!\!\!\!\!\!\!\!\!\!\!\!\!
  \raisebox{-3mm}{ \scalebox{.7}{$\begin{array}{l}\ \ \ \ \colvec{1.(2x+6y)+2.(4x+8y)\\0.(2x+6y)+1.(4x+8y)}\\[4mm] \ =\colvec{10x+22y\\4x+8y}\end{array}$}}
$$
Notice that the same final result could be achieved with a single machine:
$$
\colvec{x\\y}\raisebox{-11mm}{\includegraphics[scale=.15]{\whatIsPath//machine_matrix2.jpg} }\!\!\!\!\!\!\colvec{10x+22y\\4x+8y}\, .
$$
There is a simple matrix notation for this called {\it matrix multiplication}\index{Matrix multiplication}
$$
\begin{pmatrix}1&2\\0&1\end{pmatrix}
\begin{pmatrix}2&6\\4&8\end{pmatrix}
=\begin{pmatrix}10&22\\4&8\end{pmatrix}\, .
$$ 
Try review problem~\ref{matmult} to learn more about matrix multiplication. 

In the language\footnote{The notation $h:A\to B$ means that $h$ is a function with domain $A$ and codomain $B$. See the webwork  background  set\hwref{Background}{3} if you are unfamiliar with this notation or these terms.} 
of functions, if $$f:U\longrightarrow V\quad \mbox{and}\quad g:V\longrightarrow W$$
the new function obtained by plugging the outputs if $f$ into $g$ is called $g\circ f$,
$$
g\circ f:
U\longrightarrow  W$$
where
$$
(g\circ f)(u)=g(f(u))\, .
$$
This is called the {\it composition of functions}\index{Composition of functions}.
Matrix multiplication is the tool required for computing the composition of linear functions.


\subsection{The Matrix Detour}
Linear algebra is about linear functions, not matrices. 
%This lesson is hard to learn after a full term of working with matrices. 
%Start thinking about this on day one of the course. 
The following presentation is meant to get you thinking about this idea constantly throughout the course.
\begin{center}
\shabox{Matrices only get involved in linear algebra when certain notational choices are made.}
\end{center}
To exemplify, lets look at the derivative operator again.

\begin{example}{of how matrices come into linear algebra.}\\
Consider the equation
%question ``which quadratic function $f$ satisfies
$$\left( \frac{d}{dx}+2\right) f= x+1$$
where $f$ is unknown (the place where solutions should go) and  the linear differential operator $\frac{d}{dx}+2$ is understood to take in quadratic functions (of the form $ax^2+bx+c$) and give out other quadratic functions. 

Let's simplify the way we denote the quadratic functions; 
we will  
$${\rm denote~~}ax^2+bx+c \text{~~as~~} \colvec{a\\b\\c}_B.$$
The subscript $B$ serves to remind us of our particular notational convention; we will compare to another notational convention later. With the convention $B$ we can say
$$  \left( \frac{d}{dx}+2\right) \colvec{a\\b\\c}_B
=\left( \frac{d}{dx}+2\right)(ax^2+bx+c)$$
$$
=(2ax+b)+(2ax^2+2bx+2c) = 2a x^2+(2a+2b)x+(b+2c)
$$
$$
=\colvec{\mc{2a}\\2a+2b\\\mc{b+2c}}_B
= 
\left[ 
\begin{pmatrix}   
2&0&0\\
2&2&0\\
0&1&2
\end{pmatrix}
\colvec{a\\b\\c}\right]_B.
$$
That is, our notational convention for quadratic functions has induced a notation for the differential operator $\frac{d}{dx}+2$ as a matrix. 
We can use this notation to change the way that 
the following two equations say exactly the same thing.
$$
 \left( \frac{d}{dx}+2\right) f=x+1
 \Leftrightarrow
\left[ 
\begin{pmatrix}   
2&0&0\\
2&2&0\\
0&1&2
\end{pmatrix}
\colvec{a\\b\\c}\right]_B = \colvec{0\\1\\1}_B.$$
Our notational convention has served as an organizing principle to yield the system of equations 
$$
\begin{array}{cc}
2a&=0\\
2a+2b&=1\\
b+2c&=1
\end{array}
$$
with solution $\colvec{0\\ \frac12\\[1mm] \frac14}_{\!B}  $, where the subscript $B$ is used to remind us that this stack of numbers  encodes the vector $\frac12x+\frac14$, which is indeed the  solution to our equation since, substituting for $f$ yields the true statement $\left(\frac{d}{dx}+2\right)(\frac12x+\frac14)=x+1$. 
\end{example}

It would be nice  to have a systematic way
to rewrite any linear equation as an equivalent matrix equation. 
It will be a little while before we can learn to  organize information in a way generalizable to all linear equations, 
but keep this example in mind throughout the course. 

The general idea is presented in the picture below; sometimes a linear equation is too hard to solve as is, but by organizing information and reformulating the equation as a matrix equation the process of finding solutions becomes tractable. 
\begin{center}
{\includegraphics[width=.8\textwidth]{\whatIsPath/detourAbs} }\\
\end{center}
A simple example with the knowns ($L$ and $V$ are $\frac{d}{dx}$ and $3$, respectively) is shown below, although the detour is unnecessary in this case since you know how to anti-differentiate.
\begin{center}
{\includegraphics[width=.8\textwidth]{\whatIsPath/detourEg} }\\
\end{center}


To drive home the point that we are not studying matrices but rather linear functions, and that those linear functions can be represented as matrices under certain notational conventions, 
consider how changeable the notational conventions are. 


\begin{example}{of how a different matrix comes into the same linear algebra problem.}\\

\noindent Another possible notational convention  is to
$$\text{denote~~}a+bx+cx^2 \text{~~as~~} \colvec{a\\b\\c}_{\!\!\!B'}.$$
With this alternative notation
$$  \left( \frac{d}{dx}+2\right) \colvec{a\\b\\c}_{B'}
=\left( \frac{d}{dx}+2\right)(a+bx+cx^2)$$
$$
=(b+2cx)+(2a +2bx+2cx^2 ) = (2a+b) + (2b+2c)x+2cx^2
$$
$$
=\ccolvec{2a+b\\2b+2c\\2c}_{B'}
= 
\left[ 
\begin{pmatrix}   
2&1&0\\
0&2&2\\
0&0&2
\end{pmatrix}
\colvec{a\\b\\c}\right]_{B'}.$$
Notice that we have obtained {\it a different matrix for the same linear function}. 
The  equation we started with 
$$
 \left( \frac{d}{dx}+2\right) f=x+1
 \Leftrightarrow
\left[ 
\begin{pmatrix}   
2&1&0\\
0&2&2\\
0&0&2
\end{pmatrix}
\colvec{a\\b\\c}\right]_{B'} = \colvec{1\\1\\0}_{\!B'}$$
$$
\Leftrightarrow
\begin{array}{r}
2a+b=1\\
2b+2c=1\\
2c=0
\end{array}
$$
has the solution 
$\colvec{ \frac14\\[1mm] \frac12\\[1mm]0}$. Notice that we have obtained {\it a different 3-vector for the same vector},  since in the notational convention $B'$ this 3-vector represents $\frac14+\frac12x$. 
\end{example}

One linear function can be represented (denoted) by a huge variety of matrices. %with the same effects for any choice of representation. 
The representation only depends on how vectors are denoted as n-vectors.


%We hope this helps you understand why we will be looking at matrices so much even though they will not be our primary objects of study. But, unfortunately we can not start at the climax of our story; 
%we must cover how to use matrices per se before we cover how to change to matrix notation and then use matrices! Thus, the next chapter is devoted to the study of matrices in and of them selves. 



%\section{Where are we going?}
%
%Let us reiterate, linear algebra is about more than just matrices. It is about linear operators. 
%While matrices are among the simplest linear operators, they are the most powerful; as hinted at above, complicated and confusing operators can be re-written as matrices as long as they are linear. Before one can see that, one must have a deep understanding of matrices and their structure. This course is mostly about learning that structure.
%
%To hint at what we meant by ``structure of matrices"" lets look back on some math you already know.  
%The fundamental theorem of arithmetic says that any integer can be factored into a product of prime numbers (and bio further). 
%The fundamental theorem of algebra says that polynomials over the complex numbers can be factored into first order polynomials (and no further, e.g.~$x^4-1=(x+1)(x-1)(x+1)(x-1)$). 
%The prime numbers are the building blocks of integers and the first order polynomials are the building blocks of polynomials. We will first work toward answering the question, what are the building blocks of matrices? We will discover in chapter 2 that the building blocks are objects called elementary row operations. In Chapter 3 will categorize these objects in detail.

%This example uses augmented matrices which have not been introduced yet!
%\videoscriptlink{what_is_linear_algebra_3_3_matrix.mp4}{A~$3 \times 3$ matrix example}{scripts_what_is_linear_algebra_3_3_matrix}

%This vid uses the old first page of the text and  might be a bit confusing. 
%\videoscriptlink{what_is_linear_algebra_overview.mp4}{Video Overview}{video_what_is_overview}

%I've opted to use the word operator instead of transformation-David
%The matrix is an example of a \emph{Linear Transformation}\index{Linear Transformation!concept of}, because it takes one vector and turns it into another in a linear way:


%\References{
%Hefferon, Chapter One, Section 1\\
%Beezer, Chapter SLE, Sections WILA and SSLE\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/System_of_linear_equations}{Systems of Linear Equations}
%}


\section{Review Problems}

You probably have already noticed that understanding sets, functions and basic logical operations 
is a must to do well in linear algebra. Brush up on these skills by trying these background webwork
problems:

\begin{center}
\begin{tabular}{|c|c|}
\hline
Logic &
\hwref{Background}{1}\\ 
Sets &
\hwref{Background}{2}\\ 
Functions &
\hwref{Background}{3}\\ 
Equivalence Relations &
\hwref{Background}{4}\\ 
Proofs &
\hwref{Background}{5}\\ 
\hline
\end{tabular}
\end{center}

\noindent
Each chapter also has reading and skills WeBWorK problems:
\vspace{2mm}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading problems &
\hwrref{WhatIsLinearAlgebra}{1}, \hwrref{WhatIsLinearAlgebra}{2}\\
\hline
\end{tabular}
\vspace{4mm}

\noindent
Probably you will spend most of your time on the following review questions:

\input{\whatIsPath/problems}

%\newpage

%\begin{comment}
%There are two things to notice about  this list: First,  the range of operations in algebra seems to go beyond addition, subtraction, multiplication, and division while the range of possible objects goes beyond numbers. [In fact, its best to think of the objects (numbers) being generalized while trying to maintain
%the basic properties of addition, subtraction, {\it etc}..., as far as possible.]
%%; in fact, we mean only to hint at the variety of objects and operations found in algebra.   
%Second, these very different questions all have the same form. To emphasize this we restate them:
%\begin{example} (Algebra \hypertarget{AandD}{Questions} Restated)
%% Note to the Editor:
%% putting all in Ax=b is BAD! They emotionally want x to stand for a number, not a function,
%Solve
%$$
%{\cal A}\, {\bf x} = {\bf b}\, ,
%$$
%where
%\begin{enumerate}[(A)]
%\item \label{A}
%${\bf x}$ and ${\bf b}$ are numbers, $${\cal A}\, {\bf x}:=10 \, {\bf x}\, ,$$
%and ${\bf b}=3$.
%%What 
%%number~$x$ satisfies~$Ax=10$ where~$Ax:=5x?$\\[-.6cm]
%\item 
%${\bf x}$ and ${\bf b}$ are 3-vectors,  $${\cal A}\, {\bf x}:=\colvec{1 \\ 1\\ 0} \times {\bf x}\, ,$$\\
%\vspace{-15mm}
%and ${\bf b}=\colvec{1\\1\\0}$.
%%What vector~$v$ satisfies~$Bv = \colvec{0\\1\\1}$% \\
%%where ~$Bv:=\colvec{1 \\ 1\\ 0} \times v$?\\[-.4cm]
%\item 
%${\bf x}$ and ${\bf b}$ are polynomials in the variable $x$, $${\cal A}\, p(x):=x\int_{-1}^1 yp(y)dy\, ,$$ 
%and ${\bf b}=x$.
%%What polynomial~$g$ satisfies~$Cg=h$ where~$Cg$ is the function defined by \\[.3cm]
%%$Cg(x)=\int_{-1}^1 f(x,y)g(y) dy = h(x)$ where~$f(x,y):=xy,~h(x):=x$?\\[-.5cm]
%\item ${\bf x}$ and ${\bf b}$ are polynomials, 
%$$
%{\cal A}\,  p(x):=\frac{dp(x)}{dx}-2p(x)\, ,
%$$
%and ${\bf b}=0$
%%What polynomial~$f$ satisfies~$Df=0$ 
%%where~$Df$ is the function \\[.3cm]
%%$Df(x):=x\frac{d}{dx}f(x)-2f(x)$?\\[-5mm]
%\item  \label{E}
%${\bf x}$ and ${\bf b}$ are functions, 
%$$
%{\cal A}  \big(f(x) \big):= f''+f^3\, ,
%$$
%and ${\bf b}=\sqrt{x}$.
%%What function~$f$ satisfies~$Ef=g$ where~$Ef:=f''(x)+\big(f(x)\big)^3$ and~$g(x):=\sqrt{x}$?
%\end{enumerate}
%\end{example}
%\noindent
%\reading{1}{1}
%You probably sense already that one of our main subjects will be the  study of equations 
%that can be written in the form
%\begin{center}
%\shabox{$Ax=b$}
%\end{center}
%%\begin{eqnarray*}
%%Ax=b
%%\end{eqnarray*}
%but where $x$ and $b$ can be much more general creatures than just numbers. The technical answer to the question ``how general?'',
%is that $x$ and $b$ will be {\it vectors}. We will say much more about what this means later, but for now all you need
%to know is that vectors are objects that you can {\it add} and {\it scalar multiply}, so that the algebraic operations
%$$
%x+x'\, ,\mbox{ and } c\, x\, ,
%$$
%where $c$ is any number, make sense.
%
%You might also be wondering why we wrote ${\cal A}\big(f(x)\big)$ rather than ${\cal A} f(x)$ in part~(\ref{E}) of our example,
%or perhaps why we did not write ${\cal A}({\bf x})$ in part~(\ref{A}). In fact all the the above problems involve functions (denoted ${\cal A}$)
%of vectors. However, in the first four examples the function~${\cal A}$ (that takes in a some sort of vector and returns a vector as its output)
%is a {\it linear operator} which means that it obeys a
%very special  property called {\it linearity}. This is the ``Linear'' in Linear Algebras and we will explain it next. 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%%\newpage
%\section{Operators: Functions of Functions}
%A few comments on jargon and notation are in order before we move on. ~$D$ takes in functions like~$x^4$ and gives out functions like~$4x^4-2x^4=2x^4$. You could say that~$D$ is a function that takes in functions and gives out functions.  
%Alternatively, that awful combination of words can be avoided by using the word 
%``{\it operator}"" instead of ``function"";
%~$D$ is an operator that takes in functions and gives out functions. 
% Similarly,~$C$ is an operator that takes in, for example,~$x^3$ and gives out ~$\frac12 x$. 
%We hope you can understand why we don't always use the 
%parentheses notation for arguments of functions when we think of the function as an operator;~$Dg(x)$ looks much better~$D(g)(x)$.
%
%%Video of operators? 
%
%If we are going to think of~$B$ as an operator too we need to understand the objects it takes in and gives out in a new way.
%It takes vectors from 3-space to vectors in 3-space. 
%Such vectors  
%\hypertarget{vecs as fun}{are really functions;} 
%a vector from 3-space~$v$ is a function whose domain is just (the set containing only)~$1,2,$ and~$3$. 
%
%
%\begin{example}
%\noindent
%The vector~$v=\colvec{v_1\\v_2\\v_3}=\colvec{7\\14\\21}$ is the function that: \\[.4cm]
%\begin{center}
%\begin{tabular}{l}
%Takes in~$1$ and gives out~$v_1=7$\\[1mm]
%Takes in~$2$ and gives out~$v_2=14$\\[1mm]
%Takes in~$3$ and gives out~$v_3=21$.\\[1mm]
%\end{tabular}
%\end{center}
%\end{example}
%
%\noindent
%This is an explicit definition of a function as opposed to of the more familiar algebraic definition. In analogy to defining a function~$f$ with domain all real numbers by the algebraic statement~$f(x)=7x$, we can define the function~$v$ by~$v_i=7i$. 
%The operator~$B$ takes in such a  function and gives out another. For example~$$Bv=\colvec{1\\1\\0}\times \colvec{7\\14\\21}=\colvec{21\\-21\\7}.$$
%
%
%You can also think of vectors from two space as functions with domain (the set containing only)~$1$ and~$2$. Similarly vectors from n-space are functions of (the set containing only)~$1,2,...,n$. This includes the case~$n=1$; we can even think of~$A$ as an operator. 
%
%\reading{1}{2}
%
%
%
%Digest what you can of these ideas now, but don't worry if you feel that some of this new stuff is not sinking in immediately. We will revisit all of these ideas. The point here is not to learn about any of the particular operators~$A,B,C,D,E$ but rather to show you that the world of algebra is much bigger than the study of questions involving just exponentiation, multiplication, division, addition and subtraction of numbers. Let your imagination go wild with our examples as inspiration. Imagine all the possible integral operators like~$C$ and differential operators like~$D$. We hope you will conclude that your adventures in algebra are just beginning, and that there are many very difficult problems out there. 
%The tools you will learn in linear algebra will help you answer all four of these example questions in an efficient way with the same method. That is the long story we wish to tell. 
%\end{comment}
%
%%<picture please: flow chart with algebra problem-> linear or not linear, if linear then-> matrix equation
%
%%\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%\section{What is Linear?} 
%
%\begin{example} (Vectors) \\[-.5cm]
%\begin{enumerate}[(A)]
%\item Numbers. If
%\item What vector~$v$ from 3 space satisfies the cross product equation~$ \colvec{1 \\ 1\\ 0} \times v = \colvec{0\\1\\1}$?\\[-.4cm]
%\item What polynomial~$g$ satisfies~$\int_{-1}^1 f(x,y)g(y) dy = h(x)$, \\[.2cm]
% where~$f(x,y):=xy$ and~$h(x):=x$?\\[-.5cm]
%\item What power series~$f$ satisfies~$x\frac{d}{dx} f(x) -2f(x)=0$?
%%\item What power series~$f$ satisfies~$x\frac{d}{dx} f(x) -2f(x)=0$?
%\item What what function~$f$ satisfies~$f''(x)+f^3(x) =\sqrt{x}~$?
%\end{enumerate}
%\end{example}
%
%
%
%
%Students often ask questions like ``how do I know when to use a particular method""? A question like ``how do I know when to use integration by parts?"" is hard to  answer, but in contrast, ``when can I use  linear algebra?"" is easy to answer: %for the kinds of questions given above; 
%{\it when the operator involved is linear.} \\
%
%\noindent
%An operator~$E$ is \hypertarget{asked in chapter 1}{said to be linear} if it has two properties:\\[-.5cm]
%\begin{enumerate}
%\item {\it Homogeneity:} ``Constants can be pulled out""
%$$E(cv)=cEv\, .$$ \\[-.5cm]
%\item {\it Additivity:} ``The operator can be distributed""
%$$E(v+u)=Ev+Eu\, .$$ 
%\end{enumerate}
%
%
%
%
%\noindent
%A great example is the derivative operator: for any two functions~$f$,~$g$ and any number~$c$ the derivative operator satisfies\\[-.5cm]
%\begin{example} (The derivative operator is linear)\\
%\begin{enumerate}
%\item ~$\frac{d}{dx} (cf)=c\frac{d}{dx} f$, \\[-.5cm]
%\item~$\frac{d}{dx}(f+g)=\frac{d}{dx}f+\frac{d}{dx}g$.\\[-.5cm]
%\end{enumerate}
%\end{example}
%\noindent
%We now explicitly verify that~$B$ and~$C$ are linear. You will verify that~$A$ and~$D$ are linear in the \hyperlink{ch1probset}{problem set} that ends this chapter.  \\
%
%\begin{example} Verifying that~$B$ is linear:
%
%We need to check that $B$ is linear acting on any vector $v=\colvec{x\\ y\\ z}$ so we compute~$Bv$
%using the cross product
%$$
%Bv=\colvec{1 \\ 1\\ 0} \times \colvec{x \\ y\\ z}=\colvec{z\\ -z\\y-x}\, .
%$$
%Now, until we know what a scalar multiple $cv$ of the vector $v$ means, the problem makes no sense. 
%Here we make (the logical) choice
%$$
%c\colvec{x \\ y\\ z} := \colvec{cx\\ cy\\ cz}\, .
%$$
%Now, using this definition and our result for how $B$ acts on an arbitrary vector we find:
%$$
%B(cv)=B\colvec{cx\\ cy\\ cz}=\colvec{cz\\-cz\\cy-cx}\, .
%$$
%We hope that this coincides with $cBv$ which also compute using our result for $Bv$ and the definition
%for multiplying a vector by a number:
%$$
%cBv=c\colvec{z\\ -z\\y-x}=\colvec{cz\\ c(-z)\\c(y-x)}=\colvec{cz\\-cz\\cy-cx}\, .
%$$
%Thankfully, we got the same result for both $B(cv)$ and $cBv$, so the homogeneity property holds.
%
%We still have to check additivity. Again, this makes no sense until we define what the sum $v+u$ of two arbitrary vectors $v=\colvec{x\\y\\z}$ 
%and $u=\colvec{x'\\y'\\z'}$. For that we again make a logical choice
%$$
%\colvec{x\\y\\z}+\colvec{x'\\y'\\z'}:=\colvec{x+x'\\y+y'\\z+z'}\, .
%$$
%Again, we have to make two computations: $B(u+v)$ (first add then act with $B$) and $Bu+Bv$ (first act with $B$, then add):
%$$
%B(u+v)=B\colvec{x+x'\\y+y'\\z+z'}=\colvec{z+z'\\-(z+z')\\(y+y')-(x+x')}=\colvec{z+z'\\-z-z'\\y+y'-x-x'}\, ,
%$$
%$$
%Bu+Bv=B\colvec{x\\y\\z}+B\colvec{x'\\y'\\z'}=\colvec{z\\-z\\y-x}+\colvec{z'\\-z'\\y'-x'}=\colvec{z+z'\\-z-z'\\y+y'-x-x'}\, .
%$$
%Both computations give the same answer so now we know that $B$ also obeys additivity and can conclude that $B$ is linear.
%%AI)~$A(cx)=5cx= c5x=cAx$, \\
%%AII) A(x+y)=5(x+y)=5x+5y=Ax+Ay\\
%%\item~$B(cv)=\colvec{1 \\ 1\\ 0} \times cv
%%\!\!\!\!\!\!\!\!\!\!\!\!\!\!\stackrel{\rm \begin{array}{c}\rm\scriptstyle Using\  the\ rule\\[-2mm] \rm\scriptstyle for\ cross\ products\ of\\[-2mm]\rm
%%\scriptstyle scalar\ multiples\\\downarrow\\[1mm]\end{array}}= 
%%\!\!\!\!\!\!\!\!\!\!\!\!\!\!c\colvec{1 \\ 1\\ 0} \times v=c\, Bv$\, .\\
%%\item~$B(u+v)=\colvec{1 \\ 1\\ 0} \times (u+v)
%%\!\!\!\!\!\!\!\!\!\!\!\!\!\!\stackrel{\rm \begin{array}{c}\rm\scriptstyle Using\  the\ rule\\[-2mm] \rm\scriptstyle for\ cross\ products\\[-2mm]\rm
%%\scriptstyle of\ sums\\\downarrow\\[1mm]\end{array}}
%%=\!\!\!\!\!\!\!\!\!\!\!\!\!\!\colvec{1 \\ 1\\ 0} \times u+\colvec{1 \\ 1\\ 0} \times v =Bu+Bv\, .$\\[.5cm]
%%\end{enumerate} 
%\end{example} 
%The above example looks a little silly, this because it is really just checking something you probably already know about cross products of vectors. Namely, if $u$, $v$ and $w$ are vectors and $c$ is a number then
%$$
%w\times(cv)=c(w\times v)\quad\mbox{and}\quad  w\times(v+u)=w\times v+w\times u\, .
%$$
%You might also have noticed how important it was to know what $cu$ and $u+v$ actually meant to even solve the problem.
%In fact, one of the reasons linear algebra is so powerful, is that it can be applied to any choice for what $cu$ and $u+v$ mean
%that happens to obey a few basic rules. You will learn about these in Chapter~\ref{\vectorSpacesPath}.
%
%Here is another example involving linear operators made from integrals where we assume all the usual rules for integrals. Notice how much easier checking linearity is.
%\begin{example} Verifying that~$C$ is linear:\
%\begin{enumerate}
%\item~$C(cg)(x)= \int_{-1}^1f(x,y) cg(y)dy =c\int_{-1}^1f(x,y) g(y)dy =c\,Cg(x)$\\[.2cm]
%\item~$C(g_1+g_2)(x) =  \int_{-1}^1f(x,y) [g_1(y)+g_2(y)]dy\\[.2cm]
%\phantom{a}~~~~~
%=
%\int_{-1}^1f(x,y) g_1(y)dy
%+\int_{-1}^1f(x,y)g_2(y)dy
%=(Cg_1 +Cg_2)(x)$\\%[.4cm]
%\end{enumerate}
%\end{example}
%%DI)~$D(cf)(x)=(x\frac{d}{dx}-2)cf(x)=c(x\frac{d}{dx}-2)f(x)=cDf(x)$\\[.1cm]
%%DII)~$D(f_1+f_2)(x)=xf_1'(x)-2f_1(x) +xf_2'(x)-2f_2(x) =(Df_1+Df_2)(x)$.\\
%%You now know how to test if an operator is linear, and have some examples.  
%\noindent 
%One moral of our story is that if~$A$ is a {\it linear} operator while~$b$ and~$x$ are vectors, then~$Ax=b$ is equivalent to a matrix{\it -type} equation. 
%That is why we will begin by studying matrices.\\
%% We have only hinted at the wide range of possible examples for linear operators.
%This moral does {\it not} apply to non-linear operators:
%\begin{example} Show that~$E$ is {\it not} linear.\\[-5mm]
%\begin{enumerate}  
%\item~$E(5f)=5f''+5^3f^3 \neq 5f''+5f^3=5Ef~$\\[-5mm]
%\item Not needed.%\\[-5mm]
%\end{enumerate}  
%The operator~$E$ is not linear because it is not homogeneous.\
%\end{example}
%
%Actually,~$E$ is not additive either, but we did not need to show this. Once either one of the linearity properties is broken, the operator is not linear. (Unfortunately, you are a criminal the moment that you break just one law, no matter if you are a law abiding citizen in every other way!)
%
%\videoscriptlink{Homnotadd.mp4}{A Homogeneous Nonlinear Operator}{scripts_H}
%
%\begin{example} Show that operator~$Y$ that acts on numbers by~$Yx=5x+3$ is not linear. \\ 
%Lets check additivity first:
%$$
%Y(x+y)= 5(x+y)+3=(5x+3)+(5x)\neq (5x+3)+(5x+3)=Yx+Yy\, .
%$$
%This proves linearity, unfortunately homogeneity fails:
%$$Y(cx)=5cx+3=c (5x+3) +3 -3c= c\, Yx +3-3c\neq cYx \mbox{ if } c\neq1\, .$$
%Notice that the special case $c=1$ holds (and just says $Y(1.x)=1.Yx$), but the homogeneity
%requirement must work for {\it any} $c$. 
%
%The operator~$Y$ is neither homogeneous nor additive, and therefore not linear.
%\end{example}
%
%This example might surprise you because the graph of the equation~$y=5x+3$ is a line, and what else could linear mean other than ``line-like""? In fact, there is a linear operator associated with this equation: 
%the operator~$L$ that acts on two-vectors as 
%$$L\colvec{x\\y}=y-5x.$$ 
%The line is then the collection of solutions to 
%$$L\colvec{x\\y}=10 \Leftrightarrow y-5x=3 \,.$$ 
%Similarly, the  operator~$M$ on 3-space with 
%$$M\colvec{x\\y\\z}=2x-3y+5z$$ 
%is linear. The set of solutions to 
%$$M\colvec{x\\y\\z}=10 \Leftrightarrow 2x-3y+5z=10$$ 
%form a plane. Lines and planes are {\it level sets} of linear functions. 
%
%\videoscriptlink{Linear_Operator_on_Lines.mp4}{Linear Operators Preserve Lines}{scripts_H}
%
%
%
%%It is one thing to be able to tell which operators are linear and which are not, and it is another to have a feel for what linear operators do. Lets look at three simple example problems to get a feel for this. On the way we will introduce the main tool of linear algebra, the idea of a matrix. \\
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\noindent
%
",lesson
2,Gaussian Elimination,"\chapter{Systems of Linear Equations }
\label{systems}

\section{Gaussian Elimination}
\label{gaussElim}

\hyperlink{system2matrix}{Systems of linear equations can be written as matrix equations.}
Now you will learn  an efficient algorithm  for (maximally) simplifying a system of linear equations (or a matrix equation) -- Gaussian elimination.

%You might get the feeling that you are learning to do Gaussian elimination only so that you can tell your computer how too do it in the future. There is more than that going on here. Let us foreshadow chapter 3; as we attempt to streamline the process of elimination we will discover the building blocks of matrices. 



%In \Lecture~\ref{warmup}  we looked performed elimination on a system of equations. 
%It was nice to do this elimination by adding subtracting an multiplying wholes equations at a time. 
%Lets do another example with the system
%\begin{eqnarray}
%	x\ +\ y & = & 27\nn \\
%	2x-\ y & = &\  0\nn \,.
%\end{eqnarray}
%Adding the first equation to the second then dividing by 3 gives
%\begin{eqnarray}
%	x\ +\ 0 & = & 9\nn \\
%	2x-\ y & = &\  0\nn \,.
%\end{eqnarray}
%Subtracting rice the first from the second then dividing by $-1$ gives
%\begin{eqnarray}
%	x \phantom{\ +\ y}& = &\  9\nn \\
%	\phantom{x\ +\ }y & = & 18\, .\nn
%\end{eqnarray}
%
%\noindent
%The maximum number of terms have been eliminated from each equation, and the result is an obvious statement of the solution. 
%\\
%
%We also learned to write such a linear system using a matrix and two vectors. In this case the original linear system can be written
%
%\begin{equation*}
%    \begin{pmatrix}
%      1             &1  \\
%      2             &-1
%    \end{pmatrix}
%  \colvec{x \\ y}
%  =
%  \colvec{27 \\ 0}\, .
%\end{equation*}
%
%
%\noindent
%Likewise, the system of equations that we obtained after elimination can be written
%
%\begin{equation*}
%    \begin{pmatrix}
%      1             &0  \\
%      0             &1
%    \end{pmatrix}
%  \colvec{x \\ y}
%  =
%  \colvec{9 \\ 18} \, .
%\end{equation*}
%\\
%
%%\noindent
%By the way, the matrix $$I=    \begin{pmatrix}
%      1             &0  \\
%      0             &1
%    \end{pmatrix}$$ is called the \emph{Identity Matrix}\index{Identity matrix!$2\times2$}.  She will be very important to us. You should check that if $v$ is any vector, then $$Iv=v\, .$$
%    
%Here is a nice way to summarize your goal when performing the process we are calling elimination elimination; manipulate the system of equations until the resulting system can be written as a matrix equation with the identity matrix.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Augmented Matrix Notation}

%A useful shorthand for a  system of linear equations is an \hypertarget{augmented_matrix}{\emph{Augmented Matrix}}\index{Augmented matrix~$2\times2$}. 
Efficiency demands a new notation, called an \hypertarget{augmented_matrix}{\emph{augmented matrix}},
which we  introduce via examples: 

The linear system
$$\left\{ 
\begin{array}{rcr}
	x\ +\ y & = & 27 \\
	2x-\ y & = &\  0\, ,
\end{array}\right.
$$
is denoted by the augmented matrix

\[
\begin{amatrix}{2}
1 &1 &27 \\ 2 &-1 & 0
\end{amatrix}\, .
\]

\noindent
This notation is  simpler than the matrix one, 
\begin{equation*}
    \begin{pmatrix}
      1             &1  \\
      2             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{27 \\ 0}\, ,
\end{equation*}
although all three of the above denote the same thing. 

\Videoscriptlink{gaussian_elimination_more_background.mp4}{Augmented Matrix Notation}{script_gaussian_elimination_more}

\noindent
Another interesting rewriting is
$$
x\colvec{1\\2}+y\colvec{1\\-1}=\colvec{27\\0}\, .
$$
This tells us that we are trying to find the combination of the vectors $\colvec{1\\2}$~and $\colvec{1\\-1}$ adds up to $\colvec{27\\0}$; 
 the answer is  ``clearly'' $9\colvec{1\\2}+18\colvec{1\\-1}$.\\[1mm]


Here is a larger example.
The system
\begin{eqnarray*}
1x + 3y + 2z + 0w   &=&9 \\ 
6x + 2y + 0z   -2w  &=&0  \\
-1x+ 0 y + 1 z + 1w  &=&3 \, ,
\end{eqnarray*}
is denoted by the augmented matrix
\[
\begin{amatrix}{4}
1 & 3 & 2 & 0  & 9 \\ 
6 & 2 & 0  &\!\! -2 & 0  \\
-1& 0  & 1  & 1 & 3
\end{amatrix}\, ,
\]
%When writing a system of equations one can write out the terms with zero for coefficients or not
%\begin{eqnarray*}
%1x + 3y + 2z  \phantom{+ 0w}   =9 \\ 
%6x + 2y \phantom{ + 0z}   -2w  =0  \\
%-1x  \phantom{+0 y} + 1 z + 1w  =3 
%\end{eqnarray*}
which is equivalent to the matrix equation
\begin{eqnarray*}
\left(\begin{array}{rccr}
1 & 3 & 2 & 0   \\ 
6 & 2 & 0  & \!\!-2   \\
\!\!-1& 0  & 1  & 1 
\end{array}\right)
\colvec{ x\\ y\\z\\w}
=\colvec{ 9\\0\\3}\, .
\end{eqnarray*}
Again, we are trying to find which combination of the columns of the matrix adds up to the vector on the right hand side.



For the the general case of $r$ linear equations in $k$ unknowns,
the number of equations is the number of rows $r$ in the augmented matrix, and the number of columns $k$ in the matrix left of the vertical line is the number of unknowns, giving an augmented matrix of the form
\[
\begin{amatrix}{4}
a^1_1 & a^1_2 & \cdots & a^1_k & b^1 \\[1mm]
a^2_1 & a^2_2 & \cdots & a^2_k & b^2 \\[1mm] 
\vdots & \vdots & & \vdots & \vdots  \\[1mm]
a^r_1 & a^r_2 & \cdots & a^r_k & b^r 
\end{amatrix}.
\]
Entries left of the divide carry two indices; subscripts denote column number and  superscripts row number. We emphasize, the superscripts here do {\it not} denote exponents.  
%Aside: most people don't like  indexing by superscripts at first. However, kind of index placement will later facilitate Einstein's summation convention, which Einstein himself described as his greatest contribution to the sciences. 
Make sure you can write out the system of equations and the associated matrix equation for any augmented matrix. 

%\reading{2}{1}
\Reading{SystemsOfLinearEquations}{1}

We now have three ways of writing the same question. 
Let's put them side by side as we solve the system by strategically adding and subtracting equations. 
We will not tell you the motivation for this particular series of steps yet, but let you develop some intuition first. 

\begin{example}  (How matrix equations and augmented matrices change in elimination)
\begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	x&+&y & = & 27 \\
	2x&-& y & = &\  0 
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      1             &1  \\
      2             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{27 \\ 0}
  \Leftrightarrow
 \begin{amatrix}{2}
1 &1 &27 \\ 2 &-1 & 0
\end{amatrix}\, .
  \end{eqnarray*}
With the first equation replaced by the sum of the two equations this becomes
\begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	3x& +& 0 & = & 27 \\
	2x&-& y & = &\  0 
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      3             &0  \\
      2             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{27 \\ 0}
  \Leftrightarrow
 \begin{amatrix}{2}
3 &0 &27 \\ 2 &-1 & 0
\end{amatrix}\, .
  \end{eqnarray*}
Let the new first equation be the old first equation divided by 3:
 \begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	x& +& 0 & = & 9 \\
	2x&-& y & = &\  0 
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      1             &0  \\
      2             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{9 \\ 0}
  \Leftrightarrow
 \begin{amatrix}{2}
1 &0 &9 \\ 2 &-1 & 0
\end{amatrix}\, .
  \end{eqnarray*}
  Replace the second equation by the second equation minus two times the first equation: 
\begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	x& +& 0 & = & 9 \\
	0&-& y & = &\  -18
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      1             &0  \\
      0             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{9 \\ -18}
  \Leftrightarrow
 \begin{amatrix}{2}
1 &0 &9 \\ 0 &-1 & -18
\end{amatrix}\, .
  \end{eqnarray*}
Let the new  second equation be the old second equation divided by -1:
\begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	x &+ &0 & = & \ 9 \\
	0& + &y & = &  18
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      1             &0  \\
      0             &1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{9 \\ 18}
  \Leftrightarrow
 \begin{amatrix}{2}
1 &0 &9 \\ 0 &1 & 18
\end{amatrix}\, .
  \end{eqnarray*}
\end{example}
Did you see what the strategy was? To {\it eliminate} $y$ from the first equation and then {\it eliminate} $x$ from the second. The result was  the solution to the system. 

Here is the big idea: 
Everywhere in the instructions above we can replace the word ``equation"" with the word ``row"" and interpret them as telling us what to do with the augmented matrix instead of the system of equations.
Performed systemically, the result is the {\bf Gaussian elimination}\index{Gaussian elimination} algorithm. 
%
%\begin{example} of Gaussian elimination
%\begin{eqnarray*}
% \begin{amatrix}{2}
%1 &1 &27 \\ 2 &-1 & 0
%\end{amatrix}
%  \end{eqnarray*}
%Let the new first row be the sum of the first and second rows
%\begin{eqnarray*}
% \begin{amatrix}{2}
%3 &0 &27 \\ 2 &-1 & 0
%\end{amatrix}
%  \end{eqnarray*}
%Let the new first row be the old first row divided by 3
% \begin{eqnarray*}
% \begin{amatrix}{2}
%1 &0 &9 \\ 2 &-1 & 0
%\end{amatrix}
%  \end{eqnarray*}
%Let the new second row be the old second row minus two times the first row 
%\begin{eqnarray*}
% \begin{amatrix}{2}
%1 &0 &9 \\ 0 &-1 & -18
%\end{amatrix}
%  \end{eqnarray*}
%Let the new  second row be the old second row divided by -1
%\begin{eqnarray*}
% \begin{amatrix}{2}
%1 &0 &9 \\ 0 &1 & 18
%\end{amatrix}
%  \end{eqnarray*}
%The solution can be read off very quickly, and the notation was very minimal. 
%\end{example}
% At each step, the augmented matrices encode systems of equations which have the same solutions. Lets make this idea more formal, and introduce some notation to convey the idea.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Equivalence and the Act of Solving}


%Two augmented matrices corresponding to linear systems 
%%{\it that actually have solutions} %cmon, this idea has not even been introduced
%are said to be \hypertarget{roweq}(row) 
%\emph{equivalent}\index{Row equivalence} if they have the \emph{same} solutions.
%To denote this 

We now introduce the symbol $\sim$ which is called ``tilde"" but should be read as  ``is (row) equivalent to''
because at each step the augmented matrix changes by an operation on its rows but its solutions do not. For example, we found above that
\[
\begin{amatrix}{2}
1 &1 &27 \\ 2 &-1 &0
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &0 &9 \\ 2 &-1 & 0
\end{amatrix}
\sim
\begin{amatrix}{2}
1 & 0&9 \\   0& 1 & 18
\end{amatrix}\, .
\]
The last of these augmented matrices is our favorite!
\Videoscriptlink{gaussian_elimination_background.mp4}{Equivalence Example}{script_gaussian_elimination_background}

%\videoscriptlink{gaussian_elimination_3_3_example.mp4}{A $3 \times 3$ example}{scripts_gaussian_elimination_3_3_example}

Setting up a string of equivalences like this is a means of solving a system of linear equations. This is the main idea of section~\ref{RREF}.
This next example hints at the main trick:

\begin{example} (Using Gaussian elimination to solve a system of linear equations)
\begin{eqnarray*}
   \left.
\begin{array}{lcr}
	x +\ y & = & 5 \\
	x + 2y & = &\  8
     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{2}
1 &1 &5 \\ 1 &2 & 8
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &1 &5 \\ 0 &1 & 3
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &0 &2 \\ 0 &1 & 3
\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{lcr}
	x + 0 & = & 2 \\
	 0 + y & = &\  3
     \end{array}
   \right.
\end{eqnarray*}  
Note that in going from the first to second augmented matrix, we used the top left $1$ to make the bottom left entry zero. For this reason we call the top left entry a pivot. 
Similarly, to get from the second to third augmented matrix,  the bottom right entry (before the divide) was used to make the top right one vanish; so the bottom right entry is also called a pivot. 
\end{example}

This name {\it pivot}  is  used to indicate the matrix
entry used to ``zero out''  the other entries in its column; the pivot is the number used to eliminate another number in its column.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Reduced Row Echelon Form}\label{RREF}
For a system of two linear  equations, the goal of Gaussian elimination is to convert the part of the augmented matrix left of the dividing line into the matrix
 $$I=    \begin{pmatrix}
      1             &0  \\
      0             &1
    \end{pmatrix}\, ,$$ 
called the \emph{Identity Matrix}\index{Identity matrix!$2\times2$}, since this would give the simple statement of a solution $x=a,y=b$. The same goes for  larger systems of equations
for which the identity matrix~$I$ has 1's along its \hyperlink{diagonal}{diagonal} and all off-diagonal entries vanish:
\begin{center}
\shabox{$I=\left(\begin{array}{ccccc}1&0&\cdots&0\\0&1&&0\\ \vdots&&\ddots&\vdots\\0&0&\cdots&1\end{array}\right)$}
\end{center}

%\reading{2}{2} %a 3x3 example with one solution>
\Reading{SystemsOfLinearEquations}{2}


\vspace{1mm}
\noindent
For many systems, it is not possible to reach the identity in the augmented matrix via Gaussian elimination. In any case, a certain version of the matrix that has the maximum number of components eliminated is said to be the Row Reduced Echelon Form (RREF). 

\begin{example}\label{redundant} (Redundant equations)
 \begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	 x& + & y & = & 2 \\[1mm]
	2x& + &2y & = &  4
     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{2}
1 &1 &2 \\[1mm] 2 &2 & 4
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &1 &2 \\[1mm] 0 &0 & 0
\end{amatrix}
%\sim
%\begin{amatrix}{2}
%1 &0 &2 \\ 0 &1 & 3
%\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{ccccr}
	x &+& y & = & 2 \\[1mm]
	 0& +& 0 & = &  0
     \end{array}
   \right.
\end{eqnarray*}  
This example demonstrates if one equation is a multiple of the other the identity matrix can not be a reached. This is because the first step in elimination will make the second row a row of zeros. Notice that solutions still exists $(1,1)$ is a solution. The last augmented matrix here is in RREF; no more than two components  can be eliminated.
\end{example}

\begin{example} (Inconsistent equations)
 \begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	x &+ & y & = & 2 \\[1mm]
	2x &+ &2y & = &  5
     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{2}
1 &1 &2 \\[1mm] 2 &2 & 5
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &1 &2 \\[1mm] 0 &0 & 1
\end{amatrix}
%\sim
%\begin{amatrix}{2}
%1 &0 &2 \\ 0 &1 & 3
%\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{ccccr}
	x &+& y & = & 2 \\[1mm]
	 0 &+& 0 & = &  1
     \end{array}
   \right.
\end{eqnarray*}  
This system of equation has a solution if there exists two numbers $x$, and $y$ such that $0+0=1$. That is a tricky way of saying there are no solutions. The last form of the augmented matrix here is the RREF.
\end{example}


\begin{example} (Silly order of equations)\\
A robot might make this mistake:
 \begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	0x &+&  y & = & -2 \\[1mm]
	x& +& y & = &  7
     \end{array}
   \right\} 
   \Leftrightarrow 
\begin{amatrix}{2}
0 &1 & -2\\[1mm] 1 &1 & 7
\end{amatrix}
\sim \, \cdots\, ,
\end{eqnarray*}  
and then give up because the the upper left slot can not function as a pivot since the~0 that lives there can not be used to eliminate the zero below it. Of course, the right thing to do is to change the order of the equations before starting
 \begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	 x &+ &y & = &  7
	\\[1mm]
	0x &+ & \ y & = & -2 
	     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{2}
1 &1 & 7\\[1mm] 0 &1 & -2
\end{amatrix}
\sim 
\begin{amatrix}{2}
1 &0 & 9\\[1mm] 0 &1 & -2
\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{ccccr}
	x &+& 0 & = & 9\phantom{\, .} \\[1mm]
	 0 &+& y & = & -2\, .
     \end{array} 
   \right.
\end{eqnarray*}  
The third augmented matrix above  is the RREF of the first and second. That is to say, you can swap rows on your way to RREF.
\end{example}



For larger systems of equations redundancy and inconsistency are the obstructions to obtaining the identity matrix, and hence to a simple statement of a solution in the form $x=a,y=b,\ldots$ . 
What can we do to maximally simplify a system of equations in general?
We need to perform operations that simplify our system {\it without changing its solutions}.
Because, exchanging the order of equations, multiplying one equation by a {\it non-zero} constant or adding equations does not 
change the system's solutions, we are lead to three operations:
\begin{itemize}
\item (Row Swap) Exchange any two rows.
\item (Scalar Multiplication) Multiply any row by a non-zero constant.
\item (Row Addition) Add 
%a multiple of 
%%% Any row op can be built out of these three... and with the comment out wording there is some redundancy
one row to another row.
\end{itemize}
These are called \emph{Elementary Row Operations}\index{EROs}, or EROs for short, and are studied in detail in section~\ref{EROS}. 
Suppose now we have a general augmented matrix for which the first entry in the first row does not vanish. 
Then, using just the three EROs, we could\footnote{This is a ``brute force"" algorithm;  there will often be more efficient ways to get to RREF.} then perform the following.\\

\newpage
\begin{center}
{\bf {\Large Algorithm For Obtaining RREF:}}
\end{center}
\begin{itemize}
\item Make the leftmost nonzero entry in the top row 1 by multiplication.  
\item Then use that 1 as a pivot to eliminate everything below it. 
\item Then go to the next row and make the leftmost nonzero entry 1. 
\item Use that 1 as a pivot to eliminate everything below {\it and above it}! 
\item Go to the next row and make the leftmost nonzero entry 1... {\it etc}
\end{itemize}
In the case that the first entry of the first row is zero, we may first interchange the first row with another row whose first entry is non-vanishing  and then perform the above algorithm. If the entire first column vanishes, we may still apply the algorithm on the remaining columns.

Here is a video (with special effects!) of a hand performing the algorithm by hand. When it is done, you should try doing what it does.
\videoscriptlink{gaussian_elimination_Beginner_Elimination.mp4}{Beginner Elimination}{script_gaussian_elimination_more}

\noindent
This algorithm and its variations is known as Gaussian elimination. The endpoint of the algorithm is  an augmented matrix of the form \label{Reduced row echelon form} 
$$\left(
\begin{array}{cccccccc|c}
1       	& * & 0		& * & 0		& \cdots& 0	&*	& b^1 \\[.5mm] 
0	        & 0 & 1		& * & 0		& \cdots& 0	&*	& b^2 \\[.5mm]
0		& 0& 0		& 0 & 1		& \cdots& 0	&*	& b^3 \\[.5mm]  
\vdots  	& \vdots& \vdots	&   & \vdots &	& 	& 
\vdots			& \vdots \\[2mm]  
0		& 0&	0		&  0& 0			&  \cdots   & 1		&*	& b^k \\[.5mm]  
0		& 0 & 0		& 0 & 0		& \cdots& 0 	&0	& b^{k+1} \\[.5mm] 
\vdots  	& \vdots & \vdots	&  \vdots & \vdots	& 	& \vdots	&\vdots	& \vdots \\[.5mm]  
0		&  0 & 0		& 0 & 0		& \cdots& 0		 & 0& b^r \\ 
\end{array}\!\right).$$
This is called 
\emph{Reduced Row Echelon Form}\index{Reduced row echelon form} (RREF).
The asterisks denote the possibility of arbitrary numbers ({\it e.g.}, the second 1 in the top line of example~\ref{redundant}).

Learning to perform this algorithm by hand is the first step to learning linear algebra; it will be the primary means of computation for this course. You need to learn it well. So start practicing as soon as you can, and practice often. 


\newpage
\begin{center}
\Large{{\bf The following properties define RREF:}}
\end{center}
\begin{enumerate}
\item  In every row  the left most non-zero entry is  $1$ (and is called a pivot).
\item The pivot of any given row is always to the right of the pivot of the row above it.
\item The pivot is the only non-zero entry in its column.
\end{enumerate}
%Here are some examples:
\begin{example}\label{augrref} (Augmented matrix in RREF)
$$
\begin{amatrix}{3} 
1 & 0 & 7 & 0 \\ 
0 & 1 & 3 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 \\
\end{amatrix}
$$
\end{example}

\begin{example} (Augmented matrix NOT in RREF)
$$
\begin{amatrix}{3} 
1 & 0 & 3 & 0 \\ 
0 & 0 & 2 & 0 \\
0 & 1 & 0 & 1 \\
0 & 0 & 0 & 1 \\
\end{amatrix}
$$
Actually, this NON-example breaks all three of the rules!
\end{example}














The reason we need the asterisks in the general form of RREF is that
not every column need have a pivot, as demonstrated in examples~\ref{redundant} and~\ref{augrref}. 
Here is an example where multiple columns have no pivot:

\begin{example} (Consecutive  columns with no pivot in RREF)
 \begin{eqnarray*}
   \left.
\begin{array}{cccccccccr}
	 x & + & y &+ & z& + & 0w & = & 2 \\
	2x& +& 2y& +&2z&+&2w & = &  4
     \end{array}
   \right\} 
   &\!\Leftrightarrow\!&
\begin{amatrix}{4}
1 &1 &1 & 0 &2 \\ 
2 &2 &2 & 1 & 4
\end{amatrix}
\sim
\begin{amatrix}{4}
1 &1 &1 & 0 &2 \\ 
0 &0 &0 & 1 & 0
\end{amatrix}\\[2mm]
&\!\Leftrightarrow\!&
\left\{
\begin{array}{rrrrrr}
	x \ +\  y\  + \  z&&& = & 2 \phantom{\, .}\\
	&& w & = &  0\, .
     \end{array}
   \right.
\end{eqnarray*}  
Note that there was no hope of reaching the identity matrix, because of the shape of the augmented matrix we started with. 
\end{example}

With some practice, elimination can go  quickly. Here is an expert showing you some tricks. If you can't follow him now then come back when you have some more experience and watch again. You are going to need to get really good at this!

\videoscriptlink{gaussian_elimination_Advanced_Elimination.mp4}{Advanced Elimination}{script_gaussian_elimination_more}

It is important that you are able to convert RREF back into a system of equations. The first thing you might notice is that
if any of the numbers 
$b^{k+1},\dots, b^r$ in~\ref{Reduced row echelon form}~are non-zero then the system of equations is inconsistent and has no solutions. 
Our next task is to extract all possible solutions from an RREF  augmented matrix.

\subsection{Solution Sets and RREF}
%While RREF is not always pretty, it is certainly useful. 
%Our goal  is to solve systems of linear equations. 
RREF is a maximally simplified version of the original system of equations in the following sense: 
\begin{itemize}
\item As many coefficients of the variables as possible are $0$. 
\item As many coefficients  of the variables as possible are $1$.
\end{itemize}
It is easier to read off solutions from the maximally simplified equations than from the original equations, even when there are infinitely many solutions.

\begin{example}{(Standard approach  from a system of equations to the solution set)}
 \begin{eqnarray*}
\left.
\begin{array}{ccccccccr}
	x & +&y  && & +& 5w &   =& 1 \\
	 &&y & &   &+& 2 w & = &6 \\
	&&&& z&+&         4w & = &8
\end{array}
 \right\}
 \Leftrightarrow
 \begin{amatrix}{4} 
1 & 1 & 0 & 5 & 1 \\ 
0 & 1 & 0 & 2 & 6 \\
0 & 0 & 1 & 4 & 8 
\end{amatrix}
\sim
 \begin{amatrix}{4} 
1 & 0 & 0 & 3 & -5 \\ 
0 & 1 & 0 & 2 & 6 \\
0 & 0 & 1 & 4 & 8 
\end{amatrix}
\\[4mm]
%
\Leftrightarrow
\left\{
\begin{array}{cccccccccr}
	x &&& + &3w &  =& -5 \\[.5mm]
	&   y && +& 2 w & = &6 \\[.5mm]
        && z&+ &        4w & = &8
     \end{array}
     \right\}
\Leftrightarrow
\left\{
\begin{array}{lcrccr}
	x & =& -5& -&3w \\[.5mm]
	 y  & =& 6 &-&2w\\[.5mm]
	 z & = &8&-&4w \\[.5mm]
	w & =&&&w          
     \end{array}
     \right\}
 \\[4mm]
\Leftrightarrow
\colvec{x\\[.5mm]y\\[.5mm]z\\[.5mm]w} = \colvec{-5\\[.5mm]6\\[.5mm]8\\[.5mm]0} + w\colvec{-3\\[.5mm]-2\\[.5mm]-4\\[.5mm]1}\, .
\end{eqnarray*}
%This brings up the issue of what constitutes a solution to a system of equations; the last thing written above was a vector equation... and if it sounds wrong to say  ``an equation is a solution to an equation"" you are in good company.  
%There are infinitely many solutions to the , one for each value of $w$. 
%For example
%$$ \colvec{-5\\[.5mm]6\\[.5mm]8\\[.5mm]0} 
%+ 
%\left( -.75\right)^{\pi-1}\colvec{-3\\[.5mm]-2\\[.5mm]-4\\[.5mm]1}\, 
%$$
%is one of the solutions. Notice that this solution is not an equation; it is an ordered set of numbers that, if substituted  for the variables in the equations, yields true statements. This might be annoying to check because of the annoying number $(-.75)^{\pi-1}$ we chose to replace $w$. We just wanted to show you that ANY number can go there. You probably want a way to check a solution which is not annoying. A good check is to see if the system is solved by the expression with $w$ replaced by $0$. Indeed
%\begin{eqnarray*}
% \colvec{-5\\[.5mm]6\\[.5mm]8\\[.5mm]0} {\text ~is~a ~solution ~to \qquad}
%\begin{array}{ccccccccr}
%	x & +&y  && & +& 5w &   =& 1 \\
%	 &&y & &   &+& 2 w & = &6 \\
%	&&&& z&+&         4w & = &8
%\end{array}
%\\ 
%{\text  ~because \qquad}
%\begin{array}{ccccccccr}
%	-5 & +&6  && & +& 5(0) &   =& 1 \\
%	 &&6 & &   &+& 2 (0) & = &6 \\
%	&&&& 8&+&         4(0) & = &8
%\end{array}.
%\end{eqnarray*}
%Given that there are many solutions, what ought one report when asked to solve a system of equations? The standard thing to report is {\it the solution set}\index{solution set}\index{Solution set}; 
%There is a solution for each value of $w$. Better said 
%\begin{eqnarray*}
%{\text the ~solution~ set~ to \qquad}
%\begin{array}{ccccccccr}
%	x & +&y  && & +& 5w &   =& 1 \\
%	 &&y & &   &+& 2 w & = &6 \\
%	&&&& z&+&         4w & = &8
%\end{array}
%is \qquad\left\{    \colvec{-5\\[.5mm]6\\[.5mm]8\\[.5mm]0} 
%+ 
%\alpha \colvec{-3\\[.5mm]-2\\[.5mm]-4\\[.5mm]1} : \alpha \in \mathbb{R} \right\}
%\end{eqnarray*}
There is one solution for each value of $w$, so the solution set is 
$$
\left\{    \colvec{-5\\[.5mm]6\\[.5mm]8\\[.5mm]0} 
+ 
\alpha \colvec{-3\\[.5mm]-2\\[.5mm]-4\\[.5mm]1} : \alpha \in \mathbb{R} \right\}.
$$
\end{example}
Here is a verbal description of the preceeding example of the \hypertarget{standard approach}{{\it standard approach}}. We say that $x,y$, and $z$ are {\it pivot variables}\index{Pivot variables} because they appeared with a  pivot coefficient in RREF. 
Since $w$ never appears with a pivot  coefficient, 
 it is not a pivot variable. %We call it a free variable. 
%One way to reveal the solutions to this system of equations is to 
In the second line we put all the pivot variables on one side 
and all the {\it non-pivot variables}\index{Non-pivot variables} on the other side and added the trivial equation $w=w$ to obtain a system that allowed us to easily read off solutions.

\begin{center}
{\Large{\bf The Standard Approach To Solution Sets}}
\end{center}
\begin{enumerate}
\item Write the augmented matrix.
\item Perform EROs to reach RREF.
\item Express the pivot variables in terms of the non-pivot variables. 
\end{enumerate}
There are always exactly enough non-pivot variables to index your solutions. 
In any approach, the variables which are not expressed in terms of the other variables are called  {\it free variables}\index{free variables}. The standard approach is to use the non-pivot variables as free variables.

%vid for this?! 
Non-standard approach: solve for $w$ in terms of $z$ and substitute into the other equations. You now have an expression for each component in terms of $z$. But why pick $z$ instead of $y$ or $x$? (or $x+y$?) The standard approach not only feels natural, 
but is {\it canonical}, meaning that everyone will get the same RREF and hence choose the same variables to be free.
However, it is important to remember that so long as their {\it set} of solutions is the same, any two choices of free variables is fine.
(You might think of this as the difference between using Google Maps$^{\sf TM}$ or Mapquest$^{\sf TM}$; although their maps may look different, 
the place 
$\langle$home {\it sic}$\rangle$ 
they are describing is the same!)


When you see an RREF augmented matrix with two columns that have no pivot, you know there will be two free variables. 

\begin{example}{(Standard approach, multiple free variables)}

 \begin{eqnarray*}
 \begin{amatrix}{4} 
1 & 0 & 7 & 0 & 4 \\ 
0 & 1 & 3 & 4 & 1 \\ 
0 & 0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0 & 0 \\ 
\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{lcr}
	x \phantom{+y}    + 7z  \phantom{+w} & = 4 \\
	\phantom {x+}   y + 3z  {+4w} & = 1 \\
	%\phantom{x+y+z+}          w & = 2
     \end{array}
     \right\}
%
\\
\Leftrightarrow 
\left\{
\begin{array}{rlcrccrr}
	x & = \ 4&\! -\!&7z& \\
	 y  & = \ 1& \!-\!&3z&\!-\!&4w\\
	 z   & = &&z\\
	w & =&&&&w          
     \end{array}
     \right\}
     \Leftrightarrow
\colvec{x\\y\\z\\w} = \colvec{4\\1\\0\\0} + z\colvec{-7\\-3\\1\\0} + w\colvec{0\\-4\\0\\1}
\end{eqnarray*}
%There are infinitely many solutions; one for each pair of numbers $z,w$. 
so the solution set is 
$$  \left\{  \colvec{4\\1\\0\\0} + z\colvec{-7\\-3\\1\\0} + w\colvec{0\\-4\\0\\1} : z,w\in \mathbb{R} \right\}. $$
\end{example}

%the youtube version
%\begin{center}
%\href{http://www.youtube.com/watch?v=87iKQG6PJvM}{\raisebox{-.4cm}{\includegraphics[scale=.075]{take1.jpg}}}\hspace{1cm}\scalebox{1.2}{From RREF to a Solution Set}
%\end{center}


\begin{center}
\Videoscriptlink{solution_set.mp4}{From RREF to a Solution Set}
{}
\end{center}

You can imagine having three, four, or fifty-six non-pivot columns and the same number of free variables indexing your solutions set. 
In general a solution set to a system of equations with $n$ free variables will be of the form 
\begin{center}
\shabox{
$
\{ x^P +\mu_1 x^H _1 + \mu_2 x^H _2 + \cdots + \mu_nx^H _n : \mu_1, \dots, \mu_n \in \mathbb{R} \} .$
}
\end{center}

The parts of these solutions play special roles in the associated matrix equation. This will come up again and again long after we complete this discussion of basic calculation methods, so we will use the general language of linear algebra to give names to these parts now. \\

\noindent {\bf Definition:} A {\bf homogeneous solution} to a linear equation $Lx=v$, with $L$ and $v$ known is a vector $x^H $ such  that $Lx^H =0$ where $0$ is the zero vector. \\

\shabox{
If you have a particular solution $x^P $ to a linear equation and add a sum of multiples of homogeneous solutions to it you obtain another particular solution. }\\
%The equation $Lx=0$ is called the  homogeneous equation associated to $.



\begin{center}
\Videoscriptlink{particular_homogeneous.mp4}{\hspace{-8mm}Particular and Homogeneous Solutions\hspace{-8mm}}{}
%youtube version
%\href{http://www.youtube.com/watch?v=6a_sT06Kti4}{\raisebox{-.4cm}{\includegraphics[scale=.075]{take1.jpg}}}\hspace{1cm}\scalebox{1.2}{Particular and Homogeneous Solutions}
\end{center}



Check now that the parts of the solutions with free variables as coefficients from the previous examples are homogeneous solutions, and that by adding a homogeneous solution to a particular solution one obtains a solution to the matrix equation. This will come up over and over again. As an example without matrices, consider the differential equation $\frac{d^2}{dx^2} f=3$. A particular solution is $\frac32x^2$ while $x$ and $1$ are homogeneous solutions. The solution set is $\{ \frac32 x^2+ax +c1 ~:~a,b\in\mathbb{R} \}$. You can imagine similar differential equations with more homogeneous solutions. \\


You need to become very adept at reading off solutions sets of linear systems from the RREF
of their augmented matrix; it is a basic skill for linear algebra, and we will continue using it up to the last page of the book! 





\Videoscriptlink{elementary_row_operations_worked_examples.mp4}{\hspace{-8mm}Worked examples of Gaussian elimination\hspace{-8mm}}{scripts_elementary_row_operations_worked_examples}


%This example emphasizes different aspects.
%\begin{example}
%$$
%\begin{amatrix}{5} 
%1 & 1 & 0 & 1 & 0 & 1\\ 
%0 & 0 & 1 & 2 & 0 & 2\\ 
%0 & 0 & 0 & 0 & 1 & 3\\ 
%0 & 0 & 0 & 0 & 0 & 0
%\end{amatrix}\, .
%$$
%Here we were not told the names of the variables, so lets just call them $x_1,x_2,x_3,x_4,x_5$.
%(There are always as many of these as there are columns in the matrix before the vertical line; the number of rows,
%on the other hand is the number of linear equations.)
%
%To begin with we immediately notice that there are no pivots in the second and fourth columns so, as per the standard approach, we will be all variables in terms of $x_2$ and $x_4$. 
%Next we see from the second last row that $x_5=3$. The second row says 
%$x_3=2-2x_4=2-2 x_2$.
%The top row then gives $x_1=1-x_2-x_4=1-x_1-x_2$. Again we can write this solution as a vector
%$$
%\colvec{1\\0\\2\\0\\3}+x_1\colvec{-1\\1\\0\\0\\0}+x_2\colvec{-1\\0\\-2\\1\\0}\, .
%$$
%Observe, that since no variables were given at the beginning, we can use any symbols instead of $x_1$ and $x_2$. For example lower case greek letter lambda:
%$$
%\colvec{1\\0\\2\\0\\3}+\lambda_1\colvec{-1\\1\\0\\0\\0}+\lambda_2\colvec{-1\\0\\-2\\1\\0}\, .
%$$
%As a challenge, look carefully at this solution and make sure you can see how every part of it comes from
%the original augmented matrix without every having to reintroduce variables and equations.
%\end{example}
%
%




%\begin{theorem}
%Every augmented matrix is row-equivalent to a \emph{unique} augmented matrix in reduced row echelon form.
%\end{theorem}
%
%\noindent
%In \Lecture~\ref{elemRowOpsPath}, we will see why this is true.

%\section*{Uniqueness of RREF}
%
%\begin{theorem}\label{GJEunique} Gauss-Jordan Elimination produces a unique augmented matrix in RREF.
%\end{theorem}
%
%\begin{proof}
%Suppose Alice and Bob compute the RREF for a linear system but get different results, $A$ and $B$.  Working from the left, discard all columns except for the pivots and the first column in which $A$ and $B$ differ.  By \hyperref[colremove]{Review Problem~\ref{colremove}}, removing columns does not affect row equivalence.  Call the new, smaller, matrices $\hat{A}$ and $\hat{B}$.  The new matrices should look this: $$\hat{A}=\begin{amatrix}{1}
%I_N & a\\
%0 & 0
%\end{amatrix} \mbox{ and } \hat{B}=\begin{amatrix}{1}
%I_N & b\\
%0 & 0
%\end{amatrix}\, ,$$ where $I_N$ is an $N\times N$ identity matrix and $a$ and $b$ are vectors.
%
%Now if $\hat{A}$ and $\hat{B}$ have the same solution, then we must have $a=b$.  But this is a contradiction!  Then $A=B$.
%\end{proof}
%
%\videoscriptlink{elementary_row_operations_proof.mp4}{Explanation of the proof}{scripts_elementary_row_operations_proof}


%\References{
%Hefferon, Chapter One, Section 1
%\\
%Beezer, Chapter SLE, Section RREF
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/Row_echelon_form}{Row Echelon Form}}

%\newpage

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading problems &
\hwrref{SystemsOfLinearEquations}{1}, \hwrref{SystemsOfLinearEquations}{2}\\
Augmented matrix &  \hwref{SystemsOfLinearEquations}{6}\\
$2\times2$ systems &  \hwref{SystemsOfLinearEquations}{7},
\hwref{SystemsOfLinearEquations}{8},
\hwref{SystemsOfLinearEquations}{9},
\hwref{SystemsOfLinearEquations}{10},
\hwref{SystemsOfLinearEquations}{11},
\hwref{SystemsOfLinearEquations}{12}\\
$3\times2$ systems & 
\hwref{SystemsOfLinearEquations}{13},
\hwref{SystemsOfLinearEquations}{14}
\\
$3\times3$ systems & 
\hwref{SystemsOfLinearEquations}{15},
\hwref{SystemsOfLinearEquations}{16},
\hwref{SystemsOfLinearEquations}{17}
\\
\hline
\end{tabular}

\input{\gaussElimPath/problems}



%\chapter{\elemRowOpsTitle}
%FOR THE BOOK
\section{\elemRowOpsTitle}

\label{EROS}
%\hypertarget{Elementary Row Operations} %what is this? 

Elementary row operations are  systems of linear equations relating the old and new rows in Gaussian elimination: 


\begin{example}\label{Rsystem} (\hypertarget{Keeping track of EROs with equations between rows}{Keeping track of EROs with equations between rows})\\
We refer to the new $k$th row as $R'_k$ and the old $k$th row as $R_k$.
\begin{equation*}
\begin{array}{ccc|r}
\begin{amatrix}{3} 
0 & 1 & 1 & 7 \\ 
2 & 0 & 0& 4 \\
0& 0 & 1 & 4 \\
\end{amatrix} 
& \stackrel{R_1'=0R_1+\phantom{1}R_2+0R_3}{\stackrel{R_2'=\phantom{1}R_1+0R_2+0R_3}{ \stackrel{R_3'= 0R_1+0R_2+\phantom{1}R_3}{\stackrel{}{\sim}}}}&
\begin{amatrix}{3} 
2 & 0 & 0 & 4 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
&\colvec{R_1'\\R_2'\\R_3'}=\begin{pmatrix}0&1&0\\1&0&0\\0&0&1\end{pmatrix}\colvec{R_1\\R_2\\R_3}
\\[.8cm]
& \stackrel{R_1'=\frac12 R_1+0R_2+0R_3}{\stackrel{R_2'=0R_1+\phantom{1}R_2+0R_3}{ \stackrel{R_3'= 0R_1+0R_2+\phantom{1}R_3}{\stackrel{}{\sim}}} }&
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
&\colvec{R_1'\\R_2'\\R_3'}=\begin{pmatrix}\frac12&0&0\\0&1&0\\0&0&1\end{pmatrix}\colvec{R_1\\R_2\\R_3}
\\[.8cm]
& \stackrel{R_1'= \phantom{1}R_1+0R_2+0R_3}{\stackrel{R_2'=0R_1+\phantom{1}R_2-\phantom{1}R_3}{ \stackrel{R_3'=0R_1+0R_2+\phantom{1} R_3}{\stackrel{}{\sim}}} }&
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 0& 3 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
&\colvec{R_1'\\R_2'\\R_3'}=\begin{pmatrix}1&0&0\\0&1&\!\!-1\\0&0&1\end{pmatrix}\colvec{R_1\\R_2\\R_3}\\[3mm]\phantom{x} &&&
\end{array}
\end{equation*}
On the right, we have listed the relations between  old and new rows in matrix notation.
\Reading{SystemsOfLinearEquations}{3}
\end{example}

%\videoscriptlink{elementary_row_operations_example.mp4}{Example}{script_elementary_row_operations_example}

%\begin{center}\href{\webworkurl ReadingHomework3/1/}{Reading homework: problem 3.1}\end{center}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{EROs and  Matrices}
Interestingly, the matrix 
that describes the relationship between old and new rows 
performs the corresponding ERO on the augmented matrix.
\begin{example} (Performing EROs with Matrices)
\begin{eqnarray*}
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  
\end{pmatrix} 
\begin{amatrix}{3} 
0 & 1 & 1 & 7 \\ 
2 & 0 & 0& 4 \\
0& 0 & 1 & 4 
\end{amatrix} 
&=&
\begin{amatrix}{3} 
2 & 0 & 0 & 4 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\\[3mm] &&\qquad\quad \rotatebox{90}{$\sim$} \\[3mm]%second line
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 & 0 \\
0& 0 & 1  \\
\end{pmatrix}
\begin{amatrix}{3} 
2 & 0 & 0 & 4 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
&=&
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\\[3mm] &&\qquad\quad \rotatebox{90}{$\sim$} \\[3mm]%third line
\begin{pmatrix}
1&0&0\\
0&1&\!\!\!-1\\
0&0&1
\end{pmatrix}
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix} 
&=&
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 0& 3 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\end{eqnarray*}
Here we have multiplied the augmented matrix with the matrices that acted on rows listed on the right  of example~\ref{Rsystem}. 
\end{example}

Realizing EROs as matrices allows us to give a concrete notion of \hyperlink{ch1divide}{``dividing by a matrix''}; we can now perform manipulations on both sides of an equation in a familiar way:

\begin{example} (Undoing $A$ in $Ax=b$ slowly, for $A=6=3\cdot2$)
\begin{equation*}\begin{array}{crcr}
&6x&=&\phantom{ 3^{-1}} 12 \\[2mm]
\Leftrightarrow\ &3^{-1}6x&=&3^{-1}12 \\[2mm]
\Leftrightarrow\ & 2x&=&\phantom{3^{-1}~}4  \\[2mm]
\Leftrightarrow\ & 2^{-1}2x&=&2^{-1}~4\\[2mm] %  \Leftrightarrow 
\Leftrightarrow\ &  1x&=&\phantom{3^{-1}~} 2
\end{array}
\end{equation*}
\end{example}

\noindent
The matrices corresponding to EROs undo a matrix step by step.
\begin{example} \label{slowly}(\hypertarget{Undoing}{Undoing} $A$ in $Ax=b$ slowly, for $A=M=...$)
\begin{equation*}
\begin{array}{crcr}
& \begin{pmatrix}
0 & 1 & 1  \\ 
2 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
}
~
\begin{pmatrix}
 7 \\ 
4 \\
4\\
\end{pmatrix} 
\\[7mm] %second line
\Leftrightarrow\ &
%
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
%
\begin{pmatrix}
0 & 1 & 1  \\ 
2 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
~
\begin{pmatrix}
 7 \\ 
4 \\
4\\
\end{pmatrix} 
\\[7mm]
\Leftrightarrow\ & %third line
\phantom{-}
\begin{pmatrix}
2 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
}
~
\begin{pmatrix}
4 \\
7 \\ 
4\\
\end{pmatrix} 
\\[7mm]
\Leftrightarrow\ & %fourth line
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
2 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
%\phantom{
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 &  0\\
0& 0 & 1  \\
\end{pmatrix} 
%}
~
\begin{pmatrix}
4 \\
7 \\ 
4\\
\end{pmatrix} 
\\[7mm]
\Leftrightarrow\ & %fifth line
\phantom{
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 &  0\\
0& 0 & 1  \\
\end{pmatrix} 
}
~
\begin{pmatrix}
2 \\
7 \\ 
4\\
\end{pmatrix} 
\\[7mm]
\Leftrightarrow\ & %sixth line
%\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 & \!\!\!-1 \\
0& 0 & 1  \\
\end{pmatrix} 
%}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
%\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 & \!\! \!-1\\
0& 0  &  1  \\
\end{pmatrix} 
%}
\! \!
\begin{pmatrix}
2 \\
7 \\ 
4\\
\end{pmatrix} 
\\[7mm]
\Leftrightarrow\ &%%%%%%%%%%%%%%%%%%%%%%%7th line
\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 & -1 \\
0& 0 & 1  \\
\end{pmatrix} 
}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 &  -1\\
0& 0  &  1  \\
\end{pmatrix} 
}
\! 
\begin{pmatrix}
2 \\
3 \\ 
4\\
\end{pmatrix} .
\end{array}
\end{equation*}
\end{example}



\noindent
This is another way of thinking about Gaussian elimination which feels more like elementary algebra in the sense that you ``do something to both sides of an equation"" until you have a solution. 


%%%%%%%%%%%%%%%%%%

\subsection{Recording EROs in $(\!\, M | I \,) $}\label{EROinverse}
Just as we put together $3^{-1}2^{-1}=6^{-1}$ to get a single thing to apply to both sides of $6x=12$ to undo $6$, we should put together multiple EROs  to get a single thing that undoes our matrix. 
To do this, augment by the identity matrix (not just a single column) and then perform Gaussian elimination. 
There is no need to write the EROs as systems of equations or as matrices while doing this. 
%That is, perform the EROs without the their corresponding matrices.

\begin{example} \label{undo_a_matrix}(Collecting EROs that \hypertarget{undo a matrix}{undo a matrix})
\begin{eqnarray*}
\left(\begin{array}{ccc|ccc}
0 & 1 & 1 &1 &0 &0\\ 
2 & 0 & 0 &0&1&0\\
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)
&\!\!\sim\!\!&
\left(\begin{array}{ccc|ccc}
2 & 0 & 0 &0&1&0\\
0 & 1 & 1 &1 &0 &0\\ 
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)
\\[2mm]
&\!\!\sim\!\!&
\left(\begin{array}{ccc|ccc}
1 & 0 & 0 &0&\frac12&0\\
0 & 1 & 1 &1 &0 &0\\ 
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)
\sim
\left(\begin{array}{ccc|ccr}
1 & 0 & 0 &0&\frac12&0\\
0 & 1 & 0 &1 &0 &\!\!\!-1\\ 
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)\, .
\end{eqnarray*}
\end{example}
\noindent
As we changed the left side from the matrix $M$ to the identity matrix, the right side changed from the identity matrix to the matrix which undoes $M$. 
\begin{example} (Checking that \hypertarget{inversie}{one matrix undoes another})
\begin{eqnarray*}
\left(\begin{array}{rrr}
0&\frac12&0\\
1 &0 &\!\!\!-1\\ 
0  &0 &1\\
\end{array}  \right)
\left(\begin{array}{ccc}
0&1&1\\
2 &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
=
\left(\begin{array}{ccc}
1  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) \, .
\end{eqnarray*}
If the matrices are composed in the opposite order, the result is the same.
\begin{eqnarray*}
\left(\begin{array}{ccc}
0&1&1\\
2 &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
\left(\begin{array}{ccr}
0&\frac12&0\\
1 &0 &\!\!\!-1\\ 
0  &0 &1\\
\end{array}  \right)
=
\left(\begin{array}{ccc}
1  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) \, .
\end{eqnarray*}
\end{example}

%\reading{3}{1}

Whenever the product of two matrices $MN=I$, we say that  $N$ is the inverse\index{Inverse Matrix} of $M$ or $N=M^{-1}$. 
Conversely $M$ is the inverse of $N$;~$M=N^{-1}$.\\



In abstract generality, let $M$ be some matrix and, as always, let $I$ stand for the identity matrix. Imagine the process of performing elementary row operations to bring $M$ to the identity matrix: 
\begin{equation*}
(M | I) \sim ( E_1M| E_1)\sim (E_2E_1 M | E_2 E_1) \sim \cdots \sim (I | \cdots E_2E_1 )\, .
\end{equation*}
The ellipses ``$\cdots$'' stand for additional EROs. The result is a product of matrices that form a matrix which undoes $M$
\begin{equation*}
\cdots E_2 E_1 M =  I \, .
\end{equation*}
This is only true if the RREF of $M$ is the identity matrix.  \\

\noindent {\bf Definition}: A matrix $M$ is {\bf invertible}\index{invertiblech3} if its RREF is an identity matrix.
\begin{center}
{\Large{\bf  How to find $M^{-1}$}}\\[5mm]
 \shabox{$(M | I) \sim (I| M^{-1})$}
\end{center}

Much use is made of the fact that invertible matrices can be undone with EROs. 
To begin with, since each  elementary row operation has an inverse, 
$$
M= E_1^{-1} E_2^{-1} \cdots\, ,
$$
while the inverse of $M$ is 
\begin{equation*}
M^{-1}=\cdots E_2 E_1 \, .
\end{equation*}
This is symbolically verified by
\begin{equation*}
M^{-1}M=\cdots E_2 E_1\, E_1^{-1} E_2^{-1} \cdots
=\cdots E_2 \, E_2^{-1} \cdots = \cdots = I\, .
\end{equation*}
Thus, if $M$ is invertible, then  $M$  can be expressed as the product of EROs. (The same is true for its inverse.) This has the feel of the fundamental theorem of arithmetic (integers can be expressed as the product of primes) or the fundamental theorem of algebra (polynomials can be expressed as the product of [complex] first order polynomials); EROs are  building blocks of invertible matrices. 




\subsection{The Three Elementary Matrices}

%To use this in concrete examples, one uses the fact that i
We now work toward concrete examples and applications. 
It is surprisingly easy to translate between EROs and matrices that perform EROs.
The matrices corresponding to these kinds are close in form to the identity matrix:
\begin{itemize}
\item Row Swap: Identity matrix with two rows swapped.
\item Scalar Multiplication:  Identity matrix with one \hyperlink{diagmat}{diagonal entry} not 1.
\item Row Sum: The identity matrix with one off-\hyperlink{diagmat}{diagonal entry} not 0.
\end{itemize}


\begin{example} (Correspondences between EROs and their matrices)
\begin{itemize}
\item The row swap matrix that swaps the 2nd and 4th row is the identity matrix with the 2nd and 4th row swapped: 
$$
\begin{pmatrix}
1&0&0&0&0\\
0&0&0&1&0\\
0&0&1&0&0\\
0&1&0&0&0\\
0&0&0&0&1\\
\end{pmatrix}\, .
$$
\item
The scalar multiplication matrix that replaces the 3rd row with 7 times the 3rd row is the identity matrix with 7 in the 3rd row instead of 1:
$$
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&7&0\\
0&0&0&1\\
\end{pmatrix}\, .
$$

\item The row sum matrix that replaces the 4th row with the 4th row plus 9 times the 2nd row is the identity matrix with a 9 in the  4th row, 2nd column:
$$
\begin{pmatrix}
1&0&0&0&0&0&0\\
0&1&0&0&0&0&0\\
0&0&1&0&0&0&0\\
0&9&0&1&0&0&0\\
0&0&0&0&1&0&0\\
0&0&0&0&0&1&0\\
0&0&0&0&0&0&1\\
\end{pmatrix}\, .
$$
\end{itemize}
\end{example}

We can write an explicit factorization of a matrix into EROs by keeping track of the EROs used in getting to RREF.

\begin{example} (Express $M$ from  
\hyperlink{undo a matrix}{Example~\ref{undo_a_matrix}} as a product of EROs)\\
Note that in the previous example 
one of each of the kinds of EROs is used, in the order just given.
Elimination looked like 
\begin{eqnarray*}
M=
\left(\begin{array}{ccc}
0 & 1 & 1 \\ 
2 & 0 & 0 \\\
0& 0 & 1  
\end{array}  \right)
\stackrel{E_1}{\sim}
\left(\begin{array}{ccc}
2 & 0 & 0 \\
0 & 1 & 1 \\ 
0& 0 & 1  
\end{array}  \right)
\stackrel{E_2}{\sim}
\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 1 \\ 
0& 0 & 1  
\end{array}  \right)
\stackrel{E_3}{\sim}
\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\ 
0& 0 & 1  
\end{array}  \right)
=I\, ,
\end{eqnarray*}
where the EROs matrices are 
\begin{eqnarray*}
E_1
= \left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
,~
E_2
= \left(\begin{array}{ccc}
\frac12  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) , ~
E_3
= \left(\begin{array}{ccc}
1  &0 &0\\
0  &1 & -1\\ 
0  &0 &1\\
\end{array}  \right) \,.
\end{eqnarray*}
%Composing these gives (by matrix multiplication rules worked out in %\hyperref{}
%\begin{eqnarray*}
%E_3E_2E_1
%&= &
% \left(\begin{array}{ccc}
%1  &0 &0\\
%0  &1 & -1\\ 
%0  &0 &1\\
%\end{array}  \right)
% \left(\begin{array}{ccc}
%\frac12  &0 &0\\
%0  &1 &0\\ 
%0  &0 &1\\
%\end{array}  \right)
%\left(\begin{array}{ccc}
%0  &1 &0\\
%1  &0 &0\\ 
%0  &0 &1\\
%\end{array}  \right)
%\\ %2nd line
%&=& \left(\begin{array}{ccc}
%1 &0 &0\\
%0  &1 & -1\\ 
%0  &0 &1\\
%\end{array}  \right) 
%\left(\begin{array}{ccc}
%0  	&\frac12 	& 0\\ 
%1  	&0	 	&0\\
%0  	&0 		&1
%\end{array}  \right) 
%=%%%%%%%%%%%%%%%%
%\left(\begin{array}{ccc}
%0  	&\frac12 	& 0\\ 
%1  	&0	 	&-1\\
%0  	&0 		&1
%\end{array}  \right) 
% \, .
%\end{eqnarray*}
%We showed this was $M^{-1}$ \hyperlink{inversie}{earlier}. 
The inverse of the ERO matrices (corresponding to the description of the reverse row maniplulations)
\begin{eqnarray*}
E_1^{-1}
= \left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
,~
E_2^{-1}
= \left(\begin{array}{ccc}
2  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) , ~
E_3^{-1}
= \left(\begin{array}{ccc}
1  &0 &0\\
0  &1 & 1\\ 
0  &0 &1\\
\end{array}  \right) \,.
\end{eqnarray*}
Multiplying these gives 
\begin{eqnarray*}
E_1^{-1}E_2^{-1}E_3^{-1}
&=& 
\left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
 \left(\begin{array}{ccc}
2  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) 
\left(\begin{array}{ccc}
1  &0 &0\\
0  &1 & 1\\ 
0  &0 &1\\
\end{array}  \right) 
\\[2mm] %2nd line
&=&
\left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
 \left(\begin{array}{ccc}
2  &0 &0\\
0  &1 &1\\ 
0  &0 &1\\
\end{array}  \right) 
= %%%%%%%
\left(\begin{array}{ccc}
 0 &1 &1\\
2  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)  = M \, .
\end{eqnarray*}
\end{example}

\subsection{$LU$, $LDU$, and $PLDU$ Factorizations}\label{LUtake1}
The process of elimination can be stopped halfway to obtain decompositions frequently used in large computations in sciences and engineering. 
The first half of the elimination process is to eliminate entries below the diagonal  
leaving a matrix which is called {\it upper triangular}\index{Upper triangular matrix}. The elementary matrices which perform this part of the elimination are {\it lower triangular}\index{lower triangular}, as are their inverses. But putting together the upper triangular and lower triangular parts one obtains the so-called $LU$ factorization.


\begin{example}\label{factorize} ($LU$ \hypertarget{elldeeeww}{ factorization})
\begin{eqnarray*}
M=
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
&\stackrel{E_1}{\sim}&
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&-1&1&-1\\
\end{pmatrix}
\\[2mm]
&\stackrel{E_2}{\sim}&
~~\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&3&1\\
\end{pmatrix}
~~\stackrel{E_3}{\sim}~
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
:=U\, ,
\end{eqnarray*}
%%%%%%%%%%%%%%%%%%%%%%%%%%
where the EROs and their inverses are 
%%%%%%%%%%%%%%%
\begin{eqnarray*}
E_1=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
2&0&1&0\\
0&0&0&1\\
\end{pmatrix} \, ,~~~~
E_2=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&1&0&1\\
\end{pmatrix} \, ,~~
E_3=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&-1&1\\
\end{pmatrix} \, 
\\[2mm] %%%%%%%%second line
E_1^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&0&0&1\\
\end{pmatrix}  , \,
E_2^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&-1&0&1\\
\end{pmatrix}  , \,
E_3^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&1&1\\
\end{pmatrix} \, .
\end{eqnarray*}
Applying inverse elementary matrices to both sides of the equality  $U=E_3E_2E_1M$ gives 
$M=E_1^{-1}E_2^{-1}E_3^{-1}U$ or 
%\scalebox{.97}
\begin{eqnarray*}
 %oi vey
\begin{pmatrix}
2&0&\!\!-3&1\\
0&1&2&2\\
\!-4&0&9&2\\
0&-1&1&\!\!-1\\
\end{pmatrix}
\!\!\!\!\!\!&=&\!\!\!\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
\!-2&0&1&0\\
0&0&0&1\\
\end{pmatrix} \!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&\!\!\!-1&0&1\\
\end{pmatrix}\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&1&1\\
\end{pmatrix}\!\!\!
\begin{pmatrix}
2&0&\!\!\!-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&\!\!-3\\
\end{pmatrix}
\\[2mm]
&=&\!\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
\!-2&0&1&0\\
0&0&0&1\\
\end{pmatrix} 
%%%%%%%%
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&\!\!-1&1&1\\
\end{pmatrix}
%%%%%%%%%%
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\\[2mm]
&=&\!\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
\!-2&0&1&0\\
0&\!\!-1&1&1\\
\end{pmatrix} 
%
\begin{pmatrix}
2&0&\!\!-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&\!\!-3\\
\end{pmatrix} \, .
\end{eqnarray*}
%\end{scalebox}
This is a lower triangular matrix times an upper triangular matrix. 
\end{example}

\newpage
What if we stop at a different point in elimination? 
We could multiply rows so that the entries in the diagonal are 1 next. Note that the EROs that do this are diagonal. This gives a slightly different factorization.
\begin{example} \label{factorizes}($LDU$ factorization building from previous example)
\begin{eqnarray*}
M=
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
&\stackrel{E_3E_2E_1}{\sim}&
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\stackrel{E_4}{\sim}
\begin{pmatrix}
1&0&-\frac{3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\\[2mm]
&\stackrel{E_5}{\sim}&
\begin{pmatrix}
1&0&-\frac{3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&-3\\
\end{pmatrix}
\stackrel{E_6}{\sim}
\begin{pmatrix}
1&0&-\frac{3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&1\\
\end{pmatrix}
=:U
\end{eqnarray*}
The corresponding elementary matrices are
\begin{equation*}
\begin{aligned}
%%%%%%%%the EROs
E_4=
\begin{pmatrix}
\frac12&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix} , \, ~~
E_5=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&\frac13&0\\
0&0&0&1\\
\end{pmatrix} , \, ~~
E_6=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&-\frac13\\
\end{pmatrix} , \, 
\\[3mm]
%%%%%%%%the ERO inverses
E_4^{-1}=
\begin{pmatrix}
2&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix} , \, 
E_5^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&3&0\\
0&0&0&1\\
\end{pmatrix} , \, 
E_6^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&-3\\
\end{pmatrix} \, .
\end{aligned}
\end{equation*}
The equation $U=E_6E_5E_4E_3E_2E_1 M$ can be rearranged as
$$M=(E_1^{-1}E_2^{-1}E_3^{-1})(E_4^{-1}E_5^{-1}E_6^{-1})U.$$ 
We calculated the product of the first three factors in the previous example; it was named $L$ there, and we will reuse that name here. The product of the next three factors is diagonal and we wil name it $D$. The last factor we named $U$ (the name means something different in this example than the last example.) The $LDU$ factorization of our matrix is
\begin{eqnarray*}
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&-1&1&1\\
\end{pmatrix} 
\begin{pmatrix}
2&0&0&0\\
0&1&0&0\\
0&0&3&0\\
0&0&0&-3\\
\end{pmatrix} 
\begin{pmatrix}
1&0&\!\!-\frac{3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&1\\
\end{pmatrix}\, .
\end{eqnarray*}
\end{example}

The $LDU$ factorization of a matrix is a factorization into blocks of EROs of a various types: $L$ is the product of the inverses of EROs which eliminate below the diagonal by row addition, $D$ the product of inverses of EROs which set the diagonal elements to 1 by row multiplication, and $U$ is the product of inverses of EROs which eliminate above the diagonal by row addition.

\hypertarget{LDPU}{
You} may notice that one of the three kinds of row operation is missing from this story. 
Row exchange may  be necessary to obtain RREF. Indeed, 
so far in this chapter we have been working under the tacit assumption that 
$M$ can be brought to the identity by just row multiplication and row addition. 
If row exchange is necessary, the resulting factorization is $LDPU$ where $P$ is the product of inverses of EROs that perform row exchange. 

\begin{example} ($LDPU$ factorization, building from previous examples)
\begin{eqnarray*}
M=
\begin{pmatrix}
0&1&2&2\\
2&0&-3&1\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\stackrel{P}{\sim}
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\stackrel{E_6E_5E_4E_3E_2E_1}{\sim} L
\end{eqnarray*}
\begin{eqnarray*}
P=
\begin{pmatrix}
0&1&0&0\\
1&0&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix}
=P^{-1}
\end{eqnarray*}
\begin{eqnarray*}
M=P(E_1^{-1}E_2^{-1}E_3^{-1})(E_4^{-1}E_5^{-1}E_6^{-1}) (E_7^{-1}) U=PLDU\\
\end{eqnarray*}
%%%%%%%last line!
\begin{center}
\scalebox{.91}{$
\!\!\!\begin{pmatrix}
0&1&2&2\\
2&0&-3&1\\
-4&0&9&2\\
0&\!\!-1&1&\!\!-1\\
\end{pmatrix}
=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&\!\!-1&1&1\\
\end{pmatrix} 
\!\!\!
\begin{pmatrix}
2&0&0&0\\
0&1&0&0\\
0&0&3&0\\
0&0&1&\!\!\!-3\\
\end{pmatrix} 
\!\!\!
\begin{pmatrix}
0&1&0&0\\
1&0&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix}
\!\!\!
\begin{pmatrix}
1&0&\!\!-\frac{3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&1\\
\end{pmatrix}
$}\end{center}

\end{example}

%\References{
%Hefferon, Chapter One, Section 1.1 and 1.2
%\\
%Beezer, Chapter SLE, Section RREF
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/Row_echelon_form}{Row Echelon Form}
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/Elementary_matrix_transformations}{Elementary Matrix Operations}}


\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|l|l|}
\hline
Reading problems &
\hwrref{SystemsOfLinearEquations}{3}\\
Matrix notation &  \hwref{SystemsOfLinearEquations}{18}\\
$LU$ &  \hwref{SystemsOfLinearEquations}{19}\\
\hline
\end{tabular}


\input{\elemRowOpsPath/problems}

%\input{\elemRowOpsPath/problems}




%for book
\section{\solutionSetsTitle}
%for summer
%\chapter{\solutionSetsTitle}


Algebraic equations problems can have multiple solutions. For example $x(x-1)=0$ has  two solutions: $0$ and $1$. By contrast, equations of the form $Ax=b$ with $A$ a linear operator (with scalars the real numbers) have the following property:

\vspace{3mm}
\noindent
If $A$ is a linear operator and $b$ is  known, then $Ax=b$ has either
\begin{enumerate}
\item One solution
\item  No solutions
\item Infinitely many solutions
\end{enumerate}


\subsection{The Geometry of Solution Sets: Hyperplanes}
Consider the following algebra problems and their solutions.

\begin{enumerate}
\item $6x=12$ has one solution: $2$.
\item[2a.] $0x=12$ has no solution.
\item[2b.] $0x=0$ has infinitely many solutions; its solution set is $\mathbb{R}$.
\end{enumerate}
In each case the linear operator is a $1\times 1$ matrix. In the first case, the linear operator is invertible. 
In the other two cases it is not. 
In the first case, the solution set is a point on the number line, in  case 2b the solution set is the whole number line.

Lets examine similar situations with larger matrices: $2\times 2$ matrices.
\begin{enumerate}
\item
$\begin{pmatrix}
6	&0 	\\
0 	&2 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
12 \\ 
6
\end{pmatrix}$has  one solution: 
$\begin{pmatrix}
2 \\ 
3
\end{pmatrix}.$
%\\linear operator is invertible

\item[2a.] 
$\begin{pmatrix}
1	&3 	\\
0 	&0 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
4 \\ 
1 
\end{pmatrix}$ has no solutions.
%not in the range of the linear operator

\item[2bi.]
$\begin{pmatrix}
1	&3 	\\
0 	&0 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
4 \\ 
0
\end{pmatrix} $ has solution set 
$\left \{ 
\left(\begin{array}{c}
4 \\ 
0
\end{array} \right)
+
y\left(\begin{array}{c}
-3 \\ 
1
\end{array} \right)
: y\in \mathbb{R} \right\}.$

\item[2bii.]
$\begin{pmatrix}
0	&0 	\\
0 	&0 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
0 \\ 
0
\end{pmatrix} $
has solution set 
$\left \{ 
\left(\begin{array}{c}
x \\ 
y
\end{array} \right)
: x, y\in \mathbb{R} \right\}.$
\end{enumerate}
Again, in the first case the linear operator is invertible while in the other cases it is not. When a $2\times 2$ matrix from a matrix equation is not invertible the solution set can be empty, a line in the plane, or the plane itself.


For a system of equations with $r$ equations and $k$ veriables, one can have a number of different outcomes.  For example, consider the case of $r$ equations in three variables.  Each of these equations is the equation of a plane in three-dimensional space.  To find solutions to the system of equations, we look for the common intersection of the planes (if an intersection exists).  Here we have \hypertarget{FIVE}{five different possibilities}:

\begin{enumerate}
\item \textbf{Unique Solution.}  The planes have a unique point of intersection.

\item[2a.] \textbf{No solutions.}  Some of the equations are contradictory, so no solutions exist.

\item[2bi.] \textbf{Line.}  The planes intersect in a common line; any point on that line then gives a solution to the system of equations.

\item[2bii.] \textbf{Plane.}  Perhaps you only had one equation to begin with, or else all of the equations coincide geometrically.  In this case, you have a plane of solutions, with two free parameters.

\Videoscriptlink{solution_sets_for_systems_of_linear_equations_planes.mp4}{Planes}{solution_sets_for_systems_of_linear_equations_planes}

\item[2biii.] \textbf{All of $\mathbb{R}^3$.}  If you start with no information, then any point in $\mathbb{R}^3$ is a solution.  There are three free parameters.
\end{enumerate}

In general, for systems of equations with $k$ unknowns, there are $k+2$ possible outcomes, corresponding to the possible numbers ({\it i.e.}, $0,1,2,\dots,k$) of free parameters in the solutions set, plus the possibility of no solutions.  These types of solution sets\index{Solution set} are hyperplanes\index{Hyperplane}, generalizations of planes that behave like planes in $\mathbb{R}^3$ in many ways.

\Reading{SystemsOfLinearEquations}{4}
%\begin{center}\href{\webworkurl ReadingHomework4/1/}{Reading homework: problem 4.1}\end{center}


\Videoscriptlink{solution_sets_for_systems_of_linear_equations_overview.mp4}{Pictures and Explanation}{solution_sets_for_systems_of_linear_equations_overview}






\subsection{\! Particular Solution \hspace{-.7mm}$+$\hspace{-.7mm} Homogeneous Solutions }

Lets look at solution sets again, this time trying to get to their geometric shape.
In the \hyperlink{standard approach}{standard approach}, variables corresponding to columns that do not contain a pivot (after going to reduced row echelon form) are \emph{free}.  It is the number of free variables that determines the geometry of the solution set. 
%We called them non-pivot variables. 
%They index elements of the solution set by acting as coefficients of vectors.
%In this way the number of non-pivot columns determines (in part) the size of the solution set.  
%We can denote them with dummy variables $\mu_1, \mu_2, \ldots$. 

\begin{example}\label{npcd} (Non-pivot variables determine the gemometry of the solution set)
$$\begin{pmatrix}
1 &  0 & 1 & -1 \\ 
 0 & 1 & -1& 1  \\
 0 &0   & 0  & 0 \\
\end{pmatrix}
\colvec{x_1\\x_2\\x_3\\x_4} 
=
\colvec{1\\1\\0} 
\Leftrightarrow
\left\{
\begin{array}{lcr}
	1x_1 +0x_2+ 1x_3 - 1x_4 & = 1 \\
	0x_1 +1x_2 - 1x_3 + 1x_4 & = 1 \\
	0x_1 +0x_2 + 0x_3 + 0x_4 & = 0 
\end{array}
     \right.
$$
Following the standard approach, express the pivot variables in terms of the non-pivot variables and add ``empty equations"". Here $x_3$ and $x_4$ are non-pivot variables.  
\begin{eqnarray*}
\left.
\begin{array}{rcl}
	x_1 & = &1 -x_3+x_4 \\
	x_2 & = &1 +x_3-x_4 \\
	x_3 & = &\phantom{1+~\,}x_3\\
	x_4 & =&\phantom{1+x_3+~}x_4         
\end{array}
     \right\}
     \Leftrightarrow
\colvec{x_1\\x_2\\x_3\\x_4} 
= \colvec{1\\1\\0\\0} + x_3\colvec{-1\\1\\1\\0} + x_4\colvec{1\\-1\\0\\1}
\end{eqnarray*}
The preferred way to write a solution set $S$ is with set notation\index{Solution set!set notation};  \[S = \left\{\colvec{x_1\\x_2\\x_3\\x_4} = \colvec{1\\1\\ 0\\0 } + \mu_1 \colvec{-1\\1\\1\\0 }  + \mu_2  \colvec{1\\-1\\ 0 \\1 } : \mu_1,\mu_2\in  {\mathbb R} \right\} .\]
Notice that the first two components of the second two terms come from the non-pivot columns.
Another way to write the solution set is
\[S= \left\{  x^P  + \mu_1 x^H _1 + \mu_2 x^H _2   : \mu_1,\mu_2 \in  {\mathbb R}   \right\}\, , \]
where 
\[x^P = \colvec{1\\1\\0 \\0 }\, ,\quad x^H _1=\colvec{-1\\1\\1\\0 } \, ,\quad x^H _2= \colvec{1\\-1\\0 \\1 }\, .
\]
Here $x^P $ is a {\it particular solution} while $x^H _1$ and $x^H _2$ are called {\it homogeneous solutions}. The solution set forms a plane.
\end{example}



\subsection{Solutions and Linearity}
%
%\begin{definition}   A function $f$ is \emph{linear}\index{Linear!function} if 
%for any vectors $X,Y$  in the domain of $f$, and any scalars $\alpha,\beta$ 
%\[f(\alpha X + \beta Y) = \alpha f(X) + \beta f(Y) \,.\]
%\end{definition}

%
%
%\begin{example}
%\hypertarget{solution_sets_for_systems_of_linear_equations_concrete_example}{Consider our example system above with} 
%\[
%M=    \begin{pmatrix}
%      1  & 0  & 1 & -1  \\
%       0  & 1 & -1 & 1  \\
%        0 &0   & 0  & 0    \\
%    \end{pmatrix} \, ,\quad
%X= \colvec{x_1\\x_2\\x_3\\x_4} \mbox{ and } Y=\colvec{y_1\\y_2\\y^3 \\y^4 }\, ,
%\]
%and take for the function of vectors
%$$
%f(X)=MX\, .
%$$
%Now let us check the linearity property for $f$. 
%The property needs to hold for {\it any} scalars $\alpha$ and $\beta$, so for simplicity
%let us concentrate first on the case $\alpha=\beta=1$. This means that we need to
%compare the following two calculations:
%\begin{enumerate}
%\item First add $X+Y$, then compute $f(X+Y)$.
%\item First compute $f(X)$ and $f(Y)$, then compute the sum $f(X)+f(Y)$.
%\end{enumerate}
%The second computation is slightly easier:
%$$
%f(X) = MX 
%    =\colvec{x_1+x_3-x_4\\x_2-x_3+x_4\\0}\mbox{ and }
%f(Y) = MY   
%    =\colvec{y_1+y_3-y_4\\y_2-y_3+y_4\\0}\, ,
%$$
%(using our result above). Adding these gives
%$$
%f(X)+f(Y)=\colvec{x_1+x_3-x_4+y_1+y_3-y_4\\[1mm]x_2-x_3+x_4+y_2-y_3+y_4\\[1mm]0}\, .
%$$
%Next we perform the first computation beginning with:
%$$
%X+Y=\colvec{x_1 + y_1\\x_2+y_2\\ x_3+y_3\\ x_4+y_4}\, ,
%$$
%from which we calculate
%$$
%f(X+Y)=\colvec{x_1+y_2+x_3+y_3-(x_4+y_4)\\[1mm] x_2+y_2-(x_3+y_3)+x_4+y_4\\[1mm]0}\, .
%$$
%Distributing the minus signs and remembering that the order of adding numbers like $x_1,x_2,\ldots$ 
%does not matter, we see that the two computations give exactly the same answer.
%
%Of course, you should complain that we took a special choice of $\alpha$ and $\beta$.
%Actually, to take care of this we only need to check that $f(\alpha X)=\alpha f(X)$.
%It is your job to explain this in  \hyperref[linear]{Review Question~\ref*{linear}}
%\end{example}
%
%\noindent
%Later we will show that matrix multiplication is always linear.  Then we will know that:
Motivated by example~\ref{npcd}, we say that the matrix equation $Mx=v$ has  solution set  $\{ x^P  + \mu_1 x^H _1 + \mu_2 x^H _2\,  |\,  \mu_1,\mu_2 \in {\mathbb R} \}$.
\hyperlink{earlier}{Recall}  that matrices are linear operators.
%\[M(\alpha X + \beta Y) = \alpha MX + \beta MY\]
%
%Then 
%
%the two equations 
Thus 
$$M( x^P  + \mu_1 x^H _1 + \mu_2 x^H _2)  = Mx^P  + \mu_1Mx^H _1 + \mu_2Mx^H _2 =v\, ,$$
for \emph{any} $\mu_1, \mu_2 \in \mathbb{R}$. 
Choosing $\mu_1=\mu_2=0$, we obtain 
$$Mx^P =v\, .$$  
This is why $x^P $ is an example of a  \emph{particular solution}\index{Particular solution!an example}.

Setting $\mu_1=1, \mu_2=0$, and subtracting $Mx^P =v$ we obtain 
$$Mx^H _1=0\, .$$ 
Likewise, setting $\mu_1=0, \mu_2=1$, we obtain $$Mx^H _2=0\, .$$
Here $x^H _1$ and $x^H _2$ are examples of what are called  \emph{homogeneous} solutions\index{Homogeneous solution!an example} to the system.
They {\it do not} solve the original equation $Mx=v$, but instead its associated 
{\it homogeneous  equation}\index{homogeneous equation} $M y =0$.

We have just learnt a  fundamental lesson of linear algebra: the  solution set to $Ax=b$, where $A$ is a linear operator, consists of a particular solution plus homogeneous solutions.

\begin{center}
\shabox{ \{Solutions\} $=$ \{Particular solution $+$ Homogeneous solutions\} }
\end{center}

\begin{example}
Consider the matrix equation of example~\ref{npcd}. It has  solution set
\[S = \left\{\colvec{1\\1\\0 \\0 } + \mu_1 \colvec{-1\\1\\1\\0 } + \mu_2 \colvec{1\\-1\\ 0\\1 } \, :\,  \mu_1,\mu_2 \in \Re \right\} \, .\]
Then $Mx^P =v$ says that 
$\colvec{1\\1\\0 \\ 0}$ is a solution to the original matrix equation, which is certainly true, but this is not the only solution.\\

$Mx^H _1=0$ says that $\colvec{-1\\1\\1\\ 0}$ is a solution to the homogeneous equation.

\vspace{2mm}

$Mx^H _2=0$ says that 
$\colvec{1\\-1\\0 \\1}$ is a solution to the homogeneous equation.

\vspace{2mm}

\noindent
Notice how adding any multiple of a homogeneous solution to the particular solution yields another particular solution.
\end{example}


\Reading{SystemsOfLinearEquations}{4}
%\begin{center}\href{\webworkurl ReadingHomework4/1/}{Reading homework: problem 4.1}\end{center}
%\reading{2}{5}


%\begin{center}\href{\webworkurl ReadingHomework4/2/}{Reading homework: problem 4.2}\end{center}

%\section*{References}
%
%Hefferon, Chapter One, Section I.2
%\\
%Beezer, Chapter SLE, Section TSS
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/System_of_linear_equations}{Systems of Linear Equations}


%\subsection{The size of solution sets vs size of homogeneous solution set}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|l|l|}
\hline
Reading problems &
\hwrref{SystemsOfLinearEquations}{4},
\hwrref{SystemsOfLinearEquations}{5}
\\
Solution sets&
\hwref{SystemsOfLinearEquations}{20},
\hwref{SystemsOfLinearEquations}{21},
\hwref{SystemsOfLinearEquations}{22}\\
Geometry of solutions&
\hwref{SystemsOfLinearEquations}{23},
\hwref{SystemsOfLinearEquations}{24},
\hwref{SystemsOfLinearEquations}{25},
\hwref{SystemsOfLinearEquations}{26}\\
\hline
\end{tabular}


\input{\solutionSetsPath/problems}


\newpage


","\chapter{Systems of Linear Equations }
\label{systems}

\section{Gaussian Elimination}
\label{gaussElim}

\hyperlink{system2matrix}{Systems of linear equations can be written as matrix equations.}
Now you will learn  an efficient algorithm  for (maximally) simplifying a system of linear equations (or a matrix equation) -- Gaussian elimination.

%You might get the feeling that you are learning to do Gaussian elimination only so that you can tell your computer how too do it in the future. There is more than that going on here. Let us foreshadow chapter 3; as we attempt to streamline the process of elimination we will discover the building blocks of matrices. 



%In \Lecture~\ref{warmup}  we looked performed elimination on a system of equations. 
%It was nice to do this elimination by adding subtracting an multiplying wholes equations at a time. 
%Lets do another example with the system
%\begin{eqnarray}
%	x\ +\ y & = & 27\nn \\
%	2x-\ y & = &\  0\nn \,.
%\end{eqnarray}
%Adding the first equation to the second then dividing by 3 gives
%\begin{eqnarray}
%	x\ +\ 0 & = & 9\nn \\
%	2x-\ y & = &\  0\nn \,.
%\end{eqnarray}
%Subtracting rice the first from the second then dividing by $-1$ gives
%\begin{eqnarray}
%	x \phantom{\ +\ y}& = &\  9\nn \\
%	\phantom{x\ +\ }y & = & 18\, .\nn
%\end{eqnarray}
%
%\noindent
%The maximum number of terms have been eliminated from each equation, and the result is an obvious statement of the solution. 
%\\
%
%We also learned to write such a linear system using a matrix and two vectors. In this case the original linear system can be written
%
%\begin{equation*}
%    \begin{pmatrix}
%      1             &1  \\
%      2             &-1
%    \end{pmatrix}
%  \colvec{x \\ y}
%  =
%  \colvec{27 \\ 0}\, .
%\end{equation*}
%
%
%\noindent
%Likewise, the system of equations that we obtained after elimination can be written
%
%\begin{equation*}
%    \begin{pmatrix}
%      1             &0  \\
%      0             &1
%    \end{pmatrix}
%  \colvec{x \\ y}
%  =
%  \colvec{9 \\ 18} \, .
%\end{equation*}
%\\
%
%%\noindent
%By the way, the matrix $$I=    \begin{pmatrix}
%      1             &0  \\
%      0             &1
%    \end{pmatrix}$$ is called the \emph{Identity Matrix}\index{Identity matrix!$2\times2$}.  She will be very important to us. You should check that if $v$ is any vector, then $$Iv=v\, .$$
%    
%Here is a nice way to summarize your goal when performing the process we are calling elimination elimination; manipulate the system of equations until the resulting system can be written as a matrix equation with the identity matrix.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Augmented Matrix Notation}

%A useful shorthand for a  system of linear equations is an \hypertarget{augmented_matrix}{\emph{Augmented Matrix}}\index{Augmented matrix~$2\times2$}. 
Efficiency demands a new notation, called an \hypertarget{augmented_matrix}{\emph{augmented matrix}},
which we  introduce via examples: 

The linear system
$$\left\{ 
\begin{array}{rcr}
	x\ +\ y & = & 27 \\
	2x-\ y & = &\  0\, ,
\end{array}\right.
$$
is denoted by the augmented matrix

\[
\begin{amatrix}{2}
1 &1 &27 \\ 2 &-1 & 0
\end{amatrix}\, .
\]

\noindent
This notation is  simpler than the matrix one, 
\begin{equation*}
    \begin{pmatrix}
      1             &1  \\
      2             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{27 \\ 0}\, ,
\end{equation*}
although all three of the above denote the same thing. 

\Videoscriptlink{gaussian_elimination_more_background.mp4}{Augmented Matrix Notation}{script_gaussian_elimination_more}

\noindent
Another interesting rewriting is
$$
x\colvec{1\\2}+y\colvec{1\\-1}=\colvec{27\\0}\, .
$$
This tells us that we are trying to find the combination of the vectors $\colvec{1\\2}$~and $\colvec{1\\-1}$ adds up to $\colvec{27\\0}$; 
 the answer is  ``clearly'' $9\colvec{1\\2}+18\colvec{1\\-1}$.\\[1mm]


Here is a larger example.
The system
\begin{eqnarray*}
1x + 3y + 2z + 0w   &=&9 \\ 
6x + 2y + 0z   -2w  &=&0  \\
-1x+ 0 y + 1 z + 1w  &=&3 \, ,
\end{eqnarray*}
is denoted by the augmented matrix
\[
\begin{amatrix}{4}
1 & 3 & 2 & 0  & 9 \\ 
6 & 2 & 0  &\!\! -2 & 0  \\
-1& 0  & 1  & 1 & 3
\end{amatrix}\, ,
\]
%When writing a system of equations one can write out the terms with zero for coefficients or not
%\begin{eqnarray*}
%1x + 3y + 2z  \phantom{+ 0w}   =9 \\ 
%6x + 2y \phantom{ + 0z}   -2w  =0  \\
%-1x  \phantom{+0 y} + 1 z + 1w  =3 
%\end{eqnarray*}
which is equivalent to the matrix equation
\begin{eqnarray*}
\left(\begin{array}{rccr}
1 & 3 & 2 & 0   \\ 
6 & 2 & 0  & \!\!-2   \\
\!\!-1& 0  & 1  & 1 
\end{array}\right)
\colvec{ x\\ y\\z\\w}
=\colvec{ 9\\0\\3}\, .
\end{eqnarray*}
Again, we are trying to find which combination of the columns of the matrix adds up to the vector on the right hand side.



For the the general case of $r$ linear equations in $k$ unknowns,
the number of equations is the number of rows $r$ in the augmented matrix, and the number of columns $k$ in the matrix left of the vertical line is the number of unknowns, giving an augmented matrix of the form
\[
\begin{amatrix}{4}
a^1_1 & a^1_2 & \cdots & a^1_k & b^1 \\[1mm]
a^2_1 & a^2_2 & \cdots & a^2_k & b^2 \\[1mm] 
\vdots & \vdots & & \vdots & \vdots  \\[1mm]
a^r_1 & a^r_2 & \cdots & a^r_k & b^r 
\end{amatrix}.
\]
Entries left of the divide carry two indices; subscripts denote column number and  superscripts row number. We emphasize, the superscripts here do {\it not} denote exponents.  
%Aside: most people don't like  indexing by superscripts at first. However, kind of index placement will later facilitate Einstein's summation convention, which Einstein himself described as his greatest contribution to the sciences. 
Make sure you can write out the system of equations and the associated matrix equation for any augmented matrix. 

%\reading{2}{1}
\Reading{SystemsOfLinearEquations}{1}

We now have three ways of writing the same question. 
Let's put them side by side as we solve the system by strategically adding and subtracting equations. 
We will not tell you the motivation for this particular series of steps yet, but let you develop some intuition first. 

\begin{example}  (How matrix equations and augmented matrices change in elimination)
\begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	x&+&y & = & 27 \\
	2x&-& y & = &\  0 
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      1             &1  \\
      2             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{27 \\ 0}
  \Leftrightarrow
 \begin{amatrix}{2}
1 &1 &27 \\ 2 &-1 & 0
\end{amatrix}\, .
  \end{eqnarray*}
With the first equation replaced by the sum of the two equations this becomes
\begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	3x& +& 0 & = & 27 \\
	2x&-& y & = &\  0 
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      3             &0  \\
      2             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{27 \\ 0}
  \Leftrightarrow
 \begin{amatrix}{2}
3 &0 &27 \\ 2 &-1 & 0
\end{amatrix}\, .
  \end{eqnarray*}
Let the new first equation be the old first equation divided by 3:
 \begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	x& +& 0 & = & 9 \\
	2x&-& y & = &\  0 
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      1             &0  \\
      2             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{9 \\ 0}
  \Leftrightarrow
 \begin{amatrix}{2}
1 &0 &9 \\ 2 &-1 & 0
\end{amatrix}\, .
  \end{eqnarray*}
  Replace the second equation by the second equation minus two times the first equation: 
\begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	x& +& 0 & = & 9 \\
	0&-& y & = &\  -18
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      1             &0  \\
      0             &-1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{9 \\ -18}
  \Leftrightarrow
 \begin{amatrix}{2}
1 &0 &9 \\ 0 &-1 & -18
\end{amatrix}\, .
  \end{eqnarray*}
Let the new  second equation be the old second equation divided by -1:
\begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	x &+ &0 & = & \ 9 \\
	0& + &y & = &  18
     \end{array}
   \right\} 
   \Leftrightarrow
    \begin{pmatrix}
      1             &0  \\
      0             &1
    \end{pmatrix}
  \colvec{x \\ y}
  =
  \colvec{9 \\ 18}
  \Leftrightarrow
 \begin{amatrix}{2}
1 &0 &9 \\ 0 &1 & 18
\end{amatrix}\, .
  \end{eqnarray*}
\end{example}
Did you see what the strategy was? To {\it eliminate} $y$ from the first equation and then {\it eliminate} $x$ from the second. The result was  the solution to the system. 

Here is the big idea: 
Everywhere in the instructions above we can replace the word ``equation"" with the word ``row"" and interpret them as telling us what to do with the augmented matrix instead of the system of equations.
Performed systemically, the result is the {\bf Gaussian elimination}\index{Gaussian elimination} algorithm. 
%
%\begin{example} of Gaussian elimination
%\begin{eqnarray*}
% \begin{amatrix}{2}
%1 &1 &27 \\ 2 &-1 & 0
%\end{amatrix}
%  \end{eqnarray*}
%Let the new first row be the sum of the first and second rows
%\begin{eqnarray*}
% \begin{amatrix}{2}
%3 &0 &27 \\ 2 &-1 & 0
%\end{amatrix}
%  \end{eqnarray*}
%Let the new first row be the old first row divided by 3
% \begin{eqnarray*}
% \begin{amatrix}{2}
%1 &0 &9 \\ 2 &-1 & 0
%\end{amatrix}
%  \end{eqnarray*}
%Let the new second row be the old second row minus two times the first row 
%\begin{eqnarray*}
% \begin{amatrix}{2}
%1 &0 &9 \\ 0 &-1 & -18
%\end{amatrix}
%  \end{eqnarray*}
%Let the new  second row be the old second row divided by -1
%\begin{eqnarray*}
% \begin{amatrix}{2}
%1 &0 &9 \\ 0 &1 & 18
%\end{amatrix}
%  \end{eqnarray*}
%The solution can be read off very quickly, and the notation was very minimal. 
%\end{example}
% At each step, the augmented matrices encode systems of equations which have the same solutions. Lets make this idea more formal, and introduce some notation to convey the idea.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Equivalence and the Act of Solving}


%Two augmented matrices corresponding to linear systems 
%%{\it that actually have solutions} %cmon, this idea has not even been introduced
%are said to be \hypertarget{roweq}(row) 
%\emph{equivalent}\index{Row equivalence} if they have the \emph{same} solutions.
%To denote this 

We now introduce the symbol $\sim$ which is called ``tilde"" but should be read as  ``is (row) equivalent to''
because at each step the augmented matrix changes by an operation on its rows but its solutions do not. For example, we found above that
\[
\begin{amatrix}{2}
1 &1 &27 \\ 2 &-1 &0
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &0 &9 \\ 2 &-1 & 0
\end{amatrix}
\sim
\begin{amatrix}{2}
1 & 0&9 \\   0& 1 & 18
\end{amatrix}\, .
\]
The last of these augmented matrices is our favorite!
\Videoscriptlink{gaussian_elimination_background.mp4}{Equivalence Example}{script_gaussian_elimination_background}

%\videoscriptlink{gaussian_elimination_3_3_example.mp4}{A $3 \times 3$ example}{scripts_gaussian_elimination_3_3_example}

Setting up a string of equivalences like this is a means of solving a system of linear equations. This is the main idea of section~\ref{RREF}.
This next example hints at the main trick:

\begin{example} (Using Gaussian elimination to solve a system of linear equations)
\begin{eqnarray*}
   \left.
\begin{array}{lcr}
	x +\ y & = & 5 \\
	x + 2y & = &\  8
     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{2}
1 &1 &5 \\ 1 &2 & 8
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &1 &5 \\ 0 &1 & 3
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &0 &2 \\ 0 &1 & 3
\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{lcr}
	x + 0 & = & 2 \\
	 0 + y & = &\  3
     \end{array}
   \right.
\end{eqnarray*}  
Note that in going from the first to second augmented matrix, we used the top left $1$ to make the bottom left entry zero. For this reason we call the top left entry a pivot. 
Similarly, to get from the second to third augmented matrix,  the bottom right entry (before the divide) was used to make the top right one vanish; so the bottom right entry is also called a pivot. 
\end{example}

This name {\it pivot}  is  used to indicate the matrix
entry used to ``zero out''  the other entries in its column; the pivot is the number used to eliminate another number in its column.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Reduced Row Echelon Form}\label{RREF}
For a system of two linear  equations, the goal of Gaussian elimination is to convert the part of the augmented matrix left of the dividing line into the matrix
 $$I=    \begin{pmatrix}
      1             &0  \\
      0             &1
    \end{pmatrix}\, ,$$ 
called the \emph{Identity Matrix}\index{Identity matrix!$2\times2$}, since this would give the simple statement of a solution $x=a,y=b$. The same goes for  larger systems of equations
for which the identity matrix~$I$ has 1's along its \hyperlink{diagonal}{diagonal} and all off-diagonal entries vanish:
\begin{center}
\shabox{$I=\left(\begin{array}{ccccc}1&0&\cdots&0\\0&1&&0\\ \vdots&&\ddots&\vdots\\0&0&\cdots&1\end{array}\right)$}
\end{center}

%\reading{2}{2} %a 3x3 example with one solution>
\Reading{SystemsOfLinearEquations}{2}


\vspace{1mm}
\noindent
For many systems, it is not possible to reach the identity in the augmented matrix via Gaussian elimination. In any case, a certain version of the matrix that has the maximum number of components eliminated is said to be the Row Reduced Echelon Form (RREF). 

\begin{example}\label{redundant} (Redundant equations)
 \begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	 x& + & y & = & 2 \\[1mm]
	2x& + &2y & = &  4
     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{2}
1 &1 &2 \\[1mm] 2 &2 & 4
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &1 &2 \\[1mm] 0 &0 & 0
\end{amatrix}
%\sim
%\begin{amatrix}{2}
%1 &0 &2 \\ 0 &1 & 3
%\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{ccccr}
	x &+& y & = & 2 \\[1mm]
	 0& +& 0 & = &  0
     \end{array}
   \right.
\end{eqnarray*}  
This example demonstrates if one equation is a multiple of the other the identity matrix can not be a reached. This is because the first step in elimination will make the second row a row of zeros. Notice that solutions still exists $(1,1)$ is a solution. The last augmented matrix here is in RREF; no more than two components  can be eliminated.
\end{example}

\begin{example} (Inconsistent equations)
 \begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	x &+ & y & = & 2 \\[1mm]
	2x &+ &2y & = &  5
     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{2}
1 &1 &2 \\[1mm] 2 &2 & 5
\end{amatrix}
\sim
\begin{amatrix}{2}
1 &1 &2 \\[1mm] 0 &0 & 1
\end{amatrix}
%\sim
%\begin{amatrix}{2}
%1 &0 &2 \\ 0 &1 & 3
%\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{ccccr}
	x &+& y & = & 2 \\[1mm]
	 0 &+& 0 & = &  1
     \end{array}
   \right.
\end{eqnarray*}  
This system of equation has a solution if there exists two numbers $x$, and $y$ such that $0+0=1$. That is a tricky way of saying there are no solutions. The last form of the augmented matrix here is the RREF.
\end{example}


\begin{example} (Silly order of equations)\\
A robot might make this mistake:
 \begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	0x &+&  y & = & -2 \\[1mm]
	x& +& y & = &  7
     \end{array}
   \right\} 
   \Leftrightarrow 
\begin{amatrix}{2}
0 &1 & -2\\[1mm] 1 &1 & 7
\end{amatrix}
\sim \, \cdots\, ,
\end{eqnarray*}  
and then give up because the the upper left slot can not function as a pivot since the~0 that lives there can not be used to eliminate the zero below it. Of course, the right thing to do is to change the order of the equations before starting
 \begin{eqnarray*}
   \left.
\begin{array}{ccccr}
	 x &+ &y & = &  7
	\\[1mm]
	0x &+ & \ y & = & -2 
	     \end{array}
   \right\} 
   \Leftrightarrow
\begin{amatrix}{2}
1 &1 & 7\\[1mm] 0 &1 & -2
\end{amatrix}
\sim 
\begin{amatrix}{2}
1 &0 & 9\\[1mm] 0 &1 & -2
\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{ccccr}
	x &+& 0 & = & 9\phantom{\, .} \\[1mm]
	 0 &+& y & = & -2\, .
     \end{array} 
   \right.
\end{eqnarray*}  
The third augmented matrix above  is the RREF of the first and second. That is to say, you can swap rows on your way to RREF.
\end{example}



For larger systems of equations redundancy and inconsistency are the obstructions to obtaining the identity matrix, and hence to a simple statement of a solution in the form $x=a,y=b,\ldots$ . 
What can we do to maximally simplify a system of equations in general?
We need to perform operations that simplify our system {\it without changing its solutions}.
Because, exchanging the order of equations, multiplying one equation by a {\it non-zero} constant or adding equations does not 
change the system's solutions, we are lead to three operations:
\begin{itemize}
\item (Row Swap) Exchange any two rows.
\item (Scalar Multiplication) Multiply any row by a non-zero constant.
\item (Row Addition) Add 
%a multiple of 
%%% Any row op can be built out of these three... and with the comment out wording there is some redundancy
one row to another row.
\end{itemize}
These are called \emph{Elementary Row Operations}\index{EROs}, or EROs for short, and are studied in detail in section~\ref{EROS}. 
Suppose now we have a general augmented matrix for which the first entry in the first row does not vanish. 
Then, using just the three EROs, we could\footnote{This is a ``brute force"" algorithm;  there will often be more efficient ways to get to RREF.} then perform the following.\\

\newpage
\begin{center}
{\bf {\Large Algorithm For Obtaining RREF:}}
\end{center}
\begin{itemize}
\item Make the leftmost nonzero entry in the top row 1 by multiplication.  
\item Then use that 1 as a pivot to eliminate everything below it. 
\item Then go to the next row and make the leftmost nonzero entry 1. 
\item Use that 1 as a pivot to eliminate everything below {\it and above it}! 
\item Go to the next row and make the leftmost nonzero entry 1... {\it etc}
\end{itemize}
In the case that the first entry of the first row is zero, we may first interchange the first row with another row whose first entry is non-vanishing  and then perform the above algorithm. If the entire first column vanishes, we may still apply the algorithm on the remaining columns.

Here is a video (with special effects!) of a hand performing the algorithm by hand. When it is done, you should try doing what it does.
\videoscriptlink{gaussian_elimination_Beginner_Elimination.mp4}{Beginner Elimination}{script_gaussian_elimination_more}

\noindent
This algorithm and its variations is known as Gaussian elimination. The endpoint of the algorithm is  an augmented matrix of the form \label{Reduced row echelon form} 
$$\left(
\begin{array}{cccccccc|c}
1       	& * & 0		& * & 0		& \cdots& 0	&*	& b^1 \\[.5mm] 
0	        & 0 & 1		& * & 0		& \cdots& 0	&*	& b^2 \\[.5mm]
0		& 0& 0		& 0 & 1		& \cdots& 0	&*	& b^3 \\[.5mm]  
\vdots  	& \vdots& \vdots	&   & \vdots &	& 	& 
\vdots			& \vdots \\[2mm]  
0		& 0&	0		&  0& 0			&  \cdots   & 1		&*	& b^k \\[.5mm]  
0		& 0 & 0		& 0 & 0		& \cdots& 0 	&0	& b^{k+1} \\[.5mm] 
\vdots  	& \vdots & \vdots	&  \vdots & \vdots	& 	& \vdots	&\vdots	& \vdots \\[.5mm]  
0		&  0 & 0		& 0 & 0		& \cdots& 0		 & 0& b^r \\ 
\end{array}\!\right).$$
This is called 
\emph{Reduced Row Echelon Form}\index{Reduced row echelon form} (RREF).
The asterisks denote the possibility of arbitrary numbers ({\it e.g.}, the second 1 in the top line of example~\ref{redundant}).

Learning to perform this algorithm by hand is the first step to learning linear algebra; it will be the primary means of computation for this course. You need to learn it well. So start practicing as soon as you can, and practice often. 


\newpage
\begin{center}
\Large{{\bf The following properties define RREF:}}
\end{center}
\begin{enumerate}
\item  In every row  the left most non-zero entry is  $1$ (and is called a pivot).
\item The pivot of any given row is always to the right of the pivot of the row above it.
\item The pivot is the only non-zero entry in its column.
\end{enumerate}
%Here are some examples:
\begin{example}\label{augrref} (Augmented matrix in RREF)
$$
\begin{amatrix}{3} 
1 & 0 & 7 & 0 \\ 
0 & 1 & 3 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 \\
\end{amatrix}
$$
\end{example}

\begin{example} (Augmented matrix NOT in RREF)
$$
\begin{amatrix}{3} 
1 & 0 & 3 & 0 \\ 
0 & 0 & 2 & 0 \\
0 & 1 & 0 & 1 \\
0 & 0 & 0 & 1 \\
\end{amatrix}
$$
Actually, this NON-example breaks all three of the rules!
\end{example}














The reason we need the asterisks in the general form of RREF is that
not every column need have a pivot, as demonstrated in examples~\ref{redundant} and~\ref{augrref}. 
Here is an example where multiple columns have no pivot:

\begin{example} (Consecutive  columns with no pivot in RREF)
 \begin{eqnarray*}
   \left.
\begin{array}{cccccccccr}
	 x & + & y &+ & z& + & 0w & = & 2 \\
	2x& +& 2y& +&2z&+&2w & = &  4
     \end{array}
   \right\} 
   &\!\Leftrightarrow\!&
\begin{amatrix}{4}
1 &1 &1 & 0 &2 \\ 
2 &2 &2 & 1 & 4
\end{amatrix}
\sim
\begin{amatrix}{4}
1 &1 &1 & 0 &2 \\ 
0 &0 &0 & 1 & 0
\end{amatrix}\\[2mm]
&\!\Leftrightarrow\!&
\left\{
\begin{array}{rrrrrr}
	x \ +\  y\  + \  z&&& = & 2 \phantom{\, .}\\
	&& w & = &  0\, .
     \end{array}
   \right.
\end{eqnarray*}  
Note that there was no hope of reaching the identity matrix, because of the shape of the augmented matrix we started with. 
\end{example}

With some practice, elimination can go  quickly. Here is an expert showing you some tricks. If you can't follow him now then come back when you have some more experience and watch again. You are going to need to get really good at this!

\videoscriptlink{gaussian_elimination_Advanced_Elimination.mp4}{Advanced Elimination}{script_gaussian_elimination_more}

It is important that you are able to convert RREF back into a system of equations. The first thing you might notice is that
if any of the numbers 
$b^{k+1},\dots, b^r$ in~\ref{Reduced row echelon form}~are non-zero then the system of equations is inconsistent and has no solutions. 
Our next task is to extract all possible solutions from an RREF  augmented matrix.

\subsection{Solution Sets and RREF}
%While RREF is not always pretty, it is certainly useful. 
%Our goal  is to solve systems of linear equations. 
RREF is a maximally simplified version of the original system of equations in the following sense: 
\begin{itemize}
\item As many coefficients of the variables as possible are $0$. 
\item As many coefficients  of the variables as possible are $1$.
\end{itemize}
It is easier to read off solutions from the maximally simplified equations than from the original equations, even when there are infinitely many solutions.

\begin{example}{(Standard approach  from a system of equations to the solution set)}
 \begin{eqnarray*}
\left.
\begin{array}{ccccccccr}
	x & +&y  && & +& 5w &   =& 1 \\
	 &&y & &   &+& 2 w & = &6 \\
	&&&& z&+&         4w & = &8
\end{array}
 \right\}
 \Leftrightarrow
 \begin{amatrix}{4} 
1 & 1 & 0 & 5 & 1 \\ 
0 & 1 & 0 & 2 & 6 \\
0 & 0 & 1 & 4 & 8 
\end{amatrix}
\sim
 \begin{amatrix}{4} 
1 & 0 & 0 & 3 & -5 \\ 
0 & 1 & 0 & 2 & 6 \\
0 & 0 & 1 & 4 & 8 
\end{amatrix}
\\[4mm]
%
\Leftrightarrow
\left\{
\begin{array}{cccccccccr}
	x &&& + &3w &  =& -5 \\[.5mm]
	&   y && +& 2 w & = &6 \\[.5mm]
        && z&+ &        4w & = &8
     \end{array}
     \right\}
\Leftrightarrow
\left\{
\begin{array}{lcrccr}
	x & =& -5& -&3w \\[.5mm]
	 y  & =& 6 &-&2w\\[.5mm]
	 z & = &8&-&4w \\[.5mm]
	w & =&&&w          
     \end{array}
     \right\}
 \\[4mm]
\Leftrightarrow
\colvec{x\\[.5mm]y\\[.5mm]z\\[.5mm]w} = \colvec{-5\\[.5mm]6\\[.5mm]8\\[.5mm]0} + w\colvec{-3\\[.5mm]-2\\[.5mm]-4\\[.5mm]1}\, .
\end{eqnarray*}
%This brings up the issue of what constitutes a solution to a system of equations; the last thing written above was a vector equation... and if it sounds wrong to say  ``an equation is a solution to an equation"" you are in good company.  
%There are infinitely many solutions to the , one for each value of $w$. 
%For example
%$$ \colvec{-5\\[.5mm]6\\[.5mm]8\\[.5mm]0} 
%+ 
%\left( -.75\right)^{\pi-1}\colvec{-3\\[.5mm]-2\\[.5mm]-4\\[.5mm]1}\, 
%$$
%is one of the solutions. Notice that this solution is not an equation; it is an ordered set of numbers that, if substituted  for the variables in the equations, yields true statements. This might be annoying to check because of the annoying number $(-.75)^{\pi-1}$ we chose to replace $w$. We just wanted to show you that ANY number can go there. You probably want a way to check a solution which is not annoying. A good check is to see if the system is solved by the expression with $w$ replaced by $0$. Indeed
%\begin{eqnarray*}
% \colvec{-5\\[.5mm]6\\[.5mm]8\\[.5mm]0} {\text ~is~a ~solution ~to \qquad}
%\begin{array}{ccccccccr}
%	x & +&y  && & +& 5w &   =& 1 \\
%	 &&y & &   &+& 2 w & = &6 \\
%	&&&& z&+&         4w & = &8
%\end{array}
%\\ 
%{\text  ~because \qquad}
%\begin{array}{ccccccccr}
%	-5 & +&6  && & +& 5(0) &   =& 1 \\
%	 &&6 & &   &+& 2 (0) & = &6 \\
%	&&&& 8&+&         4(0) & = &8
%\end{array}.
%\end{eqnarray*}
%Given that there are many solutions, what ought one report when asked to solve a system of equations? The standard thing to report is {\it the solution set}\index{solution set}\index{Solution set}; 
%There is a solution for each value of $w$. Better said 
%\begin{eqnarray*}
%{\text the ~solution~ set~ to \qquad}
%\begin{array}{ccccccccr}
%	x & +&y  && & +& 5w &   =& 1 \\
%	 &&y & &   &+& 2 w & = &6 \\
%	&&&& z&+&         4w & = &8
%\end{array}
%is \qquad\left\{    \colvec{-5\\[.5mm]6\\[.5mm]8\\[.5mm]0} 
%+ 
%\alpha \colvec{-3\\[.5mm]-2\\[.5mm]-4\\[.5mm]1} : \alpha \in \mathbb{R} \right\}
%\end{eqnarray*}
There is one solution for each value of $w$, so the solution set is 
$$
\left\{    \colvec{-5\\[.5mm]6\\[.5mm]8\\[.5mm]0} 
+ 
\alpha \colvec{-3\\[.5mm]-2\\[.5mm]-4\\[.5mm]1} : \alpha \in \mathbb{R} \right\}.
$$
\end{example}
Here is a verbal description of the preceeding example of the \hypertarget{standard approach}{{\it standard approach}}. We say that $x,y$, and $z$ are {\it pivot variables}\index{Pivot variables} because they appeared with a  pivot coefficient in RREF. 
Since $w$ never appears with a pivot  coefficient, 
 it is not a pivot variable. %We call it a free variable. 
%One way to reveal the solutions to this system of equations is to 
In the second line we put all the pivot variables on one side 
and all the {\it non-pivot variables}\index{Non-pivot variables} on the other side and added the trivial equation $w=w$ to obtain a system that allowed us to easily read off solutions.

\begin{center}
{\Large{\bf The Standard Approach To Solution Sets}}
\end{center}
\begin{enumerate}
\item Write the augmented matrix.
\item Perform EROs to reach RREF.
\item Express the pivot variables in terms of the non-pivot variables. 
\end{enumerate}
There are always exactly enough non-pivot variables to index your solutions. 
In any approach, the variables which are not expressed in terms of the other variables are called  {\it free variables}\index{free variables}. The standard approach is to use the non-pivot variables as free variables.

%vid for this?! 
Non-standard approach: solve for $w$ in terms of $z$ and substitute into the other equations. You now have an expression for each component in terms of $z$. But why pick $z$ instead of $y$ or $x$? (or $x+y$?) The standard approach not only feels natural, 
but is {\it canonical}, meaning that everyone will get the same RREF and hence choose the same variables to be free.
However, it is important to remember that so long as their {\it set} of solutions is the same, any two choices of free variables is fine.
(You might think of this as the difference between using Google Maps$^{\sf TM}$ or Mapquest$^{\sf TM}$; although their maps may look different, 
the place 
$\langle$home {\it sic}$\rangle$ 
they are describing is the same!)


When you see an RREF augmented matrix with two columns that have no pivot, you know there will be two free variables. 

\begin{example}{(Standard approach, multiple free variables)}

 \begin{eqnarray*}
 \begin{amatrix}{4} 
1 & 0 & 7 & 0 & 4 \\ 
0 & 1 & 3 & 4 & 1 \\ 
0 & 0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0 & 0 \\ 
\end{amatrix}
\Leftrightarrow
\left\{
\begin{array}{lcr}
	x \phantom{+y}    + 7z  \phantom{+w} & = 4 \\
	\phantom {x+}   y + 3z  {+4w} & = 1 \\
	%\phantom{x+y+z+}          w & = 2
     \end{array}
     \right\}
%
\\
\Leftrightarrow 
\left\{
\begin{array}{rlcrccrr}
	x & = \ 4&\! -\!&7z& \\
	 y  & = \ 1& \!-\!&3z&\!-\!&4w\\
	 z   & = &&z\\
	w & =&&&&w          
     \end{array}
     \right\}
     \Leftrightarrow
\colvec{x\\y\\z\\w} = \colvec{4\\1\\0\\0} + z\colvec{-7\\-3\\1\\0} + w\colvec{0\\-4\\0\\1}
\end{eqnarray*}
%There are infinitely many solutions; one for each pair of numbers $z,w$. 
so the solution set is 
$$  \left\{  \colvec{4\\1\\0\\0} + z\colvec{-7\\-3\\1\\0} + w\colvec{0\\-4\\0\\1} : z,w\in \mathbb{R} \right\}. $$
\end{example}

%the youtube version
%\begin{center}
%\href{http://www.youtube.com/watch?v=87iKQG6PJvM}{\raisebox{-.4cm}{\includegraphics[scale=.075]{take1.jpg}}}\hspace{1cm}\scalebox{1.2}{From RREF to a Solution Set}
%\end{center}


\begin{center}
\Videoscriptlink{solution_set.mp4}{From RREF to a Solution Set}
{}
\end{center}

You can imagine having three, four, or fifty-six non-pivot columns and the same number of free variables indexing your solutions set. 
In general a solution set to a system of equations with $n$ free variables will be of the form 
\begin{center}
\shabox{
$
\{ x^P +\mu_1 x^H _1 + \mu_2 x^H _2 + \cdots + \mu_nx^H _n : \mu_1, \dots, \mu_n \in \mathbb{R} \} .$
}
\end{center}

The parts of these solutions play special roles in the associated matrix equation. This will come up again and again long after we complete this discussion of basic calculation methods, so we will use the general language of linear algebra to give names to these parts now. \\

\noindent {\bf Definition:} A {\bf homogeneous solution} to a linear equation $Lx=v$, with $L$ and $v$ known is a vector $x^H $ such  that $Lx^H =0$ where $0$ is the zero vector. \\

\shabox{
If you have a particular solution $x^P $ to a linear equation and add a sum of multiples of homogeneous solutions to it you obtain another particular solution. }\\
%The equation $Lx=0$ is called the  homogeneous equation associated to $.



\begin{center}
\Videoscriptlink{particular_homogeneous.mp4}{\hspace{-8mm}Particular and Homogeneous Solutions\hspace{-8mm}}{}
%youtube version
%\href{http://www.youtube.com/watch?v=6a_sT06Kti4}{\raisebox{-.4cm}{\includegraphics[scale=.075]{take1.jpg}}}\hspace{1cm}\scalebox{1.2}{Particular and Homogeneous Solutions}
\end{center}



Check now that the parts of the solutions with free variables as coefficients from the previous examples are homogeneous solutions, and that by adding a homogeneous solution to a particular solution one obtains a solution to the matrix equation. This will come up over and over again. As an example without matrices, consider the differential equation $\frac{d^2}{dx^2} f=3$. A particular solution is $\frac32x^2$ while $x$ and $1$ are homogeneous solutions. The solution set is $\{ \frac32 x^2+ax +c1 ~:~a,b\in\mathbb{R} \}$. You can imagine similar differential equations with more homogeneous solutions. \\


You need to become very adept at reading off solutions sets of linear systems from the RREF
of their augmented matrix; it is a basic skill for linear algebra, and we will continue using it up to the last page of the book! 





\Videoscriptlink{elementary_row_operations_worked_examples.mp4}{\hspace{-8mm}Worked examples of Gaussian elimination\hspace{-8mm}}{scripts_elementary_row_operations_worked_examples}


%This example emphasizes different aspects.
%\begin{example}
%$$
%\begin{amatrix}{5} 
%1 & 1 & 0 & 1 & 0 & 1\\ 
%0 & 0 & 1 & 2 & 0 & 2\\ 
%0 & 0 & 0 & 0 & 1 & 3\\ 
%0 & 0 & 0 & 0 & 0 & 0
%\end{amatrix}\, .
%$$
%Here we were not told the names of the variables, so lets just call them $x_1,x_2,x_3,x_4,x_5$.
%(There are always as many of these as there are columns in the matrix before the vertical line; the number of rows,
%on the other hand is the number of linear equations.)
%
%To begin with we immediately notice that there are no pivots in the second and fourth columns so, as per the standard approach, we will be all variables in terms of $x_2$ and $x_4$. 
%Next we see from the second last row that $x_5=3$. The second row says 
%$x_3=2-2x_4=2-2 x_2$.
%The top row then gives $x_1=1-x_2-x_4=1-x_1-x_2$. Again we can write this solution as a vector
%$$
%\colvec{1\\0\\2\\0\\3}+x_1\colvec{-1\\1\\0\\0\\0}+x_2\colvec{-1\\0\\-2\\1\\0}\, .
%$$
%Observe, that since no variables were given at the beginning, we can use any symbols instead of $x_1$ and $x_2$. For example lower case greek letter lambda:
%$$
%\colvec{1\\0\\2\\0\\3}+\lambda_1\colvec{-1\\1\\0\\0\\0}+\lambda_2\colvec{-1\\0\\-2\\1\\0}\, .
%$$
%As a challenge, look carefully at this solution and make sure you can see how every part of it comes from
%the original augmented matrix without every having to reintroduce variables and equations.
%\end{example}
%
%




%\begin{theorem}
%Every augmented matrix is row-equivalent to a \emph{unique} augmented matrix in reduced row echelon form.
%\end{theorem}
%
%\noindent
%In \Lecture~\ref{elemRowOpsPath}, we will see why this is true.

%\section*{Uniqueness of RREF}
%
%\begin{theorem}\label{GJEunique} Gauss-Jordan Elimination produces a unique augmented matrix in RREF.
%\end{theorem}
%
%\begin{proof}
%Suppose Alice and Bob compute the RREF for a linear system but get different results, $A$ and $B$.  Working from the left, discard all columns except for the pivots and the first column in which $A$ and $B$ differ.  By \hyperref[colremove]{Review Problem~\ref{colremove}}, removing columns does not affect row equivalence.  Call the new, smaller, matrices $\hat{A}$ and $\hat{B}$.  The new matrices should look this: $$\hat{A}=\begin{amatrix}{1}
%I_N & a\\
%0 & 0
%\end{amatrix} \mbox{ and } \hat{B}=\begin{amatrix}{1}
%I_N & b\\
%0 & 0
%\end{amatrix}\, ,$$ where $I_N$ is an $N\times N$ identity matrix and $a$ and $b$ are vectors.
%
%Now if $\hat{A}$ and $\hat{B}$ have the same solution, then we must have $a=b$.  But this is a contradiction!  Then $A=B$.
%\end{proof}
%
%\videoscriptlink{elementary_row_operations_proof.mp4}{Explanation of the proof}{scripts_elementary_row_operations_proof}


%\References{
%Hefferon, Chapter One, Section 1
%\\
%Beezer, Chapter SLE, Section RREF
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/Row_echelon_form}{Row Echelon Form}}

%\newpage

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading problems &
\hwrref{SystemsOfLinearEquations}{1}, \hwrref{SystemsOfLinearEquations}{2}\\
Augmented matrix &  \hwref{SystemsOfLinearEquations}{6}\\
$2\times2$ systems &  \hwref{SystemsOfLinearEquations}{7},
\hwref{SystemsOfLinearEquations}{8},
\hwref{SystemsOfLinearEquations}{9},
\hwref{SystemsOfLinearEquations}{10},
\hwref{SystemsOfLinearEquations}{11},
\hwref{SystemsOfLinearEquations}{12}\\
$3\times2$ systems & 
\hwref{SystemsOfLinearEquations}{13},
\hwref{SystemsOfLinearEquations}{14}
\\
$3\times3$ systems & 
\hwref{SystemsOfLinearEquations}{15},
\hwref{SystemsOfLinearEquations}{16},
\hwref{SystemsOfLinearEquations}{17}
\\
\hline
\end{tabular}

\input{\gaussElimPath/problems}



%\chapter{\elemRowOpsTitle}
%FOR THE BOOK
\section{\elemRowOpsTitle}

\label{EROS}
%\hypertarget{Elementary Row Operations} %what is this? 

Elementary row operations are  systems of linear equations relating the old and new rows in Gaussian elimination: 


\begin{example}\label{Rsystem} (\hypertarget{Keeping track of EROs with equations between rows}{Keeping track of EROs with equations between rows})\\
We refer to the new $k$th row as $R'_k$ and the old $k$th row as $R_k$.
\begin{equation*}
\begin{array}{ccc|r}
\begin{amatrix}{3} 
0 & 1 & 1 & 7 \\ 
2 & 0 & 0& 4 \\
0& 0 & 1 & 4 \\
\end{amatrix} 
& \stackrel{R_1'=0R_1+\phantom{1}R_2+0R_3}{\stackrel{R_2'=\phantom{1}R_1+0R_2+0R_3}{ \stackrel{R_3'= 0R_1+0R_2+\phantom{1}R_3}{\stackrel{}{\sim}}}}&
\begin{amatrix}{3} 
2 & 0 & 0 & 4 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
&\colvec{R_1'\\R_2'\\R_3'}=\begin{pmatrix}0&1&0\\1&0&0\\0&0&1\end{pmatrix}\colvec{R_1\\R_2\\R_3}
\\[.8cm]
& \stackrel{R_1'=\frac12 R_1+0R_2+0R_3}{\stackrel{R_2'=0R_1+\phantom{1}R_2+0R_3}{ \stackrel{R_3'= 0R_1+0R_2+\phantom{1}R_3}{\stackrel{}{\sim}}} }&
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
&\colvec{R_1'\\R_2'\\R_3'}=\begin{pmatrix}\frac12&0&0\\0&1&0\\0&0&1\end{pmatrix}\colvec{R_1\\R_2\\R_3}
\\[.8cm]
& \stackrel{R_1'= \phantom{1}R_1+0R_2+0R_3}{\stackrel{R_2'=0R_1+\phantom{1}R_2-\phantom{1}R_3}{ \stackrel{R_3'=0R_1+0R_2+\phantom{1} R_3}{\stackrel{}{\sim}}} }&
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 0& 3 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
&\colvec{R_1'\\R_2'\\R_3'}=\begin{pmatrix}1&0&0\\0&1&\!\!-1\\0&0&1\end{pmatrix}\colvec{R_1\\R_2\\R_3}\\[3mm]\phantom{x} &&&
\end{array}
\end{equation*}
On the right, we have listed the relations between  old and new rows in matrix notation.
\Reading{SystemsOfLinearEquations}{3}
\end{example}

%\videoscriptlink{elementary_row_operations_example.mp4}{Example}{script_elementary_row_operations_example}

%\begin{center}\href{\webworkurl ReadingHomework3/1/}{Reading homework: problem 3.1}\end{center}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{EROs and  Matrices}
Interestingly, the matrix 
that describes the relationship between old and new rows 
performs the corresponding ERO on the augmented matrix.
\begin{example} (Performing EROs with Matrices)
\begin{eqnarray*}
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  
\end{pmatrix} 
\begin{amatrix}{3} 
0 & 1 & 1 & 7 \\ 
2 & 0 & 0& 4 \\
0& 0 & 1 & 4 
\end{amatrix} 
&=&
\begin{amatrix}{3} 
2 & 0 & 0 & 4 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\\[3mm] &&\qquad\quad \rotatebox{90}{$\sim$} \\[3mm]%second line
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 & 0 \\
0& 0 & 1  \\
\end{pmatrix}
\begin{amatrix}{3} 
2 & 0 & 0 & 4 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
&=&
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\\[3mm] &&\qquad\quad \rotatebox{90}{$\sim$} \\[3mm]%third line
\begin{pmatrix}
1&0&0\\
0&1&\!\!\!-1\\
0&0&1
\end{pmatrix}
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix} 
&=&
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 0& 3 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\end{eqnarray*}
Here we have multiplied the augmented matrix with the matrices that acted on rows listed on the right  of example~\ref{Rsystem}. 
\end{example}

Realizing EROs as matrices allows us to give a concrete notion of \hyperlink{ch1divide}{``dividing by a matrix''}; we can now perform manipulations on both sides of an equation in a familiar way:

\begin{example} (Undoing $A$ in $Ax=b$ slowly, for $A=6=3\cdot2$)
\begin{equation*}\begin{array}{crcr}
&6x&=&\phantom{ 3^{-1}} 12 \\[2mm]
\Leftrightarrow\ &3^{-1}6x&=&3^{-1}12 \\[2mm]
\Leftrightarrow\ & 2x&=&\phantom{3^{-1}~}4  \\[2mm]
\Leftrightarrow\ & 2^{-1}2x&=&2^{-1}~4\\[2mm] %  \Leftrightarrow 
\Leftrightarrow\ &  1x&=&\phantom{3^{-1}~} 2
\end{array}
\end{equation*}
\end{example}

\noindent
The matrices corresponding to EROs undo a matrix step by step.
\begin{example} \label{slowly}(\hypertarget{Undoing}{Undoing} $A$ in $Ax=b$ slowly, for $A=M=...$)
\begin{equation*}
\begin{array}{crcr}
& \begin{pmatrix}
0 & 1 & 1  \\ 
2 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
}
~
\begin{pmatrix}
 7 \\ 
4 \\
4\\
\end{pmatrix} 
\\[7mm] %second line
\Leftrightarrow\ &
%
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
%
\begin{pmatrix}
0 & 1 & 1  \\ 
2 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
~
\begin{pmatrix}
 7 \\ 
4 \\
4\\
\end{pmatrix} 
\\[7mm]
\Leftrightarrow\ & %third line
\phantom{-}
\begin{pmatrix}
2 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
}
~
\begin{pmatrix}
4 \\
7 \\ 
4\\
\end{pmatrix} 
\\[7mm]
\Leftrightarrow\ & %fourth line
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
2 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
%\phantom{
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 &  0\\
0& 0 & 1  \\
\end{pmatrix} 
%}
~
\begin{pmatrix}
4 \\
7 \\ 
4\\
\end{pmatrix} 
\\[7mm]
\Leftrightarrow\ & %fifth line
\phantom{
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 &  0\\
0& 0 & 1  \\
\end{pmatrix} 
}
~
\begin{pmatrix}
2 \\
7 \\ 
4\\
\end{pmatrix} 
\\[7mm]
\Leftrightarrow\ & %sixth line
%\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 & \!\!\!-1 \\
0& 0 & 1  \\
\end{pmatrix} 
%}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
%\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 & \!\! \!-1\\
0& 0  &  1  \\
\end{pmatrix} 
%}
\! \!
\begin{pmatrix}
2 \\
7 \\ 
4\\
\end{pmatrix} 
\\[7mm]
\Leftrightarrow\ &%%%%%%%%%%%%%%%%%%%%%%%7th line
\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 & -1 \\
0& 0 & 1  \\
\end{pmatrix} 
}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 &  -1\\
0& 0  &  1  \\
\end{pmatrix} 
}
\! 
\begin{pmatrix}
2 \\
3 \\ 
4\\
\end{pmatrix} .
\end{array}
\end{equation*}
\end{example}



\noindent
This is another way of thinking about Gaussian elimination which feels more like elementary algebra in the sense that you ``do something to both sides of an equation"" until you have a solution. 


%%%%%%%%%%%%%%%%%%

\subsection{Recording EROs in $(\!\, M | I \,) $}\label{EROinverse}
Just as we put together $3^{-1}2^{-1}=6^{-1}$ to get a single thing to apply to both sides of $6x=12$ to undo $6$, we should put together multiple EROs  to get a single thing that undoes our matrix. 
To do this, augment by the identity matrix (not just a single column) and then perform Gaussian elimination. 
There is no need to write the EROs as systems of equations or as matrices while doing this. 
%That is, perform the EROs without the their corresponding matrices.

\begin{example} \label{undo_a_matrix}(Collecting EROs that \hypertarget{undo a matrix}{undo a matrix})
\begin{eqnarray*}
\left(\begin{array}{ccc|ccc}
0 & 1 & 1 &1 &0 &0\\ 
2 & 0 & 0 &0&1&0\\
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)
&\!\!\sim\!\!&
\left(\begin{array}{ccc|ccc}
2 & 0 & 0 &0&1&0\\
0 & 1 & 1 &1 &0 &0\\ 
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)
\\[2mm]
&\!\!\sim\!\!&
\left(\begin{array}{ccc|ccc}
1 & 0 & 0 &0&\frac12&0\\
0 & 1 & 1 &1 &0 &0\\ 
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)
\sim
\left(\begin{array}{ccc|ccr}
1 & 0 & 0 &0&\frac12&0\\
0 & 1 & 0 &1 &0 &\!\!\!-1\\ 
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)\, .
\end{eqnarray*}
\end{example}
\noindent
As we changed the left side from the matrix $M$ to the identity matrix, the right side changed from the identity matrix to the matrix which undoes $M$. 
\begin{example} (Checking that \hypertarget{inversie}{one matrix undoes another})
\begin{eqnarray*}
\left(\begin{array}{rrr}
0&\frac12&0\\
1 &0 &\!\!\!-1\\ 
0  &0 &1\\
\end{array}  \right)
\left(\begin{array}{ccc}
0&1&1\\
2 &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
=
\left(\begin{array}{ccc}
1  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) \, .
\end{eqnarray*}
If the matrices are composed in the opposite order, the result is the same.
\begin{eqnarray*}
\left(\begin{array}{ccc}
0&1&1\\
2 &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
\left(\begin{array}{ccr}
0&\frac12&0\\
1 &0 &\!\!\!-1\\ 
0  &0 &1\\
\end{array}  \right)
=
\left(\begin{array}{ccc}
1  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) \, .
\end{eqnarray*}
\end{example}

%\reading{3}{1}

Whenever the product of two matrices $MN=I$, we say that  $N$ is the inverse\index{Inverse Matrix} of $M$ or $N=M^{-1}$. 
Conversely $M$ is the inverse of $N$;~$M=N^{-1}$.\\



In abstract generality, let $M$ be some matrix and, as always, let $I$ stand for the identity matrix. Imagine the process of performing elementary row operations to bring $M$ to the identity matrix: 
\begin{equation*}
(M | I) \sim ( E_1M| E_1)\sim (E_2E_1 M | E_2 E_1) \sim \cdots \sim (I | \cdots E_2E_1 )\, .
\end{equation*}
The ellipses ``$\cdots$'' stand for additional EROs. The result is a product of matrices that form a matrix which undoes $M$
\begin{equation*}
\cdots E_2 E_1 M =  I \, .
\end{equation*}
This is only true if the RREF of $M$ is the identity matrix.  \\

\noindent {\bf Definition}: A matrix $M$ is {\bf invertible}\index{invertiblech3} if its RREF is an identity matrix.
\begin{center}
{\Large{\bf  How to find $M^{-1}$}}\\[5mm]
 \shabox{$(M | I) \sim (I| M^{-1})$}
\end{center}

Much use is made of the fact that invertible matrices can be undone with EROs. 
To begin with, since each  elementary row operation has an inverse, 
$$
M= E_1^{-1} E_2^{-1} \cdots\, ,
$$
while the inverse of $M$ is 
\begin{equation*}
M^{-1}=\cdots E_2 E_1 \, .
\end{equation*}
This is symbolically verified by
\begin{equation*}
M^{-1}M=\cdots E_2 E_1\, E_1^{-1} E_2^{-1} \cdots
=\cdots E_2 \, E_2^{-1} \cdots = \cdots = I\, .
\end{equation*}
Thus, if $M$ is invertible, then  $M$  can be expressed as the product of EROs. (The same is true for its inverse.) This has the feel of the fundamental theorem of arithmetic (integers can be expressed as the product of primes) or the fundamental theorem of algebra (polynomials can be expressed as the product of [complex] first order polynomials); EROs are  building blocks of invertible matrices. 




\subsection{The Three Elementary Matrices}

%To use this in concrete examples, one uses the fact that i
We now work toward concrete examples and applications. 
It is surprisingly easy to translate between EROs and matrices that perform EROs.
The matrices corresponding to these kinds are close in form to the identity matrix:
\begin{itemize}
\item Row Swap: Identity matrix with two rows swapped.
\item Scalar Multiplication:  Identity matrix with one \hyperlink{diagmat}{diagonal entry} not 1.
\item Row Sum: The identity matrix with one off-\hyperlink{diagmat}{diagonal entry} not 0.
\end{itemize}


\begin{example} (Correspondences between EROs and their matrices)
\begin{itemize}
\item The row swap matrix that swaps the 2nd and 4th row is the identity matrix with the 2nd and 4th row swapped: 
$$
\begin{pmatrix}
1&0&0&0&0\\
0&0&0&1&0\\
0&0&1&0&0\\
0&1&0&0&0\\
0&0&0&0&1\\
\end{pmatrix}\, .
$$
\item
The scalar multiplication matrix that replaces the 3rd row with 7 times the 3rd row is the identity matrix with 7 in the 3rd row instead of 1:
$$
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&7&0\\
0&0&0&1\\
\end{pmatrix}\, .
$$

\item The row sum matrix that replaces the 4th row with the 4th row plus 9 times the 2nd row is the identity matrix with a 9 in the  4th row, 2nd column:
$$
\begin{pmatrix}
1&0&0&0&0&0&0\\
0&1&0&0&0&0&0\\
0&0&1&0&0&0&0\\
0&9&0&1&0&0&0\\
0&0&0&0&1&0&0\\
0&0&0&0&0&1&0\\
0&0&0&0&0&0&1\\
\end{pmatrix}\, .
$$
\end{itemize}
\end{example}

We can write an explicit factorization of a matrix into EROs by keeping track of the EROs used in getting to RREF.

\begin{example} (Express $M$ from  
\hyperlink{undo a matrix}{Example~\ref{undo_a_matrix}} as a product of EROs)\\
Note that in the previous example 
one of each of the kinds of EROs is used, in the order just given.
Elimination looked like 
\begin{eqnarray*}
M=
\left(\begin{array}{ccc}
0 & 1 & 1 \\ 
2 & 0 & 0 \\\
0& 0 & 1  
\end{array}  \right)
\stackrel{E_1}{\sim}
\left(\begin{array}{ccc}
2 & 0 & 0 \\
0 & 1 & 1 \\ 
0& 0 & 1  
\end{array}  \right)
\stackrel{E_2}{\sim}
\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 1 \\ 
0& 0 & 1  
\end{array}  \right)
\stackrel{E_3}{\sim}
\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\ 
0& 0 & 1  
\end{array}  \right)
=I\, ,
\end{eqnarray*}
where the EROs matrices are 
\begin{eqnarray*}
E_1
= \left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
,~
E_2
= \left(\begin{array}{ccc}
\frac12  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) , ~
E_3
= \left(\begin{array}{ccc}
1  &0 &0\\
0  &1 & -1\\ 
0  &0 &1\\
\end{array}  \right) \,.
\end{eqnarray*}
%Composing these gives (by matrix multiplication rules worked out in %\hyperref{}
%\begin{eqnarray*}
%E_3E_2E_1
%&= &
% \left(\begin{array}{ccc}
%1  &0 &0\\
%0  &1 & -1\\ 
%0  &0 &1\\
%\end{array}  \right)
% \left(\begin{array}{ccc}
%\frac12  &0 &0\\
%0  &1 &0\\ 
%0  &0 &1\\
%\end{array}  \right)
%\left(\begin{array}{ccc}
%0  &1 &0\\
%1  &0 &0\\ 
%0  &0 &1\\
%\end{array}  \right)
%\\ %2nd line
%&=& \left(\begin{array}{ccc}
%1 &0 &0\\
%0  &1 & -1\\ 
%0  &0 &1\\
%\end{array}  \right) 
%\left(\begin{array}{ccc}
%0  	&\frac12 	& 0\\ 
%1  	&0	 	&0\\
%0  	&0 		&1
%\end{array}  \right) 
%=%%%%%%%%%%%%%%%%
%\left(\begin{array}{ccc}
%0  	&\frac12 	& 0\\ 
%1  	&0	 	&-1\\
%0  	&0 		&1
%\end{array}  \right) 
% \, .
%\end{eqnarray*}
%We showed this was $M^{-1}$ \hyperlink{inversie}{earlier}. 
The inverse of the ERO matrices (corresponding to the description of the reverse row maniplulations)
\begin{eqnarray*}
E_1^{-1}
= \left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
,~
E_2^{-1}
= \left(\begin{array}{ccc}
2  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) , ~
E_3^{-1}
= \left(\begin{array}{ccc}
1  &0 &0\\
0  &1 & 1\\ 
0  &0 &1\\
\end{array}  \right) \,.
\end{eqnarray*}
Multiplying these gives 
\begin{eqnarray*}
E_1^{-1}E_2^{-1}E_3^{-1}
&=& 
\left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
 \left(\begin{array}{ccc}
2  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) 
\left(\begin{array}{ccc}
1  &0 &0\\
0  &1 & 1\\ 
0  &0 &1\\
\end{array}  \right) 
\\[2mm] %2nd line
&=&
\left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
 \left(\begin{array}{ccc}
2  &0 &0\\
0  &1 &1\\ 
0  &0 &1\\
\end{array}  \right) 
= %%%%%%%
\left(\begin{array}{ccc}
 0 &1 &1\\
2  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)  = M \, .
\end{eqnarray*}
\end{example}

\subsection{$LU$, $LDU$, and $PLDU$ Factorizations}\label{LUtake1}
The process of elimination can be stopped halfway to obtain decompositions frequently used in large computations in sciences and engineering. 
The first half of the elimination process is to eliminate entries below the diagonal  
leaving a matrix which is called {\it upper triangular}\index{Upper triangular matrix}. The elementary matrices which perform this part of the elimination are {\it lower triangular}\index{lower triangular}, as are their inverses. But putting together the upper triangular and lower triangular parts one obtains the so-called $LU$ factorization.


\begin{example}\label{factorize} ($LU$ \hypertarget{elldeeeww}{ factorization})
\begin{eqnarray*}
M=
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
&\stackrel{E_1}{\sim}&
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&-1&1&-1\\
\end{pmatrix}
\\[2mm]
&\stackrel{E_2}{\sim}&
~~\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&3&1\\
\end{pmatrix}
~~\stackrel{E_3}{\sim}~
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
:=U\, ,
\end{eqnarray*}
%%%%%%%%%%%%%%%%%%%%%%%%%%
where the EROs and their inverses are 
%%%%%%%%%%%%%%%
\begin{eqnarray*}
E_1=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
2&0&1&0\\
0&0&0&1\\
\end{pmatrix} \, ,~~~~
E_2=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&1&0&1\\
\end{pmatrix} \, ,~~
E_3=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&-1&1\\
\end{pmatrix} \, 
\\[2mm] %%%%%%%%second line
E_1^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&0&0&1\\
\end{pmatrix}  , \,
E_2^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&-1&0&1\\
\end{pmatrix}  , \,
E_3^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&1&1\\
\end{pmatrix} \, .
\end{eqnarray*}
Applying inverse elementary matrices to both sides of the equality  $U=E_3E_2E_1M$ gives 
$M=E_1^{-1}E_2^{-1}E_3^{-1}U$ or 
%\scalebox{.97}
\begin{eqnarray*}
 %oi vey
\begin{pmatrix}
2&0&\!\!-3&1\\
0&1&2&2\\
\!-4&0&9&2\\
0&-1&1&\!\!-1\\
\end{pmatrix}
\!\!\!\!\!\!&=&\!\!\!\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
\!-2&0&1&0\\
0&0&0&1\\
\end{pmatrix} \!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&\!\!\!-1&0&1\\
\end{pmatrix}\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&1&1\\
\end{pmatrix}\!\!\!
\begin{pmatrix}
2&0&\!\!\!-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&\!\!-3\\
\end{pmatrix}
\\[2mm]
&=&\!\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
\!-2&0&1&0\\
0&0&0&1\\
\end{pmatrix} 
%%%%%%%%
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&\!\!-1&1&1\\
\end{pmatrix}
%%%%%%%%%%
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\\[2mm]
&=&\!\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
\!-2&0&1&0\\
0&\!\!-1&1&1\\
\end{pmatrix} 
%
\begin{pmatrix}
2&0&\!\!-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&\!\!-3\\
\end{pmatrix} \, .
\end{eqnarray*}
%\end{scalebox}
This is a lower triangular matrix times an upper triangular matrix. 
\end{example}

\newpage
What if we stop at a different point in elimination? 
We could multiply rows so that the entries in the diagonal are 1 next. Note that the EROs that do this are diagonal. This gives a slightly different factorization.
\begin{example} \label{factorizes}($LDU$ factorization building from previous example)
\begin{eqnarray*}
M=
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
&\stackrel{E_3E_2E_1}{\sim}&
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\stackrel{E_4}{\sim}
\begin{pmatrix}
1&0&-\frac{3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\\[2mm]
&\stackrel{E_5}{\sim}&
\begin{pmatrix}
1&0&-\frac{3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&-3\\
\end{pmatrix}
\stackrel{E_6}{\sim}
\begin{pmatrix}
1&0&-\frac{3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&1\\
\end{pmatrix}
=:U
\end{eqnarray*}
The corresponding elementary matrices are
\begin{equation*}
\begin{aligned}
%%%%%%%%the EROs
E_4=
\begin{pmatrix}
\frac12&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix} , \, ~~
E_5=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&\frac13&0\\
0&0&0&1\\
\end{pmatrix} , \, ~~
E_6=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&-\frac13\\
\end{pmatrix} , \, 
\\[3mm]
%%%%%%%%the ERO inverses
E_4^{-1}=
\begin{pmatrix}
2&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix} , \, 
E_5^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&3&0\\
0&0&0&1\\
\end{pmatrix} , \, 
E_6^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&-3\\
\end{pmatrix} \, .
\end{aligned}
\end{equation*}
The equation $U=E_6E_5E_4E_3E_2E_1 M$ can be rearranged as
$$M=(E_1^{-1}E_2^{-1}E_3^{-1})(E_4^{-1}E_5^{-1}E_6^{-1})U.$$ 
We calculated the product of the first three factors in the previous example; it was named $L$ there, and we will reuse that name here. The product of the next three factors is diagonal and we wil name it $D$. The last factor we named $U$ (the name means something different in this example than the last example.) The $LDU$ factorization of our matrix is
\begin{eqnarray*}
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&-1&1&1\\
\end{pmatrix} 
\begin{pmatrix}
2&0&0&0\\
0&1&0&0\\
0&0&3&0\\
0&0&0&-3\\
\end{pmatrix} 
\begin{pmatrix}
1&0&\!\!-\frac{3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&1\\
\end{pmatrix}\, .
\end{eqnarray*}
\end{example}

The $LDU$ factorization of a matrix is a factorization into blocks of EROs of a various types: $L$ is the product of the inverses of EROs which eliminate below the diagonal by row addition, $D$ the product of inverses of EROs which set the diagonal elements to 1 by row multiplication, and $U$ is the product of inverses of EROs which eliminate above the diagonal by row addition.

\hypertarget{LDPU}{
You} may notice that one of the three kinds of row operation is missing from this story. 
Row exchange may  be necessary to obtain RREF. Indeed, 
so far in this chapter we have been working under the tacit assumption that 
$M$ can be brought to the identity by just row multiplication and row addition. 
If row exchange is necessary, the resulting factorization is $LDPU$ where $P$ is the product of inverses of EROs that perform row exchange. 

\begin{example} ($LDPU$ factorization, building from previous examples)
\begin{eqnarray*}
M=
\begin{pmatrix}
0&1&2&2\\
2&0&-3&1\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\stackrel{P}{\sim}
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\stackrel{E_6E_5E_4E_3E_2E_1}{\sim} L
\end{eqnarray*}
\begin{eqnarray*}
P=
\begin{pmatrix}
0&1&0&0\\
1&0&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix}
=P^{-1}
\end{eqnarray*}
\begin{eqnarray*}
M=P(E_1^{-1}E_2^{-1}E_3^{-1})(E_4^{-1}E_5^{-1}E_6^{-1}) (E_7^{-1}) U=PLDU\\
\end{eqnarray*}
%%%%%%%last line!
\begin{center}
\scalebox{.91}{$
\!\!\!\begin{pmatrix}
0&1&2&2\\
2&0&-3&1\\
-4&0&9&2\\
0&\!\!-1&1&\!\!-1\\
\end{pmatrix}
=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&\!\!-1&1&1\\
\end{pmatrix} 
\!\!\!
\begin{pmatrix}
2&0&0&0\\
0&1&0&0\\
0&0&3&0\\
0&0&1&\!\!\!-3\\
\end{pmatrix} 
\!\!\!
\begin{pmatrix}
0&1&0&0\\
1&0&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix}
\!\!\!
\begin{pmatrix}
1&0&\!\!-\frac{3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&1\\
\end{pmatrix}
$}\end{center}

\end{example}

%\References{
%Hefferon, Chapter One, Section 1.1 and 1.2
%\\
%Beezer, Chapter SLE, Section RREF
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/Row_echelon_form}{Row Echelon Form}
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/Elementary_matrix_transformations}{Elementary Matrix Operations}}


\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|l|l|}
\hline
Reading problems &
\hwrref{SystemsOfLinearEquations}{3}\\
Matrix notation &  \hwref{SystemsOfLinearEquations}{18}\\
$LU$ &  \hwref{SystemsOfLinearEquations}{19}\\
\hline
\end{tabular}


\input{\elemRowOpsPath/problems}

%\input{\elemRowOpsPath/problems}




%for book
\section{\solutionSetsTitle}
%for summer
%\chapter{\solutionSetsTitle}


Algebraic equations problems can have multiple solutions. For example $x(x-1)=0$ has  two solutions: $0$ and $1$. By contrast, equations of the form $Ax=b$ with $A$ a linear operator (with scalars the real numbers) have the following property:

\vspace{3mm}
\noindent
If $A$ is a linear operator and $b$ is  known, then $Ax=b$ has either
\begin{enumerate}
\item One solution
\item  No solutions
\item Infinitely many solutions
\end{enumerate}


\subsection{The Geometry of Solution Sets: Hyperplanes}
Consider the following algebra problems and their solutions.

\begin{enumerate}
\item $6x=12$ has one solution: $2$.
\item[2a.] $0x=12$ has no solution.
\item[2b.] $0x=0$ has infinitely many solutions; its solution set is $\mathbb{R}$.
\end{enumerate}
In each case the linear operator is a $1\times 1$ matrix. In the first case, the linear operator is invertible. 
In the other two cases it is not. 
In the first case, the solution set is a point on the number line, in  case 2b the solution set is the whole number line.

Lets examine similar situations with larger matrices: $2\times 2$ matrices.
\begin{enumerate}
\item
$\begin{pmatrix}
6	&0 	\\
0 	&2 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
12 \\ 
6
\end{pmatrix}$has  one solution: 
$\begin{pmatrix}
2 \\ 
3
\end{pmatrix}.$
%\\linear operator is invertible

\item[2a.] 
$\begin{pmatrix}
1	&3 	\\
0 	&0 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
4 \\ 
1 
\end{pmatrix}$ has no solutions.
%not in the range of the linear operator

\item[2bi.]
$\begin{pmatrix}
1	&3 	\\
0 	&0 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
4 \\ 
0
\end{pmatrix} $ has solution set 
$\left \{ 
\left(\begin{array}{c}
4 \\ 
0
\end{array} \right)
+
y\left(\begin{array}{c}
-3 \\ 
1
\end{array} \right)
: y\in \mathbb{R} \right\}.$

\item[2bii.]
$\begin{pmatrix}
0	&0 	\\
0 	&0 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
0 \\ 
0
\end{pmatrix} $
has solution set 
$\left \{ 
\left(\begin{array}{c}
x \\ 
y
\end{array} \right)
: x, y\in \mathbb{R} \right\}.$
\end{enumerate}
Again, in the first case the linear operator is invertible while in the other cases it is not. When a $2\times 2$ matrix from a matrix equation is not invertible the solution set can be empty, a line in the plane, or the plane itself.


For a system of equations with $r$ equations and $k$ veriables, one can have a number of different outcomes.  For example, consider the case of $r$ equations in three variables.  Each of these equations is the equation of a plane in three-dimensional space.  To find solutions to the system of equations, we look for the common intersection of the planes (if an intersection exists).  Here we have \hypertarget{FIVE}{five different possibilities}:

\begin{enumerate}
\item \textbf{Unique Solution.}  The planes have a unique point of intersection.

\item[2a.] \textbf{No solutions.}  Some of the equations are contradictory, so no solutions exist.

\item[2bi.] \textbf{Line.}  The planes intersect in a common line; any point on that line then gives a solution to the system of equations.

\item[2bii.] \textbf{Plane.}  Perhaps you only had one equation to begin with, or else all of the equations coincide geometrically.  In this case, you have a plane of solutions, with two free parameters.

\Videoscriptlink{solution_sets_for_systems_of_linear_equations_planes.mp4}{Planes}{solution_sets_for_systems_of_linear_equations_planes}

\item[2biii.] \textbf{All of $\mathbb{R}^3$.}  If you start with no information, then any point in $\mathbb{R}^3$ is a solution.  There are three free parameters.
\end{enumerate}

In general, for systems of equations with $k$ unknowns, there are $k+2$ possible outcomes, corresponding to the possible numbers ({\it i.e.}, $0,1,2,\dots,k$) of free parameters in the solutions set, plus the possibility of no solutions.  These types of solution sets\index{Solution set} are hyperplanes\index{Hyperplane}, generalizations of planes that behave like planes in $\mathbb{R}^3$ in many ways.

\Reading{SystemsOfLinearEquations}{4}
%\begin{center}\href{\webworkurl ReadingHomework4/1/}{Reading homework: problem 4.1}\end{center}


\Videoscriptlink{solution_sets_for_systems_of_linear_equations_overview.mp4}{Pictures and Explanation}{solution_sets_for_systems_of_linear_equations_overview}






\subsection{\! Particular Solution \hspace{-.7mm}$+$\hspace{-.7mm} Homogeneous Solutions }

Lets look at solution sets again, this time trying to get to their geometric shape.
In the \hyperlink{standard approach}{standard approach}, variables corresponding to columns that do not contain a pivot (after going to reduced row echelon form) are \emph{free}.  It is the number of free variables that determines the geometry of the solution set. 
%We called them non-pivot variables. 
%They index elements of the solution set by acting as coefficients of vectors.
%In this way the number of non-pivot columns determines (in part) the size of the solution set.  
%We can denote them with dummy variables $\mu_1, \mu_2, \ldots$. 

\begin{example}\label{npcd} (Non-pivot variables determine the gemometry of the solution set)
$$\begin{pmatrix}
1 &  0 & 1 & -1 \\ 
 0 & 1 & -1& 1  \\
 0 &0   & 0  & 0 \\
\end{pmatrix}
\colvec{x_1\\x_2\\x_3\\x_4} 
=
\colvec{1\\1\\0} 
\Leftrightarrow
\left\{
\begin{array}{lcr}
	1x_1 +0x_2+ 1x_3 - 1x_4 & = 1 \\
	0x_1 +1x_2 - 1x_3 + 1x_4 & = 1 \\
	0x_1 +0x_2 + 0x_3 + 0x_4 & = 0 
\end{array}
     \right.
$$
Following the standard approach, express the pivot variables in terms of the non-pivot variables and add ``empty equations"". Here $x_3$ and $x_4$ are non-pivot variables.  
\begin{eqnarray*}
\left.
\begin{array}{rcl}
	x_1 & = &1 -x_3+x_4 \\
	x_2 & = &1 +x_3-x_4 \\
	x_3 & = &\phantom{1+~\,}x_3\\
	x_4 & =&\phantom{1+x_3+~}x_4         
\end{array}
     \right\}
     \Leftrightarrow
\colvec{x_1\\x_2\\x_3\\x_4} 
= \colvec{1\\1\\0\\0} + x_3\colvec{-1\\1\\1\\0} + x_4\colvec{1\\-1\\0\\1}
\end{eqnarray*}
The preferred way to write a solution set $S$ is with set notation\index{Solution set!set notation};  \[S = \left\{\colvec{x_1\\x_2\\x_3\\x_4} = \colvec{1\\1\\ 0\\0 } + \mu_1 \colvec{-1\\1\\1\\0 }  + \mu_2  \colvec{1\\-1\\ 0 \\1 } : \mu_1,\mu_2\in  {\mathbb R} \right\} .\]
Notice that the first two components of the second two terms come from the non-pivot columns.
Another way to write the solution set is
\[S= \left\{  x^P  + \mu_1 x^H _1 + \mu_2 x^H _2   : \mu_1,\mu_2 \in  {\mathbb R}   \right\}\, , \]
where 
\[x^P = \colvec{1\\1\\0 \\0 }\, ,\quad x^H _1=\colvec{-1\\1\\1\\0 } \, ,\quad x^H _2= \colvec{1\\-1\\0 \\1 }\, .
\]
Here $x^P $ is a {\it particular solution} while $x^H _1$ and $x^H _2$ are called {\it homogeneous solutions}. The solution set forms a plane.
\end{example}



\subsection{Solutions and Linearity}
%
%\begin{definition}   A function $f$ is \emph{linear}\index{Linear!function} if 
%for any vectors $X,Y$  in the domain of $f$, and any scalars $\alpha,\beta$ 
%\[f(\alpha X + \beta Y) = \alpha f(X) + \beta f(Y) \,.\]
%\end{definition}

%
%
%\begin{example}
%\hypertarget{solution_sets_for_systems_of_linear_equations_concrete_example}{Consider our example system above with} 
%\[
%M=    \begin{pmatrix}
%      1  & 0  & 1 & -1  \\
%       0  & 1 & -1 & 1  \\
%        0 &0   & 0  & 0    \\
%    \end{pmatrix} \, ,\quad
%X= \colvec{x_1\\x_2\\x_3\\x_4} \mbox{ and } Y=\colvec{y_1\\y_2\\y^3 \\y^4 }\, ,
%\]
%and take for the function of vectors
%$$
%f(X)=MX\, .
%$$
%Now let us check the linearity property for $f$. 
%The property needs to hold for {\it any} scalars $\alpha$ and $\beta$, so for simplicity
%let us concentrate first on the case $\alpha=\beta=1$. This means that we need to
%compare the following two calculations:
%\begin{enumerate}
%\item First add $X+Y$, then compute $f(X+Y)$.
%\item First compute $f(X)$ and $f(Y)$, then compute the sum $f(X)+f(Y)$.
%\end{enumerate}
%The second computation is slightly easier:
%$$
%f(X) = MX 
%    =\colvec{x_1+x_3-x_4\\x_2-x_3+x_4\\0}\mbox{ and }
%f(Y) = MY   
%    =\colvec{y_1+y_3-y_4\\y_2-y_3+y_4\\0}\, ,
%$$
%(using our result above). Adding these gives
%$$
%f(X)+f(Y)=\colvec{x_1+x_3-x_4+y_1+y_3-y_4\\[1mm]x_2-x_3+x_4+y_2-y_3+y_4\\[1mm]0}\, .
%$$
%Next we perform the first computation beginning with:
%$$
%X+Y=\colvec{x_1 + y_1\\x_2+y_2\\ x_3+y_3\\ x_4+y_4}\, ,
%$$
%from which we calculate
%$$
%f(X+Y)=\colvec{x_1+y_2+x_3+y_3-(x_4+y_4)\\[1mm] x_2+y_2-(x_3+y_3)+x_4+y_4\\[1mm]0}\, .
%$$
%Distributing the minus signs and remembering that the order of adding numbers like $x_1,x_2,\ldots$ 
%does not matter, we see that the two computations give exactly the same answer.
%
%Of course, you should complain that we took a special choice of $\alpha$ and $\beta$.
%Actually, to take care of this we only need to check that $f(\alpha X)=\alpha f(X)$.
%It is your job to explain this in  \hyperref[linear]{Review Question~\ref*{linear}}
%\end{example}
%
%\noindent
%Later we will show that matrix multiplication is always linear.  Then we will know that:
Motivated by example~\ref{npcd}, we say that the matrix equation $Mx=v$ has  solution set  $\{ x^P  + \mu_1 x^H _1 + \mu_2 x^H _2\,  |\,  \mu_1,\mu_2 \in {\mathbb R} \}$.
\hyperlink{earlier}{Recall}  that matrices are linear operators.
%\[M(\alpha X + \beta Y) = \alpha MX + \beta MY\]
%
%Then 
%
%the two equations 
Thus 
$$M( x^P  + \mu_1 x^H _1 + \mu_2 x^H _2)  = Mx^P  + \mu_1Mx^H _1 + \mu_2Mx^H _2 =v\, ,$$
for \emph{any} $\mu_1, \mu_2 \in \mathbb{R}$. 
Choosing $\mu_1=\mu_2=0$, we obtain 
$$Mx^P =v\, .$$  
This is why $x^P $ is an example of a  \emph{particular solution}\index{Particular solution!an example}.

Setting $\mu_1=1, \mu_2=0$, and subtracting $Mx^P =v$ we obtain 
$$Mx^H _1=0\, .$$ 
Likewise, setting $\mu_1=0, \mu_2=1$, we obtain $$Mx^H _2=0\, .$$
Here $x^H _1$ and $x^H _2$ are examples of what are called  \emph{homogeneous} solutions\index{Homogeneous solution!an example} to the system.
They {\it do not} solve the original equation $Mx=v$, but instead its associated 
{\it homogeneous  equation}\index{homogeneous equation} $M y =0$.

We have just learnt a  fundamental lesson of linear algebra: the  solution set to $Ax=b$, where $A$ is a linear operator, consists of a particular solution plus homogeneous solutions.

\begin{center}
\shabox{ \{Solutions\} $=$ \{Particular solution $+$ Homogeneous solutions\} }
\end{center}

\begin{example}
Consider the matrix equation of example~\ref{npcd}. It has  solution set
\[S = \left\{\colvec{1\\1\\0 \\0 } + \mu_1 \colvec{-1\\1\\1\\0 } + \mu_2 \colvec{1\\-1\\ 0\\1 } \, :\,  \mu_1,\mu_2 \in \Re \right\} \, .\]
Then $Mx^P =v$ says that 
$\colvec{1\\1\\0 \\ 0}$ is a solution to the original matrix equation, which is certainly true, but this is not the only solution.\\

$Mx^H _1=0$ says that $\colvec{-1\\1\\1\\ 0}$ is a solution to the homogeneous equation.

\vspace{2mm}

$Mx^H _2=0$ says that 
$\colvec{1\\-1\\0 \\1}$ is a solution to the homogeneous equation.

\vspace{2mm}

\noindent
Notice how adding any multiple of a homogeneous solution to the particular solution yields another particular solution.
\end{example}


\Reading{SystemsOfLinearEquations}{4}
%\begin{center}\href{\webworkurl ReadingHomework4/1/}{Reading homework: problem 4.1}\end{center}
%\reading{2}{5}


%\begin{center}\href{\webworkurl ReadingHomework4/2/}{Reading homework: problem 4.2}\end{center}

%\section*{References}
%
%Hefferon, Chapter One, Section I.2
%\\
%Beezer, Chapter SLE, Section TSS
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/System_of_linear_equations}{Systems of Linear Equations}


%\subsection{The size of solution sets vs size of homogeneous solution set}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|l|l|}
\hline
Reading problems &
\hwrref{SystemsOfLinearEquations}{4},
\hwrref{SystemsOfLinearEquations}{5}
\\
Solution sets&
\hwref{SystemsOfLinearEquations}{20},
\hwref{SystemsOfLinearEquations}{21},
\hwref{SystemsOfLinearEquations}{22}\\
Geometry of solutions&
\hwref{SystemsOfLinearEquations}{23},
\hwref{SystemsOfLinearEquations}{24},
\hwref{SystemsOfLinearEquations}{25},
\hwref{SystemsOfLinearEquations}{26}\\
\hline
\end{tabular}


\input{\solutionSetsPath/problems}


\newpage


",lesson
3,Elementary Row Operations,"
\chapter{\elemRowOpsTitle}

%\hypertarget{Elementary Row Operations} %what is this? 

Elementary row operations are  systems of linear equations between old and new  rows. 


\begin{example} (\hypertarget{Keeping track of EROs with equations between rows}{Keeping track of EROs with equations between rows})\\
We will refer to the new $k$th row as $R'_k$ and the old $k$th row as $R_k$.
\begin{eqnarray*}
\begin{amatrix}{3} 
0 & 1 & 1 & 7 \\ 
2 & 0 & 0& 4 \\
0& 0 & 1 & 4 \\
\end{amatrix} 
& \stackrel{R_1'=0R_1+\phantom{1}R_2+0R_3}{\stackrel{R_2'=\phantom{1}R_1+0R_2+0R_3}{ \stackrel{R_3'= 0R_1+0R_2+\phantom{1}R_3}{\sim}}}&
\begin{amatrix}{3} 
2 & 0 & 0 & 4 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\\[.2cm]
& \stackrel{R_1'=\frac12 R_1+0R_2+0R_3}{\stackrel{R_2'=0R_1+\phantom{1}R_2+0R_3}{ \stackrel{R_3'= 0R_1+0R_2+\phantom{1}R_3}{\sim}} }&
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\\[.2cm]
& \stackrel{R_1'= \phantom{1}R_1+0R_2+0R_3}{\stackrel{R_2'=0R_1+\phantom{1}R_2-\phantom{1}R_3}{ \stackrel{R_3'=0R_1+0R_2+\phantom{1} R_3}{\sim}} }&
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 0& 3 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\end{eqnarray*}
\end{example}

%\videoscriptlink{elementary_row_operations_example.mp4}{Example}{script_elementary_row_operations_example}

%\begin{center}\href{\webworkurl ReadingHomework3/1/}{Reading homework: problem 3.1}\end{center}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{EROs as Matrices}
The matrix describing the system of equations between rows performs the corresponding ERO on the augmented matrix:
\begin{example} (Performing EROs with Matrices)\\
\begin{eqnarray*}
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  
\end{pmatrix} 
\begin{amatrix}{3} 
0 & 1 & 1 & 7 \\ 
2 & 0 & 0& 4 \\
0& 0 & 1 & 4 
\end{amatrix} 
=
\begin{amatrix}{3} 
2 & 0 & 0 & 4 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\\ %second line
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 & 0 \\
0& 0 & 1  \\
\end{pmatrix}
\begin{amatrix}{3} 
2 & 0 & 0 & 4 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
=
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\\ %third line
\begin{pmatrix}
1&0&0\\
0&1&-1\\
0&0&1
\end{pmatrix}
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix} 
=
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 0& 3 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\end{eqnarray*}
\end{example}

This obviously involved more writing than Gaussian elimination. 
The point here is that realizing EROs as matrices allows us to make concrete the notion of ``dividing by a matrix'' \hyperlink{ch1divide}{alluded to in chapter 1}; we can now perform manipulations on both sides of an equation in a familiar way.

\begin{example} (Undoing $A$ in $Ax=b$ slowly, using $A=6=3\cdot2$)
\begin{eqnarray*}
6x&=&\phantom{ 3^{-1}} 12 \\
3^{-1}6x&=&3^{-1}12 \\
 2x&=&\phantom{3^{-1}~}4  \\
 2^{-1}2x&=&2^{-1}~4\\ %  \Leftrightarrow 
  1x&=&\phantom{3^{-1}~} 2
\end{eqnarray*}
\end{example}

\noindent
In particular, matrices corresponding to EROs undo a matrix step by step.
\begin{example} (\hypertarget{Undoing} $A$ in $Ax=b$ slowly, Using $A=M=...$)
\begin{eqnarray*}
\begin{pmatrix}
0 & 1 & 1  \\ 
2 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
}
~
\begin{pmatrix}
 7 \\ 
4 \\
4\\
\end{pmatrix} 
\\\nn %second line
%
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
%
\begin{pmatrix}
0 & 1 & 1  \\ 
2 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
~
\begin{pmatrix}
 7 \\ 
4 \\
4\\
\end{pmatrix} 
\\\nn %third line
\phantom{-}
\begin{pmatrix}
2 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
}
~
\begin{pmatrix}
4 \\
7 \\ 
4\\
\end{pmatrix} 
\\ \nn %fourth line
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
2 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
%\phantom{
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 &  0\\
0& 0 & 1  \\
\end{pmatrix} 
%}
~
\begin{pmatrix}
4 \\
7 \\ 
4\\
\end{pmatrix} 
\\\nn %fifth line
\phantom{
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 &  0\\
0& 0 & 1  \\
\end{pmatrix} 
}
~
\begin{pmatrix}
2 \\
7 \\ 
4\\
\end{pmatrix} 
\\ \nn %sixth line
%\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 & -1 \\
0& 0 & 1  \\
\end{pmatrix} 
%}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
%\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 &  -1\\
0& 0  &  1  \\
\end{pmatrix} 
%}
\! \!
\begin{pmatrix}
2 \\
7 \\ 
4\\
\end{pmatrix} 
\\\nn%%%%%%%%%%%%%%%%%%%%%%%7th line
\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 & -1 \\
0& 0 & 1  \\
\end{pmatrix} 
}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 &  -1\\
0& 0  &  1  \\
\end{pmatrix} 
}
\! 
\begin{pmatrix}
2 \\
3 \\ 
4\\
\end{pmatrix} 
\end{eqnarray*}
\end{example}



\noindent
This is another way of thinking about what is happening in the process of elimination which feels more like elementary algebra in the sense that you ``do something to both sides of an equation"" until you have a solution. 


%%%%%%%%%%%%%%%%%%

\section{Recording EROs in $[M | I ] $}
Just as we put together $3^{-1}2^{-1}=6^{-1}$ to get a single thing to apply to both sides of $6x=12$ to undo $6$, we should put together multiple EROs  to get a single thing that undoes our matrix. 
To do this, augment by the identity matrix (not just a single column) and then perform Gaussian elimination. 
There is no need to write the EROs as systems of equations or as matrices while doing this. 
%That is, perform the EROs without the their corresponding matrices.

\begin{example} (Collecting EROs that \hypertarget{undo a matrix}{undo a matrix})
\begin{eqnarray*}
\left(\begin{array}{ccc|ccc}
0 & 1 & 1 &1 &0 &0\\ 
2 & 0 & 0 &0&1&0\\
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)
\sim
&\left(\begin{array}{ccc|ccc}
2 & 0 & 0 &0&1&0\\
0 & 1 & 1 &1 &0 &0\\ 
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)&
\\
\sim
&\left(\begin{array}{ccc|ccc}
1 & 0 & 0 &0&\frac12&0\\
0 & 1 & 1 &1 &0 &0\\ 
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)&
\sim
\left(\begin{array}{ccc|ccc}
1 & 0 & 0 &0&\frac12&0\\
0 & 1 & 0 &1 &0 &-1\\ 
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)
\end{eqnarray*}
\end{example}
\noindent
As we changed the left slot from the matrix $M$ to the identity matrix, the right slot changed from the identity matrix to the matrix which undoes $M$: 
\begin{example} (Checking that \hypertarget{inversie}{one matrix undoes another})
\begin{eqnarray*}
\left(\begin{array}{ccc}
0&\frac12&0\\
1 &0 &-1\\ 
0  &0 &1\\
\end{array}  \right)
\left(\begin{array}{ccc}
0&1&1\\
2 &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
=
\left(\begin{array}{ccc}
1  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) \, .
\end{eqnarray*}
If the matrices are composed in the opposite order, the result is the same.
\begin{eqnarray*}
\left(\begin{array}{ccc}
0&1&1\\
2 &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
\left(\begin{array}{ccc}
0&\frac12&0\\
1 &0 &-1\\ 
0  &0 &1\\
\end{array}  \right)
=
\left(\begin{array}{ccc}
1  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) \, .
\end{eqnarray*}
\end{example}

%\reading{3}{1}


In abstract generality, let $M$ be some matrix and, as always, let $I$ stand for the identity matrix. Imagine the process of performing elementary row operations to bring $M$ to the identity matrix. 
\begin{equation*}
(M | I) \sim ( E_1M| E_1)\sim (E_2E_1 M | E_2 E_1) \sim \cdots \sim (I | \cdots E_2E_1 )
\end{equation*}
Ellipses stand for additional EROs. The result is a product of matrices that form a matrix which undoes $M$
\begin{equation*}
\cdots E_2 E_1 M =  I 
\end{equation*}
This is only true if RREF of $M$ is the identity matrix.  
In that case, we say $M$ is {\it invertible}\index{invertiblech3}. 


Much use is made of the fact that invertible matrices can be undone with EROs. 
To begin with, since each  elementary row operation has an inverse, 
$$
M= E_1^{-1} E_2^{-1} \cdots
$$
while the inverse of $M$ is 
\begin{equation*}
M^{-1}=\cdots E_2 E_1 
\end{equation*}
This is symbolically verified as
\begin{equation*}
M^{-1}M=\cdots E_2 E_1\, E_1^{-1} E_2^{-1} \cdots
=\cdots E_2 \, E_2^{-1} \cdots = \cdots = I
\end{equation*}
Thus, if $M$ is invertible then  $M$ and can be expressed as the product of EROs. (The same is true for its inverse.) This has the feel of the fundamental theorem of arithmetic (integers can be expressed as the product of primes) or the fundamental theorem of algebra (polynomials can be expressed as the product of first order polynomials); EROs are the building blocks of invertible matrices. 




\section{Three Kinds of EROs, Three Kinds of Matrices}

%To use this in concrete examples, one uses the fact that i
We now work toward concrete examples and applications. 
It is surprisingly easy to translate between EROs as descriptions of rows and as matrices.
The matrices corresponding to these kinds are close in form to the identity matrix:
\begin{itemize}
\item Row Swap: Identity matrix with two rows swapped
\item Scalar Multiplication:  Identity matrix with one diagonal entry not 1.
\item Row Sum: The identity matrix with one off diagonal entry not 0.
\end{itemize}


\begin{example} (Correspondences between EROs and their matrices)
\begin{itemize}
\item The row swap matrix that swaps the 2nd and 4th row is the identity matrix with the 2nd and 4th row swapped; 
$$
\begin{pmatrix}
1&0&0&0&0\\
0&0&0&1&0\\
0&0&1&0&0\\
0&1&0&0&0\\
0&0&0&0&1\\
\end{pmatrix}
$$
\item
The scalar multiplication matrix that replaces the 3rd row with 7 times the 3rd row is the identity matrix with 7 in the 3rd row instead of 1; 
$$
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&7&0\\
0&0&0&1\\
\end{pmatrix}
$$

\item The row sum matrix that replaces the 4th row with the 4th row plus 9 times the 2nd row is the identity matrix with a 9 in the  4th row, 2nd column
$$
\begin{pmatrix}
1&0&0&0&0&0&0\\
0&1&0&0&0&0&0\\
0&0&1&0&0&0&0\\
0&9&0&1&0&0&0\\
0&0&0&0&1&0&0\\
0&0&0&0&0&1&0\\
0&0&0&0&0&0&1\\
\end{pmatrix}
$$
\end{itemize}
\end{example}

We can write an explicit factorization of a matrix into EROs by keeping track of the EROs used in getting to RREF.

\begin{example} (Express $M$ from the  
\hyperlink{undo a matrix}{earlier example} as a product of EROs)\\
Note that in the previous example 
one of each of the kinds of EROs is used, in the order just given.
Elimination looked like 
\begin{eqnarray*}
M=
\left(\begin{array}{ccc}
0 & 1 & 1 \\ 
2 & 0 & 0 \\\
0& 0 & 1  
\end{array}  \right)
\stackrel{E_1}{\sim}
\left(\begin{array}{ccc}
2 & 0 & 0 \\
0 & 1 & 1 \\ 
0& 0 & 1  
\end{array}  \right)
\stackrel{E_2}{\sim}
\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 1 \\ 
0& 0 & 1  
\end{array}  \right)
\stackrel{E_3}{\sim}
\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\ 
0& 0 & 1  
\end{array}  \right)
=I
\end{eqnarray*}
where the EROs matrices are 
\begin{eqnarray*}
E_1
= \left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
,~
E_2
= \left(\begin{array}{ccc}
\frac12  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) , ~
E_3
= \left(\begin{array}{ccc}
1  &0 &0\\
0  &1 & -1\\ 
0  &0 &1\\
\end{array}  \right) \,.
\end{eqnarray*}
%Composing these gives (by matrix multiplication rules worked out in %\hyperref{}
%\begin{eqnarray*}
%E_3E_2E_1
%&= &
% \left(\begin{array}{ccc}
%1  &0 &0\\
%0  &1 & -1\\ 
%0  &0 &1\\
%\end{array}  \right)
% \left(\begin{array}{ccc}
%\frac12  &0 &0\\
%0  &1 &0\\ 
%0  &0 &1\\
%\end{array}  \right)
%\left(\begin{array}{ccc}
%0  &1 &0\\
%1  &0 &0\\ 
%0  &0 &1\\
%\end{array}  \right)
%\\ %2nd line
%&=& \left(\begin{array}{ccc}
%1 &0 &0\\
%0  &1 & -1\\ 
%0  &0 &1\\
%\end{array}  \right) 
%\left(\begin{array}{ccc}
%0  	&\frac12 	& 0\\ 
%1  	&0	 	&0\\
%0  	&0 		&1
%\end{array}  \right) 
%=%%%%%%%%%%%%%%%%
%\left(\begin{array}{ccc}
%0  	&\frac12 	& 0\\ 
%1  	&0	 	&-1\\
%0  	&0 		&1
%\end{array}  \right) 
% \, .
%\end{eqnarray*}
%We showed this was $M^{-1}$ \hyperlink{inversie}{earlier}. 
The inverse of the ERO matrices (corresponding to the description of the reverse row maniplulations)
\begin{eqnarray*}
E_1^{-1}
= \left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
,~
E_2^{-1}
= \left(\begin{array}{ccc}
2  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) , ~
E_3^{-1}
= \left(\begin{array}{ccc}
1  &0 &0\\
0  &1 & 1\\ 
0  &0 &1\\
\end{array}  \right) \,.
\end{eqnarray*}
Multiplying these gives 
\begin{eqnarray*}
E_1^{-1}E_2^{-1}E_3^{-1}
&=& 
\left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
 \left(\begin{array}{ccc}
2  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) 
\left(\begin{array}{ccc}
1  &0 &0\\
0  &1 & 1\\ 
0  &0 &1\\
\end{array}  \right) 
\\ %2nd line
&=&
\left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
 \left(\begin{array}{ccc}
2  &0 &0\\
0  &1 &1\\ 
0  &0 &1\\
\end{array}  \right) 
= %%%%%%%
\left(\begin{array}{ccc}
 0 &1 &1\\
2  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)  = M \, .
\end{eqnarray*}
\end{example}

\section{$LU$, $LDU$, and $LDPU$ Factorizations}
This process of elimination can be stopped half way to obtain decompositions frequently used in large computations in sciences and engineering. 
The first half of the elimination process is to eliminate entries below the diagonal. 
leaving a matrix which is called {\it upper triangular}\index{upper triangular}. The ERO matrices which do this part of the elimination are {\it lower triangular}\index{lower triangular}, as are their inverses. But putting together the upper triangular and lower triangular parts one obtains the so called $LU$ factorization.

\newpage
\begin{example} ($LU$ \hypertarget{elldeeeww}{ factorization})
\begin{eqnarray*}
M=
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
&\stackrel{E_1}{\sim}&
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&-1&1&-1\\
\end{pmatrix}
\\
&\stackrel{E_2}{\sim}&
~~\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&3&1\\
\end{pmatrix}
~~\stackrel{E_3}{\sim}~
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
:=U
\end{eqnarray*}

%%%%%%%%%%%%%%%%%%%%%%%%%%
where the EROs and their inverses are 
%%%%%%%%%%%%%%%
\begin{eqnarray*}
E_1=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
2&0&1&0\\
0&0&0&1\\
\end{pmatrix} \, ,~~~~
E_2=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&1&0&1\\
\end{pmatrix} \, ,~~
E_3=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&-1&1\\
\end{pmatrix} \, 
\\ %%%%%%%%second line
E_1^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&0&0&1\\
\end{pmatrix}  , \,
E_2^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&-1&0&1\\
\end{pmatrix}  , \,
E_3^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&1&1\\
\end{pmatrix} \, .
\end{eqnarray*}
applying the inverses of the EROs to both sides of the equality  $U=E_3E_2E_1M$ gives 
$M=E_1^{-1}E_2^{-1}E_3^{-1}U$ or 
\begin{eqnarray*}
\!\!\! %oi vey
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\!\!\!\!\!\!&=&\!\!\!\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&0&0&1\\
\end{pmatrix} \!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&-1&0&1\\
\end{pmatrix}\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&1&1\\
\end{pmatrix}\!\!\!
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\\
&=&\!\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&0&0&1\\
\end{pmatrix} 
%%%%%%%%
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&-1&1&1\\
\end{pmatrix}
%%%%%%%%%%
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\\
&=&\!\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&-1&1&1\\
\end{pmatrix} 
%
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix} \, .
\end{eqnarray*}
This is a lower triangular matrix times an upper triangular matrix. 
\end{example}

\newpage
What if we stop at a different point in elimination? 
We could multiply rows so that the entries in the diagonal are 1 next. Note that the EROs that do this are diagonal. This gives a slightly different factorization.
\begin{example} ($LDU$ factorization building from previous example)
\begin{eqnarray*}
M=
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\stackrel{E_3E_2E_1}{\sim}
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\stackrel{E_4}{\sim}
\begin{pmatrix}
1&0&\frac{-3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\\
\stackrel{E_5}{\sim}
\begin{pmatrix}
1&0&\frac{-3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&-3\\
\end{pmatrix}
\stackrel{E_6}{\sim}
\begin{pmatrix}
1&0&\frac{-3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&1\\
\end{pmatrix}
=:U
\\
%%%%%%%%the EROs
E_4=
\begin{pmatrix}
\frac12&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix} , \, ~~
E_5=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&\frac13&0\\
0&0&0&1\\
\end{pmatrix} , \, ~~
E_6=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&-\frac13\\
\end{pmatrix} , \, 
\\
%%%%%%%%the ERO inverses
E_4^{-1}=
\begin{pmatrix}
2&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix} , \, 
E_5^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&3&0\\
0&0&0&1\\
\end{pmatrix} , \, 
E_6^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&-3\\
\end{pmatrix} , \, 
\end{eqnarray*}
The equation $U=E_6E_5E_4E_3E_2E_1 M$ can be rearranged into  
$$M=(E_1^{-1}E_2^{-1}E_3^{-1})(E_4^{-1}E_5^{-1}E_6^{-1})U.$$ 
We calculated the product of the first three factors in the previous example; it was named $L$ there, and we will reuse that name here. The product of the next three factors is diagonal and we wil name it $D$. The last factor we named $U$ (the name means something different in this example than the last example.) The $LDU$ factorization of our matrix is
\begin{eqnarray*}
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&-1&1&1\\
\end{pmatrix} 
\begin{pmatrix}
2&0&0&0\\
0&1&0&0\\
0&0&3&0\\
0&0&1&-3\\
\end{pmatrix} 
\begin{pmatrix}
1&0&\frac{-3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&1\\
\end{pmatrix}
\end{eqnarray*}
\end{example}

The $LDU$ factorization of a matrix is a factorization into blocks of EROs of a various types: $L$ is the product of the inverses of EROs which eliminate below the diagonal by row addition, $D$ the product of inverses of EROs which set the diagonal elements to 1 by row multiplication, and $U$ is the product of inverses of EROs which eliminate above the diagonal by row addition.

\hypertarget{LDPU}{
You} may notice that one of the three kinds of row operation is missing from this story. 
Row exchange my very well be necessary to obtain RREF. Indeed, 
so far in this chapter we have been working under the tacit assumption that 
$M$ can be brought to the identity by just row multiplication and row addition. 
If row exchange is necessary, the resulting factorization is $LDPU$ where $P$ is the product of inverses of EROs that perform row exchange. 

\begin{example} ($LDPU$ factorization, building from previous examples)
\begin{eqnarray*}
M=
\begin{pmatrix}
0&1&2&2\\
2&0&-3&1\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\stackrel{E_7}{\sim}
\begin{pmatrix}
0&1&2&2\\
2&0&-3&1\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\stackrel{E_6E_5E_4E_3E_2E_1}{\sim} L
\end{eqnarray*}
\begin{eqnarray*}
E_7=
\begin{pmatrix}
0&1&0&0\\
1&0&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix}
=E_7^{-1}
\end{eqnarray*}
\begin{eqnarray*}
M=(E_1^{-1}E_2^{-1}E_3^{-1})(E_4^{-1}E_5^{-1}E_6^{-1}) (E_7^{-1}) U=LDPU\\
\end{eqnarray*}
%%%%%%%last line!
\begin{eqnarray*}
\!\!\!\begin{pmatrix}
0&1&2&2\\
2&0&-3&1\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\!\!\!=\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&-1&1&1\\
\end{pmatrix} 
\!\!\!
\begin{pmatrix}
2&0&0&0\\
0&1&0&0\\
0&0&3&0\\
0&0&1&-3\\
\end{pmatrix} 
\!\!\!
\begin{pmatrix}
0&1&0&0\\
1&0&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix}
\!\!\!
\begin{pmatrix}
1&0&\frac{-3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&1\\
\end{pmatrix}
\end{eqnarray*}

\end{example}

%\References{
%Hefferon, Chapter One, Section 1.1 and 1.2
%\\
%Beezer, Chapter SLE, Section RREF
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/Row_echelon_form}{Row Echelon Form}
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/Elementary_matrix_transformations}{Elementary Matrix Operations}}

\section{Review Problems}
\input{\elemRowOpsPath/problems}

\newpage

","
\chapter{\elemRowOpsTitle}

%\hypertarget{Elementary Row Operations} %what is this? 

Elementary row operations are  systems of linear equations between old and new  rows. 


\begin{example} (\hypertarget{Keeping track of EROs with equations between rows}{Keeping track of EROs with equations between rows})\\
We will refer to the new $k$th row as $R'_k$ and the old $k$th row as $R_k$.
\begin{eqnarray*}
\begin{amatrix}{3} 
0 & 1 & 1 & 7 \\ 
2 & 0 & 0& 4 \\
0& 0 & 1 & 4 \\
\end{amatrix} 
& \stackrel{R_1'=0R_1+\phantom{1}R_2+0R_3}{\stackrel{R_2'=\phantom{1}R_1+0R_2+0R_3}{ \stackrel{R_3'= 0R_1+0R_2+\phantom{1}R_3}{\sim}}}&
\begin{amatrix}{3} 
2 & 0 & 0 & 4 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\\[.2cm]
& \stackrel{R_1'=\frac12 R_1+0R_2+0R_3}{\stackrel{R_2'=0R_1+\phantom{1}R_2+0R_3}{ \stackrel{R_3'= 0R_1+0R_2+\phantom{1}R_3}{\sim}} }&
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\\[.2cm]
& \stackrel{R_1'= \phantom{1}R_1+0R_2+0R_3}{\stackrel{R_2'=0R_1+\phantom{1}R_2-\phantom{1}R_3}{ \stackrel{R_3'=0R_1+0R_2+\phantom{1} R_3}{\sim}} }&
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 0& 3 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\end{eqnarray*}
\end{example}

%\videoscriptlink{elementary_row_operations_example.mp4}{Example}{script_elementary_row_operations_example}

%\begin{center}\href{\webworkurl ReadingHomework3/1/}{Reading homework: problem 3.1}\end{center}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{EROs as Matrices}
The matrix describing the system of equations between rows performs the corresponding ERO on the augmented matrix:
\begin{example} (Performing EROs with Matrices)\\
\begin{eqnarray*}
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  
\end{pmatrix} 
\begin{amatrix}{3} 
0 & 1 & 1 & 7 \\ 
2 & 0 & 0& 4 \\
0& 0 & 1 & 4 
\end{amatrix} 
=
\begin{amatrix}{3} 
2 & 0 & 0 & 4 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\\ %second line
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 & 0 \\
0& 0 & 1  \\
\end{pmatrix}
\begin{amatrix}{3} 
2 & 0 & 0 & 4 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
=
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\\ %third line
\begin{pmatrix}
1&0&0\\
0&1&-1\\
0&0&1
\end{pmatrix}
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 1& 7 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix} 
=
\begin{amatrix}{3} 
1 & 0 & 0 & 2 \\
0 & 1 & 0& 3 \\
0 & 0 & 1 & 4 \\ 
\end{amatrix}
\end{eqnarray*}
\end{example}

This obviously involved more writing than Gaussian elimination. 
The point here is that realizing EROs as matrices allows us to make concrete the notion of ``dividing by a matrix'' \hyperlink{ch1divide}{alluded to in chapter 1}; we can now perform manipulations on both sides of an equation in a familiar way.

\begin{example} (Undoing $A$ in $Ax=b$ slowly, using $A=6=3\cdot2$)
\begin{eqnarray*}
6x&=&\phantom{ 3^{-1}} 12 \\
3^{-1}6x&=&3^{-1}12 \\
 2x&=&\phantom{3^{-1}~}4  \\
 2^{-1}2x&=&2^{-1}~4\\ %  \Leftrightarrow 
  1x&=&\phantom{3^{-1}~} 2
\end{eqnarray*}
\end{example}

\noindent
In particular, matrices corresponding to EROs undo a matrix step by step.
\begin{example} (\hypertarget{Undoing} $A$ in $Ax=b$ slowly, Using $A=M=...$)
\begin{eqnarray*}
\begin{pmatrix}
0 & 1 & 1  \\ 
2 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
}
~
\begin{pmatrix}
 7 \\ 
4 \\
4\\
\end{pmatrix} 
\\\nn %second line
%
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
%
\begin{pmatrix}
0 & 1 & 1  \\ 
2 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
~
\begin{pmatrix}
 7 \\ 
4 \\
4\\
\end{pmatrix} 
\\\nn %third line
\phantom{-}
\begin{pmatrix}
2 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
0 & 1 & 0  \\ 
1 & 0 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
}
~
\begin{pmatrix}
4 \\
7 \\ 
4\\
\end{pmatrix} 
\\ \nn %fourth line
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
2 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
%\phantom{
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 &  0\\
0& 0 & 1  \\
\end{pmatrix} 
%}
~
\begin{pmatrix}
4 \\
7 \\ 
4\\
\end{pmatrix} 
\\\nn %fifth line
\phantom{
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 & 0 \\
0& 0 & 1  \\
\end{pmatrix} 
}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
\frac12 & 0 & 0  \\ 
0 & 1 &  0\\
0& 0 & 1  \\
\end{pmatrix} 
}
~
\begin{pmatrix}
2 \\
7 \\ 
4\\
\end{pmatrix} 
\\ \nn %sixth line
%\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 & -1 \\
0& 0 & 1  \\
\end{pmatrix} 
%}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 1  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
%\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 &  -1\\
0& 0  &  1  \\
\end{pmatrix} 
%}
\! \!
\begin{pmatrix}
2 \\
7 \\ 
4\\
\end{pmatrix} 
\\\nn%%%%%%%%%%%%%%%%%%%%%%%7th line
\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 & -1 \\
0& 0 & 1  \\
\end{pmatrix} 
}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0  \\ 
0& 0 & 1  \\
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y \\
z 
\end{pmatrix} 
&=&
\phantom{
\begin{pmatrix}
1 & 0 & 0  \\ 
0 & 1 &  -1\\
0& 0  &  1  \\
\end{pmatrix} 
}
\! 
\begin{pmatrix}
2 \\
3 \\ 
4\\
\end{pmatrix} 
\end{eqnarray*}
\end{example}



\noindent
This is another way of thinking about what is happening in the process of elimination which feels more like elementary algebra in the sense that you ``do something to both sides of an equation"" until you have a solution. 


%%%%%%%%%%%%%%%%%%

\section{Recording EROs in $[M | I ] $}
Just as we put together $3^{-1}2^{-1}=6^{-1}$ to get a single thing to apply to both sides of $6x=12$ to undo $6$, we should put together multiple EROs  to get a single thing that undoes our matrix. 
To do this, augment by the identity matrix (not just a single column) and then perform Gaussian elimination. 
There is no need to write the EROs as systems of equations or as matrices while doing this. 
%That is, perform the EROs without the their corresponding matrices.

\begin{example} (Collecting EROs that \hypertarget{undo a matrix}{undo a matrix})
\begin{eqnarray*}
\left(\begin{array}{ccc|ccc}
0 & 1 & 1 &1 &0 &0\\ 
2 & 0 & 0 &0&1&0\\
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)
\sim
&\left(\begin{array}{ccc|ccc}
2 & 0 & 0 &0&1&0\\
0 & 1 & 1 &1 &0 &0\\ 
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)&
\\
\sim
&\left(\begin{array}{ccc|ccc}
1 & 0 & 0 &0&\frac12&0\\
0 & 1 & 1 &1 &0 &0\\ 
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)&
\sim
\left(\begin{array}{ccc|ccc}
1 & 0 & 0 &0&\frac12&0\\
0 & 1 & 0 &1 &0 &-1\\ 
0& 0 & 1   &0  &0 &1\\
\end{array}  \right)
\end{eqnarray*}
\end{example}
\noindent
As we changed the left slot from the matrix $M$ to the identity matrix, the right slot changed from the identity matrix to the matrix which undoes $M$: 
\begin{example} (Checking that \hypertarget{inversie}{one matrix undoes another})
\begin{eqnarray*}
\left(\begin{array}{ccc}
0&\frac12&0\\
1 &0 &-1\\ 
0  &0 &1\\
\end{array}  \right)
\left(\begin{array}{ccc}
0&1&1\\
2 &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
=
\left(\begin{array}{ccc}
1  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) \, .
\end{eqnarray*}
If the matrices are composed in the opposite order, the result is the same.
\begin{eqnarray*}
\left(\begin{array}{ccc}
0&1&1\\
2 &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
\left(\begin{array}{ccc}
0&\frac12&0\\
1 &0 &-1\\ 
0  &0 &1\\
\end{array}  \right)
=
\left(\begin{array}{ccc}
1  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) \, .
\end{eqnarray*}
\end{example}

%\reading{3}{1}


In abstract generality, let $M$ be some matrix and, as always, let $I$ stand for the identity matrix. Imagine the process of performing elementary row operations to bring $M$ to the identity matrix. 
\begin{equation*}
(M | I) \sim ( E_1M| E_1)\sim (E_2E_1 M | E_2 E_1) \sim \cdots \sim (I | \cdots E_2E_1 )
\end{equation*}
Ellipses stand for additional EROs. The result is a product of matrices that form a matrix which undoes $M$
\begin{equation*}
\cdots E_2 E_1 M =  I 
\end{equation*}
This is only true if RREF of $M$ is the identity matrix.  
In that case, we say $M$ is {\it invertible}\index{invertiblech3}. 


Much use is made of the fact that invertible matrices can be undone with EROs. 
To begin with, since each  elementary row operation has an inverse, 
$$
M= E_1^{-1} E_2^{-1} \cdots
$$
while the inverse of $M$ is 
\begin{equation*}
M^{-1}=\cdots E_2 E_1 
\end{equation*}
This is symbolically verified as
\begin{equation*}
M^{-1}M=\cdots E_2 E_1\, E_1^{-1} E_2^{-1} \cdots
=\cdots E_2 \, E_2^{-1} \cdots = \cdots = I
\end{equation*}
Thus, if $M$ is invertible then  $M$ and can be expressed as the product of EROs. (The same is true for its inverse.) This has the feel of the fundamental theorem of arithmetic (integers can be expressed as the product of primes) or the fundamental theorem of algebra (polynomials can be expressed as the product of first order polynomials); EROs are the building blocks of invertible matrices. 




\section{Three Kinds of EROs, Three Kinds of Matrices}

%To use this in concrete examples, one uses the fact that i
We now work toward concrete examples and applications. 
It is surprisingly easy to translate between EROs as descriptions of rows and as matrices.
The matrices corresponding to these kinds are close in form to the identity matrix:
\begin{itemize}
\item Row Swap: Identity matrix with two rows swapped
\item Scalar Multiplication:  Identity matrix with one diagonal entry not 1.
\item Row Sum: The identity matrix with one off diagonal entry not 0.
\end{itemize}


\begin{example} (Correspondences between EROs and their matrices)
\begin{itemize}
\item The row swap matrix that swaps the 2nd and 4th row is the identity matrix with the 2nd and 4th row swapped; 
$$
\begin{pmatrix}
1&0&0&0&0\\
0&0&0&1&0\\
0&0&1&0&0\\
0&1&0&0&0\\
0&0&0&0&1\\
\end{pmatrix}
$$
\item
The scalar multiplication matrix that replaces the 3rd row with 7 times the 3rd row is the identity matrix with 7 in the 3rd row instead of 1; 
$$
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&7&0\\
0&0&0&1\\
\end{pmatrix}
$$

\item The row sum matrix that replaces the 4th row with the 4th row plus 9 times the 2nd row is the identity matrix with a 9 in the  4th row, 2nd column
$$
\begin{pmatrix}
1&0&0&0&0&0&0\\
0&1&0&0&0&0&0\\
0&0&1&0&0&0&0\\
0&9&0&1&0&0&0\\
0&0&0&0&1&0&0\\
0&0&0&0&0&1&0\\
0&0&0&0&0&0&1\\
\end{pmatrix}
$$
\end{itemize}
\end{example}

We can write an explicit factorization of a matrix into EROs by keeping track of the EROs used in getting to RREF.

\begin{example} (Express $M$ from the  
\hyperlink{undo a matrix}{earlier example} as a product of EROs)\\
Note that in the previous example 
one of each of the kinds of EROs is used, in the order just given.
Elimination looked like 
\begin{eqnarray*}
M=
\left(\begin{array}{ccc}
0 & 1 & 1 \\ 
2 & 0 & 0 \\\
0& 0 & 1  
\end{array}  \right)
\stackrel{E_1}{\sim}
\left(\begin{array}{ccc}
2 & 0 & 0 \\
0 & 1 & 1 \\ 
0& 0 & 1  
\end{array}  \right)
\stackrel{E_2}{\sim}
\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 1 \\ 
0& 0 & 1  
\end{array}  \right)
\stackrel{E_3}{\sim}
\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\ 
0& 0 & 1  
\end{array}  \right)
=I
\end{eqnarray*}
where the EROs matrices are 
\begin{eqnarray*}
E_1
= \left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
,~
E_2
= \left(\begin{array}{ccc}
\frac12  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) , ~
E_3
= \left(\begin{array}{ccc}
1  &0 &0\\
0  &1 & -1\\ 
0  &0 &1\\
\end{array}  \right) \,.
\end{eqnarray*}
%Composing these gives (by matrix multiplication rules worked out in %\hyperref{}
%\begin{eqnarray*}
%E_3E_2E_1
%&= &
% \left(\begin{array}{ccc}
%1  &0 &0\\
%0  &1 & -1\\ 
%0  &0 &1\\
%\end{array}  \right)
% \left(\begin{array}{ccc}
%\frac12  &0 &0\\
%0  &1 &0\\ 
%0  &0 &1\\
%\end{array}  \right)
%\left(\begin{array}{ccc}
%0  &1 &0\\
%1  &0 &0\\ 
%0  &0 &1\\
%\end{array}  \right)
%\\ %2nd line
%&=& \left(\begin{array}{ccc}
%1 &0 &0\\
%0  &1 & -1\\ 
%0  &0 &1\\
%\end{array}  \right) 
%\left(\begin{array}{ccc}
%0  	&\frac12 	& 0\\ 
%1  	&0	 	&0\\
%0  	&0 		&1
%\end{array}  \right) 
%=%%%%%%%%%%%%%%%%
%\left(\begin{array}{ccc}
%0  	&\frac12 	& 0\\ 
%1  	&0	 	&-1\\
%0  	&0 		&1
%\end{array}  \right) 
% \, .
%\end{eqnarray*}
%We showed this was $M^{-1}$ \hyperlink{inversie}{earlier}. 
The inverse of the ERO matrices (corresponding to the description of the reverse row maniplulations)
\begin{eqnarray*}
E_1^{-1}
= \left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
,~
E_2^{-1}
= \left(\begin{array}{ccc}
2  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) , ~
E_3^{-1}
= \left(\begin{array}{ccc}
1  &0 &0\\
0  &1 & 1\\ 
0  &0 &1\\
\end{array}  \right) \,.
\end{eqnarray*}
Multiplying these gives 
\begin{eqnarray*}
E_1^{-1}E_2^{-1}E_3^{-1}
&=& 
\left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
 \left(\begin{array}{ccc}
2  &0 &0\\
0  &1 &0\\ 
0  &0 &1\\
\end{array}  \right) 
\left(\begin{array}{ccc}
1  &0 &0\\
0  &1 & 1\\ 
0  &0 &1\\
\end{array}  \right) 
\\ %2nd line
&=&
\left(\begin{array}{ccc}
0  &1 &0\\
1  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)
 \left(\begin{array}{ccc}
2  &0 &0\\
0  &1 &1\\ 
0  &0 &1\\
\end{array}  \right) 
= %%%%%%%
\left(\begin{array}{ccc}
 0 &1 &1\\
2  &0 &0\\ 
0  &0 &1\\
\end{array}  \right)  = M \, .
\end{eqnarray*}
\end{example}

\section{$LU$, $LDU$, and $LDPU$ Factorizations}
This process of elimination can be stopped half way to obtain decompositions frequently used in large computations in sciences and engineering. 
The first half of the elimination process is to eliminate entries below the diagonal. 
leaving a matrix which is called {\it upper triangular}\index{upper triangular}. The ERO matrices which do this part of the elimination are {\it lower triangular}\index{lower triangular}, as are their inverses. But putting together the upper triangular and lower triangular parts one obtains the so called $LU$ factorization.

\newpage
\begin{example} ($LU$ \hypertarget{elldeeeww}{ factorization})
\begin{eqnarray*}
M=
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
&\stackrel{E_1}{\sim}&
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&-1&1&-1\\
\end{pmatrix}
\\
&\stackrel{E_2}{\sim}&
~~\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&3&1\\
\end{pmatrix}
~~\stackrel{E_3}{\sim}~
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
:=U
\end{eqnarray*}

%%%%%%%%%%%%%%%%%%%%%%%%%%
where the EROs and their inverses are 
%%%%%%%%%%%%%%%
\begin{eqnarray*}
E_1=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
2&0&1&0\\
0&0&0&1\\
\end{pmatrix} \, ,~~~~
E_2=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&1&0&1\\
\end{pmatrix} \, ,~~
E_3=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&-1&1\\
\end{pmatrix} \, 
\\ %%%%%%%%second line
E_1^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&0&0&1\\
\end{pmatrix}  , \,
E_2^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&-1&0&1\\
\end{pmatrix}  , \,
E_3^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&1&1\\
\end{pmatrix} \, .
\end{eqnarray*}
applying the inverses of the EROs to both sides of the equality  $U=E_3E_2E_1M$ gives 
$M=E_1^{-1}E_2^{-1}E_3^{-1}U$ or 
\begin{eqnarray*}
\!\!\! %oi vey
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\!\!\!\!\!\!&=&\!\!\!\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&0&0&1\\
\end{pmatrix} \!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&-1&0&1\\
\end{pmatrix}\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&1&1\\
\end{pmatrix}\!\!\!
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\\
&=&\!\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&0&0&1\\
\end{pmatrix} 
%%%%%%%%
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&-1&1&1\\
\end{pmatrix}
%%%%%%%%%%
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\\
&=&\!\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&-1&1&1\\
\end{pmatrix} 
%
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix} \, .
\end{eqnarray*}
This is a lower triangular matrix times an upper triangular matrix. 
\end{example}

\newpage
What if we stop at a different point in elimination? 
We could multiply rows so that the entries in the diagonal are 1 next. Note that the EROs that do this are diagonal. This gives a slightly different factorization.
\begin{example} ($LDU$ factorization building from previous example)
\begin{eqnarray*}
M=
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\stackrel{E_3E_2E_1}{\sim}
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\stackrel{E_4}{\sim}
\begin{pmatrix}
1&0&\frac{-3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&3&4\\
0&0&0&-3\\
\end{pmatrix}
\\
\stackrel{E_5}{\sim}
\begin{pmatrix}
1&0&\frac{-3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&-3\\
\end{pmatrix}
\stackrel{E_6}{\sim}
\begin{pmatrix}
1&0&\frac{-3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&1\\
\end{pmatrix}
=:U
\\
%%%%%%%%the EROs
E_4=
\begin{pmatrix}
\frac12&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix} , \, ~~
E_5=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&\frac13&0\\
0&0&0&1\\
\end{pmatrix} , \, ~~
E_6=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&-\frac13\\
\end{pmatrix} , \, 
\\
%%%%%%%%the ERO inverses
E_4^{-1}=
\begin{pmatrix}
2&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix} , \, 
E_5^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&3&0\\
0&0&0&1\\
\end{pmatrix} , \, 
E_6^{-1}=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&-3\\
\end{pmatrix} , \, 
\end{eqnarray*}
The equation $U=E_6E_5E_4E_3E_2E_1 M$ can be rearranged into  
$$M=(E_1^{-1}E_2^{-1}E_3^{-1})(E_4^{-1}E_5^{-1}E_6^{-1})U.$$ 
We calculated the product of the first three factors in the previous example; it was named $L$ there, and we will reuse that name here. The product of the next three factors is diagonal and we wil name it $D$. The last factor we named $U$ (the name means something different in this example than the last example.) The $LDU$ factorization of our matrix is
\begin{eqnarray*}
\begin{pmatrix}
2&0&-3&1\\
0&1&2&2\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
=
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&-1&1&1\\
\end{pmatrix} 
\begin{pmatrix}
2&0&0&0\\
0&1&0&0\\
0&0&3&0\\
0&0&1&-3\\
\end{pmatrix} 
\begin{pmatrix}
1&0&\frac{-3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&1\\
\end{pmatrix}
\end{eqnarray*}
\end{example}

The $LDU$ factorization of a matrix is a factorization into blocks of EROs of a various types: $L$ is the product of the inverses of EROs which eliminate below the diagonal by row addition, $D$ the product of inverses of EROs which set the diagonal elements to 1 by row multiplication, and $U$ is the product of inverses of EROs which eliminate above the diagonal by row addition.

\hypertarget{LDPU}{
You} may notice that one of the three kinds of row operation is missing from this story. 
Row exchange my very well be necessary to obtain RREF. Indeed, 
so far in this chapter we have been working under the tacit assumption that 
$M$ can be brought to the identity by just row multiplication and row addition. 
If row exchange is necessary, the resulting factorization is $LDPU$ where $P$ is the product of inverses of EROs that perform row exchange. 

\begin{example} ($LDPU$ factorization, building from previous examples)
\begin{eqnarray*}
M=
\begin{pmatrix}
0&1&2&2\\
2&0&-3&1\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\stackrel{E_7}{\sim}
\begin{pmatrix}
0&1&2&2\\
2&0&-3&1\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\stackrel{E_6E_5E_4E_3E_2E_1}{\sim} L
\end{eqnarray*}
\begin{eqnarray*}
E_7=
\begin{pmatrix}
0&1&0&0\\
1&0&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix}
=E_7^{-1}
\end{eqnarray*}
\begin{eqnarray*}
M=(E_1^{-1}E_2^{-1}E_3^{-1})(E_4^{-1}E_5^{-1}E_6^{-1}) (E_7^{-1}) U=LDPU\\
\end{eqnarray*}
%%%%%%%last line!
\begin{eqnarray*}
\!\!\!\begin{pmatrix}
0&1&2&2\\
2&0&-3&1\\
-4&0&9&2\\
0&-1&1&-1\\
\end{pmatrix}
\!\!\!=\!\!\!
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
-2&0&1&0\\
0&-1&1&1\\
\end{pmatrix} 
\!\!\!
\begin{pmatrix}
2&0&0&0\\
0&1&0&0\\
0&0&3&0\\
0&0&1&-3\\
\end{pmatrix} 
\!\!\!
\begin{pmatrix}
0&1&0&0\\
1&0&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix}
\!\!\!
\begin{pmatrix}
1&0&\frac{-3}{2}&\frac{1}{2}\\
0&1&2&2\\
0&0&1&\frac43\\
0&0&0&1\\
\end{pmatrix}
\end{eqnarray*}

\end{example}

%\References{
%Hefferon, Chapter One, Section 1.1 and 1.2
%\\
%Beezer, Chapter SLE, Section RREF
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/Row_echelon_form}{Row Echelon Form}
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/Elementary_matrix_transformations}{Elementary Matrix Operations}}

\section{Review Problems}
\input{\elemRowOpsPath/problems}

\newpage

",lesson
4,Solution Sets For Systems Of Linear Equations,"\chapter{\solutionSetsTitle}

Algebra problems can have multiple solutions. For example $x(x-1)=0$ has  two solutions: $0$ and $1$. By contrast, equations of the form $Ax=b$ with $A$ a linear operator have have the following property.\\



If $A$ is a linear operator and $b$ is a known then $Ax=b$ has either
\begin{enumerate}
\item One solution
\item  No solutions
\item Infinitely many solutions
\end{enumerate}


\section{The Geometry of Solution Sets: Hyperplanes}
Consider the following algebra problems and their solutions

\begin{enumerate}
\item $6x=12$, one solution: $2$
\item $0x=12$, no solution
\item $0x=0$, one solution for each number: $x$
\end{enumerate}
In each case the linear operator is a $1\times 1$ matrix. In the first case, the linear operator is invertible. 
In the other two cases it is not. 
In the first case, the solution set is a point on the number line, in the third case the solution set is the whole number line.

Lets examine similar situations with larger matrices.
\begin{enumerate}
\item
$\begin{pmatrix}
6	&0 	\\
0 	&2 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
12 \\ 
6
\end{pmatrix}$, one solution: 
$\begin{pmatrix}
2 \\ 
3
\end{pmatrix}$
%\\linear operator is invertible

\item 
$\begin{pmatrix}
1	&3 	\\
0 	&0 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
4 \\ 
1 
\end{pmatrix}$, no solutions
%not in the range of the linear operator

\item 
$\begin{pmatrix}
1	&3 	\\
0 	&0 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
4 \\ 
0
\end{pmatrix} $, one solution for each number $y$: 
$\begin{pmatrix}
4-3y \\ 
y
\end{pmatrix} $

\item 
$\begin{pmatrix}
0	&0 	\\
0 	&0 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
0 \\ 
0
\end{pmatrix} $, one solution for each pair of numbers $x,y$:
$\begin{pmatrix}
x\\ 
y
\end{pmatrix} $
\end{enumerate}
Again, in the first case the linear operator is invertible while in the other cases it is not. When the operator is not invertible the solution set can be empty, a line in the plane or the plane itself.


For a system of equations with $r$ equations and $k$ veriables, one can have a number of different outcomes.  For example, consider the case of $r$ equations in three variables.  Each of these equations is the equation of a plane in three-dimensional space.  To find solutions to the system of equations, we look for the common intersection of the planes (if an intersection exists).  Here we have five different possibilities:

\begin{enumerate}
\item \textbf{Unique Solution.}  The planes have a unique point of intersection.

\item \textbf{No solutions.}  Some of the equations are contradictory, so no solutions exist.

\item \textbf{Line.}  The planes intersect in a common line; any point on that line then gives a solution to the system of equations.

\item \textbf{Plane.}  Perhaps you only had one equation to begin with, or else all of the equations coincide geometrically.  In this case, you have a plane of solutions, with two free parameters.

\videoscriptlink{solution_sets_for_systems_of_linear_equations_planes.mp4}{Planes}{solution_sets_for_systems_of_linear_equations_planes}

\item \textbf{All of $\mathbb{R}^3$.}  If you start with no information, then any point in $\mathbb{R}^3$ is a solution.  There are three free parameters.
\end{enumerate}

In general, for systems of equations with $k$ unknowns, there are $k+2$ possible outcomes, corresponding to the possible numbers (i.e. $0,1,2,\dots,k$) of free parameters in the solutions set plus the possibility of no solutions.  These types of ``solution sets''\index{Solution set} are ``hyperplanes''\index{Hyperplane}, generalizations of planes the behave like planes in $\mathbb{R}^3$ in many ways.

\videoscriptlink{solution_sets_for_systems_of_linear_equations_overview.mp4}{Pictures and Explanation}{solution_sets_for_systems_of_linear_equations_overview}

\vspace{3mm}

\reading{4}{1}
%\begin{center}\href{\webworkurl ReadingHomework4/1/}{Reading homework: problem 4.1}\end{center}



\section{Particular Solution $+$ Homogeneous solutions }

In the \hyperlink{standard approach}{standard approach}, variables corresponding to columns that do not contain a pivot (after going to reduced row echelon form) are \emph{free}.  
We called them non-pivot variables. 
They index elements of the solutions set by acting as coefficients of vectors.
%In this way the number of non-pivot columns determines (in part) the size of the solution set.  
%We can denote them with dummy variables $\mu_1, \mu_2, \ldots$. 

\begin{example} (Non-pivot columns determine terms of the solutions)
$$\begin{pmatrix}
1 &  0 & 1 & -1 \\ 
 0 & 1 & -1& 1  \\
 0 &0   & 0  & 0 \\
\end{pmatrix}
\colvec{x_1\\x_2\\x_3\\x_4} 
=
\colvec{1\\1\\0} 
\Leftrightarrow
\left\{
\begin{array}{lcr}
	1x_1 +0x_2+ 1x_3 - 1x_4 & = 1 \\
	0x_1 +1x_2 - 1x_3 + 1x_4 & = 1 \\
	0x_1 +0x_2 + 0x_3 + 0x_4 & = 0 
\end{array}
     \right.
$$
Following the standard approach, express the pivot variables in terms of the non-pivot variables and add ``freebee equations"". Here $x_3$ and $x_4$ are non-pivot variables.  
\begin{eqnarray*}
\left.
\begin{array}{rcl}
	x_1 & = &1 -x_3+x_4 \\
	x_2 & = &1 +x_3-x_4 \\
	x_3 & = &\phantom{1+~\,}x_3\\
	x_4 & =&\phantom{1+x_3+~}x_4         
\end{array}
     \right\}
     \Leftrightarrow
\colvec{x_1\\x_2\\x_3\\x_4} 
= \colvec{1\\1\\0\\0} + x_3\colvec{-1\\1\\1\\0} + x_4\colvec{1\\-1\\0\\1}
\end{eqnarray*}
The preferred way to write a solution set is with set notation\index{Solution set!set notation}.  \[S = \left\{\colvec{x_1\\x_2\\x_3\\x_4} = \colvec{1\\1\\ 0\\0 } + \mu_1 \colvec{-1\\1\\1\\0 }  + \mu_2  \colvec{1\\-1\\ 0 \\1 } : \mu_1,\mu_2\in  {\mathbb R} \right\} \]
Notice that the first two components of the second two terms come from the non-pivot columns
Another way to write the solution set is
\[S= \left\{  X_0 + \mu_1 Y_1 + \mu_2 Y_2   : \mu_1,\mu_2 \in  {\mathbb R}   \right\} \]
where 
\[X_0= \colvec{1\\1\\0 \\0 }, Y_1=\colvec{-1\\1\\1\\0 } , Y_2= \colvec{1\\-1\\0 \\1 }
\]
\end{example}
Here $X_0$ is called a particular solution while $Y_1$ and $Y_2$ are called homogeneous solutions. 



\section{Linearity and these parts}
%
%\begin{definition}   A function $f$ is \emph{linear}\index{Linear!function} if 
%for any vectors $X,Y$  in the domain of $f$, and any scalars $\alpha,\beta$ 
%\[f(\alpha X + \beta Y) = \alpha f(X) + \beta f(Y) \,.\]
%\end{definition}

%
%
%\begin{example}
%\hypertarget{solution_sets_for_systems_of_linear_equations_concrete_example}{Consider our example system above with} 
%\[
%M=    \begin{pmatrix}
%      1  & 0  & 1 & -1  \\
%       0  & 1 & -1 & 1  \\
%        0 &0   & 0  & 0    \\
%    \end{pmatrix} \, ,\quad
%X= \colvec{x_1\\x_2\\x_3\\x_4} \mbox{ and } Y=\colvec{y_1\\y_2\\y^3 \\y^4 }\, ,
%\]
%and take for the function of vectors
%$$
%f(X)=MX\, .
%$$
%Now let us check the linearity property for $f$. 
%The property needs to hold for {\it any} scalars $\alpha$ and $\beta$, so for simplicity
%let us concentrate first on the case $\alpha=\beta=1$. This means that we need to
%compare the following two calculations:
%\begin{enumerate}
%\item First add $X+Y$, then compute $f(X+Y)$.
%\item First compute $f(X)$ and $f(Y)$, then compute the sum $f(X)+f(Y)$.
%\end{enumerate}
%The second computation is slightly easier:
%$$
%f(X) = MX 
%    =\colvec{x_1+x_3-x_4\\x_2-x_3+x_4\\0}\mbox{ and }
%f(Y) = MY   
%    =\colvec{y_1+y_3-y_4\\y_2-y_3+y_4\\0}\, ,
%$$
%(using our result above). Adding these gives
%$$
%f(X)+f(Y)=\colvec{x_1+x_3-x_4+y_1+y_3-y_4\\[1mm]x_2-x_3+x_4+y_2-y_3+y_4\\[1mm]0}\, .
%$$
%Next we perform the first computation beginning with:
%$$
%X+Y=\colvec{x_1 + y_1\\x_2+y_2\\ x_3+y_3\\ x_4+y_4}\, ,
%$$
%from which we calculate
%$$
%f(X+Y)=\colvec{x_1+y_2+x_3+y_3-(x_4+y_4)\\[1mm] x_2+y_2-(x_3+y_3)+x_4+y_4\\[1mm]0}\, .
%$$
%Distributing the minus signs and remembering that the order of adding numbers like $x_1,x_2,\ldots$ 
%does not matter, we see that the two computations give exactly the same answer.
%
%Of course, you should complain that we took a special choice of $\alpha$ and $\beta$.
%Actually, to take care of this we only need to check that $f(\alpha X)=\alpha f(X)$.
%It is your job to explain this in  \hyperref[linear]{Review Question~\ref*{linear}}
%\end{example}
%
%\noindent
%Later we will show that matrix multiplication is always linear.  Then we will know that:
With the previous example in mind, lets say that the matrix equation $MX=V$ has  solution set  $\{ X_0 + \mu_1 Y_1 + \mu_2 Y_2):\mu_1,\mu_2 \in {\mathbb R} \}$.
Recall from \hyperlink{{Matrices are linear operators}}{earlier} that matrices are linear.
%\[M(\alpha X + \beta Y) = \alpha MX + \beta MY\]
%
%Then 
%
%the two equations 
Thus 
$$M( X_0 + \mu_1 Y_1 + \mu_2 Y_2)  = MX_0 + \mu_1MY_1 + \mu_2MY_2 =V$$
for \emph{any} $\mu_1, \mu_2 \in \mathbb{R}$. 
Choosing $\mu_1=\mu_2=0$, we obtain 
$$MX_0=V\, .$$  
This is why $X_0$ is an example of a  \emph{particular solution}\index{Particular solution!an example}.

%Given a particular solution to the system, we can then deduce that $\mu_1MY_1 + \mu_2MY_2 = 0$.  
Setting $\mu_1=1, \mu_2=0$, and using the particular solution $MX_0=V$, we obtain 
$$MY_1=0\, .$$ 
Likewise, setting $\mu_1=0, \mu_2=1$, we obtain $$MY_2=0\, .$$
Here $Y_1$ and $Y_2$ are examples of what are called  \emph{homogeneous} solutions\index{Homogeneous solution!an example} to the system.
They {\it do not} solve the original equation $MX=V$, but instead its associated 
{\it homogeneous  equation}\index{homogeneous equation} $M Y =0$.

One of the fundamental lessons of linear algebra: the  solution set to $Ax=b$ with $A$ a linear operator consists of a particular solution plus homogeneous solutions.

\begin{center}
\shabox{
general solution $=$ particular solution $+$ homogeneous solutions.}
\end{center}

\begin{example}
Consider the matrix equation of the previous example. It has  solution set
\[S = \left\{\colvec{x_1\\x_2\\x_3\\x_4} = \colvec{1\\1\\0 \\0 } + \mu_1 \colvec{-1\\1\\1\\0 } + \mu_2 \colvec{1\\-1\\ 0\\1 } \right\} \]
Then $MX_0=V$ says that $\colvec{x_1\\x_2\\x_3\\x_4} = 
\colvec{1\\1\\0 \\ 0}$ solves the original matrix equation, which is certainly true, but this is not the only solution.

$MY_1=0$ says that $\colvec{x_1\\x_2\\x_3\\x_4} = \colvec{-1\\1\\1\\ 0}
$ solves the homogeneous equation.

\vspace{2mm}

$MY_2=0$ says that $\colvec{x_1\\x_2\\x_3\\x_4} = 
\colvec{1\\-1\\0 \\1}$ solves the homogeneous equation.

\vspace{2mm}

\noindent
Notice how adding any multiple of a homogeneous solution to the particular solution yields another particular solution.
\end{example}

%\begin{definition}
%Let $M$ a matrix and $V$ a vector.  Given the linear system $MX=V$, we call $X_0$ a \emph{particular solution}\index{Particular solution} if $MX_0=V$.  We call $Y$ a \emph{homogeneous solution} if $MY=0$.  
%The linear system 
%$$MX=0$$ is called the (associated) \emph{homogeneous system}\index{Homogeneous system}.
%\end{definition}
%
%If $X_0$ is a particular solution, then the general solution\index{General solution} to the system is\footnote{The notation \(S=\{X_0+Y : MY=0\}\) is read, ``\(S\) is the set of all \(X_0+Y\) such that \(MY=0,\)'' and means exactly that. Sometimes a pipe \(|\) is used instead of a colon.}:
%
%\[S= \{X_0+Y : MY=0 \} \]

\reading{4}{2}
%\begin{center}\href{\webworkurl ReadingHomework4/2/}{Reading homework: problem 4.2}\end{center}

%\section*{References}
%
%Hefferon, Chapter One, Section I.2
%\\
%Beezer, Chapter SLE, Section TSS
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/System_of_linear_equations}{Systems of Linear Equations}


%\section{The size of solution sets vs size of homogeneous solution set}


\section{Review Problems}

\input{\solutionSetsPath/problems}


\newpage
","\chapter{\solutionSetsTitle}

Algebra problems can have multiple solutions. For example $x(x-1)=0$ has  two solutions: $0$ and $1$. By contrast, equations of the form $Ax=b$ with $A$ a linear operator have have the following property.\\



If $A$ is a linear operator and $b$ is a known then $Ax=b$ has either
\begin{enumerate}
\item One solution
\item  No solutions
\item Infinitely many solutions
\end{enumerate}


\section{The Geometry of Solution Sets: Hyperplanes}
Consider the following algebra problems and their solutions

\begin{enumerate}
\item $6x=12$, one solution: $2$
\item $0x=12$, no solution
\item $0x=0$, one solution for each number: $x$
\end{enumerate}
In each case the linear operator is a $1\times 1$ matrix. In the first case, the linear operator is invertible. 
In the other two cases it is not. 
In the first case, the solution set is a point on the number line, in the third case the solution set is the whole number line.

Lets examine similar situations with larger matrices.
\begin{enumerate}
\item
$\begin{pmatrix}
6	&0 	\\
0 	&2 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
12 \\ 
6
\end{pmatrix}$, one solution: 
$\begin{pmatrix}
2 \\ 
3
\end{pmatrix}$
%\\linear operator is invertible

\item 
$\begin{pmatrix}
1	&3 	\\
0 	&0 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
4 \\ 
1 
\end{pmatrix}$, no solutions
%not in the range of the linear operator

\item 
$\begin{pmatrix}
1	&3 	\\
0 	&0 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
4 \\ 
0
\end{pmatrix} $, one solution for each number $y$: 
$\begin{pmatrix}
4-3y \\ 
y
\end{pmatrix} $

\item 
$\begin{pmatrix}
0	&0 	\\
0 	&0 	
\end{pmatrix} 
\begin{pmatrix}
 x \\ 
y 
\end{pmatrix} 
=
\begin{pmatrix}
0 \\ 
0
\end{pmatrix} $, one solution for each pair of numbers $x,y$:
$\begin{pmatrix}
x\\ 
y
\end{pmatrix} $
\end{enumerate}
Again, in the first case the linear operator is invertible while in the other cases it is not. When the operator is not invertible the solution set can be empty, a line in the plane or the plane itself.


For a system of equations with $r$ equations and $k$ veriables, one can have a number of different outcomes.  For example, consider the case of $r$ equations in three variables.  Each of these equations is the equation of a plane in three-dimensional space.  To find solutions to the system of equations, we look for the common intersection of the planes (if an intersection exists).  Here we have five different possibilities:

\begin{enumerate}
\item \textbf{Unique Solution.}  The planes have a unique point of intersection.

\item \textbf{No solutions.}  Some of the equations are contradictory, so no solutions exist.

\item \textbf{Line.}  The planes intersect in a common line; any point on that line then gives a solution to the system of equations.

\item \textbf{Plane.}  Perhaps you only had one equation to begin with, or else all of the equations coincide geometrically.  In this case, you have a plane of solutions, with two free parameters.

\videoscriptlink{solution_sets_for_systems_of_linear_equations_planes.mp4}{Planes}{solution_sets_for_systems_of_linear_equations_planes}

\item \textbf{All of $\mathbb{R}^3$.}  If you start with no information, then any point in $\mathbb{R}^3$ is a solution.  There are three free parameters.
\end{enumerate}

In general, for systems of equations with $k$ unknowns, there are $k+2$ possible outcomes, corresponding to the possible numbers (i.e. $0,1,2,\dots,k$) of free parameters in the solutions set plus the possibility of no solutions.  These types of ``solution sets''\index{Solution set} are ``hyperplanes''\index{Hyperplane}, generalizations of planes the behave like planes in $\mathbb{R}^3$ in many ways.

\videoscriptlink{solution_sets_for_systems_of_linear_equations_overview.mp4}{Pictures and Explanation}{solution_sets_for_systems_of_linear_equations_overview}

\vspace{3mm}

\reading{4}{1}
%\begin{center}\href{\webworkurl ReadingHomework4/1/}{Reading homework: problem 4.1}\end{center}



\section{Particular Solution $+$ Homogeneous solutions }

In the \hyperlink{standard approach}{standard approach}, variables corresponding to columns that do not contain a pivot (after going to reduced row echelon form) are \emph{free}.  
We called them non-pivot variables. 
They index elements of the solutions set by acting as coefficients of vectors.
%In this way the number of non-pivot columns determines (in part) the size of the solution set.  
%We can denote them with dummy variables $\mu_1, \mu_2, \ldots$. 

\begin{example} (Non-pivot columns determine terms of the solutions)
$$\begin{pmatrix}
1 &  0 & 1 & -1 \\ 
 0 & 1 & -1& 1  \\
 0 &0   & 0  & 0 \\
\end{pmatrix}
\colvec{x_1\\x_2\\x_3\\x_4} 
=
\colvec{1\\1\\0} 
\Leftrightarrow
\left\{
\begin{array}{lcr}
	1x_1 +0x_2+ 1x_3 - 1x_4 & = 1 \\
	0x_1 +1x_2 - 1x_3 + 1x_4 & = 1 \\
	0x_1 +0x_2 + 0x_3 + 0x_4 & = 0 
\end{array}
     \right.
$$
Following the standard approach, express the pivot variables in terms of the non-pivot variables and add ``freebee equations"". Here $x_3$ and $x_4$ are non-pivot variables.  
\begin{eqnarray*}
\left.
\begin{array}{rcl}
	x_1 & = &1 -x_3+x_4 \\
	x_2 & = &1 +x_3-x_4 \\
	x_3 & = &\phantom{1+~\,}x_3\\
	x_4 & =&\phantom{1+x_3+~}x_4         
\end{array}
     \right\}
     \Leftrightarrow
\colvec{x_1\\x_2\\x_3\\x_4} 
= \colvec{1\\1\\0\\0} + x_3\colvec{-1\\1\\1\\0} + x_4\colvec{1\\-1\\0\\1}
\end{eqnarray*}
The preferred way to write a solution set is with set notation\index{Solution set!set notation}.  \[S = \left\{\colvec{x_1\\x_2\\x_3\\x_4} = \colvec{1\\1\\ 0\\0 } + \mu_1 \colvec{-1\\1\\1\\0 }  + \mu_2  \colvec{1\\-1\\ 0 \\1 } : \mu_1,\mu_2\in  {\mathbb R} \right\} \]
Notice that the first two components of the second two terms come from the non-pivot columns
Another way to write the solution set is
\[S= \left\{  X_0 + \mu_1 Y_1 + \mu_2 Y_2   : \mu_1,\mu_2 \in  {\mathbb R}   \right\} \]
where 
\[X_0= \colvec{1\\1\\0 \\0 }, Y_1=\colvec{-1\\1\\1\\0 } , Y_2= \colvec{1\\-1\\0 \\1 }
\]
\end{example}
Here $X_0$ is called a particular solution while $Y_1$ and $Y_2$ are called homogeneous solutions. 



\section{Linearity and these parts}
%
%\begin{definition}   A function $f$ is \emph{linear}\index{Linear!function} if 
%for any vectors $X,Y$  in the domain of $f$, and any scalars $\alpha,\beta$ 
%\[f(\alpha X + \beta Y) = \alpha f(X) + \beta f(Y) \,.\]
%\end{definition}

%
%
%\begin{example}
%\hypertarget{solution_sets_for_systems_of_linear_equations_concrete_example}{Consider our example system above with} 
%\[
%M=    \begin{pmatrix}
%      1  & 0  & 1 & -1  \\
%       0  & 1 & -1 & 1  \\
%        0 &0   & 0  & 0    \\
%    \end{pmatrix} \, ,\quad
%X= \colvec{x_1\\x_2\\x_3\\x_4} \mbox{ and } Y=\colvec{y_1\\y_2\\y^3 \\y^4 }\, ,
%\]
%and take for the function of vectors
%$$
%f(X)=MX\, .
%$$
%Now let us check the linearity property for $f$. 
%The property needs to hold for {\it any} scalars $\alpha$ and $\beta$, so for simplicity
%let us concentrate first on the case $\alpha=\beta=1$. This means that we need to
%compare the following two calculations:
%\begin{enumerate}
%\item First add $X+Y$, then compute $f(X+Y)$.
%\item First compute $f(X)$ and $f(Y)$, then compute the sum $f(X)+f(Y)$.
%\end{enumerate}
%The second computation is slightly easier:
%$$
%f(X) = MX 
%    =\colvec{x_1+x_3-x_4\\x_2-x_3+x_4\\0}\mbox{ and }
%f(Y) = MY   
%    =\colvec{y_1+y_3-y_4\\y_2-y_3+y_4\\0}\, ,
%$$
%(using our result above). Adding these gives
%$$
%f(X)+f(Y)=\colvec{x_1+x_3-x_4+y_1+y_3-y_4\\[1mm]x_2-x_3+x_4+y_2-y_3+y_4\\[1mm]0}\, .
%$$
%Next we perform the first computation beginning with:
%$$
%X+Y=\colvec{x_1 + y_1\\x_2+y_2\\ x_3+y_3\\ x_4+y_4}\, ,
%$$
%from which we calculate
%$$
%f(X+Y)=\colvec{x_1+y_2+x_3+y_3-(x_4+y_4)\\[1mm] x_2+y_2-(x_3+y_3)+x_4+y_4\\[1mm]0}\, .
%$$
%Distributing the minus signs and remembering that the order of adding numbers like $x_1,x_2,\ldots$ 
%does not matter, we see that the two computations give exactly the same answer.
%
%Of course, you should complain that we took a special choice of $\alpha$ and $\beta$.
%Actually, to take care of this we only need to check that $f(\alpha X)=\alpha f(X)$.
%It is your job to explain this in  \hyperref[linear]{Review Question~\ref*{linear}}
%\end{example}
%
%\noindent
%Later we will show that matrix multiplication is always linear.  Then we will know that:
With the previous example in mind, lets say that the matrix equation $MX=V$ has  solution set  $\{ X_0 + \mu_1 Y_1 + \mu_2 Y_2):\mu_1,\mu_2 \in {\mathbb R} \}$.
Recall from \hyperlink{{Matrices are linear operators}}{earlier} that matrices are linear.
%\[M(\alpha X + \beta Y) = \alpha MX + \beta MY\]
%
%Then 
%
%the two equations 
Thus 
$$M( X_0 + \mu_1 Y_1 + \mu_2 Y_2)  = MX_0 + \mu_1MY_1 + \mu_2MY_2 =V$$
for \emph{any} $\mu_1, \mu_2 \in \mathbb{R}$. 
Choosing $\mu_1=\mu_2=0$, we obtain 
$$MX_0=V\, .$$  
This is why $X_0$ is an example of a  \emph{particular solution}\index{Particular solution!an example}.

%Given a particular solution to the system, we can then deduce that $\mu_1MY_1 + \mu_2MY_2 = 0$.  
Setting $\mu_1=1, \mu_2=0$, and using the particular solution $MX_0=V$, we obtain 
$$MY_1=0\, .$$ 
Likewise, setting $\mu_1=0, \mu_2=1$, we obtain $$MY_2=0\, .$$
Here $Y_1$ and $Y_2$ are examples of what are called  \emph{homogeneous} solutions\index{Homogeneous solution!an example} to the system.
They {\it do not} solve the original equation $MX=V$, but instead its associated 
{\it homogeneous  equation}\index{homogeneous equation} $M Y =0$.

One of the fundamental lessons of linear algebra: the  solution set to $Ax=b$ with $A$ a linear operator consists of a particular solution plus homogeneous solutions.

\begin{center}
\shabox{
general solution $=$ particular solution $+$ homogeneous solutions.}
\end{center}

\begin{example}
Consider the matrix equation of the previous example. It has  solution set
\[S = \left\{\colvec{x_1\\x_2\\x_3\\x_4} = \colvec{1\\1\\0 \\0 } + \mu_1 \colvec{-1\\1\\1\\0 } + \mu_2 \colvec{1\\-1\\ 0\\1 } \right\} \]
Then $MX_0=V$ says that $\colvec{x_1\\x_2\\x_3\\x_4} = 
\colvec{1\\1\\0 \\ 0}$ solves the original matrix equation, which is certainly true, but this is not the only solution.

$MY_1=0$ says that $\colvec{x_1\\x_2\\x_3\\x_4} = \colvec{-1\\1\\1\\ 0}
$ solves the homogeneous equation.

\vspace{2mm}

$MY_2=0$ says that $\colvec{x_1\\x_2\\x_3\\x_4} = 
\colvec{1\\-1\\0 \\1}$ solves the homogeneous equation.

\vspace{2mm}

\noindent
Notice how adding any multiple of a homogeneous solution to the particular solution yields another particular solution.
\end{example}

%\begin{definition}
%Let $M$ a matrix and $V$ a vector.  Given the linear system $MX=V$, we call $X_0$ a \emph{particular solution}\index{Particular solution} if $MX_0=V$.  We call $Y$ a \emph{homogeneous solution} if $MY=0$.  
%The linear system 
%$$MX=0$$ is called the (associated) \emph{homogeneous system}\index{Homogeneous system}.
%\end{definition}
%
%If $X_0$ is a particular solution, then the general solution\index{General solution} to the system is\footnote{The notation \(S=\{X_0+Y : MY=0\}\) is read, ``\(S\) is the set of all \(X_0+Y\) such that \(MY=0,\)'' and means exactly that. Sometimes a pipe \(|\) is used instead of a colon.}:
%
%\[S= \{X_0+Y : MY=0 \} \]

\reading{4}{2}
%\begin{center}\href{\webworkurl ReadingHomework4/2/}{Reading homework: problem 4.2}\end{center}

%\section*{References}
%
%Hefferon, Chapter One, Section I.2
%\\
%Beezer, Chapter SLE, Section TSS
%\\
%Wikipedia, \href{http://en.wikipedia.org/wiki/System_of_linear_equations}{Systems of Linear Equations}


%\section{The size of solution sets vs size of homogeneous solution set}


\section{Review Problems}

\input{\solutionSetsPath/problems}


\newpage
",lesson
5,Simplex,"\chapter{The Simplex Method}

In Chapter~\ref{systems}, you learned how to handle systems of linear equations.
However there are many situations in which inequalities appear instead of equalities. 
In such cases we are often interested 
in an optimal solution extremizing a particular quantity of interest. 
Questions like this
are a focus of fields such as 
\href{http://en.wikipedia.org/wiki/Mathematical_optimization}{mathematical optimization}
 and 
 \href{ http://en.wikipedia.org/wiki/Operations_research}{operations research}.
For the case where the functions involved are linear,  these problems go under the
title  
\href{http://en.wikipedia.org/wiki/Linear_programming}{
{{\it linear programming}}\index{linear programming}}. Originally these ideas were driven by military applications,
but by now are ubiquitous in science and industry. Gigantic computers are dedicated to implementing linear programming methods such as
George Dantzig's 
\href{http://en.wikipedia.org/wiki/Simplex_algorithm}{simplex algorithm}--the topic of this chapter.

\section{Pablo's Problem}

Let us begin with an example. Consider again Pablo the nutritionist of problem~\ref{Pablo}, chapter~\ref{warmup}.
The Conundrum City school board has employed Pablo to design their school lunch program. Unfortunately for Pablo, 
their requirements are rather tricky:

\begin{example}(Pablo's problem)\\
The Conundrum City school board is heavily influenced by the local fruit grower's association. They have stipulated
that children eat at least 7 oranges and 5 apples per week. Parents and teachers have agreed that eating at least 15 pieces of fruit per
week is a good thing, but school janitors argue that too much fruit makes a terrible mess, so that children should eat no more than
25 pieces of fruit per week. 
\begin{center}
\includegraphics[scale=.3]{simplex/Pablo.jpg}
\end{center}
\noindent
Finally Pablo knows that oranges have twice as much sugar as apples
and that apples have 5 grams of sugar each. Too much sugar is unhealthy, so Pablo wants to keep the children's sugar intake as low 
as possible. How many oranges and apples should Pablo suggest that the school board put on the menu?
\end{example}

This is a rather gnarly word problem. Our first step is to restate it as mathematics, stripping away all the extraneous information:

\begin{example}(Pablo's problem restated)\\
Let $x$ be the number of apples and $y$ be the number of oranges. These must obey
$$
x\geq5\, \quad\mbox{and}\quad y\geq7\, ,
$$
to fulfill the school board's politically motivated wishes. The teacher's and parent's fruit requirement means that
$$
x+y\geq 15\, ,
$$
but to keep the canteen tidy
$$
x+y\leq 25\, .
$$
Now let 
$$s=5x+10y\, .$$
This linear function of $(x,y)$ represents the grams of sugar in $x$ apples and $y$ oranges.
The problem is asking us to minimize $s$ subject to the four linear inequalities listed above.
\end{example}

\section{Graphical Solutions}\label{graph}

Before giving a more general algorithm for handling this problem and problems like it, we note that when
the number of variables is small (preferably~2), a graphical technique can be used.

Inequalities, such as the four given in Pablo's problem, are often called {\it constraints}, and values of the variables that 
satisfy these constraints comprise the so-called {\it feasible region}. 
Since there are only two variables, this is easy to plot:

\begin{example}(Constraints and feasible region)
Pablo's constraints are
\begin{eqnarray*}
&x\geq 5&\\
&y\geq 7&\\[2mm]
&15\leq x+y\leq25\, .&
\end{eqnarray*}
Plotted in the $(x,y)$ plane, this gives:
\vspace{-5mm}
\begin{center}
\hspace{-1cm}\includegraphics[scale=.32]{simplex/feasible.jpg}
\end{center}
\end{example}

You might be able to see the solution to Pablo's problem already. Oranges are very sugary, so they should be kept low, thus $y=7$.
Also, the less fruit the better, so the answer had better lie on the line $x+y=15$. Hence, the answer must be at the vertex 
$(8,7)$. Actually this is a general feature of linear programming problems, the optimal answer must lie at a vertex of the feasible region. Rather
than prove this, lets look at a plot of the linear function $s(x,y)=5x+10y$.

\begin{example}(The sugar function)\\
Plotting the sugar function requires three dimensions:
\vspace{-1.5mm}
\begin{center}
\hspace{-1cm}\includegraphics[scale=.38]{simplex/sugar.jpg}
\end{center}
\end{example} 

The plot of a linear function of two variables is a plane through the origin. 
Restricting the variables to the feasible region gives some lamina in 3-space.
Since the function we want to optimize is linear (and assumedly non-zero), 
if we pick a point in the middle of this lamina, we can always increase/decrease 
the function by moving out to an edge and, in turn, along that edge to a corner.
Applying this to the above picture,  we see that Pablo's best option is 110 grams of sugar a week, in the form of 
8 apples and 7 oranges.

It is worthwhile to contrast the optimization problem for a linear function with the non-linear case you may have seen in calculus courses:
\vspace{-1.5mm}
\begin{center}
\hspace{-1cm}\includegraphics[scale=.33]{simplex/linvsnonlin.jpg}
\end{center}
Here we have plotted the curve $f(x)=d$ in the case where the function $f$ is linear and non-linear. 
To optimize $f$ in the interval $[a,b]$, for the linear case we just need to compute and compare the values $f(a)$ 
and $f(b)$. In contrast, for non-linear functions it is necessary to also compute the derivative $df/dx$ to study whether
there are extrema {\bf inside} the interval.

\section{Dantzig's Algorithm}\label{dantzig}

In simple situations a graphical method might suffice, but in many applications there may be thousands or even millions of variables 
and constraints. Clearly an algorithm that can be implemented on a computer is needed. The {\it simplex algorithm} (usually attributed to George Dantzig) provides exactly that. It begins with a standard problem:

\begin{problem}
Maximize $f(x_1,\ldots,x_n)$ where $f$ is linear, $x_i\geq 0$ ($i=1,\ldots, n$) subject~to  
$$
Mx = v\, ,\qquad x:=\ccolvec{x_1\\\vdots \\ x_n}\, ,
$$
where the $m\times n$ matrix $M$ and $m\times 1$ column vector $v$ are given.
\end{problem}
 
This is solved by arranging the information in an augmented matrix and then applying EROs.
To see how this works lets try an example.

\begin{example}\label{stdprob}
Maximize $f(x,y,z,w)=3x-3y-z+4w$ subject to constraints
\begin{equation*}
\begin{array}{rcccl}
c_1&:=&x+y+z+w&=&5\\[2mm]
c_2&:=&x+2y+3z+2w&=&6\, ,
\end{array}
\end{equation*}
where $x\geq0$, $y\geq0$, $z\geq0$ and $w\geq 0$.
\end{example}
  The key observation is this: Suppose we are trying to maximize $f(x_1,\ldots,x_n)$ subject to 
  a constraint $c(x_1,\ldots,x_n)=k$ for some constant $k$ ($c$ and $k$ would  be the entries of 
  $Mx$ and $v$, respectively, in the above). Then we can also try to maximize
  $$
 f(x_1,\ldots,x_n)+\alpha c(x_1,\ldots,x_n)\, 
  $$
  because this is only a constant shift $f\to f+\alpha k$. Choosing $\alpha$ carefully can lead to a simple form for the function we are extremizing.



\begin{example} (Setting up an augmented matrix):

Since we are interested in the optimum value of $f$, we treat it as an additional variable and add one further equation
$$
-3x+3y+z-4w+f=0\, .
$$
We arrange this equation and the two constraints in an augmented matrix
$$
\begin{array}{ccc}
\left(\begin{array}{rrrrr|r}
1&1&1&1&0&5\\[1mm]
1&2&3&2&0&6\\\hline
\!-3&3&1&\!\!-4&\ 1&\ 0
\end{array}\right)
\quad &\Leftrightarrow &% \quad 
\left\{
\begin{array}{lcl}
c_1&=&5\\[1mm]
c_2&=&6\\[1mm]
f&=&3x-3y-z+4w
\end{array}\right.
\end{array}.
$$
Keep in mind that the first four columns correspond to the positive variables $(x,y,z,w)$ and that
the last row has the information of the function $f$. The general case is depicted in figure~\ref{augD}.
\end{example}

\begin{figure}
\begin{center}
\begin{tabular}{ll}
\ \ $\overbrace{\phantom{VERYVERYERY}}^{\mbox{\tiny variables (incl. slack and artificial)}}$\ 
$\overbrace{\phantom{F}}^{\mbox{\tiny objective}}$&\\
$
\left(
\begin{array}{c|c}
\phantom{VERYVERYERYFATS}&\phantom{S}\\
\\
\\\hline
\\
\end{array}
\right)
$ &$\begin{array}{l} \\ \leftarrow  \mbox{constraint equations} \\[6mm] \leftarrow \mbox{objective equation}\end{array}$\\
\hspace{4.3cm}$\begin{array}{c}\uparrow\\ \mbox{objective value}\end{array}\!\!\!\!\!\!\!\!$
\end{tabular}
\end{center}
\caption{Arranging the information of an optimization problem in an augmented matrix.\label{augD}}
\end{figure}



Now the system is written as an augmented matrix where the last row encodes the objective function and the other rows the constraints. Clearly 
we can perform row operations on the constraint rows since this will not change the solutions to the constraints. 
Moreover, we can add any amount of the constraint rows to the last row,
since this just amounts to adding a constant to the function we want to extremize.

\begin{example} (Performing EROs)

We scan the last row, and notice the (most negative) coefficient $-4$. Na\""ively
you might think that this is good because this multiplies the positive variable $w$
and only helps the objective function $f=4w+\cdots$. However, what this actually means
is that the variable $w$ will be positive and thus determined by the constraints. Therefore we want to remove it 
from the objective function. We can zero out this entry by performing a row operation. For that, either of the first two rows could be used. 
To decide which, we remember that  we still have to solve solve the constraints for variables that are positive. Hence we should try 
to keep the first two entries in the last column positive. Hence 
we choose the row which will add the smallest constant to $f$ when we zero out the $-4$: Look at the last column (where the values of the constraints are stored). We see that adding four times the 
first row to the last row would zero out the $-4$ entry but add $20$ to $f$, while adding two  times the second row to the last row would also zero out the $-4$ but only add $12$ to $f$. (You can follow this by watching what happens to the last entry in the last row.)
So we perform the latter row operation and obtain the following:
$$
\begin{array}{c|c}
\left(
\begin{array}{rrrrr|r}
1&1&1&1&0&5\\[1mm]
1&2&3&2&0&6\\\hline
\!-1&7&7&0&\ 1&\ 12
\end{array}\right)
\quad\quad  &\quad 
\begin{array}{rcl}
c_1&=&5\\[1mm]
c_2&=&6\\
f&=&12+x-7y-7z\, .
\end{array}
\end{array}
$$
We do not want to undo any of our good work when we perform further row operations, so now we use the second row to zero out all other entries in the fourth column. This is achieved by subtracting half the second row from the first:
$$
\begin{array}{c|c}
\left(
\begin{array}{rrrrr|r}
\frac12&0&-\frac12&0&0&2\\[1mm]
1&2&3&2&0&6\\\hline
\!-1&7&7&0&\ 1&\ 12
\end{array}\right)
\quad\quad  &\quad 
\begin{array}{rcl}
c_1-\frac12 c_2&=&2\\[1mm]
c_2&=&6\\
f&=&12+x-7y-7z\, .
\end{array}
\end{array}
$$
Precisely because we chose the second row to perform our row operations, all entries in the last column remain positive. This allows us to continue the algorithm.

We now repeat the above procedure: There is a $-1$ in the first column of the last row. We want to zero it out while adding as little to
$f$ as possible. This is achieved by adding twice the first row to the last row:
$$
\begin{array}{c|c}
\left(
\begin{array}{rrrrr|r}
\frac12&0&-\frac12&0&0&2\\[1mm]
1&2&3&2&0&6\\\hline
0&7&6&0&\ 1&\ 16
\end{array}\right)
\quad\quad  &\quad 
\begin{array}{rcl}
c_1-\frac12 c_2&=&2\\[1mm]
c_2&=&6\\
f&=&16-7y-6z\, .
\end{array}
\end{array}
$$
The Dantzig algorithm terminates if all the coefficients in the last row (save perhaps for the last entry which encodes the value of the objective) are positive.
To see why we are done, lets write out what our row operations have done in terms of the function $f$ and the constraints $(c_1,c_2)$.
First we have
$$
f=16-7y-6z
$$
with both $y$ and $z$ positive. Hence to maximize $f$ we should choose $y=0=z$. In which case we obtain our optimum value
$$
\underline{f=16\, .}
$$
Finally, we check that the constraints can be solved with $y=0=z$ and positive $(x,w)$. Indeed, they can by taking $x=4$, $w=1$.
\end{example} 

\section{Pablo Meets Dantzig} 
Oftentimes, it takes a few tricks to bring a given problem into the standard  form of example~\ref{stdprob}. In Pablo's case, this goes as follows.

\begin{example}
Pablo's variables $x$ and $y$ do not obey $x_i\geq 0$. Therefore define new variables 
$$
x_1=x-5\, , \quad x_2=y-7\, .
$$
The conditions on the fruit $15\leq x+y\leq 25$ are inequalities, 
$$
x_1+x_2\geq 3\, ,\quad x_1+x_2\leq 13\, ,
$$ 
so are not of the form $Mx = v$. To achieve this we introduce two new positive variables $x_3\geq0$, $x_4\geq 4$ and write
$$
c_1:=x_1+x_2-x_3=3\, ,\quad c_2:=x_1 + x_2 + x_4=13\, .
$$
These are called {\it slack variables} because they take up the ``slack'' required to convert inequality to equality.
This pair of equations can now be written as $Mx=v$,
$$
\begin{pmatrix}
1 &1&\!\!-1&0\\
1&1&0&1
\end{pmatrix}
\colvec{x_1\\x_2\\x_3\\x_4}=
\colvec{3\\\!13}\, .
$$
Finally, Pablo wants to minimize sugar $s=5x+10y$, but the standard problem maximizes $f$. Thus the so-called {\it objective function}
$f=-s+95=-5x_1-10x_2$. (Notice that it makes no difference whether we maximize $-s$ or $-s+95$, we choose the latter since it is
a linear function of $(x_1,x_2)$.)
Now we can build an augmented matrix whose last row reflects  the objective function equation $5 x_1+10 x_2 +f=0$:
$$
\left(\begin{array}{rrrrr|r}
1&1&-1&0&0&3\\
1&1&0&1&0&13\\\hline
5&10&\ 0&0&1&0
\end{array}\right)\, .
$$
Here it seems that the simplex algorithm already terminates because the last row only has positive coefficients, so that setting $x_1=0=x_2$
would be optimal. However, this does not solve the constraints (for positive values of the slack variables $x_3$ and $x_4$).
Thus one more (very dirty) trick is needed. We add two more, positive, (so-called) {\it artificial variables} $x_5$ and $x_6$ to the problem
which we use to shift each constraint 
$$
c_1\to c_1-x_5\, ,\qquad c_2\to c_2-x_6\, .
$$
The idea being that for large positive~$\alpha$,  the modified objective function $$f-\alpha x_5 - \alpha x_6$$
is only maximal when the artificial variables vanish so the underlying problem is unchanged. 
Lets take $\alpha=10$ (our solution will not depend on this choice) so that our augmented matrix reads 
\begin{eqnarray*}&&
\left(\begin{array}{rrrrrrr|r}
1&1&-1&0&1&0&0&3\\
1&1&0&1&0&1&0&13\\\hline
5&10&\ 0&0&10&10&1&0
\end{array}\right)\\[2mm]
&\stackrel{\small R_3'=R_3-10 R_1-10R_2 }\sim&
\left(\begin{array}{rrrrrrr|r}
1&1&-1&0&1&0&0&3\\
1&1&0&1&0&1&0&13\\\hline
\!-15&-10&\ 10&-10&0&0&1&-160
\end{array}\right)
\, .
\end{eqnarray*}
Here we performed one row operation to zero out the coefficients of the artificial variables.
Now we are ready to run the simplex algorithm exactly as in section~\ref{dantzig}. The first row operation 
uses the~$1$ in the top of the first column to zero out the most negative entry in the last row:
\begin{eqnarray*}&&
\left(\begin{array}{rrrrrrr|r}
1&1&-1&0&1&0&0&3\\
1&1&0&1&0&1&0&13\\\hline
0&5&-5&-10&15&0&1&-115
\end{array}\right)\\[2mm]
&\stackrel{\small R_2'=R_2-R_1}\sim&
\left(\begin{array}{rrrrrrr|r}
1&1&1&0&1&0&0&3\\
0&0&1&1&-1&1&0&10\\\hline
0&5&-5&-10&15&0&1&-115
\end{array}\right)\\[2mm]
&\stackrel{\small R_3'=R_3+10R_2}\sim&
\left(\begin{array}{rrrrrrr|r}
1&1&1&0&1&0&0&3\\
0&0&1&1&-1&1&0&10\\\hline
0&5&5&0&5&10&1&-15
\end{array}\right)
\, .
\end{eqnarray*}
Now the variables $(x_2,x_3,x_5,x_6)$ have zero coefficients so must be set to zero to maximize $f$.
The optimum value is $f=-15$ so $s=-f+95=110$ exactly as before. Finally, to solve the constraints $x_1=3$
and $x_4=10$ so that $x=8$ and $y=7$ which also agrees with our previous result.
 \end{example}


Clearly, performed by hand,  the simplex algorithm was slow and complex for Pablo's problem. However, the key point is
that it is an algorithm that can be fed to a computer. For problems with many variables, this method is much faster than 
simply checking all vertices as we did in section~\ref{graph}.

\section{Review Problems}

%\section{Review Problems}


\input{simplex/problems}

","\chapter{The Simplex Method}

In Chapter~\ref{systems}, you learned how to handle systems of linear equations.
However there are many situations in which inequalities appear instead of equalities. 
In such cases we are often interested 
in an optimal solution extremizing a particular quantity of interest. 
Questions like this
are a focus of fields such as 
\href{http://en.wikipedia.org/wiki/Mathematical_optimization}{mathematical optimization}
 and 
 \href{ http://en.wikipedia.org/wiki/Operations_research}{operations research}.
For the case where the functions involved are linear,  these problems go under the
title  
\href{http://en.wikipedia.org/wiki/Linear_programming}{
{{\it linear programming}}\index{linear programming}}. Originally these ideas were driven by military applications,
but by now are ubiquitous in science and industry. Gigantic computers are dedicated to implementing linear programming methods such as
George Dantzig's 
\href{http://en.wikipedia.org/wiki/Simplex_algorithm}{simplex algorithm}--the topic of this chapter.

\section{Pablo's Problem}

Let us begin with an example. Consider again Pablo the nutritionist of problem~\ref{Pablo}, chapter~\ref{warmup}.
The Conundrum City school board has employed Pablo to design their school lunch program. Unfortunately for Pablo, 
their requirements are rather tricky:

\begin{example}(Pablo's problem)\\
The Conundrum City school board is heavily influenced by the local fruit grower's association. They have stipulated
that children eat at least 7 oranges and 5 apples per week. Parents and teachers have agreed that eating at least 15 pieces of fruit per
week is a good thing, but school janitors argue that too much fruit makes a terrible mess, so that children should eat no more than
25 pieces of fruit per week. 
\begin{center}
\includegraphics[scale=.3]{simplex/Pablo.jpg}
\end{center}
\noindent
Finally Pablo knows that oranges have twice as much sugar as apples
and that apples have 5 grams of sugar each. Too much sugar is unhealthy, so Pablo wants to keep the children's sugar intake as low 
as possible. How many oranges and apples should Pablo suggest that the school board put on the menu?
\end{example}

This is a rather gnarly word problem. Our first step is to restate it as mathematics, stripping away all the extraneous information:

\begin{example}(Pablo's problem restated)\\
Let $x$ be the number of apples and $y$ be the number of oranges. These must obey
$$
x\geq5\, \quad\mbox{and}\quad y\geq7\, ,
$$
to fulfill the school board's politically motivated wishes. The teacher's and parent's fruit requirement means that
$$
x+y\geq 15\, ,
$$
but to keep the canteen tidy
$$
x+y\leq 25\, .
$$
Now let 
$$s=5x+10y\, .$$
This linear function of $(x,y)$ represents the grams of sugar in $x$ apples and $y$ oranges.
The problem is asking us to minimize $s$ subject to the four linear inequalities listed above.
\end{example}

\section{Graphical Solutions}\label{graph}

Before giving a more general algorithm for handling this problem and problems like it, we note that when
the number of variables is small (preferably~2), a graphical technique can be used.

Inequalities, such as the four given in Pablo's problem, are often called {\it constraints}, and values of the variables that 
satisfy these constraints comprise the so-called {\it feasible region}. 
Since there are only two variables, this is easy to plot:

\begin{example}(Constraints and feasible region)
Pablo's constraints are
\begin{eqnarray*}
&x\geq 5&\\
&y\geq 7&\\[2mm]
&15\leq x+y\leq25\, .&
\end{eqnarray*}
Plotted in the $(x,y)$ plane, this gives:
\vspace{-5mm}
\begin{center}
\hspace{-1cm}\includegraphics[scale=.32]{simplex/feasible.jpg}
\end{center}
\end{example}

You might be able to see the solution to Pablo's problem already. Oranges are very sugary, so they should be kept low, thus $y=7$.
Also, the less fruit the better, so the answer had better lie on the line $x+y=15$. Hence, the answer must be at the vertex 
$(8,7)$. Actually this is a general feature of linear programming problems, the optimal answer must lie at a vertex of the feasible region. Rather
than prove this, lets look at a plot of the linear function $s(x,y)=5x+10y$.

\begin{example}(The sugar function)\\
Plotting the sugar function requires three dimensions:
\vspace{-1.5mm}
\begin{center}
\hspace{-1cm}\includegraphics[scale=.38]{simplex/sugar.jpg}
\end{center}
\end{example} 

The plot of a linear function of two variables is a plane through the origin. 
Restricting the variables to the feasible region gives some lamina in 3-space.
Since the function we want to optimize is linear (and assumedly non-zero), 
if we pick a point in the middle of this lamina, we can always increase/decrease 
the function by moving out to an edge and, in turn, along that edge to a corner.
Applying this to the above picture,  we see that Pablo's best option is 110 grams of sugar a week, in the form of 
8 apples and 7 oranges.

It is worthwhile to contrast the optimization problem for a linear function with the non-linear case you may have seen in calculus courses:
\vspace{-1.5mm}
\begin{center}
\hspace{-1cm}\includegraphics[scale=.33]{simplex/linvsnonlin.jpg}
\end{center}
Here we have plotted the curve $f(x)=d$ in the case where the function $f$ is linear and non-linear. 
To optimize $f$ in the interval $[a,b]$, for the linear case we just need to compute and compare the values $f(a)$ 
and $f(b)$. In contrast, for non-linear functions it is necessary to also compute the derivative $df/dx$ to study whether
there are extrema {\bf inside} the interval.

\section{Dantzig's Algorithm}\label{dantzig}

In simple situations a graphical method might suffice, but in many applications there may be thousands or even millions of variables 
and constraints. Clearly an algorithm that can be implemented on a computer is needed. The {\it simplex algorithm} (usually attributed to George Dantzig) provides exactly that. It begins with a standard problem:

\begin{problem}
Maximize $f(x_1,\ldots,x_n)$ where $f$ is linear, $x_i\geq 0$ ($i=1,\ldots, n$) subject~to  
$$
Mx = v\, ,\qquad x:=\ccolvec{x_1\\\vdots \\ x_n}\, ,
$$
where the $m\times n$ matrix $M$ and $m\times 1$ column vector $v$ are given.
\end{problem}
 
This is solved by arranging the information in an augmented matrix and then applying EROs.
To see how this works lets try an example.

\begin{example}\label{stdprob}
Maximize $f(x,y,z,w)=3x-3y-z+4w$ subject to constraints
\begin{equation*}
\begin{array}{rcccl}
c_1&:=&x+y+z+w&=&5\\[2mm]
c_2&:=&x+2y+3z+2w&=&6\, ,
\end{array}
\end{equation*}
where $x\geq0$, $y\geq0$, $z\geq0$ and $w\geq 0$.
\end{example}
  The key observation is this: Suppose we are trying to maximize $f(x_1,\ldots,x_n)$ subject to 
  a constraint $c(x_1,\ldots,x_n)=k$ for some constant $k$ ($c$ and $k$ would  be the entries of 
  $Mx$ and $v$, respectively, in the above). Then we can also try to maximize
  $$
 f(x_1,\ldots,x_n)+\alpha c(x_1,\ldots,x_n)\, 
  $$
  because this is only a constant shift $f\to f+\alpha k$. Choosing $\alpha$ carefully can lead to a simple form for the function we are extremizing.



\begin{example} (Setting up an augmented matrix):

Since we are interested in the optimum value of $f$, we treat it as an additional variable and add one further equation
$$
-3x+3y+z-4w+f=0\, .
$$
We arrange this equation and the two constraints in an augmented matrix
$$
\begin{array}{ccc}
\left(\begin{array}{rrrrr|r}
1&1&1&1&0&5\\[1mm]
1&2&3&2&0&6\\\hline
\!-3&3&1&\!\!-4&\ 1&\ 0
\end{array}\right)
\quad &\Leftrightarrow &% \quad 
\left\{
\begin{array}{lcl}
c_1&=&5\\[1mm]
c_2&=&6\\[1mm]
f&=&3x-3y-z+4w
\end{array}\right.
\end{array}.
$$
Keep in mind that the first four columns correspond to the positive variables $(x,y,z,w)$ and that
the last row has the information of the function $f$. The general case is depicted in figure~\ref{augD}.
\end{example}

\begin{figure}
\begin{center}
\begin{tabular}{ll}
\ \ $\overbrace{\phantom{VERYVERYERY}}^{\mbox{\tiny variables (incl. slack and artificial)}}$\ 
$\overbrace{\phantom{F}}^{\mbox{\tiny objective}}$&\\
$
\left(
\begin{array}{c|c}
\phantom{VERYVERYERYFATS}&\phantom{S}\\
\\
\\\hline
\\
\end{array}
\right)
$ &$\begin{array}{l} \\ \leftarrow  \mbox{constraint equations} \\[6mm] \leftarrow \mbox{objective equation}\end{array}$\\
\hspace{4.3cm}$\begin{array}{c}\uparrow\\ \mbox{objective value}\end{array}\!\!\!\!\!\!\!\!$
\end{tabular}
\end{center}
\caption{Arranging the information of an optimization problem in an augmented matrix.\label{augD}}
\end{figure}



Now the system is written as an augmented matrix where the last row encodes the objective function and the other rows the constraints. Clearly 
we can perform row operations on the constraint rows since this will not change the solutions to the constraints. 
Moreover, we can add any amount of the constraint rows to the last row,
since this just amounts to adding a constant to the function we want to extremize.

\begin{example} (Performing EROs)

We scan the last row, and notice the (most negative) coefficient $-4$. Na\""ively
you might think that this is good because this multiplies the positive variable $w$
and only helps the objective function $f=4w+\cdots$. However, what this actually means
is that the variable $w$ will be positive and thus determined by the constraints. Therefore we want to remove it 
from the objective function. We can zero out this entry by performing a row operation. For that, either of the first two rows could be used. 
To decide which, we remember that  we still have to solve solve the constraints for variables that are positive. Hence we should try 
to keep the first two entries in the last column positive. Hence 
we choose the row which will add the smallest constant to $f$ when we zero out the $-4$: Look at the last column (where the values of the constraints are stored). We see that adding four times the 
first row to the last row would zero out the $-4$ entry but add $20$ to $f$, while adding two  times the second row to the last row would also zero out the $-4$ but only add $12$ to $f$. (You can follow this by watching what happens to the last entry in the last row.)
So we perform the latter row operation and obtain the following:
$$
\begin{array}{c|c}
\left(
\begin{array}{rrrrr|r}
1&1&1&1&0&5\\[1mm]
1&2&3&2&0&6\\\hline
\!-1&7&7&0&\ 1&\ 12
\end{array}\right)
\quad\quad  &\quad 
\begin{array}{rcl}
c_1&=&5\\[1mm]
c_2&=&6\\
f&=&12+x-7y-7z\, .
\end{array}
\end{array}
$$
We do not want to undo any of our good work when we perform further row operations, so now we use the second row to zero out all other entries in the fourth column. This is achieved by subtracting half the second row from the first:
$$
\begin{array}{c|c}
\left(
\begin{array}{rrrrr|r}
\frac12&0&-\frac12&0&0&2\\[1mm]
1&2&3&2&0&6\\\hline
\!-1&7&7&0&\ 1&\ 12
\end{array}\right)
\quad\quad  &\quad 
\begin{array}{rcl}
c_1-\frac12 c_2&=&2\\[1mm]
c_2&=&6\\
f&=&12+x-7y-7z\, .
\end{array}
\end{array}
$$
Precisely because we chose the second row to perform our row operations, all entries in the last column remain positive. This allows us to continue the algorithm.

We now repeat the above procedure: There is a $-1$ in the first column of the last row. We want to zero it out while adding as little to
$f$ as possible. This is achieved by adding twice the first row to the last row:
$$
\begin{array}{c|c}
\left(
\begin{array}{rrrrr|r}
\frac12&0&-\frac12&0&0&2\\[1mm]
1&2&3&2&0&6\\\hline
0&7&6&0&\ 1&\ 16
\end{array}\right)
\quad\quad  &\quad 
\begin{array}{rcl}
c_1-\frac12 c_2&=&2\\[1mm]
c_2&=&6\\
f&=&16-7y-6z\, .
\end{array}
\end{array}
$$
The Dantzig algorithm terminates if all the coefficients in the last row (save perhaps for the last entry which encodes the value of the objective) are positive.
To see why we are done, lets write out what our row operations have done in terms of the function $f$ and the constraints $(c_1,c_2)$.
First we have
$$
f=16-7y-6z
$$
with both $y$ and $z$ positive. Hence to maximize $f$ we should choose $y=0=z$. In which case we obtain our optimum value
$$
\underline{f=16\, .}
$$
Finally, we check that the constraints can be solved with $y=0=z$ and positive $(x,w)$. Indeed, they can by taking $x=4$, $w=1$.
\end{example} 

\section{Pablo Meets Dantzig} 
Oftentimes, it takes a few tricks to bring a given problem into the standard  form of example~\ref{stdprob}. In Pablo's case, this goes as follows.

\begin{example}
Pablo's variables $x$ and $y$ do not obey $x_i\geq 0$. Therefore define new variables 
$$
x_1=x-5\, , \quad x_2=y-7\, .
$$
The conditions on the fruit $15\leq x+y\leq 25$ are inequalities, 
$$
x_1+x_2\geq 3\, ,\quad x_1+x_2\leq 13\, ,
$$ 
so are not of the form $Mx = v$. To achieve this we introduce two new positive variables $x_3\geq0$, $x_4\geq 4$ and write
$$
c_1:=x_1+x_2-x_3=3\, ,\quad c_2:=x_1 + x_2 + x_4=13\, .
$$
These are called {\it slack variables} because they take up the ``slack'' required to convert inequality to equality.
This pair of equations can now be written as $Mx=v$,
$$
\begin{pmatrix}
1 &1&\!\!-1&0\\
1&1&0&1
\end{pmatrix}
\colvec{x_1\\x_2\\x_3\\x_4}=
\colvec{3\\\!13}\, .
$$
Finally, Pablo wants to minimize sugar $s=5x+10y$, but the standard problem maximizes $f$. Thus the so-called {\it objective function}
$f=-s+95=-5x_1-10x_2$. (Notice that it makes no difference whether we maximize $-s$ or $-s+95$, we choose the latter since it is
a linear function of $(x_1,x_2)$.)
Now we can build an augmented matrix whose last row reflects  the objective function equation $5 x_1+10 x_2 +f=0$:
$$
\left(\begin{array}{rrrrr|r}
1&1&-1&0&0&3\\
1&1&0&1&0&13\\\hline
5&10&\ 0&0&1&0
\end{array}\right)\, .
$$
Here it seems that the simplex algorithm already terminates because the last row only has positive coefficients, so that setting $x_1=0=x_2$
would be optimal. However, this does not solve the constraints (for positive values of the slack variables $x_3$ and $x_4$).
Thus one more (very dirty) trick is needed. We add two more, positive, (so-called) {\it artificial variables} $x_5$ and $x_6$ to the problem
which we use to shift each constraint 
$$
c_1\to c_1-x_5\, ,\qquad c_2\to c_2-x_6\, .
$$
The idea being that for large positive~$\alpha$,  the modified objective function $$f-\alpha x_5 - \alpha x_6$$
is only maximal when the artificial variables vanish so the underlying problem is unchanged. 
Lets take $\alpha=10$ (our solution will not depend on this choice) so that our augmented matrix reads 
\begin{eqnarray*}&&
\left(\begin{array}{rrrrrrr|r}
1&1&-1&0&1&0&0&3\\
1&1&0&1&0&1&0&13\\\hline
5&10&\ 0&0&10&10&1&0
\end{array}\right)\\[2mm]
&\stackrel{\small R_3'=R_3-10 R_1-10R_2 }\sim&
\left(\begin{array}{rrrrrrr|r}
1&1&-1&0&1&0&0&3\\
1&1&0&1&0&1&0&13\\\hline
\!-15&-10&\ 10&-10&0&0&1&-160
\end{array}\right)
\, .
\end{eqnarray*}
Here we performed one row operation to zero out the coefficients of the artificial variables.
Now we are ready to run the simplex algorithm exactly as in section~\ref{dantzig}. The first row operation 
uses the~$1$ in the top of the first column to zero out the most negative entry in the last row:
\begin{eqnarray*}&&
\left(\begin{array}{rrrrrrr|r}
1&1&-1&0&1&0&0&3\\
1&1&0&1&0&1&0&13\\\hline
0&5&-5&-10&15&0&1&-115
\end{array}\right)\\[2mm]
&\stackrel{\small R_2'=R_2-R_1}\sim&
\left(\begin{array}{rrrrrrr|r}
1&1&1&0&1&0&0&3\\
0&0&1&1&-1&1&0&10\\\hline
0&5&-5&-10&15&0&1&-115
\end{array}\right)\\[2mm]
&\stackrel{\small R_3'=R_3+10R_2}\sim&
\left(\begin{array}{rrrrrrr|r}
1&1&1&0&1&0&0&3\\
0&0&1&1&-1&1&0&10\\\hline
0&5&5&0&5&10&1&-15
\end{array}\right)
\, .
\end{eqnarray*}
Now the variables $(x_2,x_3,x_5,x_6)$ have zero coefficients so must be set to zero to maximize $f$.
The optimum value is $f=-15$ so $s=-f+95=110$ exactly as before. Finally, to solve the constraints $x_1=3$
and $x_4=10$ so that $x=8$ and $y=7$ which also agrees with our previous result.
 \end{example}


Clearly, performed by hand,  the simplex algorithm was slow and complex for Pablo's problem. However, the key point is
that it is an algorithm that can be fed to a computer. For problems with many variables, this method is much faster than 
simply checking all vertices as we did in section~\ref{graph}.

\section{Review Problems}

%\section{Review Problems}


\input{simplex/problems}

",lesson
6,Vectors In Space N Vectors,"\chapter{\vectorsInSpaceTitle}
\label{vectorsinspace}

%In vector calculus classes, you  encountered three-dimensional vectors.  Now we will develop the notion of $n$-vectors and learn some of their properties.
%
%\videoscriptlink{vectors_in_space_n_vectors_overview.mp4}{Overview}{scripts_vectors_in_space_n_vectors}

%We begin by looking at the space $\mathbb{R}^n$, which we can think of as the space of points with $n$ coordinates.  We then specify an \emph{origin} $O$, a favorite point in $\mathbb{R}^n$.  Now given any other point $P$, we can draw a \emph{vector} $v$ from $O$ to $P$.  Just as in $\mathbb{R}^3$, a vector has a \emph{magnitude} and a \emph{direction}.  
%
%\hypertarget{choosing_the_origin}{If} $O$ has coordinates $(o^1, \ldots, o^n)$ and $p$ has coordinates $(p^1, \ldots, p^n)$, then the \emph{components} of the vector $v$ are $\colvec{p^1 - o^1 \\ p^2- o^2 \\ \vdots \\ p^n - o^n }$.  This construction allows us to put the origin anywhere that seems most convenient in $\mathbb{R}^n$, not just at the point with zero coordinates:
%
%\begin{center}
%\includegraphics[scale =.3]{vectorOP.jpg}
%\end{center}

To continue our linear algebra journey, we must discuss $n$-vectors with an arbitrarily large number of components. 
The simplest way to think about these is as  ordered  lists of numbers, 
$$a=\ccolvec{a^1 \\ \vdots \\ a^n } .$$
%It is also convenient to think of n-vectors as functions with domain the set 
%$\{1,\dots,n\}$, as \hyperlink{vecs as fun}{discussed in chapter 1}. These two ideas give us the following two equivalent notions of the set of all $n$ vectors,
%$$
%{\mathbb{R}}^n =\left\{ \colvec{a^1 \\ \vdots \\ a^n } | \,  a^1,\dots a^n \in \mathbb{R} \right\}
%=\{ a:\{1,\dots,n\}\to \mathbb{R}\}
%$$
%
%Sometimes the notation $\mathbb{R}^{ \{1,\cdots,n\} }$ is used to emphasize the idea of functions from $   \{1,\cdots,n\} $ to $\mathbb{R}$.
\noindent
{\it 
Do not be confused by our use of a superscript to label components of a vector. Here $a^2$ denotes
the second component of the vector $a$, rather than the number $a$ squared!}  

\vspace{1mm}
We emphasize  that order matters: 
\begin{example} (Order of Components Matters)
$$\colvec{7 \\4 \\ 2\\ 5 } 
\neq \colvec{7 \\2 \\4 \\5 } .$$
\end{example}
The set of all $n$-vectors is denoted $\mathbb{R}^n$. As an equation
$$
{\mathbb{R}}^n :=\left\{ \colvec{a^1 \\ \vdots\ \  \\ a^n } \middle\vert \,  a^1,\dots, a^n \in \mathbb{R} \right\} \,.
$$
%; $2$-vectors and $3$-vectors are o

%\begin{remark}
%\hypertarget{note_points_versus_vectors}{A quick note on points versus vectors.} We might sometimes interpret a point and a vector as the same object, but they are slightly different concepts and should be treated as such. 
%%For more details, see \hyperref[points_vs_vectors]{Appendix~\ref*{points_vs_vectors}}
%\end{remark}


%\vspace{2mm}
%\noindent
%{\it 
%Do not be confused by our use of a superscript to label components of a vector. Here $v^2$ denotes
%the second component of a vector $v$, rather than a number $v$ squared!}  
%\vspace{4mm}

\section{Addition and Scalar Multiplication in ${\mathbb{R}}^n$}
A simple but important property of $n$-vectors is that we can {\it add} two $n$-vectors together and {\it multiply} one $n$-vector by a scalar:

\begin{definition} Given two $n$-vectors \(a\) and \(b\) whose components are given by 
$$a=\ccolvec{a^1 \\ \vdots  \\ a^n } \mbox{ and }\  b=\ccolvec{b^1 \\ \vdots  \\ b^n }$$ their \emph{sum}\index{Vector addition!$n$-vectors} is $$a+b := \ccolvec{ a^1+b^1 \\ \vdots \\ a^n+b^n}\, .$$  
Given a scalar $\lambda$, the \emph{scalar multiple}\index{Scalar multiplication!$n$-vectors} $$\lambda a := \ccolvec{ \lambda a^1 \\ \vdots \\ \lambda a^n}\, .$$
\end{definition}

\begin{example}
Let 
$$a=\colvec{1\\2\\3\\4} \mbox{ and }\ b = \colvec{4\\3\\2\\1}\, .$$
Then, for example,
$$a+b= \colvec{5\\5\\5\\5} \mbox{ and }\  3a - 2b= \colvec{-5\\0\\5\\10}\, .$$
\end{example}

%Notice that these are the same rules we saw in \hyperref[vectoroperations]{Lecture 4}! 

%In Lectures 1-4, we thought of a vector as being a list of numbers which captured information about a linear system. Now we are thinking of a vector as a magnitude and a direction in \(\mathbb{R}^n,\) and luckily the same rules apply.

A special vector is the \emph{zero vector}\index{Zero vector!$n$-vectors}. 
%connecting the origin to itself.  
All of its components are zero:
$$
0=\ccolvec{0\\[-.5mm] \vdots \\ 0}=:0_n\, .
$$

In Euclidean geometry---the study of ${\mathbb R}^n$ with lengths and angles defined as in section~\ref{dirmag}
---$n$-vectors are used to label points  $P$ and the zero vector labels the origin $O$. In this sense,
  the zero vector is the only one  with zero magnitude, and the only one which points in no particular direction.  

\section{Hyperplanes}
Vectors in ${\mathbb R}^n$ are impossible to visualize unless $n$ is 1,2, or 3. However, familiar objects like lines and planes still make sense for any value of $n$:
The line $L$ along the direction defined by a vector $v$ and through a point $P$ labeled by a vector~$u$  can be written as 
$$L=\{ u + tv ~|~ t \in \mathbb{R} \}\, .$$  
Sometimes, since we know that a point $P$ corresponds to a vector,  we will be lazy and just write $L=\{P + tv ~|~ t \in \mathbb{R} \}$.  

\begin{example} %A line in ${\mathbb{R}}^4$.\\[.2cm]
$\left\{ \colvec{1\\2\\3\\4} + t\colvec{1\\0\\0\\0} \middle\arrowvert t \in \mathbb{R} \right\}$ describes a line in ${\mathbb{R}}^4$ parallel to the $x_1$-axis.
\end{example}



%unrelated thought! 
%Any scalar multiple of a non-zero vector lies in the line determined by that vector.

Given two non-zero vectors $u,v$, they will \emph{usually} determine a plane, 
\begin{center}
\includegraphics[scale=.28]{uvplane.jpg}
\end{center}
unless both vectors are in the same line,  in which case, one of the vectors is a scalar multiple of the other.  The sum of $u$ and $v$ corresponds to laying the two vectors head-to-tail and drawing the connecting vector.  If $u$ and $v$ determine a plane, then their sum lies in the plane determined by $u$ and~$v$.
\begin{center}
\includegraphics[scale=.28]{u+v.jpg}
\end{center}
The plane determined by two vectors $u$ and $v$ can be written as 
$$\{ P + su + tv ~|~ s, t \in \mathbb{R} \}\, .$$  


\begin{example}{(A plane in a higher dimensional space)} \label{2hyperplaneeg}
$$\left\{ \colvec{3\\1\\4\\1\\5\\9} + s\colvec{1\\0\\0\\0\\0\\0} + t\colvec{0\\1\\0\\0\\0\\0} \middle\arrowvert s, t \in \mathbb{R} \right\}$$ describes a plane in 6-dimensional space parallel to the $xy$-plane.
\end{example}

\Videoscriptlink{vectors_in_space_parametric.mp4}{Parametric Notation}{scripts_vectors_in_space_n_vectors_parametric}

\vspace{2mm}
We can generalize the notion of a plane with the following recursive definition. (That is, infinitely many things are defined in the following line.)

\begin{definition} A set of $k+1$ vectors $P,v_1, \ldots, v_k$ in $\mathbb{R}^n$ with $k\leq n$ determines a $k$-dimensional {\bf hyperplane}\index{Hyperplane}, 
\[
\left\{ P + \sum_{i=1}^{k} \lambda_iv_i\,  |\,  \lambda_i \in \mathbb{R} \right\}
\]
unless any of the vectors $v_j$ lives in the $(k-1)$-dimensional hyperplane determined by the other $k-1$ vectors
 \[
\left\{ 0 + \sum_{i\neq j}^{k} \lambda_iv_i\,  |\,  \lambda_i \in \mathbb{R} \right\}.
\]
%If the $k$ vectors do determine a $k$-dimensional hyperplane, then any point in the hyperplane can be written as:
\end{definition}


\begin{example}{(3+1 vectors that do not specify a  3-dimensional hyperplane)}
$$S:=\left\{ \colvec{3\\1\\4\\1\\5\\9} + s\colvec{1\\0\\0\\0\\0\\0} + t\colvec{0\\1\\0\\0\\0\\0} +u \colvec{1\\1\\0\\0\\0\\0}\middle\arrowvert s, t,u \in \mathbb{R} \right\}$$ 
is not a 3-dimensional hyperplane because 
$$\colvec{1\\1\\0\\0\\0\\0}=1\colvec{1\\0\\0\\0\\0\\0} + 1\colvec{0\\1\\0\\0\\0\\0} \in  \left\{     s\colvec{1\\0\\0\\0\\0\\0} + t\colvec{0\\1\\0\\0\\0\\0}  \middle\arrowvert  s,t \in \mathbb{R} \right\}.$$
In fact, the set could be rewritten as 
$$S=\left\{ \colvec{3\\1\\4\\1\\5\\9} + (s+u)\colvec{1\\0\\0\\0\\0\\0} + (t+u)\colvec{0\\1\\0\\0\\0\\0} \middle\arrowvert s, t,u \in \mathbb{R} \right\}$$
$$=
\left\{ \colvec{3\\1\\4\\1\\5\\9} + a\colvec{1\\0\\0\\0\\0\\0} + b\colvec{0\\1\\0\\0\\0\\0} \middle\arrowvert a,b \in \mathbb{R} \right\}$$ 
and so is actually the same 2-dimensional hyperplane in $\mathbb{R}^6$  as in example~\ref{2hyperplaneeg}.
\end{example}
















You might sometimes encounter the word ``hyperplane"" without the qualifier ``k-dimensional. 
When the dimension $k$ is not specified, one usually assumes that $k=n-1$ for a hyperplane inside ${\mathbb R}^n$. This is the kind of object that is specified by one algebraic equation in $n$ variables. 

\begin{example}{(Specifying a plane with one linear algebraic equation.)}\\
The solution set to 
$$
x_1+x_2+x_3+x_4+x_5=1 \Leftrightarrow 
\colvec{x_1\\x_2\\x_3\\x_4\\x_5} =
\ccolvec{
1-x_2-x_3-x_4-x_5\\
\phantom{1-}x_2\phantom{-x_3-x_4-x_5}\\
\phantom{1-x_2-}x_3\phantom{-x_4-x_5}\\
\phantom{1-x_2-x_3-}x_4\phantom{-x_5}\\
\phantom{1-x_2-x_3-x_4-}x_5
}
$$
is
$$\left\{ \colvec{1\\0\\0\\0\\0} + 
s_2 \colvec{-1\\1\\0\\0\\0} + 
s_3 \colvec{-1\\0\\1\\0\\0} + 
s_4 \colvec{-1\\0\\0\\1\\0} + 
s_5 \colvec{-1\\1\\0\\0\\1}
\middle\arrowvert s_2,s_3,s_4,s_5 \in \mathbb{R} \right\},$$
a 4-dimensional hyperplane in $\mathbb{R}^5$.
\end{example}


\section{Directions and Magnitudes}\label{dirmag}

Consider the {\it Euclidean length}\index{Euclidean length} of an $n$-vector: 
\[
\|v\| := \sqrt{(v^1)^2 + (v^2)^2+\cdots+(v^n)^2}\ =\ \sqrt{ \sum_{i=1}^n (v^i)^2 }\: .
\]
Using the Law of Cosines\index{Law of Cosines}, we can then figure out the angle between two vectors.  Given two vectors $v$ and $u$ that {\it span} a plane in $\mathbb{R}^n$, we can then connect the ends of $v$ and $u$ with the vector $v-u$. 
 \begin{center}
\includegraphics[scale=.24]{triangleineq.jpg}
\end{center}
Then the Law of Cosines states that:

\[ 
\|v-u\|^2 = \|u\|^2 + \|v\|^2 - 2\|u\|\,  \|v\| \cos \theta 
\]
Then isolate $\cos \theta$:

\begin{eqnarray*}
\|v-u\|^2 - \|u\|^2 - \|v\|^2 &=& (v^1-u^1)^2 + \cdots + (v^n-u^n)^2 \\
& & \quad - \big((u^1)^2 + \cdots + (u^n)^2\big) \\
& & \quad - \big((v^1)^2 + \cdots + (v^n)^2\big) \\
& = & -2 u^1v^1 - \cdots - 2u^nv^n
\end{eqnarray*}
Thus, 
\[
\|u\|\, \|v\| \cos \theta = u^1v^1 + \cdots + u^nv^n\, .
\]
Note that in the above discussion, we have assumed (correctly) that Euclidean lengths in ${\mathbb R}^n$
give the usual notion of lengths of vectors for any plane in ${\mathbb R}^n$. This now motivates the definition of the dot product.

\begin{definition} 
The {\bf dot product}\index{Dot product} of $u=\ccolvec{u^1 \\ \vdots \\ u^n }$ and $v=\ccolvec{v^1 \\ \vdots \\ v^n }$ is 
$$u\dotprod v := u^1v^1 + \cdots + u^nv^n\, .$$
\end{definition} 

\begin{example} of the dot product of two vectors from $\R^{100}$.
$$ \ccolvec{1\\2\\3\\4\\ \vdots \\ 100} \cdot 
\colvec{1\\1\\1\\1\\ \vdots \\ 1} = 1+2+3+\cdots+100=\frac12.100.101=5050.
$$
The sum above is the one Gau\ss, according to legend, could do in kindergarten\index{Kindergarten}.
\end{example}

\begin{definition} 
The {\bf length} (or {\bf norm} or {\bf magnitude}) of an n-vector\index{Length of a vector}\index{Norm|seealso{Length of a vector}}\index{Magnitude|seealso{Length of a vector}} $v$ is 
$$\|v\| := \sqrt{v\dotprod v }\, .$$
\end{definition} 

\begin{example} 
of the norm of a vector from $\R^{101}$.\\
$$\left\| \ccolvec{1\\2\\3\\4\\ \vdots \\ 101}\right\|
= 
\sqrt{\sum_{i=1}^{101} i^2} =   \sqrt{37,961}.
$$
\end{example}

\begin{definition} 
The {\bf angle}\index{Angle between vectors} $\theta$ between two vectors is determined by the formula $$u\dotprod v = \|u\|\|v\|\cos \theta\, .$$
\end{definition}

\begin{example} of an angle between two vectors form $\R^{101}$.\\
The angle between $ \ccolvec{1\\2\\3\\4\\ \vdots \\ 101} $ and 
$
\colvec{1\\0\\1\\0\\ \vdots \\ 1} $
is
$ 
\arccos  \left( \frac{10,201 }{\sqrt{37,916} \sqrt{51} } \right).
$
\end{example}




\begin{definition} 
Two vectors are \hypertarget{orthog}{{\bf orthogonal}}\index{Orthogonal} (or {\bf perpendicular}) if their dot product is zero.
\end{definition} 


\begin{example} of vectors from $\R^{101}$ that are orthogonal to each other.\\
$$
\colvec{1\\1\\1 \\ \vdots \\1} \cdot \colvec{1\\-1\\1\\ \vdots \\-1} =0.
$$
\end{example}

Notice that the zero vector $0_n$ from $\mathbb{R}^n$ is orthogonal to every vector in~$\mathbb{R}^n$; $0_n\cdot v=0$ for all $v \in \mathbb{R}^n$.

\vspace{2mm}\noindent
The dot product has some important properties; it is
\begin{enumerate}
\item  \emph{symmetric}:  
$$u\dotprod v = v\dotprod u\, ,$$
\item \emph{Distributive}:  $$u\dotprod (v+w) = u\dotprod v + u\dotprod w\, ,$$
\item \emph{Bilinear} (which is to say, linear in both $u$ and $v$):  
$$ u\dotprod (cv+dw) = c \, u\dotprod v +d \, u\dotprod w\, ,$$ and 
$$(cu+dw)\dotprod v = c\, u\dotprod v + d\, w\dotprod v\, .$$
\item \emph{Positive Definite}: $$u\dotprod u \geq 0\, ,$$ and  $u\dotprod u = 0$ only when $u$ itself is the $0$-vector.
\end{enumerate}

There are, in fact, many different useful ways to define lengths of vectors.  Notice in the definition above that we first defined the dot product, and then defined everything else in terms of the dot product.  So if we change our idea of the dot product, we change our notion of length and angle as well.  The dot product determines the \emph{Euclidean length and angle} between two vectors.  

Other definitions of length and angle arise from \index{Inner product}{\bf \hypertarget{inner_product}{inner products}}, which have all of the properties listed above
(except that in some contexts the positive definite requirement is relaxed). Instead of writing $\dotprod$ for other inner products, we usually write $\langle u,v \rangle$ to avoid confusion.

%\href{\webworkurl ReadingHomework5/1/}{Reading homework: problem 5.1}
\Reading{VectorsInSpace}{1}

\begin{example}\label{lorentzex}
Consider a four-dimensional space, with a special direction which we will call ``time''.  The \hypertarget{lorentzian_metric}{\emph{Lorentzian inner product}} on $\mathbb{R}^4$ is given by $\langle u,v \rangle = u^1v^1 + u^2v^2 + u^3v^3 - u^4v^4$.  This is of central importance in Einstein's theory of special relativity. Note, in particular,  that it is not positive definite.
As a result, the ``squared-length'' of a vector with coordinates $x, y, z$ and $t$ is $\|v\|^2 = x^2 + y^2 + z^2 - t^2$.  Notice that it is possible for $\|v\|^2\leq 0$ even with non-vanishing $v$!  
The physical interpretation of this inner product depends on the sign of the inner product; two space time points $X_1:=(x_1,y_1,z_1,t_1),~X_2:=(x_2,y_2,z_2,t_2)$  are 
\begin{itemize}
\item separated by a distance $\sqrt{ \langle X_1, X_2\rangle }$  if $\langle X_1, X_2\rangle \geq 0$.\\
\item separated by a time $\sqrt{ -\langle X_1, X_2\rangle }$  if $\langle X_1, X_2\rangle \leq 0.$
\end{itemize}
In particular, the difference in time coordinates $t_2-t_1$ is not the time between the two points! (Compare this to using polar coordinates for which the distance between two points $(r,\theta_1)$ and $(r,\theta_2)$ is not $\theta_2-\theta_1$; coordinate differences are not necessarily distances.)
\end{example}



\begin{theorem}[Cauchy-Schwarz Inequality]\index{Cauchy--Schwarz inequality}
For any non-zero vectors $u$ and~$v$ with an inner-product $\langle\:\  ,\:\,  \rangle$
\[ 
\frac{|\langle u,v \rangle|}{\|u\|\, \|v\|} \leq 1.
\]
\end{theorem}

The easiest proof would use the definition of the angle between two vectors and the fact that $\cos \theta \leq 1$. However, strictly speaking speaking we did not check our assumption that we could apply the Law of Cosines
to the Euclidean length in ${\mathbb R}^n$. There is, however a simple algebraic proof. 
\begin{proof}
Let $\alpha$ be any
real number and consider the following positive, quadratic polynomial in $\alpha$
$$
0\leq \langle u+\alpha v,u+\alpha v\rangle = \langle u,u\rangle +2\alpha \langle u,v\rangle +\alpha^2 \langle v,v\rangle\, .
$$
%(You should carefully check for yourself exactly which properties of an inner product were used to write down the above inequality! )
%Next, a tiny calculus computation shows that 
Since any quadratic $a\alpha^2 + 2b \alpha + c$ takes its minimal value
$c-\frac{b^2}{a}$
when $\alpha=-\frac{b}{2a}$,  and the inequality should hold for even this minimum value of the polynomial 
$$
0\leq \langle u,u\rangle -\frac{\langle u,v\rangle^2}{\langle v,v\rangle}\, 
\Leftrightarrow 
\frac{|\langle u,v \rangle|}{\|u\|\, \|v\|} \leq 1.
$$
\end{proof}

\begin{theorem}[Triangle Inequality]\index{Triangle inequality}
For any $u,v\in \mathbb{R}^n$
\[ \|u+v\| \leq \|u\| + \|v\| .\]
\end{theorem}

\begin{proof}

\begin{eqnarray*}
\|u+v\|^2 & = & (u+v)\dotprod (u+v) \\
 	& = & u\dotprod u + 2 u\dotprod v + v\dotprod v \\
	& = & \|u\|^2 + \|v\|^2 + 2\,  \|u\|\,  \|v\| \cos \theta \\
	& = & \left(\|u\| + \|v\|\right)^2 + 2 \, \|u\| \, \|v\| (\cos \theta -1) \\
	& \leq & \left(\|u\| + \|v\|\right)^2	.
\end{eqnarray*}

\noindent
That is, the square of the left-hand side of the triangle inequality is $\leq$ the square of the right-hand side. Since both the things being squared are positive, the inequality holds without the square;
\[ \|u+v\| \leq  \|u\| + \|v\|   \]

\end{proof}

The triangle inequality is also ``self-evident'' when examining a sketch of $u$, $v$ and $u+v$.
\begin{center}
\includegraphics[scale=.25]{triangleagain.jpg}
\end{center}

\begin{example}
Let 
$$a=\colvec{1\\2\\3\\4} \mbox{ and }\ b = \colvec{4\\3\\2\\1}\, ,$$
so that
$$   a\dotprod a= b\dotprod b 
=1 +2^2+3^2+4^2=30 $$ $$\Rightarrow \|a\|=\sqrt{30} =\| b \| \mbox{ and } \ 
\big(\|a\|+\|b\|\big)^2=(2\sqrt{30})^2=120\, .$$
Since
$$a+b= \colvec{5\\5\\5\\5}\, , \quad $$
we have $$\|a+b\|^2=5^2+5^2+5^2+5^2=100<120=\big(\|a\|+\|b\|\big)^2$$
as predicted by the triangle inequality.

Notice also that $a\dotprod b=1.4+2.3+3.2+4.1=20< \sqrt{30}.\sqrt{30}=30=\|a\|\, \|b\|$ 
in accordance with the Cauchy--Schwarz inequality.
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework5/2/}{Reading homework: problem 5.2}\end{center}
\Reading{VectorsInSpace}{2}


\section{Vectors, Lists and Functions: $\R^S$}
If you were  going shopping you might make something like the following list.

\begin{center}
\includegraphics[scale =.3]{list.jpg}
\end{center}

\noindent
We could represent this information mathematically as a set, $$S=\{\rm apple, orange, onion, milk, carrot\}\, .$$ There is no information
of ordering here and no information about how many carrots you will buy. This set by itself is not a vector; how would we add such sets to 
one another?

If you were a more careful shopper your list might look like the following.

\begin{center}
\includegraphics[scale =.5]{shoplist}
\end{center}

\noindent
What you have really done here is assign a number to each element of the set $S$. In other words, the second list is a function
$$
f:S\longrightarrow {\mathbb R}\, .
$$
Given two lists like the second one above, we could easily add them -- if you plan to buy 5 apples and I am buying 3 apples, together we
will buy 8 apples! In fact, the second list is really a 5-vector in disguise. 

In general
it is helpful to think of an $n$-vector as a function whose domain is the set 
$\{1,\dots,n\}$.
%, as \hyperlink{vecs as fun}{discussed in chapter 1}. 
This is equivalent to thinking of an $n$-vector as an ordered list of $n$ numbers.
These two ideas give us  two equivalent notions for the set of all $n$-vectors:
$$
{\mathbb{R}}^n :=\left\{ \ccolvec{a^1 \\ \vdots \\ a^n } \middle\vert \,  a^1,\dots ,a^n \in \mathbb{R} \right\}
=\{ a:\{1,\dots,n\}\to \mathbb{R}\} =: \mathbb{R}^{ \{1,\cdots,n\} }
$$
The notation $\mathbb{R}^{ \{1,\cdots,n\} }$ is used to denote the set of all functions from $   \{1,\dots,n\} $ to $\mathbb{R}$. 

Similarly, for any set $S$ the notation $\mathbb{R}^S$ denotes the set of functions from $S$ to~$\mathbb{R}$:
$$
{\mathbb R}^S:=\{ f:S\to {\mathbb R}\}\, .
$$
When $S$ is an ordered set like $\{1,\dots,n\}$, it is natural to write the components in order. When the elements of $S$ do not have a natural ordering, doing so might cause confusion. 



\begin{example}
{Consider the set } $S=\{*, \star, \# \}$ from \hyperlink{Consider the set}{chapter 1 review problem}~\ref{ch1rev}. A particular element of $\mathbb{R}^S$ is the function $a$ explicitly defined by 
$$ a^{\star}=3, a^{\#}=5, a^{*}=-2.$$
It is not natural to write 
$$
a=\colvec{3 \\ 5 \\ -2 } ~{\rm or} ~a=\colvec{-2\\ 3 \\ 5 } 
$$
because the elements of $S$ do not have an ordering, since as sets $\{*, \star, \# \}=\{\star, \#,*\}$.
%, like the elements of $\{ 1,2,3\}$. 
\end{example}


In this important way, $\mathbb{R}^S$ seems different from $\mathbb{R}^3$. 
What is more evident are the similarities; since we can add two functions, we can add two elements of~$\mathbb{R}^S$:

\begin{example} Addition in $\mathbb{R}^{\{*, \star, \# \}}$\\
%\begin{eqnarray*}
If $a,b \in \mathbb{R}^{\{*, \star, \# \}}$ 
such that 
$$a^{\star}=3, a^{\#}=5, a^{*}=-2$$ 
and 
$$b^{\star}=-2, b^{\#}=4, b^{*}=13$$
then $a+b \in \mathbb{R}^S$ is the function such that
$$(a+b)^{\star}=3-2=1, (a+b)^{\#}=5+4=9, (a+b)^{*}=-2+13=11\, .$$
\end{example}

Also, since we can multiply functions by  numbers, there is a notion of scalar multiplication on $\mathbb{R}^S$.

\begin{example} Scalar Multiplication in $\mathbb{R}^S$\\
If $a \in \mathbb{R}^{\{*, \star, \# \}}$ such that
$$a^{\star}=3, a^{\#}=5, a^{*}=-2$$
then $3 a \in \mathbb{R}^{\{*, \star, \# \}}$ is the function such that 
$$(3a)^{\star}=3\cdot3=9, (3a)^{\#}=3\cdot5=15, (3a)^{*}=3(-2)=-6\, .$$

\end{example}

We visualize $\mathbb{R}^2$ and  $\mathbb{R}^3$ in terms of axes. We have a more abstract picture of  $\mathbb{R}^4$,  $\mathbb{R}^5$ and  $\mathbb{R}^n$ for larger $n$ while  $\mathbb{R}^S$ seems even more abstract. However, when thought of as a simple ``shopping list'',
you can see that vectors in $\mathbb{R}^S$ in fact, can describe everyday objects.
In chapter~\ref{vectorSpaces} we introduce the  general definition of a vector space that unifies all these different notions of a vector.

%\section*{References}
%
%Hefferon: Chapter One.II
%\\
%Beezer: Chapter V, Section VO, Subsection VEASM
%\\
%Beezer: Chapter V, Section O, Subsections IP-N
%\\
%Relevant Wikipedia Articles:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Dot_product}{Dot Product}
%\item \href{http://en.wikipedia.org/wiki/Inner_product_space}{Inner Product Space}
%\item \href{http://en.wikipedia.org/wiki/Minkowski_metric}{Minkowski Metric}
%\end{itemize} 


\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading problems &
\hwrref{VectorsInSpace}{1}, \hwrref{VectorsInSpace}{2}\\
Vector operations &  \hwref{VectorsInSpace}{3}\\
Vectors and lines &  \hwref{VectorsInSpace}{4}\\
Vectors and planes &\hwref{VectorsInSpace}{5}\\
Lines, planes and vectors & \hwref{VectorsInSpace}{6},\hwref{VectorsInSpace}{7}\\
Equation of a plane &\hwref{VectorsInSpace}{8},\hwref{VectorsInSpace}{9}\\
Angle between a line and plane &\hwref{VectorsInSpace}{10}
\\
\hline
\end{tabular}


\input{\vectorsInSpacePath/problems}

\newpage
","\chapter{\vectorsInSpaceTitle}
\label{vectorsinspace}

%In vector calculus classes, you  encountered three-dimensional vectors.  Now we will develop the notion of $n$-vectors and learn some of their properties.
%
%\videoscriptlink{vectors_in_space_n_vectors_overview.mp4}{Overview}{scripts_vectors_in_space_n_vectors}

%We begin by looking at the space $\mathbb{R}^n$, which we can think of as the space of points with $n$ coordinates.  We then specify an \emph{origin} $O$, a favorite point in $\mathbb{R}^n$.  Now given any other point $P$, we can draw a \emph{vector} $v$ from $O$ to $P$.  Just as in $\mathbb{R}^3$, a vector has a \emph{magnitude} and a \emph{direction}.  
%
%\hypertarget{choosing_the_origin}{If} $O$ has coordinates $(o^1, \ldots, o^n)$ and $p$ has coordinates $(p^1, \ldots, p^n)$, then the \emph{components} of the vector $v$ are $\colvec{p^1 - o^1 \\ p^2- o^2 \\ \vdots \\ p^n - o^n }$.  This construction allows us to put the origin anywhere that seems most convenient in $\mathbb{R}^n$, not just at the point with zero coordinates:
%
%\begin{center}
%\includegraphics[scale =.3]{vectorOP.jpg}
%\end{center}

To continue our linear algebra journey, we must discuss $n$-vectors with an arbitrarily large number of components. 
The simplest way to think about these is as  ordered  lists of numbers, 
$$a=\ccolvec{a^1 \\ \vdots \\ a^n } .$$
%It is also convenient to think of n-vectors as functions with domain the set 
%$\{1,\dots,n\}$, as \hyperlink{vecs as fun}{discussed in chapter 1}. These two ideas give us the following two equivalent notions of the set of all $n$ vectors,
%$$
%{\mathbb{R}}^n =\left\{ \colvec{a^1 \\ \vdots \\ a^n } | \,  a^1,\dots a^n \in \mathbb{R} \right\}
%=\{ a:\{1,\dots,n\}\to \mathbb{R}\}
%$$
%
%Sometimes the notation $\mathbb{R}^{ \{1,\cdots,n\} }$ is used to emphasize the idea of functions from $   \{1,\cdots,n\} $ to $\mathbb{R}$.
\noindent
{\it 
Do not be confused by our use of a superscript to label components of a vector. Here $a^2$ denotes
the second component of the vector $a$, rather than the number $a$ squared!}  

\vspace{1mm}
We emphasize  that order matters: 
\begin{example} (Order of Components Matters)
$$\colvec{7 \\4 \\ 2\\ 5 } 
\neq \colvec{7 \\2 \\4 \\5 } .$$
\end{example}
The set of all $n$-vectors is denoted $\mathbb{R}^n$. As an equation
$$
{\mathbb{R}}^n :=\left\{ \colvec{a^1 \\ \vdots\ \  \\ a^n } \middle\vert \,  a^1,\dots, a^n \in \mathbb{R} \right\} \,.
$$
%; $2$-vectors and $3$-vectors are o

%\begin{remark}
%\hypertarget{note_points_versus_vectors}{A quick note on points versus vectors.} We might sometimes interpret a point and a vector as the same object, but they are slightly different concepts and should be treated as such. 
%%For more details, see \hyperref[points_vs_vectors]{Appendix~\ref*{points_vs_vectors}}
%\end{remark}


%\vspace{2mm}
%\noindent
%{\it 
%Do not be confused by our use of a superscript to label components of a vector. Here $v^2$ denotes
%the second component of a vector $v$, rather than a number $v$ squared!}  
%\vspace{4mm}

\section{Addition and Scalar Multiplication in ${\mathbb{R}}^n$}
A simple but important property of $n$-vectors is that we can {\it add} two $n$-vectors together and {\it multiply} one $n$-vector by a scalar:

\begin{definition} Given two $n$-vectors \(a\) and \(b\) whose components are given by 
$$a=\ccolvec{a^1 \\ \vdots  \\ a^n } \mbox{ and }\  b=\ccolvec{b^1 \\ \vdots  \\ b^n }$$ their \emph{sum}\index{Vector addition!$n$-vectors} is $$a+b := \ccolvec{ a^1+b^1 \\ \vdots \\ a^n+b^n}\, .$$  
Given a scalar $\lambda$, the \emph{scalar multiple}\index{Scalar multiplication!$n$-vectors} $$\lambda a := \ccolvec{ \lambda a^1 \\ \vdots \\ \lambda a^n}\, .$$
\end{definition}

\begin{example}
Let 
$$a=\colvec{1\\2\\3\\4} \mbox{ and }\ b = \colvec{4\\3\\2\\1}\, .$$
Then, for example,
$$a+b= \colvec{5\\5\\5\\5} \mbox{ and }\  3a - 2b= \colvec{-5\\0\\5\\10}\, .$$
\end{example}

%Notice that these are the same rules we saw in \hyperref[vectoroperations]{Lecture 4}! 

%In Lectures 1-4, we thought of a vector as being a list of numbers which captured information about a linear system. Now we are thinking of a vector as a magnitude and a direction in \(\mathbb{R}^n,\) and luckily the same rules apply.

A special vector is the \emph{zero vector}\index{Zero vector!$n$-vectors}. 
%connecting the origin to itself.  
All of its components are zero:
$$
0=\ccolvec{0\\[-.5mm] \vdots \\ 0}=:0_n\, .
$$

In Euclidean geometry---the study of ${\mathbb R}^n$ with lengths and angles defined as in section~\ref{dirmag}
---$n$-vectors are used to label points  $P$ and the zero vector labels the origin $O$. In this sense,
  the zero vector is the only one  with zero magnitude, and the only one which points in no particular direction.  

\section{Hyperplanes}
Vectors in ${\mathbb R}^n$ are impossible to visualize unless $n$ is 1,2, or 3. However, familiar objects like lines and planes still make sense for any value of $n$:
The line $L$ along the direction defined by a vector $v$ and through a point $P$ labeled by a vector~$u$  can be written as 
$$L=\{ u + tv ~|~ t \in \mathbb{R} \}\, .$$  
Sometimes, since we know that a point $P$ corresponds to a vector,  we will be lazy and just write $L=\{P + tv ~|~ t \in \mathbb{R} \}$.  

\begin{example} %A line in ${\mathbb{R}}^4$.\\[.2cm]
$\left\{ \colvec{1\\2\\3\\4} + t\colvec{1\\0\\0\\0} \middle\arrowvert t \in \mathbb{R} \right\}$ describes a line in ${\mathbb{R}}^4$ parallel to the $x_1$-axis.
\end{example}



%unrelated thought! 
%Any scalar multiple of a non-zero vector lies in the line determined by that vector.

Given two non-zero vectors $u,v$, they will \emph{usually} determine a plane, 
\begin{center}
\includegraphics[scale=.28]{uvplane.jpg}
\end{center}
unless both vectors are in the same line,  in which case, one of the vectors is a scalar multiple of the other.  The sum of $u$ and $v$ corresponds to laying the two vectors head-to-tail and drawing the connecting vector.  If $u$ and $v$ determine a plane, then their sum lies in the plane determined by $u$ and~$v$.
\begin{center}
\includegraphics[scale=.28]{u+v.jpg}
\end{center}
The plane determined by two vectors $u$ and $v$ can be written as 
$$\{ P + su + tv ~|~ s, t \in \mathbb{R} \}\, .$$  


\begin{example}{(A plane in a higher dimensional space)} \label{2hyperplaneeg}
$$\left\{ \colvec{3\\1\\4\\1\\5\\9} + s\colvec{1\\0\\0\\0\\0\\0} + t\colvec{0\\1\\0\\0\\0\\0} \middle\arrowvert s, t \in \mathbb{R} \right\}$$ describes a plane in 6-dimensional space parallel to the $xy$-plane.
\end{example}

\Videoscriptlink{vectors_in_space_parametric.mp4}{Parametric Notation}{scripts_vectors_in_space_n_vectors_parametric}

\vspace{2mm}
We can generalize the notion of a plane with the following recursive definition. (That is, infinitely many things are defined in the following line.)

\begin{definition} A set of $k+1$ vectors $P,v_1, \ldots, v_k$ in $\mathbb{R}^n$ with $k\leq n$ determines a $k$-dimensional {\bf hyperplane}\index{Hyperplane}, 
\[
\left\{ P + \sum_{i=1}^{k} \lambda_iv_i\,  |\,  \lambda_i \in \mathbb{R} \right\}
\]
unless any of the vectors $v_j$ lives in the $(k-1)$-dimensional hyperplane determined by the other $k-1$ vectors
 \[
\left\{ 0 + \sum_{i\neq j}^{k} \lambda_iv_i\,  |\,  \lambda_i \in \mathbb{R} \right\}.
\]
%If the $k$ vectors do determine a $k$-dimensional hyperplane, then any point in the hyperplane can be written as:
\end{definition}


\begin{example}{(3+1 vectors that do not specify a  3-dimensional hyperplane)}
$$S:=\left\{ \colvec{3\\1\\4\\1\\5\\9} + s\colvec{1\\0\\0\\0\\0\\0} + t\colvec{0\\1\\0\\0\\0\\0} +u \colvec{1\\1\\0\\0\\0\\0}\middle\arrowvert s, t,u \in \mathbb{R} \right\}$$ 
is not a 3-dimensional hyperplane because 
$$\colvec{1\\1\\0\\0\\0\\0}=1\colvec{1\\0\\0\\0\\0\\0} + 1\colvec{0\\1\\0\\0\\0\\0} \in  \left\{     s\colvec{1\\0\\0\\0\\0\\0} + t\colvec{0\\1\\0\\0\\0\\0}  \middle\arrowvert  s,t \in \mathbb{R} \right\}.$$
In fact, the set could be rewritten as 
$$S=\left\{ \colvec{3\\1\\4\\1\\5\\9} + (s+u)\colvec{1\\0\\0\\0\\0\\0} + (t+u)\colvec{0\\1\\0\\0\\0\\0} \middle\arrowvert s, t,u \in \mathbb{R} \right\}$$
$$=
\left\{ \colvec{3\\1\\4\\1\\5\\9} + a\colvec{1\\0\\0\\0\\0\\0} + b\colvec{0\\1\\0\\0\\0\\0} \middle\arrowvert a,b \in \mathbb{R} \right\}$$ 
and so is actually the same 2-dimensional hyperplane in $\mathbb{R}^6$  as in example~\ref{2hyperplaneeg}.
\end{example}
















You might sometimes encounter the word ``hyperplane"" without the qualifier ``k-dimensional. 
When the dimension $k$ is not specified, one usually assumes that $k=n-1$ for a hyperplane inside ${\mathbb R}^n$. This is the kind of object that is specified by one algebraic equation in $n$ variables. 

\begin{example}{(Specifying a plane with one linear algebraic equation.)}\\
The solution set to 
$$
x_1+x_2+x_3+x_4+x_5=1 \Leftrightarrow 
\colvec{x_1\\x_2\\x_3\\x_4\\x_5} =
\ccolvec{
1-x_2-x_3-x_4-x_5\\
\phantom{1-}x_2\phantom{-x_3-x_4-x_5}\\
\phantom{1-x_2-}x_3\phantom{-x_4-x_5}\\
\phantom{1-x_2-x_3-}x_4\phantom{-x_5}\\
\phantom{1-x_2-x_3-x_4-}x_5
}
$$
is
$$\left\{ \colvec{1\\0\\0\\0\\0} + 
s_2 \colvec{-1\\1\\0\\0\\0} + 
s_3 \colvec{-1\\0\\1\\0\\0} + 
s_4 \colvec{-1\\0\\0\\1\\0} + 
s_5 \colvec{-1\\1\\0\\0\\1}
\middle\arrowvert s_2,s_3,s_4,s_5 \in \mathbb{R} \right\},$$
a 4-dimensional hyperplane in $\mathbb{R}^5$.
\end{example}


\section{Directions and Magnitudes}\label{dirmag}

Consider the {\it Euclidean length}\index{Euclidean length} of an $n$-vector: 
\[
\|v\| := \sqrt{(v^1)^2 + (v^2)^2+\cdots+(v^n)^2}\ =\ \sqrt{ \sum_{i=1}^n (v^i)^2 }\: .
\]
Using the Law of Cosines\index{Law of Cosines}, we can then figure out the angle between two vectors.  Given two vectors $v$ and $u$ that {\it span} a plane in $\mathbb{R}^n$, we can then connect the ends of $v$ and $u$ with the vector $v-u$. 
 \begin{center}
\includegraphics[scale=.24]{triangleineq.jpg}
\end{center}
Then the Law of Cosines states that:

\[ 
\|v-u\|^2 = \|u\|^2 + \|v\|^2 - 2\|u\|\,  \|v\| \cos \theta 
\]
Then isolate $\cos \theta$:

\begin{eqnarray*}
\|v-u\|^2 - \|u\|^2 - \|v\|^2 &=& (v^1-u^1)^2 + \cdots + (v^n-u^n)^2 \\
& & \quad - \big((u^1)^2 + \cdots + (u^n)^2\big) \\
& & \quad - \big((v^1)^2 + \cdots + (v^n)^2\big) \\
& = & -2 u^1v^1 - \cdots - 2u^nv^n
\end{eqnarray*}
Thus, 
\[
\|u\|\, \|v\| \cos \theta = u^1v^1 + \cdots + u^nv^n\, .
\]
Note that in the above discussion, we have assumed (correctly) that Euclidean lengths in ${\mathbb R}^n$
give the usual notion of lengths of vectors for any plane in ${\mathbb R}^n$. This now motivates the definition of the dot product.

\begin{definition} 
The {\bf dot product}\index{Dot product} of $u=\ccolvec{u^1 \\ \vdots \\ u^n }$ and $v=\ccolvec{v^1 \\ \vdots \\ v^n }$ is 
$$u\dotprod v := u^1v^1 + \cdots + u^nv^n\, .$$
\end{definition} 

\begin{example} of the dot product of two vectors from $\R^{100}$.
$$ \ccolvec{1\\2\\3\\4\\ \vdots \\ 100} \cdot 
\colvec{1\\1\\1\\1\\ \vdots \\ 1} = 1+2+3+\cdots+100=\frac12.100.101=5050.
$$
The sum above is the one Gau\ss, according to legend, could do in kindergarten\index{Kindergarten}.
\end{example}

\begin{definition} 
The {\bf length} (or {\bf norm} or {\bf magnitude}) of an n-vector\index{Length of a vector}\index{Norm|seealso{Length of a vector}}\index{Magnitude|seealso{Length of a vector}} $v$ is 
$$\|v\| := \sqrt{v\dotprod v }\, .$$
\end{definition} 

\begin{example} 
of the norm of a vector from $\R^{101}$.\\
$$\left\| \ccolvec{1\\2\\3\\4\\ \vdots \\ 101}\right\|
= 
\sqrt{\sum_{i=1}^{101} i^2} =   \sqrt{37,961}.
$$
\end{example}

\begin{definition} 
The {\bf angle}\index{Angle between vectors} $\theta$ between two vectors is determined by the formula $$u\dotprod v = \|u\|\|v\|\cos \theta\, .$$
\end{definition}

\begin{example} of an angle between two vectors form $\R^{101}$.\\
The angle between $ \ccolvec{1\\2\\3\\4\\ \vdots \\ 101} $ and 
$
\colvec{1\\0\\1\\0\\ \vdots \\ 1} $
is
$ 
\arccos  \left( \frac{10,201 }{\sqrt{37,916} \sqrt{51} } \right).
$
\end{example}




\begin{definition} 
Two vectors are \hypertarget{orthog}{{\bf orthogonal}}\index{Orthogonal} (or {\bf perpendicular}) if their dot product is zero.
\end{definition} 


\begin{example} of vectors from $\R^{101}$ that are orthogonal to each other.\\
$$
\colvec{1\\1\\1 \\ \vdots \\1} \cdot \colvec{1\\-1\\1\\ \vdots \\-1} =0.
$$
\end{example}

Notice that the zero vector $0_n$ from $\mathbb{R}^n$ is orthogonal to every vector in~$\mathbb{R}^n$; $0_n\cdot v=0$ for all $v \in \mathbb{R}^n$.

\vspace{2mm}\noindent
The dot product has some important properties; it is
\begin{enumerate}
\item  \emph{symmetric}:  
$$u\dotprod v = v\dotprod u\, ,$$
\item \emph{Distributive}:  $$u\dotprod (v+w) = u\dotprod v + u\dotprod w\, ,$$
\item \emph{Bilinear} (which is to say, linear in both $u$ and $v$):  
$$ u\dotprod (cv+dw) = c \, u\dotprod v +d \, u\dotprod w\, ,$$ and 
$$(cu+dw)\dotprod v = c\, u\dotprod v + d\, w\dotprod v\, .$$
\item \emph{Positive Definite}: $$u\dotprod u \geq 0\, ,$$ and  $u\dotprod u = 0$ only when $u$ itself is the $0$-vector.
\end{enumerate}

There are, in fact, many different useful ways to define lengths of vectors.  Notice in the definition above that we first defined the dot product, and then defined everything else in terms of the dot product.  So if we change our idea of the dot product, we change our notion of length and angle as well.  The dot product determines the \emph{Euclidean length and angle} between two vectors.  

Other definitions of length and angle arise from \index{Inner product}{\bf \hypertarget{inner_product}{inner products}}, which have all of the properties listed above
(except that in some contexts the positive definite requirement is relaxed). Instead of writing $\dotprod$ for other inner products, we usually write $\langle u,v \rangle$ to avoid confusion.

%\href{\webworkurl ReadingHomework5/1/}{Reading homework: problem 5.1}
\Reading{VectorsInSpace}{1}

\begin{example}\label{lorentzex}
Consider a four-dimensional space, with a special direction which we will call ``time''.  The \hypertarget{lorentzian_metric}{\emph{Lorentzian inner product}} on $\mathbb{R}^4$ is given by $\langle u,v \rangle = u^1v^1 + u^2v^2 + u^3v^3 - u^4v^4$.  This is of central importance in Einstein's theory of special relativity. Note, in particular,  that it is not positive definite.
As a result, the ``squared-length'' of a vector with coordinates $x, y, z$ and $t$ is $\|v\|^2 = x^2 + y^2 + z^2 - t^2$.  Notice that it is possible for $\|v\|^2\leq 0$ even with non-vanishing $v$!  
The physical interpretation of this inner product depends on the sign of the inner product; two space time points $X_1:=(x_1,y_1,z_1,t_1),~X_2:=(x_2,y_2,z_2,t_2)$  are 
\begin{itemize}
\item separated by a distance $\sqrt{ \langle X_1, X_2\rangle }$  if $\langle X_1, X_2\rangle \geq 0$.\\
\item separated by a time $\sqrt{ -\langle X_1, X_2\rangle }$  if $\langle X_1, X_2\rangle \leq 0.$
\end{itemize}
In particular, the difference in time coordinates $t_2-t_1$ is not the time between the two points! (Compare this to using polar coordinates for which the distance between two points $(r,\theta_1)$ and $(r,\theta_2)$ is not $\theta_2-\theta_1$; coordinate differences are not necessarily distances.)
\end{example}



\begin{theorem}[Cauchy-Schwarz Inequality]\index{Cauchy--Schwarz inequality}
For any non-zero vectors $u$ and~$v$ with an inner-product $\langle\:\  ,\:\,  \rangle$
\[ 
\frac{|\langle u,v \rangle|}{\|u\|\, \|v\|} \leq 1.
\]
\end{theorem}

The easiest proof would use the definition of the angle between two vectors and the fact that $\cos \theta \leq 1$. However, strictly speaking speaking we did not check our assumption that we could apply the Law of Cosines
to the Euclidean length in ${\mathbb R}^n$. There is, however a simple algebraic proof. 
\begin{proof}
Let $\alpha$ be any
real number and consider the following positive, quadratic polynomial in $\alpha$
$$
0\leq \langle u+\alpha v,u+\alpha v\rangle = \langle u,u\rangle +2\alpha \langle u,v\rangle +\alpha^2 \langle v,v\rangle\, .
$$
%(You should carefully check for yourself exactly which properties of an inner product were used to write down the above inequality! )
%Next, a tiny calculus computation shows that 
Since any quadratic $a\alpha^2 + 2b \alpha + c$ takes its minimal value
$c-\frac{b^2}{a}$
when $\alpha=-\frac{b}{2a}$,  and the inequality should hold for even this minimum value of the polynomial 
$$
0\leq \langle u,u\rangle -\frac{\langle u,v\rangle^2}{\langle v,v\rangle}\, 
\Leftrightarrow 
\frac{|\langle u,v \rangle|}{\|u\|\, \|v\|} \leq 1.
$$
\end{proof}

\begin{theorem}[Triangle Inequality]\index{Triangle inequality}
For any $u,v\in \mathbb{R}^n$
\[ \|u+v\| \leq \|u\| + \|v\| .\]
\end{theorem}

\begin{proof}

\begin{eqnarray*}
\|u+v\|^2 & = & (u+v)\dotprod (u+v) \\
 	& = & u\dotprod u + 2 u\dotprod v + v\dotprod v \\
	& = & \|u\|^2 + \|v\|^2 + 2\,  \|u\|\,  \|v\| \cos \theta \\
	& = & \left(\|u\| + \|v\|\right)^2 + 2 \, \|u\| \, \|v\| (\cos \theta -1) \\
	& \leq & \left(\|u\| + \|v\|\right)^2	.
\end{eqnarray*}

\noindent
That is, the square of the left-hand side of the triangle inequality is $\leq$ the square of the right-hand side. Since both the things being squared are positive, the inequality holds without the square;
\[ \|u+v\| \leq  \|u\| + \|v\|   \]

\end{proof}

The triangle inequality is also ``self-evident'' when examining a sketch of $u$, $v$ and $u+v$.
\begin{center}
\includegraphics[scale=.25]{triangleagain.jpg}
\end{center}

\begin{example}
Let 
$$a=\colvec{1\\2\\3\\4} \mbox{ and }\ b = \colvec{4\\3\\2\\1}\, ,$$
so that
$$   a\dotprod a= b\dotprod b 
=1 +2^2+3^2+4^2=30 $$ $$\Rightarrow \|a\|=\sqrt{30} =\| b \| \mbox{ and } \ 
\big(\|a\|+\|b\|\big)^2=(2\sqrt{30})^2=120\, .$$
Since
$$a+b= \colvec{5\\5\\5\\5}\, , \quad $$
we have $$\|a+b\|^2=5^2+5^2+5^2+5^2=100<120=\big(\|a\|+\|b\|\big)^2$$
as predicted by the triangle inequality.

Notice also that $a\dotprod b=1.4+2.3+3.2+4.1=20< \sqrt{30}.\sqrt{30}=30=\|a\|\, \|b\|$ 
in accordance with the Cauchy--Schwarz inequality.
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework5/2/}{Reading homework: problem 5.2}\end{center}
\Reading{VectorsInSpace}{2}


\section{Vectors, Lists and Functions: $\R^S$}
If you were  going shopping you might make something like the following list.

\begin{center}
\includegraphics[scale =.3]{list.jpg}
\end{center}

\noindent
We could represent this information mathematically as a set, $$S=\{\rm apple, orange, onion, milk, carrot\}\, .$$ There is no information
of ordering here and no information about how many carrots you will buy. This set by itself is not a vector; how would we add such sets to 
one another?

If you were a more careful shopper your list might look like the following.

\begin{center}
\includegraphics[scale =.5]{shoplist}
\end{center}

\noindent
What you have really done here is assign a number to each element of the set $S$. In other words, the second list is a function
$$
f:S\longrightarrow {\mathbb R}\, .
$$
Given two lists like the second one above, we could easily add them -- if you plan to buy 5 apples and I am buying 3 apples, together we
will buy 8 apples! In fact, the second list is really a 5-vector in disguise. 

In general
it is helpful to think of an $n$-vector as a function whose domain is the set 
$\{1,\dots,n\}$.
%, as \hyperlink{vecs as fun}{discussed in chapter 1}. 
This is equivalent to thinking of an $n$-vector as an ordered list of $n$ numbers.
These two ideas give us  two equivalent notions for the set of all $n$-vectors:
$$
{\mathbb{R}}^n :=\left\{ \ccolvec{a^1 \\ \vdots \\ a^n } \middle\vert \,  a^1,\dots ,a^n \in \mathbb{R} \right\}
=\{ a:\{1,\dots,n\}\to \mathbb{R}\} =: \mathbb{R}^{ \{1,\cdots,n\} }
$$
The notation $\mathbb{R}^{ \{1,\cdots,n\} }$ is used to denote the set of all functions from $   \{1,\dots,n\} $ to $\mathbb{R}$. 

Similarly, for any set $S$ the notation $\mathbb{R}^S$ denotes the set of functions from $S$ to~$\mathbb{R}$:
$$
{\mathbb R}^S:=\{ f:S\to {\mathbb R}\}\, .
$$
When $S$ is an ordered set like $\{1,\dots,n\}$, it is natural to write the components in order. When the elements of $S$ do not have a natural ordering, doing so might cause confusion. 



\begin{example}
{Consider the set } $S=\{*, \star, \# \}$ from \hyperlink{Consider the set}{chapter 1 review problem}~\ref{ch1rev}. A particular element of $\mathbb{R}^S$ is the function $a$ explicitly defined by 
$$ a^{\star}=3, a^{\#}=5, a^{*}=-2.$$
It is not natural to write 
$$
a=\colvec{3 \\ 5 \\ -2 } ~{\rm or} ~a=\colvec{-2\\ 3 \\ 5 } 
$$
because the elements of $S$ do not have an ordering, since as sets $\{*, \star, \# \}=\{\star, \#,*\}$.
%, like the elements of $\{ 1,2,3\}$. 
\end{example}


In this important way, $\mathbb{R}^S$ seems different from $\mathbb{R}^3$. 
What is more evident are the similarities; since we can add two functions, we can add two elements of~$\mathbb{R}^S$:

\begin{example} Addition in $\mathbb{R}^{\{*, \star, \# \}}$\\
%\begin{eqnarray*}
If $a,b \in \mathbb{R}^{\{*, \star, \# \}}$ 
such that 
$$a^{\star}=3, a^{\#}=5, a^{*}=-2$$ 
and 
$$b^{\star}=-2, b^{\#}=4, b^{*}=13$$
then $a+b \in \mathbb{R}^S$ is the function such that
$$(a+b)^{\star}=3-2=1, (a+b)^{\#}=5+4=9, (a+b)^{*}=-2+13=11\, .$$
\end{example}

Also, since we can multiply functions by  numbers, there is a notion of scalar multiplication on $\mathbb{R}^S$.

\begin{example} Scalar Multiplication in $\mathbb{R}^S$\\
If $a \in \mathbb{R}^{\{*, \star, \# \}}$ such that
$$a^{\star}=3, a^{\#}=5, a^{*}=-2$$
then $3 a \in \mathbb{R}^{\{*, \star, \# \}}$ is the function such that 
$$(3a)^{\star}=3\cdot3=9, (3a)^{\#}=3\cdot5=15, (3a)^{*}=3(-2)=-6\, .$$

\end{example}

We visualize $\mathbb{R}^2$ and  $\mathbb{R}^3$ in terms of axes. We have a more abstract picture of  $\mathbb{R}^4$,  $\mathbb{R}^5$ and  $\mathbb{R}^n$ for larger $n$ while  $\mathbb{R}^S$ seems even more abstract. However, when thought of as a simple ``shopping list'',
you can see that vectors in $\mathbb{R}^S$ in fact, can describe everyday objects.
In chapter~\ref{vectorSpaces} we introduce the  general definition of a vector space that unifies all these different notions of a vector.

%\section*{References}
%
%Hefferon: Chapter One.II
%\\
%Beezer: Chapter V, Section VO, Subsection VEASM
%\\
%Beezer: Chapter V, Section O, Subsections IP-N
%\\
%Relevant Wikipedia Articles:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Dot_product}{Dot Product}
%\item \href{http://en.wikipedia.org/wiki/Inner_product_space}{Inner Product Space}
%\item \href{http://en.wikipedia.org/wiki/Minkowski_metric}{Minkowski Metric}
%\end{itemize} 


\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading problems &
\hwrref{VectorsInSpace}{1}, \hwrref{VectorsInSpace}{2}\\
Vector operations &  \hwref{VectorsInSpace}{3}\\
Vectors and lines &  \hwref{VectorsInSpace}{4}\\
Vectors and planes &\hwref{VectorsInSpace}{5}\\
Lines, planes and vectors & \hwref{VectorsInSpace}{6},\hwref{VectorsInSpace}{7}\\
Equation of a plane &\hwref{VectorsInSpace}{8},\hwref{VectorsInSpace}{9}\\
Angle between a line and plane &\hwref{VectorsInSpace}{10}
\\
\hline
\end{tabular}


\input{\vectorsInSpacePath/problems}

\newpage
",lesson
7,Vector Spaces,"\chapter{\vectorSpacesTitle}
\label{vectorSpaces}

%Thus far we have thought of vectors as lists of numbers% in $\mathbb{R}^n$.  As it turns out, The notion of a vector applies to a much more general class of structures than this.
As suggested at the end of chapter~\ref{vectorsinspace}, the vector spaces $\mathbb{R}^n$ are not the only vector spaces. 
We now give a general definition that  includes $\Re^n$ for all values of $n$, and $\mathbb{R}^{S}$ for all sets $S$, and more.
%The main idea is to  define vectors based on their most important properties.  
%Vectors in $\mathbb{R}^n$ will fit our definition, but so will many other extremely useful notions of vectors. 
This mathematical structure
is applicable to a wide range of real-world problems and allows for tremendous economy of thought; the idea of a basis for a vector space will drive home the main idea of vector spaces; they are sets with very simple structure. 


The two key properties of vectors are that they can be added together and multiplied by scalars. 
Thus, before giving a rigorous definition of vector spaces, we restate the main idea.\\

\begin{center} \shabox{ 
 A vector space is a set that is closed under addition and scalar~multiplication.}
\end{center}



\begin{definition} A {\bf vector space}\index{Vector space}\label{vectorspace} $(V,+,.\, ,{\mathbb R})$ is a set $V$ with two operations $+$ and~$\cdot$ satisfying the following properties for all $u, v \in V$ and $c, d \in \mathbb{R}$:

\begin{itemize}

\item[(+i)] (Additive Closure)\index{Closure!additive} $u+v \in V$.  {\it Adding two vectors gives a vector.}

\item[(+ii)] (Additive Commutativity) $u+v=v+u$.  {\it Order of addition does not matter.}

\item[(+iii)] (Additive Associativity) $(u+v)+w = u+(v+w)$.  {\it Order of adding many vectors does not matter.}

\item[(+iv)] (Zero) There is a special vector $0_V \in V$ such that $u+0_V = u$ for all $u$ in $V$.

\item[(+v)] (Additive Inverse) For every $u \in V$ there exists $w \in V$ such that $u+w=0_V$.

\item[($\cdot$ i)] (Multiplicative Closure)\index{Closure!multiplicative} $c\cdot v \in V$.  {\it Scalar times a vector is a vector.}

\item[($\cdot$ ii)] (Distributivity) $(c+d) \cdot v= c\cdot v + d\cdot v$.  {\it Scalar multiplication distributes over addition of scalars.}

\item[($\cdot$ iii)] (Distributivity) $c\cdot (u+v)= c\cdot u + c\cdot v$.  {\it Scalar multiplication distributes over addition of vectors.}

\item[($\cdot$ iv)] (Associativity) $ (cd)\cdot v = c \cdot (d \cdot v)$. 

\item[($\cdot$ v)] (Unity) $1\cdot v = v$ for all $v \in V$.
\end{itemize}
\end{definition}

\Videoscriptlink{vector_spaces_definition_example.mp4}{Examples of each rule}{scripts_vector_spaces_definition_example}

\begin{remark}
Rather than writing $(V,+,.\, ,{\mathbb R})$, we will often say ``let $V$ be a vector space over ${\mathbb R}$''. If it is obvious that the numbers used are real numbers, then ``let $V$ be a vector space'' suffices.
Also, don't confuse the scalar product $\cdot$ with the dot product~$\dotprod$.  The scalar product is a function that takes as its two inputs one number and one vector  and returns a vector as its output.  This can be written 
$$\cdot \colon \mathbb{R}\times V \rightarrow V\, .$$ 
Similarly
$$
+:V\times V \rightarrow V\, .
$$
On the other hand, the dot product takes two vectors and returns a number.  Succinctly: $\dotprod \colon V\times V \rightarrow \Re$.
Once the properties of a vector space have been verified, we'll just write scalar multiplication with juxtaposition $cv=c\cdot v$, though, to keep our notation efficient. 
\end{remark}


\section{Examples of Vector Spaces}
%\begin{remark}
%It isn't hard to devise strange rules for addition or scalar multiplication that break some or all of the rules listed above.
%\end{remark}

One can  find many interesting vector spaces, such as the following:
\Videoscriptlink{vector_spaces_example.mp4}{Example of a vector space}{scripts_vector_spaces_example}

\begin{example}
\[ \mathbb{R}^\mathbb{N} = \{f \mid f \colon \mathbb{N} \rightarrow \Re \} \]
Here the vector space is the set of functions that take in a natural number~$n$ and return a real number.  The addition is just addition of functions: $(f_1 + f_2)(n) = f_1(n) + f_2(n)$.  Scalar multiplication is just as simple: $c \cdot f(n) = cf(n)$.

%We can think of these functions as infinite column vectors: $f(0)$ is the first entry, $f(1)$ is the second entry, and so on.  Then for example the function $f(n) = n^3$ would look like this:

%\[
%f(n) = \colvec{0\\1\\8\\27\\ \vdots \\ n^3 \\ \vdots}
%\]
%Alternatively, $V$ is the space of sequences: $f = \{f_1, f_2, \ldots, f_n, \ldots \}$.

We can think of these functions as infinitely large ordered lists of numbers: \(f(1)=1^3=1\) is the first component, \(f(2)=2^3=8\) is the second, and so on. Then for example the function \(f(n)=n^3\) would look like this:
\[
f=\ccolvec{1\\ 8\\ 27\\ \vdots\\ n^3\\ \vdots }.
\]
Thinking this way, \(\Re^\mathbb{N}\) is the space of all infinite sequences. 
Because we can not write a list infinitely long (without infinite time and ink), one can not define an element of this space explicitly; definitions that are implicit, as above, or algebraic as in $f(n)=n^3$ (for all $n \in \mathbb{N}$) suffice.

Let's check some axioms.

\begin{itemize}
\item[(+i)] (Additive Closure) $(f_1 + f_2)(n)=f_1(n) +f_2(n)$ is indeed a function 
$\mathbb{N} \rightarrow \Re$, since the sum of two real numbers is a real number.

\item[(+iv)] (Zero) We need to propose a zero vector.  The constant zero function $g(n) = 0$ works because then $f(n) + g(n) = f(n) + 0 = f(n)$.
\end{itemize}
The other axioms should also be checked.  This can be done using  properties of the real numbers.
\Reading{VectorSpaces}{1}
\end{example}

\begin{example} The space of functions of one real variable.\\
\[ \mathbb{R}^\mathbb{R} = \{f \mid f \colon \Re \to \Re \} \]
The addition is point-wise $$(f+g)(x)=f(x)+g(x)\, ,$$ as is scalar multiplication $$c\cdot f(x)=cf(x)\, .$$  
To check that $\Re^\Re$ is a vector space use the properties of addition of functions and scalar multiplication of functions as in the previous example. 


We can not write out an explicit definition for one of these functions either, there are not only infinitely many components, but even infinitely many components between any two components!  
You are familiar with algebraic definitions like $f(x)=e^{x^2-x+5}$. However, most vectors in this vector space can not be defined algebraically. For example, the nowhere continuous function 
\begin{displaymath}
   f(x) = \left\{
     \begin{array}{lr}
       1\, , & x \in \mathbb{Q}\\[2mm]
       0\, , &  x \notin \mathbb{Q}
     \end{array}
   \right. .
\end{displaymath} 
\end{example}

\begin{example} $\Re^{ \{*, \star, \# \}} = \{ f : \{*, \star, \# \} \to \Re \}$. Again, the properties of addition and scalar multiplication of functions show that this is a vector space.
\end{example}

You can probably figure out how to show that $\Re^S$ is vector space for any set $S$.  
This might lead you to guess that all vector spaces are of the form $\Re^S$ for some set $S$. The following is a counterexample. 

\begin{example}
Another very important example of a vector space is the space of all differentiable functions: 
\[
\left\{ f \colon \Re\rightarrow \Re \, \Big|\, \frac{d}{dx}f \text{ exists} \right\}.
\]

From calculus, we know that the sum of any two differentiable functions is differentiable, since the derivative distributes over addition.  A scalar multiple of a function is also differentiable, since the derivative commutes with scalar multiplication ($\frac{d}{d x}(cf)=c\frac{d}{dx}f$).  The zero function is just the function such that $0(x)=0$ for every $x$.  The rest of the vector space properties are inherited from addition and scalar multiplication in $\Re$.
\end{example}

Similarly, the set of functions with at least $k$ derivatives is always a vector space, as is the space of functions with infinitely many derivatives. 
None of these examples can be written as $\Re^S$ for some set $S$.
Despite our emphasis on such examples, it is also not true that all vector spaces consist of functions. Examples are somewhat esoteric, so we omit them.

Another important class of examples is vector spaces that live inside $\Re^n$ but are not themselves $\Re^n$. 

\begin{example} (Solution set to a homogeneous linear equation.)\\
Let 
\[ M = \begin{pmatrix}
      1 & 1 &1 \\
      2&2&2 \\
      3&3&3
    \end{pmatrix}.\]
    The solution set to the homogeneous equation $Mx=0$ is 
$$\left\{ c_1\colvec{-1\\1\\0} + c_2 \colvec{-1\\0\\1} \middle\vert c_1,c_2\in \Re \right\}.$$
    This set is not equal to $\Re^3$ since it does not contain, for example,  $\colvec{1\\0\\0}$. 
The sum of any two solutions is a solution, for example 
$$
    \left[ 2\colvec{-1\\1\\0} + 3 \colvec{-1\\0\\1} \right] 
+ \left [ 7\colvec{-1\\1\\0} + 5 \colvec{-1\\0\\1} \right]
=
 9\colvec{-1\\1\\0} + 8 \colvec{-1\\0\\1} 
$$
and any scalar multiple of a solution is a solution
$$
4\left[ 5\colvec{-1\\1\\0} - 3 \colvec{-1\\0\\1} \right]
=      20\colvec{-1\\1\\0} - 12 \colvec{-1\\0\\1} . 
$$
This example is called a {\it subspace} because it gives a vector space inside another vector space. See chapter~\ref{subspacesspanning}
for details. Indeed, because it is determined by the linear map given by the matrix $M$, it is called $\ker M$, or in words, the {\it kernel} of $M$,
for this see chapter~\ref{kernelrank}. 
\end{example}

Similarly, the solution set to any homogeneous linear equation is a vector space:
Additive and multiplicative closure follow from the following statement, made using linearity of matrix multiplication:
$${\rm If}~Mx_1=0 ~\mbox{and}~Mx_2=0~ \mbox{then} ~M(c_1x_1 + c_2x_2)=c_1Mx_1+c_2Mx_2=0+0=0.$$ 
A powerful result, called the subspace theorem (see chapter~\ref{subspacesspanning}) guarantees, based on the closure properties alone, that homogeneous
solution sets are vector spaces.

More generally, if $V$ is any vector space, then any hyperplane through the origin of $V$ is a vector space. 

\begin{example} Consider the functions $f(x)=e^x$ and $g(x)=e^{2x}$ in $\Re^\Re$. By taking combinations of these two vectors we can form the plane $\{ c_1 f+ c_2 g | c_1,c_2 \in \Re\}$
inside of $\Re^\Re$. This is a vector space; 
some examples of vectors in it are 
$4e^x-31e^{2x},~\pi e^{2x}-4e^x$ and $\frac12e^{2x}$. 
\end{example}

A hyperplane which does not contain the origin cannot be a vector space because it fails condition (+iv).

It is also possible to build new vector spaces from old ones using the product of sets. Remember that if $V$ and $W$ are sets, then
their product is the new set
$$
V\times W = \{(v,w)|v\in V, w\in W\}\, ,
$$
or in words, all ordered pairs of elements from $V$ and $W$.
In fact $V\times W$ is a vector space if $V$ and $W$ are. We have actually been using this fact already:

\begin{example}
The real numbers~${\mathbb R}$ form a vector space (over ${\mathbb R}$). The new vector space
$${\mathbb R}\times {\mathbb R}=\{(x,y)|x\in{\mathbb R}, y\in {\mathbb R}\}$$
has addition and scalar multiplication defined by
$$
(x,y)+(x',y')=(x+x',y+y')\, \mbox{ and } c.(x,y)=(cx,cy)\, .
$$
Of course, this is just the vector space ${\mathbb R}^2={\mathbb R}^{\{1,2\}}$. 
\end{example}

\subsection{Non-Examples} 
The solution set to a linear non-homogeneous equation is not a vector space because it does not contain the zero vector and therefore fails (iv).

\begin{example} 
The solution set to 
\[  \begin{pmatrix}
      1 & 1 \\
      0 & 0 
    \end{pmatrix} \colvec{x\\y} = \colvec{1\\0} \]
is  $\left\{ \colvec{1\\0} + c \colvec{-1\\1} \Big|\, c \in \Re \right\}$.
The vector $\colvec{0\\0}$ is not in this set.
\end{example}
Do notice that if just one of the vector space rules is broken, the example is not a vector space.

Most sets of $n$-vectors are not vector spaces. 
\begin{example} 
$P:=\left \{ \colvec{a\\b} \Big| \,a,b \geq 0 \right\}$ is not a vector space because the set fails ($\cdot$i) since 
$\colvec{1\\1}\in P$ but $-2\colvec{1\\1} =\colvec{-2\\-2} \notin P$.
\end{example}


Sets of functions other than those of the form $\Re^S$ should be carefully checked for compliance with the definition of a vector space.


\begin{example}
The set of all functions which are nowhere zero 
\[\left\{ f \colon \Re\rightarrow \Re \mid f(x)\neq 0 {\rm ~for~any}~x\in\Re \right\}\, ,\]
does not form a vector space because it does not satisfy (+i). The functions $f(x)=x^2+1$ and $g(x)= -5$ are in the set, but their sum $(f+g)(x)=x^2-4=(x+2)(x-2)$ is not since $(f+g)(2)=0$.
\end{example}


\section{Other Fields} \label{otherfields}
Above, we defined vector spaces over the real numbers.  One can actually define vector spaces over any \emph{field}.
This is referred to as choosing a different {\it base field}\index{Base field}.
  A field is a collection of ``numbers'' satisfying  properties which are listed in appendix~\ref{fields}.
An example of a field is the complex numbers, 
\[
\mathbb{C}= \left\{x+iy \mid i^2=-1, x,y\in \Re \right\}.
\]

\begin{example}
In quantum physics, vector spaces over $\mathbb{C}$ describe all possible states a physical system %of particles 
can have.  
For example,
\[
V= \left\{ \colvec{\lambda \\ \mu} \mid \lambda, \mu \in \mathbb{C}\right\}
\]
is the set of possible states for an electron's spin. The vectors \scalebox{.9}{$\colvec{1 \\ 0}$}  and~\scalebox{.9}{$\colvec{0 \\ 1}$} describe, respectively,  an  electron with spin ``up'' and ``down'' along a given direction.  
Other vectors, like \scalebox{.9}{$\colvec{i \\ -i}$} are permissible, since the base field is the complex numbers. Such states represent a mixture of spin up and spin down for the given direction (a rather counterintuitive yet experimentally verifiable concept), but a given spin in some other direction.
\end{example}

Complex numbers are very useful because of a special property that they enjoy: every polynomial over the complex numbers factors into a product of linear polynomials.  For example, the polynomial $$x^2+1$$ doesn't factor over  real numbers, but over complex numbers it factors into $$(x+i)(x-i)\, .$$ In other words, there are {\it two} solutions to $$x^2=-1,$$
$x=i$ and $x=-i$.
 This property  has far-reaching consequences: often in mathematics problems that are very difficult using only real numbers become relatively simple when working over the complex numbers.  This phenomenon occurs when diagonalizing matrices, see chapter~\ref{sec:diagonalization}.

The rational numbers $\mathbb{Q}$ are also a field. This  field is important in computer algebra: a real number given by an infinite string of numbers after the decimal point can't be stored by a computer.  So instead rational approximations are used.  Since the rationals are a field, the mathematics of vector spaces still apply to this special case.

Another very useful field is bits 
$$
B_2={\mathbb Z}_2=\{0,1\}\, ,
$$
with the addition and multiplication rules
$$
\begin{array}{c|cc}
+&0&1\\\hline
0&0&1\\
1&1&0
\end{array}\qquad
\begin{array}{c|cc}
\times&0&1\\\hline
0&0&0\\
1&0&1
\end{array}
$$
These rules can be summarized by the relation $2=0$. For bits, it follows that $-1=1$!
%In this class, we will work mainly over the real numbers and the complex numbers, and occasionally work over $\mathbb{Z}_2 = \{0, 1\}$ where $1 + 1 = 0$. 
%For more on fields in general, see \hyperref[fields]{Appendix~\ref*{fields}}; however 

The theory of fields is typically covered in a class on abstract algebra or Galois theory\index{Galois}.


%\section*{References}
%
%Hefferon, Chapter One, Section I.1
%\\
%Beezer, Chapter VS, Section VS
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Vector_space}{Vector Space}
%\item \href{http://en.wikipedia.org/wiki/Field_(mathematics)}{Field}
%\item \href{http://en.wikipedia.org/wiki/Spin_1/2}{Spin $\frac{1}{2}$}
%\item \href{http://en.wikipedia.org/wiki/Spin_1/2}{Galois Theory}
%
%\end{itemize}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading problems &
\hwrref{VectorSpaces}{1}\\
Addition and inverse& \hwref{VectorSpaces}{2}\\
\hline
\end{tabular}

\input{\vectorSpacesPath/problems}


\newpage
","\chapter{\vectorSpacesTitle}
\label{vectorSpaces}

%Thus far we have thought of vectors as lists of numbers% in $\mathbb{R}^n$.  As it turns out, The notion of a vector applies to a much more general class of structures than this.
As suggested at the end of chapter~\ref{vectorsinspace}, the vector spaces $\mathbb{R}^n$ are not the only vector spaces. 
We now give a general definition that  includes $\Re^n$ for all values of $n$, and $\mathbb{R}^{S}$ for all sets $S$, and more.
%The main idea is to  define vectors based on their most important properties.  
%Vectors in $\mathbb{R}^n$ will fit our definition, but so will many other extremely useful notions of vectors. 
This mathematical structure
is applicable to a wide range of real-world problems and allows for tremendous economy of thought; the idea of a basis for a vector space will drive home the main idea of vector spaces; they are sets with very simple structure. 


The two key properties of vectors are that they can be added together and multiplied by scalars. 
Thus, before giving a rigorous definition of vector spaces, we restate the main idea.\\

\begin{center} \shabox{ 
 A vector space is a set that is closed under addition and scalar~multiplication.}
\end{center}



\begin{definition} A {\bf vector space}\index{Vector space}\label{vectorspace} $(V,+,.\, ,{\mathbb R})$ is a set $V$ with two operations $+$ and~$\cdot$ satisfying the following properties for all $u, v \in V$ and $c, d \in \mathbb{R}$:

\begin{itemize}

\item[(+i)] (Additive Closure)\index{Closure!additive} $u+v \in V$.  {\it Adding two vectors gives a vector.}

\item[(+ii)] (Additive Commutativity) $u+v=v+u$.  {\it Order of addition does not matter.}

\item[(+iii)] (Additive Associativity) $(u+v)+w = u+(v+w)$.  {\it Order of adding many vectors does not matter.}

\item[(+iv)] (Zero) There is a special vector $0_V \in V$ such that $u+0_V = u$ for all $u$ in $V$.

\item[(+v)] (Additive Inverse) For every $u \in V$ there exists $w \in V$ such that $u+w=0_V$.

\item[($\cdot$ i)] (Multiplicative Closure)\index{Closure!multiplicative} $c\cdot v \in V$.  {\it Scalar times a vector is a vector.}

\item[($\cdot$ ii)] (Distributivity) $(c+d) \cdot v= c\cdot v + d\cdot v$.  {\it Scalar multiplication distributes over addition of scalars.}

\item[($\cdot$ iii)] (Distributivity) $c\cdot (u+v)= c\cdot u + c\cdot v$.  {\it Scalar multiplication distributes over addition of vectors.}

\item[($\cdot$ iv)] (Associativity) $ (cd)\cdot v = c \cdot (d \cdot v)$. 

\item[($\cdot$ v)] (Unity) $1\cdot v = v$ for all $v \in V$.
\end{itemize}
\end{definition}

\Videoscriptlink{vector_spaces_definition_example.mp4}{Examples of each rule}{scripts_vector_spaces_definition_example}

\begin{remark}
Rather than writing $(V,+,.\, ,{\mathbb R})$, we will often say ``let $V$ be a vector space over ${\mathbb R}$''. If it is obvious that the numbers used are real numbers, then ``let $V$ be a vector space'' suffices.
Also, don't confuse the scalar product $\cdot$ with the dot product~$\dotprod$.  The scalar product is a function that takes as its two inputs one number and one vector  and returns a vector as its output.  This can be written 
$$\cdot \colon \mathbb{R}\times V \rightarrow V\, .$$ 
Similarly
$$
+:V\times V \rightarrow V\, .
$$
On the other hand, the dot product takes two vectors and returns a number.  Succinctly: $\dotprod \colon V\times V \rightarrow \Re$.
Once the properties of a vector space have been verified, we'll just write scalar multiplication with juxtaposition $cv=c\cdot v$, though, to keep our notation efficient. 
\end{remark}


\section{Examples of Vector Spaces}
%\begin{remark}
%It isn't hard to devise strange rules for addition or scalar multiplication that break some or all of the rules listed above.
%\end{remark}

One can  find many interesting vector spaces, such as the following:
\Videoscriptlink{vector_spaces_example.mp4}{Example of a vector space}{scripts_vector_spaces_example}

\begin{example}
\[ \mathbb{R}^\mathbb{N} = \{f \mid f \colon \mathbb{N} \rightarrow \Re \} \]
Here the vector space is the set of functions that take in a natural number~$n$ and return a real number.  The addition is just addition of functions: $(f_1 + f_2)(n) = f_1(n) + f_2(n)$.  Scalar multiplication is just as simple: $c \cdot f(n) = cf(n)$.

%We can think of these functions as infinite column vectors: $f(0)$ is the first entry, $f(1)$ is the second entry, and so on.  Then for example the function $f(n) = n^3$ would look like this:

%\[
%f(n) = \colvec{0\\1\\8\\27\\ \vdots \\ n^3 \\ \vdots}
%\]
%Alternatively, $V$ is the space of sequences: $f = \{f_1, f_2, \ldots, f_n, \ldots \}$.

We can think of these functions as infinitely large ordered lists of numbers: \(f(1)=1^3=1\) is the first component, \(f(2)=2^3=8\) is the second, and so on. Then for example the function \(f(n)=n^3\) would look like this:
\[
f=\ccolvec{1\\ 8\\ 27\\ \vdots\\ n^3\\ \vdots }.
\]
Thinking this way, \(\Re^\mathbb{N}\) is the space of all infinite sequences. 
Because we can not write a list infinitely long (without infinite time and ink), one can not define an element of this space explicitly; definitions that are implicit, as above, or algebraic as in $f(n)=n^3$ (for all $n \in \mathbb{N}$) suffice.

Let's check some axioms.

\begin{itemize}
\item[(+i)] (Additive Closure) $(f_1 + f_2)(n)=f_1(n) +f_2(n)$ is indeed a function 
$\mathbb{N} \rightarrow \Re$, since the sum of two real numbers is a real number.

\item[(+iv)] (Zero) We need to propose a zero vector.  The constant zero function $g(n) = 0$ works because then $f(n) + g(n) = f(n) + 0 = f(n)$.
\end{itemize}
The other axioms should also be checked.  This can be done using  properties of the real numbers.
\Reading{VectorSpaces}{1}
\end{example}

\begin{example} The space of functions of one real variable.\\
\[ \mathbb{R}^\mathbb{R} = \{f \mid f \colon \Re \to \Re \} \]
The addition is point-wise $$(f+g)(x)=f(x)+g(x)\, ,$$ as is scalar multiplication $$c\cdot f(x)=cf(x)\, .$$  
To check that $\Re^\Re$ is a vector space use the properties of addition of functions and scalar multiplication of functions as in the previous example. 


We can not write out an explicit definition for one of these functions either, there are not only infinitely many components, but even infinitely many components between any two components!  
You are familiar with algebraic definitions like $f(x)=e^{x^2-x+5}$. However, most vectors in this vector space can not be defined algebraically. For example, the nowhere continuous function 
\begin{displaymath}
   f(x) = \left\{
     \begin{array}{lr}
       1\, , & x \in \mathbb{Q}\\[2mm]
       0\, , &  x \notin \mathbb{Q}
     \end{array}
   \right. .
\end{displaymath} 
\end{example}

\begin{example} $\Re^{ \{*, \star, \# \}} = \{ f : \{*, \star, \# \} \to \Re \}$. Again, the properties of addition and scalar multiplication of functions show that this is a vector space.
\end{example}

You can probably figure out how to show that $\Re^S$ is vector space for any set $S$.  
This might lead you to guess that all vector spaces are of the form $\Re^S$ for some set $S$. The following is a counterexample. 

\begin{example}
Another very important example of a vector space is the space of all differentiable functions: 
\[
\left\{ f \colon \Re\rightarrow \Re \, \Big|\, \frac{d}{dx}f \text{ exists} \right\}.
\]

From calculus, we know that the sum of any two differentiable functions is differentiable, since the derivative distributes over addition.  A scalar multiple of a function is also differentiable, since the derivative commutes with scalar multiplication ($\frac{d}{d x}(cf)=c\frac{d}{dx}f$).  The zero function is just the function such that $0(x)=0$ for every $x$.  The rest of the vector space properties are inherited from addition and scalar multiplication in $\Re$.
\end{example}

Similarly, the set of functions with at least $k$ derivatives is always a vector space, as is the space of functions with infinitely many derivatives. 
None of these examples can be written as $\Re^S$ for some set $S$.
Despite our emphasis on such examples, it is also not true that all vector spaces consist of functions. Examples are somewhat esoteric, so we omit them.

Another important class of examples is vector spaces that live inside $\Re^n$ but are not themselves $\Re^n$. 

\begin{example} (Solution set to a homogeneous linear equation.)\\
Let 
\[ M = \begin{pmatrix}
      1 & 1 &1 \\
      2&2&2 \\
      3&3&3
    \end{pmatrix}.\]
    The solution set to the homogeneous equation $Mx=0$ is 
$$\left\{ c_1\colvec{-1\\1\\0} + c_2 \colvec{-1\\0\\1} \middle\vert c_1,c_2\in \Re \right\}.$$
    This set is not equal to $\Re^3$ since it does not contain, for example,  $\colvec{1\\0\\0}$. 
The sum of any two solutions is a solution, for example 
$$
    \left[ 2\colvec{-1\\1\\0} + 3 \colvec{-1\\0\\1} \right] 
+ \left [ 7\colvec{-1\\1\\0} + 5 \colvec{-1\\0\\1} \right]
=
 9\colvec{-1\\1\\0} + 8 \colvec{-1\\0\\1} 
$$
and any scalar multiple of a solution is a solution
$$
4\left[ 5\colvec{-1\\1\\0} - 3 \colvec{-1\\0\\1} \right]
=      20\colvec{-1\\1\\0} - 12 \colvec{-1\\0\\1} . 
$$
This example is called a {\it subspace} because it gives a vector space inside another vector space. See chapter~\ref{subspacesspanning}
for details. Indeed, because it is determined by the linear map given by the matrix $M$, it is called $\ker M$, or in words, the {\it kernel} of $M$,
for this see chapter~\ref{kernelrank}. 
\end{example}

Similarly, the solution set to any homogeneous linear equation is a vector space:
Additive and multiplicative closure follow from the following statement, made using linearity of matrix multiplication:
$${\rm If}~Mx_1=0 ~\mbox{and}~Mx_2=0~ \mbox{then} ~M(c_1x_1 + c_2x_2)=c_1Mx_1+c_2Mx_2=0+0=0.$$ 
A powerful result, called the subspace theorem (see chapter~\ref{subspacesspanning}) guarantees, based on the closure properties alone, that homogeneous
solution sets are vector spaces.

More generally, if $V$ is any vector space, then any hyperplane through the origin of $V$ is a vector space. 

\begin{example} Consider the functions $f(x)=e^x$ and $g(x)=e^{2x}$ in $\Re^\Re$. By taking combinations of these two vectors we can form the plane $\{ c_1 f+ c_2 g | c_1,c_2 \in \Re\}$
inside of $\Re^\Re$. This is a vector space; 
some examples of vectors in it are 
$4e^x-31e^{2x},~\pi e^{2x}-4e^x$ and $\frac12e^{2x}$. 
\end{example}

A hyperplane which does not contain the origin cannot be a vector space because it fails condition (+iv).

It is also possible to build new vector spaces from old ones using the product of sets. Remember that if $V$ and $W$ are sets, then
their product is the new set
$$
V\times W = \{(v,w)|v\in V, w\in W\}\, ,
$$
or in words, all ordered pairs of elements from $V$ and $W$.
In fact $V\times W$ is a vector space if $V$ and $W$ are. We have actually been using this fact already:

\begin{example}
The real numbers~${\mathbb R}$ form a vector space (over ${\mathbb R}$). The new vector space
$${\mathbb R}\times {\mathbb R}=\{(x,y)|x\in{\mathbb R}, y\in {\mathbb R}\}$$
has addition and scalar multiplication defined by
$$
(x,y)+(x',y')=(x+x',y+y')\, \mbox{ and } c.(x,y)=(cx,cy)\, .
$$
Of course, this is just the vector space ${\mathbb R}^2={\mathbb R}^{\{1,2\}}$. 
\end{example}

\subsection{Non-Examples} 
The solution set to a linear non-homogeneous equation is not a vector space because it does not contain the zero vector and therefore fails (iv).

\begin{example} 
The solution set to 
\[  \begin{pmatrix}
      1 & 1 \\
      0 & 0 
    \end{pmatrix} \colvec{x\\y} = \colvec{1\\0} \]
is  $\left\{ \colvec{1\\0} + c \colvec{-1\\1} \Big|\, c \in \Re \right\}$.
The vector $\colvec{0\\0}$ is not in this set.
\end{example}
Do notice that if just one of the vector space rules is broken, the example is not a vector space.

Most sets of $n$-vectors are not vector spaces. 
\begin{example} 
$P:=\left \{ \colvec{a\\b} \Big| \,a,b \geq 0 \right\}$ is not a vector space because the set fails ($\cdot$i) since 
$\colvec{1\\1}\in P$ but $-2\colvec{1\\1} =\colvec{-2\\-2} \notin P$.
\end{example}


Sets of functions other than those of the form $\Re^S$ should be carefully checked for compliance with the definition of a vector space.


\begin{example}
The set of all functions which are nowhere zero 
\[\left\{ f \colon \Re\rightarrow \Re \mid f(x)\neq 0 {\rm ~for~any}~x\in\Re \right\}\, ,\]
does not form a vector space because it does not satisfy (+i). The functions $f(x)=x^2+1$ and $g(x)= -5$ are in the set, but their sum $(f+g)(x)=x^2-4=(x+2)(x-2)$ is not since $(f+g)(2)=0$.
\end{example}


\section{Other Fields} \label{otherfields}
Above, we defined vector spaces over the real numbers.  One can actually define vector spaces over any \emph{field}.
This is referred to as choosing a different {\it base field}\index{Base field}.
  A field is a collection of ``numbers'' satisfying  properties which are listed in appendix~\ref{fields}.
An example of a field is the complex numbers, 
\[
\mathbb{C}= \left\{x+iy \mid i^2=-1, x,y\in \Re \right\}.
\]

\begin{example}
In quantum physics, vector spaces over $\mathbb{C}$ describe all possible states a physical system %of particles 
can have.  
For example,
\[
V= \left\{ \colvec{\lambda \\ \mu} \mid \lambda, \mu \in \mathbb{C}\right\}
\]
is the set of possible states for an electron's spin. The vectors \scalebox{.9}{$\colvec{1 \\ 0}$}  and~\scalebox{.9}{$\colvec{0 \\ 1}$} describe, respectively,  an  electron with spin ``up'' and ``down'' along a given direction.  
Other vectors, like \scalebox{.9}{$\colvec{i \\ -i}$} are permissible, since the base field is the complex numbers. Such states represent a mixture of spin up and spin down for the given direction (a rather counterintuitive yet experimentally verifiable concept), but a given spin in some other direction.
\end{example}

Complex numbers are very useful because of a special property that they enjoy: every polynomial over the complex numbers factors into a product of linear polynomials.  For example, the polynomial $$x^2+1$$ doesn't factor over  real numbers, but over complex numbers it factors into $$(x+i)(x-i)\, .$$ In other words, there are {\it two} solutions to $$x^2=-1,$$
$x=i$ and $x=-i$.
 This property  has far-reaching consequences: often in mathematics problems that are very difficult using only real numbers become relatively simple when working over the complex numbers.  This phenomenon occurs when diagonalizing matrices, see chapter~\ref{sec:diagonalization}.

The rational numbers $\mathbb{Q}$ are also a field. This  field is important in computer algebra: a real number given by an infinite string of numbers after the decimal point can't be stored by a computer.  So instead rational approximations are used.  Since the rationals are a field, the mathematics of vector spaces still apply to this special case.

Another very useful field is bits 
$$
B_2={\mathbb Z}_2=\{0,1\}\, ,
$$
with the addition and multiplication rules
$$
\begin{array}{c|cc}
+&0&1\\\hline
0&0&1\\
1&1&0
\end{array}\qquad
\begin{array}{c|cc}
\times&0&1\\\hline
0&0&0\\
1&0&1
\end{array}
$$
These rules can be summarized by the relation $2=0$. For bits, it follows that $-1=1$!
%In this class, we will work mainly over the real numbers and the complex numbers, and occasionally work over $\mathbb{Z}_2 = \{0, 1\}$ where $1 + 1 = 0$. 
%For more on fields in general, see \hyperref[fields]{Appendix~\ref*{fields}}; however 

The theory of fields is typically covered in a class on abstract algebra or Galois theory\index{Galois}.


%\section*{References}
%
%Hefferon, Chapter One, Section I.1
%\\
%Beezer, Chapter VS, Section VS
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Vector_space}{Vector Space}
%\item \href{http://en.wikipedia.org/wiki/Field_(mathematics)}{Field}
%\item \href{http://en.wikipedia.org/wiki/Spin_1/2}{Spin $\frac{1}{2}$}
%\item \href{http://en.wikipedia.org/wiki/Spin_1/2}{Galois Theory}
%
%\end{itemize}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading problems &
\hwrref{VectorSpaces}{1}\\
Addition and inverse& \hwref{VectorSpaces}{2}\\
\hline
\end{tabular}

\input{\vectorSpacesPath/problems}


\newpage
",lesson
8,Linear Transformations,"
\chapter{\linTransTitle}
\label{sec:linearTransformation}

The main objects of study in any course in linear algebra are linear functions:

\begin{definition}
A function $L \colon V\rightarrow W$ is {\bf linear} if $V$ and $W$ are vector spaces and 
\begin{center}
\shabox{$
L(ru + sv) = rL(u) + sL(v) $}
\end{center}
 for all $u,v \in V$ and $r,s \in \Re$.
\end{definition}

\Reading{LinearTransformations}{1}
%\begin{center}\href{\webworkurl ReadingHomework7/1/}{Reading homework: problem 7.1}\end{center}


\begin{remark}
We will often refer to linear functions by names like ``linear map''\index{Linear Map}, ``linear operator''\index{Linear Operator} or ``linear transformation''\index{Linear Transformation}. In some contexts
you will also see the name ``homomorphism''\index{Homomorphism} which generally is applied to functions from one kind of set to the same kind of set while respecting any  structures on the sets; linear maps are from vector spaces to vector spaces that respect scalar multiplication and addition, the two structures on vector spaces. It is common to denote a linear function by capital $L$ as a reminder of its linearity, but sometimes we will use just $f$, after all we are just studying very special functions.
\end{remark}

The definition above coincides with the \hyperlink{twopart}{two part} description in Chapter~\ref{warmup};
the case $r=1,s=1$ describes additivity, while  $s=0$ describes homogeneity. 
We are now ready to learn the powerful consequences of linearity.

\section{The Consequence of Linearity}

Now that we have a sufficiently general notion of vector space 
it is time to talk about why linear operators are so special. 
Think about what is required to fully specify a real function of one variable. 
%We typically deal with functions that have simple algebraic descriptions like 
%$f(x)=x^2$ or $g(x)=\ln(x)$. 
%There are many more functions than these. 
%Imagine assigning a random  output to each of the 
%One output for each input.
%from $\Re^\mathbb{N}$. 
One output must be specified for each input. 
That is an infinite amount of information. 

By contrast, even though a linear function can have infinitely many elements in its domain, it is specified by a very small amount of information. 

\begin{example} (One output specifies infinitely many)\\ 
If you know that the function $L$ is linear and that 
$$L\colvec{1\\0}  =\colvec{5\\3}$$ 
then you do not need any more information to figure out 
$$L\colvec{2\\0},~L\colvec{3\\0}~,L\colvec{4\\0},~L\colvec{5\\0} ,{\rm ~\mbox{\it etc}}\ldots, $$ 
because by homogeneity
$$L\colvec{5\\0}=L\left[ 5\colvec{1\\0} \right] = 5 L\colvec{1\\0}=5\colvec{5\\3}=\colvec{25\\15}.$$
In this way an an infinite number of outputs is specified by just one.
\end{example}

\begin{example}(Two outputs in $\mathbb{R}^2$ specifies all outputs)\\
Likewise, if you  know that $L$ is linear and that
$$
L\colvec{1\\0}=\colvec{5\\3} {\rm ~and~} L\colvec{0\\1}= \colvec{2\\2}
$$ 
then you don't need any more information to compute
$$L\colvec{1\\1}$$ because by additivity
$$
L\colvec{1\\1}= L \left[ \colvec{1\\0} + \colvec{0\\1} \right] 
=L\colvec{1\\0} + L \colvec{0\\1} = \colvec{5\\3} +\colvec{2\\2} =\colvec{7\\5}.
$$
In fact, since every vector in $\Re^2$ can be expressed as 
$$
\colvec{x\\y}= x\colvec{1\\0}+y\colvec{0\\1}\, ,
$$ 
we know how $L$ acts on every vector from 
$\Re^2$ by linearity based on just  two pieces of information;
$$
L\colvec{x\\y}
= L \left[ x\colvec{1\\0}+y\colvec{0\\1} \right]
= x L\colvec{1\\0}+y L\colvec{0\\1} 
= x \colvec{5\\3}+ y\colvec{2\\2} =\colvec{5x+2y\\3x+2y}.
$$
Thus, the value of $L$ at infinitely many inputs is completely specified by its value at just two inputs.
(We can see now that $L$ acts in exactly the way the matrix 
$$
\begin{pmatrix}
5&2\\
3&2 \end{pmatrix}
$$
acts on vectors from $\Re^2$.)
\end{example}

\Reading{LinearTransformations}{2}

This is the reason that linear functions are so nice;
they are secretly very simple functions by virtue of two characteristics:
\begin{enumerate}\item They act on vector spaces.
\item They act additively and homogeneously. 
\end{enumerate}


A linear transformation with domain $\Re^3$ is completely specified by the way it acts on the three vectors 
$$
\colvec{1\\0\\0}\, ,\:\colvec{0\\1\\0}\, ,\:\colvec{0\\0\\1}\, .
$$
Similarly, a linear transformation with domain $\Re^n$ is completely specified by its action on the $n$ different $n$-vectors that have exactly one non-zero component, and its matrix form can be read off this information. However, not all linear functions have such nice domains.


%%%%%%%%%%%%%%%%%%%%%%

\section{Linear Functions on Hyperplanes }
It is not always so easy to write a linear operator as a matrix. 
Generally, this will amount to solving a linear systems problem. Examining a linear function whose domain is a hyperplane is instructive.

\begin{example}\label{Vdef} Let $$V=\left\{  c_1\colvec{1\\1\\0} +c_2\colvec{0\\1\\1} \middle| c_1,c_2\in \Re \right\} $$ and consider $L:V\to \Re^3$ be a linear function that obeys 
$$
L\colvec{1\\1\\0 } = \colvec{0\\1\\0},\qquad
L\colvec{0\\1\\1 } = \colvec{0\\1\\0}.
$$
By linearity this specifies the action of $L$ on any vector from $V$ as
$$
L\left[ c_1\colvec{1\\1\\0 } + c_2 \colvec{0\\1\\1} \right]= (c_1+c_2)\colvec{0\\1\\0}.
$$
The domain of $L$ is a plane and its range is the line through the origin in the $x_2$ direction. 


It is not clear how to formulate $L$ as a matrix; 
since
\begin{eqnarray*}
L\ccolvec{c_1\\\!\!c_1+c_2\!\!\\c_2} = 
\begin{pmatrix}
0&0&0\\
1&0&1\\
0&0&0
\end{pmatrix}
\ccolvec{c_1\\\!\!c_1+c_2\!\!\\c_2} =(c_1+c_2)\colvec{0\\1\\0}\, ,
\end{eqnarray*}
{\it or} 
\begin{eqnarray*}
L\ccolvec{c_1\\\!\!c_1+c_2\!\!\\c_2} = 
\begin{pmatrix}
0&0&0\\
0&1&0\\
0&0&0
\end{pmatrix}
\ccolvec{c_1\\\!\!c_1+c_2\!\!\\c_2} =(c_1+c_2)\colvec{0\\1\\0}\, ,
\end{eqnarray*}
you might suspect that  $L$ is equivalent to one of these $3\times3$ matrices. It is not. By the natural domain convention, all $3\times3$ matrices have $\Re^3$ as their domain, and the domain of $L$ is smaller than that. 
When we do realize this $L$ as a matrix it will be as a  $3\times2$ matrix. We can tell because the domain of $L$ is 2 dimensional and the codomain is $3$ dimensional. (You probably already know that the plane has dimension~2, and a line is 1~dimensional, but the careful definition of ``dimension'' takes some work; this is tackled in Chapter~\ref{basisdimension}.) This leads us to write
$$
L\left[ c_1\colvec{1\\1\\0 } + c_2 \colvec{0\\1\\1} \right]=c_1\colvec{0\\1\\0}+c_2\colvec{0\\1\\0}=\begin{pmatrix}0&0\\1&1\\0&0\end{pmatrix}\colvec{c_1\\c_2}\, .
$$
This makes sense, but requires a {\it warning}: The matrix $\begin{pmatrix}0&0\\1&1\\0&0\end{pmatrix}$ specifies $L$  so long as you also provide the information that you are labeling points in the plane $V$
by the two numbers $(c_1,c_2)$.
\end{example} 




%Recall that the key properties of vector spaces are vector addition and scalar multiplication.  Now suppose we have two vector spaces $V$ and $W$ and a map $L$ between them:
%\[
%L \colon V \rightarrow W
%\]
%Now, both $V$ and $W$ have notions of vector addition and scalar multiplication.  It would be ideal if the map $L$ \emph{preserved} these operations.  In other words, if adding vectors and then applying $L$ were the same as applying $L$ to vectors and then adding them.  Likewise, it would be nice if, when multiplying by a scalar, it didn't matter whether we multiplied before or after applying~$L$.  In formulas, this means that for any $u,v \in V$ and $c \in \Re$:
%\begin{eqnarray*}
%L(u+v) &=& L(u)+L(v) \\[2mm]
%L(cv) &= &cL(v)
%\end{eqnarray*}
%
%Combining these two requirements into one equation, we get the definition of a linear function\index{Linear function} or linear transformation\index{Linear transformation}.


%Notice that on the left the addition and scalar multiplication occur in $V$, while on the right the operations occur in $W$.
%This is often called the {\it linearity property}\index{Linearity} of a linear transformation.



%\begin{example}
%Take $L \colon \Re^3\rightarrow \Re^3$ defined by:
%
%\[
%L\colvec{x\\y\\z} = \colvec{ x+y\\y+z\\0 }
%\]
%The domain of $L$ is $\Re^3$. The range is not all of $\Re^3$, but just the plane comprised of vecotrs whose third component is zero. We say that $L$ transforms $\Re^3$ into this plane.\\
%
%We now check linearity.  Call $u = \colvec{x\\y\\z}$ and $v=\colvec{a\\b\\c}$.  
%
%\begin{eqnarray*}
%L(ru + sv) & = & L\left( r \colvec{x\\y\\z} + s \colvec{a\\b\\c} \right) \\
% & = & L\left( \colvec{rx\\ry\\rz} + \colvec{sa\\sb\\sc} \right) \\
% & = & L \colvec{rx+sa\\ry+sb\\rz+sx}  \\
% & = & \colvec{rx+sa+ry+sb\\ry+sb+rz+sx\\0}
%\end{eqnarray*}
%On the other hand,
%
%\begin{eqnarray*}
%rL(u) + sL(v) & = & rL\colvec{x\\y\\z} + sL\colvec{a\\b\\c}\\
% & = & r\colvec{x+y\\y+z\\0} + s\colvec{a+b\\b+c\\0}\\
% & = & \colvec{rx+ry\\ry+rz\\0} + \colvec{sa+sb\\sb+sc\\0}\\
% & = & \colvec{rx+sa+ry+sb\\ry+sb+rz+sx\\0}
%\end{eqnarray*}
%Then the two sides of the linearity requirement are equal, so $L$ is a linear transformation.
%
%We can write the linear transformation $L$ in the previous example using a matrix like so:
%\[
%L\colvec{x\\y\\z} = \colvec{x+y\\y+z\\0} = 
%\begin{pmatrix}
%1 & 1 & 0 \\
%0 & 1 & 1 \\
%0 & 0 & 0 \\
%\end{pmatrix}\colvec{x\\y\\z}
%\]
%
%\begin{center}\href{\webworkurl ReadingHomework7/2/}{Reading homework: problem 7.2}\end{center}
%\end{example}

%\videoscriptlink{linear_transformations_example.mp4}{A linear and non-linear example}{scripts_linear_transformations_example}

\section{Linear Differential Operators}

Your calculus class became much easier when you stopped using the limit definition of the derivative,  learned the power rule, and started using linearity of the derivative operator.

\begin{example}
Let $V$ be the vector space of polynomials of degree 2 or less with standard addition and scalar multiplication;
\[
V := \{a_0\cdot1 + a_1x + a_2 x^2 \, | \,  a_0,a_1,a_2 \in \Re \}
\]
Let $\frac{d}{dx} \colon V\rightarrow V$ be \hypertarget{derivative_linear}{the derivative operator.}  
The following three equations, along with linearity of the derivative operator, allow one to take the derivative of any 2nd degree polynomial:
$$
\frac{d}{dx} 1=0,~\frac{d}{dx}x=1,~\frac{d}{dx}x^2=2x\,. 
$$
In particular
$$
\frac{d}{dx} (a_01 + a_1x + a_2 x^2) = 
 a_0\frac{d}{dx}1 + a_1 \frac{d}{dx} x + a_2 \frac{d}{dx} x^2  
 = 0+a_1+2a_2x.
$$
Thus, the derivative acting any of the infinitely many second order polynomials is determined by its action for just three inputs.
%The full statement of linearity of $\frac{d}{dx}$ is:  
%For 2nd order polynomials $p_1,p_2$ and numbers $r,s$, 
%\[
%\frac{d}{dx}(rp_1 + s p_2) = r \frac{dp_1}{dx} + s \frac{dp_2}{dx} .
%\]
%We can represent a polynomial as a ``semi-infinite vector'', like so:
%\[
%a_0 + a_1x + \cdots + a_nx^n \longleftrightarrow 
%\colvec{ a_0 \\ a_1 \\ \vdots \\ a_n \\ 0 \\ 0 \\ \vdots }
%\]

%Then we have:
%\[
%\frac{d}{dx}(a_0 + a_1x + \cdots + a_nx^n) = a_1 + 2a_2x + \cdots + na_{n}x^{n-1} \\
%\longleftrightarrow 
%\colvec{ a_1 \\ 2a_2 \\ \vdots \\ na_n \\ 0 \\ 0 \\ \vdots }
%\]
%
%One could then write the derivative as an ``infinite matrix'':
%\[
%\frac{d}{dx} \longleftrightarrow 
%\begin{pmatrix}
%0 & 1 & 0 & 0 & \cdots \\
%0 & 0 & 2 & 0 & \cdots \\
%0 & 0 & 0 & 3 & \cdots \\
%\vdots & \vdots & \vdots & \vdots &  \\
%\end{pmatrix}
%\]
\end{example}


\section{Bases (Take 1)} 
The central idea of linear algebra is to exploit the hidden simplicity of linear functions. 
It ends up there is a lot of freedom in how to do this. That freedom is what makes linear algebra powerful.

You saw that a linear operator acting on $\Re^2$ is completely specified by how it acts on the pair of vectors $\colvec{1\\0}$ and $\colvec{0\\1}$. 
In fact, any linear operator acting on $\Re^2$ is also completely specified by how it acts on the pair of vectors $\colvec{1\\1}$ and $\colvec{1\\-1}$.

\begin{example} The linear operator $L$ is a linear operator then it is completely specified \hypertarget{nonstandard r2 basis}{by the two equalities} 
$$
L\colvec{1\\1}= \colvec{2\\4}, {\rm ~and~} L\colvec{1\\-1}=\colvec{6\\8}.
$$ 
This is because any vector $\colvec{x\\y}$ in $\Re^2$ is a sum of multiples of
$\colvec{1\\1}$ and $\colvec{1\\-1}$ which can be calculated via a linear systems problem as follows:
\begin{eqnarray*}&&
\colvec{x\\y}=a\colvec{1\\1}+b\colvec{1\\-1}\\[1mm]
&\Leftrightarrow& 
\begin{pmatrix}
1&1\\
1&-1
\end{pmatrix}
\colvec{a\\b}
=\colvec{x\\y}\\[1mm]
&\Leftrightarrow& 
\begin{amatrix}{2}
1&1&x\\
1&-1&y
\end{amatrix}
\sim \begin{amatrix}{2}
1&0& \frac{x+y}{2}\\
0&1&\frac{x-y}2
\end{amatrix}\\[1mm]
&\Leftrightarrow&
\left\{ 
\begin{array}{l}
a=\frac{x+y}{2}\\
b=\frac{x-y}{2}\, .
\end{array}
\right.
\end{eqnarray*}
Thus
$$
\colvec{x\\[2mm]y}=\frac{x+y}{2}\colvec{1\\[2mm]1}+\frac{x-y}{2}\colvec{1\\[2mm]-1}\, .
$$
We can then calculate how $L$ acts on any vector by first expressing the vector as  a sum of multiples and then applying linearity;
\begin{eqnarray*}
L\colvec{x\\y}
&=&L\left[    \frac{x+y}{2} \colvec{1\\1} + \frac{x-y}{2} \colvec{1\\-1}  \right]\\[1mm]
&=&\frac{x+y}{2} L \colvec{1\\1} + \frac{x-y}{2} L \colvec{1\\-1} \\[2mm]
&=&\frac{x+y}{2} \colvec{2\\4} + \frac{x-y}{2}  \colvec{6\\8} \\[1mm]
&=&\ccolvec{x+y \\ 2(x+y)} +  \colvec{3(x-y)\\4(x-y)}\\[1mm]
&=&\ccolvec{4x-2y \\ 6x-2y}
\end{eqnarray*}
Thus $L$ is completely specified by its value at just two inputs. 
%In fact, we find that $L$ is equivalent to a matrix, 
%$$
%L\colvec{a\\b}=
%\begin{pmatrix}
%4&-2\\
%6&-1
%\end{pmatrix}
%\colvec{a\\b}.
%$$
%but only by virtue of it having domain and targert $\Re^2$.
\end{example}

It should not surprise you to learn there are infinitely many pairs of vectors from $\Re^2$ 
with the property that any vector can be expressed as a linear combination of them; any pair that when used as columns of a matrix gives an invertible matrix works. Such a pair is called a {\it basis}\index{basis} for $\Re^2$.

Similarly, there are infinitely many triples of vectors with the property that any vector from $\Re^3$ can be expressed as a linear combination of them: these are the triples that used as columns of a matrix give an invertible matrix. Such a triple is called a basis for $\Re^3$.

In a similar spirit, there are infinitely many pairs of vectors with the property that every vector in 
$$V=\left\{  c_1\colvec{1\\1\\0} +c_2\colvec{0\\1\\1} \middle\vert \, c_1,c_2\in \Re \right\} $$ 
can be expressed as a linear combination of them. Some examples are 
$$V=
\left\{ c_1\colvec{1\\1\\0} +c_2\colvec{0\\2\\2}  \middle\vert c_1,c_2\in \Re \right\} 
=\left\{c_1 \colvec{1\\1\\0}+c_2 \colvec{1\\3\\2}  \middle\vert c_1,c_2\in \Re \right\} 
%\\
%=\left\{  c_1\colvec{2\\4\\2} +c_2 \colvec{1\\3\\2}  | c_1,c_2\in \Re \right\} 
$$
Such a pair is a  called a basis for $V$.



%\begin{remark}[Foreshadowing Dimension.]

You probably have some intuitive notion of what dimension\index{Dimension!concept of} means
(the careful mathematical definition is given in chapter~\ref{sec:dimension}).
%Some of the examples of vector spaces we have worked with have been finite dimensional.  (For example, $\Re^n$ will turn out to have dimension $n$.)  
%The polynomial example above is an example of an infinite dimensional vector space.  
Roughly speaking, dimension is the number of independent directions available.  To figure out the dimension of a vector space, I stand at the origin, and pick a direction.  If there are any vectors in my vector space that aren't in that direction, then I choose another direction that isn't in the line determined by the direction I chose.  If there are any vectors in my vector space not in the plane determined by the first two directions, then I choose one of them as my next direction.  In other words, I choose a collection of \emph{independent} vectors in the vector space (independent vectors are defined in Chapter~\ref{linearind}).  
A minimal set of independent vectors is called a {\it basis}\index{basis} (see Chapter~\ref{basisdimension} for the precise definition). 
The number of vectors in my basis is the dimension of the vector space. 
Every vector space has many bases, but all bases for a particular vector space have the same number of vectors. Thus dimension is a well-defined concept. 

The fact that every vector space (over~$\Re$) has infinitely many bases is actually very useful. 
Often a good choice of  basis can reduce the time required to run a calculation in dramatic ways! 

In summary:

\begin{center}
\shabox{A basis is a set of vectors in terms of which it is possible to uniquely express any other vector.}
\end{center}
%For finite dimensional vector spaces, linear transformations can always be represented by matrices.  For that reason, we will start studying matrices intensively in the next few lectures.
%\end{remark}


%\section*{References}
%
%Hefferon, Chapter Three, Section II.  (Note that Hefferon uses the term \emph{homomorphism} for a linear map.  `Homomorphism' is a very general term which in mathematics means `Structure-preserving map.'  A linear map preserves the linear structure of a vector space, and is thus a type of homomorphism.)
%\\[2mm]
%Beezer, Chapter LT, Section LT, sections LT, LTC, and MLT.
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Linear_transformation}{Linear Transformation}
%\item \href{http://en.wikipedia.org/wiki/Dimension_(linear_algebra)}{Dimension}
%\end{itemize}
%

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading problems &
\hwrref{LinearTransformations}{1}, \hwrref{LinearTransformations}{2}\\
Linear? & \hwref{LinearTransformations}{3}\\
Matrix $\times$ vector & \hwref{LinearTransformations}{4}, \hwref{LinearTransformations}{5}\\
Linearity & \hwref{LinearTransformations}{6}, \hwref{LinearTransformations}{7}\\
\hline
\end{tabular}
\input{\linTransPath/problems}

\newpage
","
\chapter{\linTransTitle}
\label{sec:linearTransformation}

The main objects of study in any course in linear algebra are linear functions:

\begin{definition}
A function $L \colon V\rightarrow W$ is {\bf linear} if $V$ and $W$ are vector spaces and 
\begin{center}
\shabox{$
L(ru + sv) = rL(u) + sL(v) $}
\end{center}
 for all $u,v \in V$ and $r,s \in \Re$.
\end{definition}

\Reading{LinearTransformations}{1}
%\begin{center}\href{\webworkurl ReadingHomework7/1/}{Reading homework: problem 7.1}\end{center}


\begin{remark}
We will often refer to linear functions by names like ``linear map''\index{Linear Map}, ``linear operator''\index{Linear Operator} or ``linear transformation''\index{Linear Transformation}. In some contexts
you will also see the name ``homomorphism''\index{Homomorphism} which generally is applied to functions from one kind of set to the same kind of set while respecting any  structures on the sets; linear maps are from vector spaces to vector spaces that respect scalar multiplication and addition, the two structures on vector spaces. It is common to denote a linear function by capital $L$ as a reminder of its linearity, but sometimes we will use just $f$, after all we are just studying very special functions.
\end{remark}

The definition above coincides with the \hyperlink{twopart}{two part} description in Chapter~\ref{warmup};
the case $r=1,s=1$ describes additivity, while  $s=0$ describes homogeneity. 
We are now ready to learn the powerful consequences of linearity.

\section{The Consequence of Linearity}

Now that we have a sufficiently general notion of vector space 
it is time to talk about why linear operators are so special. 
Think about what is required to fully specify a real function of one variable. 
%We typically deal with functions that have simple algebraic descriptions like 
%$f(x)=x^2$ or $g(x)=\ln(x)$. 
%There are many more functions than these. 
%Imagine assigning a random  output to each of the 
%One output for each input.
%from $\Re^\mathbb{N}$. 
One output must be specified for each input. 
That is an infinite amount of information. 

By contrast, even though a linear function can have infinitely many elements in its domain, it is specified by a very small amount of information. 

\begin{example} (One output specifies infinitely many)\\ 
If you know that the function $L$ is linear and that 
$$L\colvec{1\\0}  =\colvec{5\\3}$$ 
then you do not need any more information to figure out 
$$L\colvec{2\\0},~L\colvec{3\\0}~,L\colvec{4\\0},~L\colvec{5\\0} ,{\rm ~\mbox{\it etc}}\ldots, $$ 
because by homogeneity
$$L\colvec{5\\0}=L\left[ 5\colvec{1\\0} \right] = 5 L\colvec{1\\0}=5\colvec{5\\3}=\colvec{25\\15}.$$
In this way an an infinite number of outputs is specified by just one.
\end{example}

\begin{example}(Two outputs in $\mathbb{R}^2$ specifies all outputs)\\
Likewise, if you  know that $L$ is linear and that
$$
L\colvec{1\\0}=\colvec{5\\3} {\rm ~and~} L\colvec{0\\1}= \colvec{2\\2}
$$ 
then you don't need any more information to compute
$$L\colvec{1\\1}$$ because by additivity
$$
L\colvec{1\\1}= L \left[ \colvec{1\\0} + \colvec{0\\1} \right] 
=L\colvec{1\\0} + L \colvec{0\\1} = \colvec{5\\3} +\colvec{2\\2} =\colvec{7\\5}.
$$
In fact, since every vector in $\Re^2$ can be expressed as 
$$
\colvec{x\\y}= x\colvec{1\\0}+y\colvec{0\\1}\, ,
$$ 
we know how $L$ acts on every vector from 
$\Re^2$ by linearity based on just  two pieces of information;
$$
L\colvec{x\\y}
= L \left[ x\colvec{1\\0}+y\colvec{0\\1} \right]
= x L\colvec{1\\0}+y L\colvec{0\\1} 
= x \colvec{5\\3}+ y\colvec{2\\2} =\colvec{5x+2y\\3x+2y}.
$$
Thus, the value of $L$ at infinitely many inputs is completely specified by its value at just two inputs.
(We can see now that $L$ acts in exactly the way the matrix 
$$
\begin{pmatrix}
5&2\\
3&2 \end{pmatrix}
$$
acts on vectors from $\Re^2$.)
\end{example}

\Reading{LinearTransformations}{2}

This is the reason that linear functions are so nice;
they are secretly very simple functions by virtue of two characteristics:
\begin{enumerate}\item They act on vector spaces.
\item They act additively and homogeneously. 
\end{enumerate}


A linear transformation with domain $\Re^3$ is completely specified by the way it acts on the three vectors 
$$
\colvec{1\\0\\0}\, ,\:\colvec{0\\1\\0}\, ,\:\colvec{0\\0\\1}\, .
$$
Similarly, a linear transformation with domain $\Re^n$ is completely specified by its action on the $n$ different $n$-vectors that have exactly one non-zero component, and its matrix form can be read off this information. However, not all linear functions have such nice domains.


%%%%%%%%%%%%%%%%%%%%%%

\section{Linear Functions on Hyperplanes }
It is not always so easy to write a linear operator as a matrix. 
Generally, this will amount to solving a linear systems problem. Examining a linear function whose domain is a hyperplane is instructive.

\begin{example}\label{Vdef} Let $$V=\left\{  c_1\colvec{1\\1\\0} +c_2\colvec{0\\1\\1} \middle| c_1,c_2\in \Re \right\} $$ and consider $L:V\to \Re^3$ be a linear function that obeys 
$$
L\colvec{1\\1\\0 } = \colvec{0\\1\\0},\qquad
L\colvec{0\\1\\1 } = \colvec{0\\1\\0}.
$$
By linearity this specifies the action of $L$ on any vector from $V$ as
$$
L\left[ c_1\colvec{1\\1\\0 } + c_2 \colvec{0\\1\\1} \right]= (c_1+c_2)\colvec{0\\1\\0}.
$$
The domain of $L$ is a plane and its range is the line through the origin in the $x_2$ direction. 


It is not clear how to formulate $L$ as a matrix; 
since
\begin{eqnarray*}
L\ccolvec{c_1\\\!\!c_1+c_2\!\!\\c_2} = 
\begin{pmatrix}
0&0&0\\
1&0&1\\
0&0&0
\end{pmatrix}
\ccolvec{c_1\\\!\!c_1+c_2\!\!\\c_2} =(c_1+c_2)\colvec{0\\1\\0}\, ,
\end{eqnarray*}
{\it or} 
\begin{eqnarray*}
L\ccolvec{c_1\\\!\!c_1+c_2\!\!\\c_2} = 
\begin{pmatrix}
0&0&0\\
0&1&0\\
0&0&0
\end{pmatrix}
\ccolvec{c_1\\\!\!c_1+c_2\!\!\\c_2} =(c_1+c_2)\colvec{0\\1\\0}\, ,
\end{eqnarray*}
you might suspect that  $L$ is equivalent to one of these $3\times3$ matrices. It is not. By the natural domain convention, all $3\times3$ matrices have $\Re^3$ as their domain, and the domain of $L$ is smaller than that. 
When we do realize this $L$ as a matrix it will be as a  $3\times2$ matrix. We can tell because the domain of $L$ is 2 dimensional and the codomain is $3$ dimensional. (You probably already know that the plane has dimension~2, and a line is 1~dimensional, but the careful definition of ``dimension'' takes some work; this is tackled in Chapter~\ref{basisdimension}.) This leads us to write
$$
L\left[ c_1\colvec{1\\1\\0 } + c_2 \colvec{0\\1\\1} \right]=c_1\colvec{0\\1\\0}+c_2\colvec{0\\1\\0}=\begin{pmatrix}0&0\\1&1\\0&0\end{pmatrix}\colvec{c_1\\c_2}\, .
$$
This makes sense, but requires a {\it warning}: The matrix $\begin{pmatrix}0&0\\1&1\\0&0\end{pmatrix}$ specifies $L$  so long as you also provide the information that you are labeling points in the plane $V$
by the two numbers $(c_1,c_2)$.
\end{example} 




%Recall that the key properties of vector spaces are vector addition and scalar multiplication.  Now suppose we have two vector spaces $V$ and $W$ and a map $L$ between them:
%\[
%L \colon V \rightarrow W
%\]
%Now, both $V$ and $W$ have notions of vector addition and scalar multiplication.  It would be ideal if the map $L$ \emph{preserved} these operations.  In other words, if adding vectors and then applying $L$ were the same as applying $L$ to vectors and then adding them.  Likewise, it would be nice if, when multiplying by a scalar, it didn't matter whether we multiplied before or after applying~$L$.  In formulas, this means that for any $u,v \in V$ and $c \in \Re$:
%\begin{eqnarray*}
%L(u+v) &=& L(u)+L(v) \\[2mm]
%L(cv) &= &cL(v)
%\end{eqnarray*}
%
%Combining these two requirements into one equation, we get the definition of a linear function\index{Linear function} or linear transformation\index{Linear transformation}.


%Notice that on the left the addition and scalar multiplication occur in $V$, while on the right the operations occur in $W$.
%This is often called the {\it linearity property}\index{Linearity} of a linear transformation.



%\begin{example}
%Take $L \colon \Re^3\rightarrow \Re^3$ defined by:
%
%\[
%L\colvec{x\\y\\z} = \colvec{ x+y\\y+z\\0 }
%\]
%The domain of $L$ is $\Re^3$. The range is not all of $\Re^3$, but just the plane comprised of vecotrs whose third component is zero. We say that $L$ transforms $\Re^3$ into this plane.\\
%
%We now check linearity.  Call $u = \colvec{x\\y\\z}$ and $v=\colvec{a\\b\\c}$.  
%
%\begin{eqnarray*}
%L(ru + sv) & = & L\left( r \colvec{x\\y\\z} + s \colvec{a\\b\\c} \right) \\
% & = & L\left( \colvec{rx\\ry\\rz} + \colvec{sa\\sb\\sc} \right) \\
% & = & L \colvec{rx+sa\\ry+sb\\rz+sx}  \\
% & = & \colvec{rx+sa+ry+sb\\ry+sb+rz+sx\\0}
%\end{eqnarray*}
%On the other hand,
%
%\begin{eqnarray*}
%rL(u) + sL(v) & = & rL\colvec{x\\y\\z} + sL\colvec{a\\b\\c}\\
% & = & r\colvec{x+y\\y+z\\0} + s\colvec{a+b\\b+c\\0}\\
% & = & \colvec{rx+ry\\ry+rz\\0} + \colvec{sa+sb\\sb+sc\\0}\\
% & = & \colvec{rx+sa+ry+sb\\ry+sb+rz+sx\\0}
%\end{eqnarray*}
%Then the two sides of the linearity requirement are equal, so $L$ is a linear transformation.
%
%We can write the linear transformation $L$ in the previous example using a matrix like so:
%\[
%L\colvec{x\\y\\z} = \colvec{x+y\\y+z\\0} = 
%\begin{pmatrix}
%1 & 1 & 0 \\
%0 & 1 & 1 \\
%0 & 0 & 0 \\
%\end{pmatrix}\colvec{x\\y\\z}
%\]
%
%\begin{center}\href{\webworkurl ReadingHomework7/2/}{Reading homework: problem 7.2}\end{center}
%\end{example}

%\videoscriptlink{linear_transformations_example.mp4}{A linear and non-linear example}{scripts_linear_transformations_example}

\section{Linear Differential Operators}

Your calculus class became much easier when you stopped using the limit definition of the derivative,  learned the power rule, and started using linearity of the derivative operator.

\begin{example}
Let $V$ be the vector space of polynomials of degree 2 or less with standard addition and scalar multiplication;
\[
V := \{a_0\cdot1 + a_1x + a_2 x^2 \, | \,  a_0,a_1,a_2 \in \Re \}
\]
Let $\frac{d}{dx} \colon V\rightarrow V$ be \hypertarget{derivative_linear}{the derivative operator.}  
The following three equations, along with linearity of the derivative operator, allow one to take the derivative of any 2nd degree polynomial:
$$
\frac{d}{dx} 1=0,~\frac{d}{dx}x=1,~\frac{d}{dx}x^2=2x\,. 
$$
In particular
$$
\frac{d}{dx} (a_01 + a_1x + a_2 x^2) = 
 a_0\frac{d}{dx}1 + a_1 \frac{d}{dx} x + a_2 \frac{d}{dx} x^2  
 = 0+a_1+2a_2x.
$$
Thus, the derivative acting any of the infinitely many second order polynomials is determined by its action for just three inputs.
%The full statement of linearity of $\frac{d}{dx}$ is:  
%For 2nd order polynomials $p_1,p_2$ and numbers $r,s$, 
%\[
%\frac{d}{dx}(rp_1 + s p_2) = r \frac{dp_1}{dx} + s \frac{dp_2}{dx} .
%\]
%We can represent a polynomial as a ``semi-infinite vector'', like so:
%\[
%a_0 + a_1x + \cdots + a_nx^n \longleftrightarrow 
%\colvec{ a_0 \\ a_1 \\ \vdots \\ a_n \\ 0 \\ 0 \\ \vdots }
%\]

%Then we have:
%\[
%\frac{d}{dx}(a_0 + a_1x + \cdots + a_nx^n) = a_1 + 2a_2x + \cdots + na_{n}x^{n-1} \\
%\longleftrightarrow 
%\colvec{ a_1 \\ 2a_2 \\ \vdots \\ na_n \\ 0 \\ 0 \\ \vdots }
%\]
%
%One could then write the derivative as an ``infinite matrix'':
%\[
%\frac{d}{dx} \longleftrightarrow 
%\begin{pmatrix}
%0 & 1 & 0 & 0 & \cdots \\
%0 & 0 & 2 & 0 & \cdots \\
%0 & 0 & 0 & 3 & \cdots \\
%\vdots & \vdots & \vdots & \vdots &  \\
%\end{pmatrix}
%\]
\end{example}


\section{Bases (Take 1)} 
The central idea of linear algebra is to exploit the hidden simplicity of linear functions. 
It ends up there is a lot of freedom in how to do this. That freedom is what makes linear algebra powerful.

You saw that a linear operator acting on $\Re^2$ is completely specified by how it acts on the pair of vectors $\colvec{1\\0}$ and $\colvec{0\\1}$. 
In fact, any linear operator acting on $\Re^2$ is also completely specified by how it acts on the pair of vectors $\colvec{1\\1}$ and $\colvec{1\\-1}$.

\begin{example} The linear operator $L$ is a linear operator then it is completely specified \hypertarget{nonstandard r2 basis}{by the two equalities} 
$$
L\colvec{1\\1}= \colvec{2\\4}, {\rm ~and~} L\colvec{1\\-1}=\colvec{6\\8}.
$$ 
This is because any vector $\colvec{x\\y}$ in $\Re^2$ is a sum of multiples of
$\colvec{1\\1}$ and $\colvec{1\\-1}$ which can be calculated via a linear systems problem as follows:
\begin{eqnarray*}&&
\colvec{x\\y}=a\colvec{1\\1}+b\colvec{1\\-1}\\[1mm]
&\Leftrightarrow& 
\begin{pmatrix}
1&1\\
1&-1
\end{pmatrix}
\colvec{a\\b}
=\colvec{x\\y}\\[1mm]
&\Leftrightarrow& 
\begin{amatrix}{2}
1&1&x\\
1&-1&y
\end{amatrix}
\sim \begin{amatrix}{2}
1&0& \frac{x+y}{2}\\
0&1&\frac{x-y}2
\end{amatrix}\\[1mm]
&\Leftrightarrow&
\left\{ 
\begin{array}{l}
a=\frac{x+y}{2}\\
b=\frac{x-y}{2}\, .
\end{array}
\right.
\end{eqnarray*}
Thus
$$
\colvec{x\\[2mm]y}=\frac{x+y}{2}\colvec{1\\[2mm]1}+\frac{x-y}{2}\colvec{1\\[2mm]-1}\, .
$$
We can then calculate how $L$ acts on any vector by first expressing the vector as  a sum of multiples and then applying linearity;
\begin{eqnarray*}
L\colvec{x\\y}
&=&L\left[    \frac{x+y}{2} \colvec{1\\1} + \frac{x-y}{2} \colvec{1\\-1}  \right]\\[1mm]
&=&\frac{x+y}{2} L \colvec{1\\1} + \frac{x-y}{2} L \colvec{1\\-1} \\[2mm]
&=&\frac{x+y}{2} \colvec{2\\4} + \frac{x-y}{2}  \colvec{6\\8} \\[1mm]
&=&\ccolvec{x+y \\ 2(x+y)} +  \colvec{3(x-y)\\4(x-y)}\\[1mm]
&=&\ccolvec{4x-2y \\ 6x-2y}
\end{eqnarray*}
Thus $L$ is completely specified by its value at just two inputs. 
%In fact, we find that $L$ is equivalent to a matrix, 
%$$
%L\colvec{a\\b}=
%\begin{pmatrix}
%4&-2\\
%6&-1
%\end{pmatrix}
%\colvec{a\\b}.
%$$
%but only by virtue of it having domain and targert $\Re^2$.
\end{example}

It should not surprise you to learn there are infinitely many pairs of vectors from $\Re^2$ 
with the property that any vector can be expressed as a linear combination of them; any pair that when used as columns of a matrix gives an invertible matrix works. Such a pair is called a {\it basis}\index{basis} for $\Re^2$.

Similarly, there are infinitely many triples of vectors with the property that any vector from $\Re^3$ can be expressed as a linear combination of them: these are the triples that used as columns of a matrix give an invertible matrix. Such a triple is called a basis for $\Re^3$.

In a similar spirit, there are infinitely many pairs of vectors with the property that every vector in 
$$V=\left\{  c_1\colvec{1\\1\\0} +c_2\colvec{0\\1\\1} \middle\vert \, c_1,c_2\in \Re \right\} $$ 
can be expressed as a linear combination of them. Some examples are 
$$V=
\left\{ c_1\colvec{1\\1\\0} +c_2\colvec{0\\2\\2}  \middle\vert c_1,c_2\in \Re \right\} 
=\left\{c_1 \colvec{1\\1\\0}+c_2 \colvec{1\\3\\2}  \middle\vert c_1,c_2\in \Re \right\} 
%\\
%=\left\{  c_1\colvec{2\\4\\2} +c_2 \colvec{1\\3\\2}  | c_1,c_2\in \Re \right\} 
$$
Such a pair is a  called a basis for $V$.



%\begin{remark}[Foreshadowing Dimension.]

You probably have some intuitive notion of what dimension\index{Dimension!concept of} means
(the careful mathematical definition is given in chapter~\ref{sec:dimension}).
%Some of the examples of vector spaces we have worked with have been finite dimensional.  (For example, $\Re^n$ will turn out to have dimension $n$.)  
%The polynomial example above is an example of an infinite dimensional vector space.  
Roughly speaking, dimension is the number of independent directions available.  To figure out the dimension of a vector space, I stand at the origin, and pick a direction.  If there are any vectors in my vector space that aren't in that direction, then I choose another direction that isn't in the line determined by the direction I chose.  If there are any vectors in my vector space not in the plane determined by the first two directions, then I choose one of them as my next direction.  In other words, I choose a collection of \emph{independent} vectors in the vector space (independent vectors are defined in Chapter~\ref{linearind}).  
A minimal set of independent vectors is called a {\it basis}\index{basis} (see Chapter~\ref{basisdimension} for the precise definition). 
The number of vectors in my basis is the dimension of the vector space. 
Every vector space has many bases, but all bases for a particular vector space have the same number of vectors. Thus dimension is a well-defined concept. 

The fact that every vector space (over~$\Re$) has infinitely many bases is actually very useful. 
Often a good choice of  basis can reduce the time required to run a calculation in dramatic ways! 

In summary:

\begin{center}
\shabox{A basis is a set of vectors in terms of which it is possible to uniquely express any other vector.}
\end{center}
%For finite dimensional vector spaces, linear transformations can always be represented by matrices.  For that reason, we will start studying matrices intensively in the next few lectures.
%\end{remark}


%\section*{References}
%
%Hefferon, Chapter Three, Section II.  (Note that Hefferon uses the term \emph{homomorphism} for a linear map.  `Homomorphism' is a very general term which in mathematics means `Structure-preserving map.'  A linear map preserves the linear structure of a vector space, and is thus a type of homomorphism.)
%\\[2mm]
%Beezer, Chapter LT, Section LT, sections LT, LTC, and MLT.
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Linear_transformation}{Linear Transformation}
%\item \href{http://en.wikipedia.org/wiki/Dimension_(linear_algebra)}{Dimension}
%\end{itemize}
%

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading problems &
\hwrref{LinearTransformations}{1}, \hwrref{LinearTransformations}{2}\\
Linear? & \hwref{LinearTransformations}{3}\\
Matrix $\times$ vector & \hwref{LinearTransformations}{4}, \hwref{LinearTransformations}{5}\\
Linearity & \hwref{LinearTransformations}{6}, \hwref{LinearTransformations}{7}\\
\hline
\end{tabular}
\input{\linTransPath/problems}

\newpage
",lesson
9,Matrices,"\chapter{\matricesTitle}
\label{Matrices}

Matrices are a powerful tool for calculations involving linear transformations.
It is important to understand how to find the matrix of a linear transformation 
and the properties of matrices.

\section{Linear Transformations and Matrices}

Ordered, finite-dimensional, bases for  vector spaces allows us to express linear operators as matrices.
%We represent vectors from $\Re^n$ as matrices with one column, so lets begin there.

\subsection{Basis Notation}
\hypertarget{Basis notation}{~}

\noindent
A basis allows us to efficiently label arbitrary vectors in terms of column vectors. Here is an  example.
\begin{example}
Let $$V=\left\{\begin{pmatrix}a&b\\c&d\end{pmatrix}\middle| \,a,b,c,d\in {\mathbb R}\right\}$$ be the vector space of $2\times 2$ real matrices,
with addition and scalar multiplication defined componentwise. 
One choice of basis is the ordered set (or list) of matrices
$$B=\left(\begin{pmatrix}1&0\\0&0\end{pmatrix},\begin{pmatrix}0&1\\0&0\end{pmatrix},\begin{pmatrix}0&0\\ 1&0\end{pmatrix},\begin{pmatrix}0&0\\ 0&1\end{pmatrix}\right)=:(e_1^1,e_2^1,e^2_1,e^2_2)\, .$$ 
Given a particular vector and a basis, your job is to write that vector as a sum of multiples of basis elements. Here
an arbitrary vector $v\in V$ is just a matrix, so we write
\begin{eqnarray*}
v\ =\ \begin{pmatrix}a&b\\c&d\end{pmatrix}&=&\quad\!\! \begin{pmatrix}a&0\\0&0\end{pmatrix}+\begin{pmatrix}0&b\\0&0\end{pmatrix}+\begin{pmatrix}0&0\\ c&0\end{pmatrix}+\begin{pmatrix}0&0\\0&d\end{pmatrix}\\[1mm]
&=&a\begin{pmatrix}1&0\\0&0\end{pmatrix}+b\begin{pmatrix}0&1\\0&0\end{pmatrix}+c\begin{pmatrix}0&0\\ 1&0\end{pmatrix}
+d\begin{pmatrix}0&0\\0&1\end{pmatrix}\\[1mm]
&=&a\,  e^1_1+b \, e^1_2+c \, e^2_1+d \, e^2_2\, .
\end{eqnarray*}
The coefficients $(a,b,c,d)$ of the basis vectors $(e^1_1,e^1_2,e^2_1,e^2_2)$ encode the information of which matrix the vector $v$ is.
We store them in column vector by writing
$$
v=a \, e^1_1+b \, e^1_2+c\,  e^2_1+d \, e^2_2=: (e^1_1,e^1_2,e^2_1,e^2_2)\colvec{a\\b\\c\\d}=:\colvec{a \\ b\\c\\d}_B\, .
$$
The 4-vector $\colvec{a\\b\\c\\d} \in \mathbb{R}^4$ encodes the vector $\begin{pmatrix}a&b\\c&d\end{pmatrix}\in V$ but is NOT equal to it! \\(After all, $v$ is a matrix so could not equal a column vector.) Both notations on the right hand side of the above equation really stand for the vector obtained by multiplying the coefficients stored in the column vector by the corresponding basis element and then summing over them.
\end{example}
 
Next, lets consider a tautological example showing how to label column vectors in terms of column vectors:

\begin{example} (Standard Basis of $\Re^2$)\index{Standard basis!for ${\mathbb R}^2$}\\
The vectors 
$$
e_1=\colvec{1\\0},~~~
e_2=\colvec{0\\1}
$$
are called the standard basis vectors of $\Re^2=\Re^{\{1,2\}}$. 
Their description as functions of~$\{1,2\}$ are 
\begin{displaymath}
   e_1 (k)= \left\{
     \begin{array}{lr}
       1 & {\rm ~if~} k=1\\
       0 & {\rm ~if~} k=2
     \end{array}
   \right. \, ,~~
   e_2 (k)= \left\{
     \begin{array}{ll}
       0 & {\rm ~if~} k=1\\
       1 & {\rm ~if~} k=2\, .
     \end{array}
   \right.
\end{displaymath} 
It is  natural to assign these  the order: $e_1$ is first and $e_2$ is second.
An arbitrary vector~$v$ of $\Re^2$ can be written as  
$$
v=\colvec{x\\y}=x e_1+ ye_2 .
$$
To emphasize that we are using the standard basis we define the list (or ordered set) $$E=(e_1,e_2)\, ,$$ and write 
$$
\colvec{x\\y}_E:=(e_1,e_2)\colvec{x\\y}:=x e_1+ ye_2=v .
$$
You should read this equation by saying:
\begin{quote}
``The column vector of the vector $v$ in the basis $E$ is $\colvec{x\\y}$.''
\end{quote}

Again, the first notation of a column vector with a subscript~$E$ refers to the vector obtained by multiplying each basis vector
by the corresponding scalar listed in the column and then  summing these, {\it i.e.} $xe_1+y e_2$. 
The second notation denotes exactly the same thing but we first list the basis elements and then the column vector; a useful trick
because this can be read in the same way as matrix multiplication of a row vector times a column vector--except that the entries of the row vector are themselves vectors! 
\end{example}

You should already try to write down the standard basis vectors for $\Re^n$ for other values of $n$ and express an arbitrary vector in $\Re^n$  in terms of them.%webwork problem? 

The last example probably seems pedantic because column vectors are already just ordered lists of numbers and the basis notation 
has simply allowed us to ``re-express'' these as lists of numbers. Of course, this objection does not apply to more complicated vector spaces like our first matrix example. Moreover, as we saw \hypertarget{nonstandard r2 basis}{earlier}, there are infinitely many other pairs of vectors in $\Re^2$ that form a basis.

\begin{example}(A Non-Standard Basis of $\Re^2=\Re^{\{1,2\}}$)\\
$$
b=\colvec{1\\1}\, ,\quad \beta=\colvec{1\\-1}.
$$
As functions of $\{1,2\}$ they read
\begin{displaymath}
   b (k)= \left\{
     \begin{array}{lr}
       1 & {\rm ~if~} k=1\\
       1 & {\rm ~if~} k=2
     \end{array}
   \right. \, ,~~
   \beta (k)= \left\{
     \begin{array}{rl}
       1 & {\rm ~if~} k=1\\
       -1 & {\rm ~if~} k=2\, .
     \end{array}
   \right.
\end{displaymath} 

Notice something important: there is no reason to say that $\beta$ comes before $b$ or vice versa. That is, there is no {\it a priori} reason to give these basis elements one order or the other. 
However, it will be necessary to give the basis elements an order if we want to use them to encode other vectors.  We choose one arbitrarily; let $$B=(b,\beta)$$ be the ordered basis. Note that for an unordered set we use the $\{\}$ parentheses while for lists or ordered sets we use $()$. 

As before we define 
$$
\colvec{x\\y}_B :=(b,\beta)\colvec{x \\ y}:= xb+ y \beta\, . 
$$
You might think that the numbers $x$ and $y$ denote exactly the same vector as in the previous example. However, they do not. Inserting
the actual vectors that $b$ and $\beta$ represent we have
$$
xb+ y \beta=x\colvec{1\\1}+y\colvec{1\\-1}=\colvec{x+y\\x-y}\, .
$$
Thus, to contrast, we have
$$
\colvec{x\\y}_B=\colvec{x+y\\x-y} \mbox{ and } \colvec{x\\y}_E=\colvec{x\\y}
$$
Only in the standard basis $E$ does the column vector of $v$ agree with the column vector that $v$ actually is!

%This equation shows how a pair of numbers in ordered basis $B$ gives a pair of numbers in the standard basis $E$. The stacks of numbers we have dealt with in the past were always specifying vectors in the standard basis. 

%The other order for the basis, $B'=(\beta,b)$ yields
%$$
%\colvec{x\\y}_B' := x\beta+ y b =x\colvec{1\\-1}+y \colvec{1\\1}=\colvec{x-y\\x+y}_E.
%$$
%Again, order matters. 
\end{example}

Based on the above example, you might think that our aim would be to find the ``standard basis'' for any problem. 
In fact, this is far from the truth. Notice, for example that the vector $$v=\colvec{1\\1}=e_1+e_2=b$$ written in the standard basis~$E$ is just
$$
v=\colvec{1\\1}_E\, ,
$$
which was easy to calculate. But in the basis~$B$ we find
$$
v=\colvec{1\\0}_B\, ,
$$
which is actually a simpler column vector!
The fact that there are many bases for any given vector space allows us to choose a basis in which our computation is easiest. 
In any case, the standard basis only makes sense for $\Re^n$. Suppose your vector space was the set of solutions to a differential equation--what would a standard basis then be?



\begin{example} (A Basis For a Hyperplane)\\
Lets again consider the hyperplane
$$
V=\left\{  c_1\colvec{1\\1\\0} +c_2\colvec{0\\1\\1} \middle| c_1,c_2\in \Re \right\} 
$$
One possible choice of ordered basis is 
$$b_1=\colvec{1\\1\\0},~~~b_2=\colvec{0\\1\\1}, ~~~
B=(b_1,b_2).
$$
With this choice
$$
\colvec{x\\y}_B := xb_1+ y b_2 
=x\colvec{1\\1\\0}+y \colvec{0\\1\\1}=\ccolvec{x\\x+y\\y}_E.
$$
With the other choice of order $B'=(b_2, b_1)$ 
$$
\colvec{x\\y}_{\!B'} := xb_2+ y b_1 
=x\colvec{0\\1\\1}+y \colvec{1\\1\\0}=\ccolvec{y\\x+y\\x}_E.
$$
We see that the  order of basis elements matters. 
\end{example}

Finding the column vector of a given vector in a given basis usually amounts to a linear systems problem:

\begin{example} (Pauli Matrices)\\
Let $$V=\left\{\begin{pmatrix}z&u\\v&-z\end{pmatrix}\middle| z,u,v\in{\mathbb C}\right\}$$\
be the vector space of \hyperlink{TRACE}{trace}-free complex-valued matrices (over~${\mathbb C}$) with basis
$$
B=(\sigma_x,\sigma_y,\sigma_z)\, ,
$$ where
$$
\sigma_x=\begin{pmatrix}0&1\\1&0\end{pmatrix}\, ,\quad
\sigma_y=\begin{pmatrix}0&-i\\i&0\end{pmatrix}\, ,\quad
\sigma_z=\begin{pmatrix}1&0\\0&-1\end{pmatrix}\, .
$$
These three matrices are the famous {\it Pauli matrices}\index{Pauli Matrices}; they are used to describe electrons in quantum theory, or qubits in quantum computation.
Let 
$$
v=\begin{pmatrix}
-2+i&1+i\\3-i&2-i
\end{pmatrix}\, .
$$
Find the column vector of $v$ in the basis~$B$.

For this we must solve the equation
$$
\begin{pmatrix}
-2+i&1+i\\3-i&2-i
\end{pmatrix}
=\alpha^x \begin{pmatrix}0&1\\1&0\end{pmatrix}+\alpha^y
\begin{pmatrix}0&-i\\i&0\end{pmatrix}+\alpha^z
\begin{pmatrix}1&0\\0&-1\end{pmatrix}\, .
$$
This gives four equations, {\it i.e.} a linear systems problem, for the $\alpha$'s
$$
\left\{
\begin{array}{rrrrr}
\alpha^x&\!\!-\ i\alpha^y&&=&1+i\\
\alpha^x&\!\!+\ i\alpha^y&&=&3-i\\
&&\alpha^z&=&-2+i\\
&&-\alpha^z&=&2-i\\
\end{array}
\right.
$$
with solution
$$
\alpha^x=2\, ,\quad \alpha^y=2-2i\, ,\quad \alpha^z=-2+i\, .
$$
Thus
$$
v=\begin{pmatrix}
-2+i&1+i\\3-i&2-i
\end{pmatrix}=\ccolvec{2\\\ 2-i\\\!\!-2+i }_B\, .
$$
\end{example}

To summarize, the {\it column vector 
of a vector}\index{Column vector! of a vector} $v$ in an ordered basis $B=(b_1,b_2,\ldots,b_n)$,
$$\ccolvec{\alpha^1\\\alpha^2\\\vdots\\\alpha^n}\, ,$$ is defined by solving the linear systems problem
$$
v=\alpha^1 b_1 + \alpha^2b_2 +\cdots + \alpha^n b_n = \sum_{i=1}^n \alpha^i b_i\, .
$$
The numbers $(\alpha^1,\alpha^2,\ldots,\alpha^n)$ are called the {\it components of the vector}\index{Components of a vector}~$v$.
Two useful shorthand notations for this are
$$
v=\ccolvec{\alpha^1\\\alpha^2\\\vdots\\\alpha^n}_B = (b_1,b_2,\ldots,b_n)\ccolvec{\alpha^1\\\alpha^2\\\vdots\\\alpha^n}\, .
$$

\subsection{From Linear Operators to Matrices}

Chapter~\ref{sec:linearTransformation} showed that linear functions are very special kinds of functions; 
they are fully specified by their values on any basis for their domain. 
A matrix  records how a linear operator maps an element of the basis to a sum of multiples in the target space basis.

More carefully, if $L$ is a linear operator from $V$ to $W$ 
then the matrix for~$L$ in the ordered bases $B=(b_1,b_2,\dots)$ for $V$ and $B'=(\beta_1,\beta_2,\dots)$ for~$W$, is the
 array of numbers %$M$ specified by 
% $$
% Lv_B=(Mv)_{B'}
% $$
%
 $m_{i}^j$ specified by 
$$L(b_i)= m_{i}^1\beta_1^{\phantom{1}}+\dots +m_{i}^j \beta_j^{\phantom{1}}+\cdots$$

\begin{remark}
To calculate the matrix of a linear transformation you must compute what the linear transformation does to every input basis vector and then write the answers in terms of the output basis vectors:
\begin{equation*}
\begin{split}
\big(L(b_1)&,L(b_2),\ldots,L(b_j),\ldots\big)\\[3mm]
&
=\scalebox{.76}{$
\Big((\beta_1,\beta_2,\ldots,\beta_j,\ldots)\ccolvec{m^1_1\\[.5mm]m^2_2\\\vdots\\m^j_1\\\vdots},
(\beta_1,\beta_2,\ldots,\beta_j,\ldots)\ccolvec{m^1_2\\[.5mm]m^2_2\\\vdots\\m^j_2\\\vdots},\cdots,
(\beta_1,\beta_2,\ldots,\beta_j,\ldots)\ccolvec{m^1_i\\[.5mm]m^2_i\\\vdots\\m^j_i\\\vdots},\cdots\Big)$}\\[3mm]
&
=(\beta_1,\beta_2,\ldots,\beta_j,\ldots)\begin{pmatrix}
m_1^1 & m_2^1 & \cdots & m_i^1 &\cdots\\[.5mm]
m_1^2 & m_2^2 & \cdots & m_i^2 &\cdots\\
\multicolumn{1}{c}{\vdots}& \multicolumn{1}{c}{\vdots} &   & \multicolumn{1}{c}{\vdots} &\\
m_1^j & m_2^j & \cdots & m_i^j& \cdots \\
\multicolumn{1}{c}{\vdots}&\multicolumn{1}{c}{\vdots}&&\multicolumn{1}{c}{\vdots}
\end{pmatrix}
\end{split}
\end{equation*}
\end{remark}


\begin{example}
Consider 
$L:V\to \Re^3$ (as in example~\ref{Vdef}) defined by 
$$
L\colvec{1\\1\\0 } = \colvec{0\\1\\0}\, ,\quad
L\colvec{0\\1\\1 } = \colvec{0\\1\\0}\, .
$$
By linearity this specifies the action of $L$ on any vector from $V$ as
$$
L\left[ c_1\colvec{1\\1\\0 } + c_2 \colvec{0\\1\\1} \right]= (c_1+c_2)\colvec{0\\1\\0}.
$$
We had trouble expressing this linear operator as a matrix. Lets take input basis
$$
B=\left(\colvec{1\\1\\0 } ,\colvec{0\\1\\1 }\right)=:(b_1,b_2)\, ,
$$
and output basis 
$$
E=\left(\colvec{1\\0\\0 } ,\colvec{0\\1\\0 } ,\colvec{0\\0\\1 }\right)\, .
$$
Then
$$
L b_1 = 0 e_1 + 1e_2+ 0  e_3 \, ,
$$
$$
L b_2 = 0 e_1 + 1e_2+ 0  e_3 \, ,
$$
or 
$$
\big(Lb_1, L b_2) = \big( (e_1,e_2,e_3)\colvec{0\\1\\0}, (e_1,e_2,e_3)\colvec{0\\1\\0}\big)=(e_1,e_2,e_3)\begin{pmatrix}0&0\\1&1\\0&0
\end{pmatrix}\, .
$$
The matrix on the right is the matrix of $L$ in these bases.
More succinctly we could write
$$
L\colvec{x\\y}_B=(x+y) \colvec{0\\1\\0}_E
$$
and thus see that $L$ acts like the matrix 
$
\begin{pmatrix}
0&0\\
1&1\\
0&0
\end{pmatrix}
$.

Hence
$$L\colvec{x\\y}_B
=\left(  \begin{pmatrix}
0&0\\
1&1\\
0&0
\end{pmatrix}
\colvec{x\\y} \right)_E\, ;
$$
given input and output bases, the linear operator is now encoded by a matrix.
\end{example}
This is the general rule for this chapter:



\begin{center}
\shabox{
Linear operators become matrices when given ordered~input~and~output~bases. }
\end{center}

\Reading{Matrices}{1}

\begin{example} Lets compute a matrix for the derivative operator acting on the vector space of  polynomials of degree 2 or less:
\[
V = \{a_01 + a_1x + a_2 x^2 \,|\,  a_0,a_1,a_2 \in \Re \}\, .
\]
In the ordered basis $B=( 1,x,x^2)$ we write 
\[
\colvec{a\\b\\c}_{\!B}= a\cdot 1 + b x + c x^2
\]
and
\[
\frac{d}{dx}  \colvec{a\\b\\c}_{\!B}=b\cdot 1 +2c x +0x^2=\ccolvec{b\\2c\\0}_B
\]
In the ordered basis $B$ for both domain and range
\[
\frac{d}{dx} 
\stackrel{B}{\mapsto}
\begin{pmatrix}
0&1&0\\
0&0&2\\
0&0&0
\end{pmatrix}\]
Notice this last line makes no sense without explaining which bases we are using!
\end{example}







%\section*{References}
%Hefferon, Chapter Three, Section IV, parts 1-3.
%\\
%Beezer, Chapter M, Section MM.
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Matrix_multiplication}{Matrix Multiplication}
%\end{itemize}


\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}\hline
Reading problem&\hwrref{Matrices}{1}
\\
Matrix of a Linear Transformation & \hwref{Matrices}{9}, \hwref{Matrices}{10},
 \hwref{Matrices}{11},
 \hwref{Matrices}{12},
 \hwref{Matrices}{13}\\\hline
\end{tabular}


\input{\matricesPath/problems}


\section{\propMatricesTitle} \label{properties_matrices}

The objects of study in linear algebra are linear operators. 
We have seen that linear operators can be represented as matrices through choices of ordered bases, and that matrices provide a means of efficient computation. 

We now begin an in depth study of matrices.

\begin{definition}
An $r\times k$ {\bf matrix}\index{Matrix} $M=(m^i_j)$ for $i=1, \ldots, r; j=1, \ldots, k$ is a rectangular array of real (or complex) numbers:
\label{matrixnotation}
\[M = 
\begin{pmatrix}
m_1^1 & m_2^1 & \cdots & m_k^1 \\[1mm]
m_1^2 & m_2^2 & \cdots & m_k^2 \\
\multicolumn{1}{c}{\vdots} & \multicolumn{1}{c}{\vdots} &   & \multicolumn{1}{c}{\vdots} \\
m_1^r & m_2^r & \cdots & m_k^r \\
\end{pmatrix}\, .
\]
The numbers $m^i_j$ are called {\bf entries}\index{Matrix!entries of}.  The superscript indexes the row of the matrix and the subscript indexes the column of the matrix in which $m_j^i$ appears.
\end{definition}

%It is often useful to consider matrices whose entries are more general than the real numbers, so we allow that possibility.

An $r\times 1$ matrix $v = (v^r_1) = (v^r)$ is called a \emph{column vector}\index{Column vector}, written $$v = \ccolvec{v^1\\v^2\\ \vdots \\ v^r }\, .$$  A $1\times k$ matrix $v = (v^1_k) = (v_k)$ is called a \emph{row vector}\index{Row vector}, written $$v = \rowvec{v_1 & v_2 & \cdots & v_k }\, .$$  
The {\it transpose of a column vector}\index{Transpose!of a column vector} is the corresponding row vector and vice versa:

\begin{example}
Let 
$$
v=\colvec{1\\2\\3}\, .
$$
Then
$$
v^T=\rowvec{1 &2 &3}\, ,
$$
and $(v^T)^T=v$. This is an example of an {\it involution}\index{Involution}, namely an operation which when performed twice does nothing.
\end{example}

A matrix is  an efficient way to store information.

\begin{example}
In computer graphics, you may have encountered image files with a .gif extension.  These files are actually just matrices: at the start of the file the size of the matrix is given, after which each number is a matrix entry  indicating the color of a particular pixel in the image.

This matrix  then has its rows shuffled a bit: by listing, say, every eighth row, a web browser downloading the file can start displaying an incomplete version of the picture before the download is complete.

Finally, a compression algorithm is applied to the matrix to reduce the file size.
\end{example}



\begin{example}
Graphs occur in many applications, ranging from telephone networks to airline routes.  In the subject of \emph{graph theory}\index{Graph theory}, a graph is just a collection of vertices and some edges connecting vertices.  A matrix can be used to indicate how many edges attach one vertex to another.

\begin{center}
\includegraphics[width=10cm]{notes8-0.png}
\end{center}
For example, the graph pictured above would have the following matrix, where $m^i_j$ indicates the number of edges between the vertices labeled $i$~and~$j$:

\[
M = \begin{pmatrix}
1 & 2 & 1 & 1 \\
2 & 0 & 1 & 0 \\
1 & 1 & 0 & 1 \\
1 & 0 & 1 & 3 \\
\end{pmatrix}
\]
This is an example of a \emph{symmetric matrix}, since $m_j^i = m_i^j$.

\Videoscriptlink{matrices_example.mp4}{Adjacency Matrix Example}{scripts_matrices_example}
\end{example}

The set of all $r\times k$ matrices 
$${\mathbb M}_k^r:=\{(m^i_j)|m^i_j\in {\mathbb R};\,  
i\in \{1,\dots,r\} ;\, 
j\in \{1\dots k\} \}\, ,$$ 
is itself  a vector space with  addition and scalar multiplication defined as follows:

\Shabox{1}{\begin{tabular}{c}
$M+N = (m_j^i) + (n_j^i) = ( m_j^i + n_j^i )$\\[2mm]
$rM = r(m_j^i) = (rm_j^i)$\end{tabular}}

In other words, addition just adds corresponding entries in two matrices, and scalar multiplication multiplies every entry.
Notice that $M_1^n = \Re^n$ is just the vector space of column vectors.

Recall that we can multiply an \(r \times k\) matrix by a \(k \times 1\) column vector to produce a \(r \times 1\) column vector using the rule
\[MV = \big(\sum_{j=1}^k m_j^i v^j\big)\, .\]

This suggests the rule for multiplying an \(r \times k\) matrix \(M\) by a \(k \times s\) matrix~\(N\): our \(k \times s\) matrix \(N\) consists of \(s\) column vectors side-by-side, each of dimension \(k \times 1.\) We can multiply our \(r \times k\) matrix \(M\) by each of these \(s\) column vectors using the rule we already know, obtaining \(s\) column vectors each of dimension \(r \times 1.\) If we place these \(s\) column vectors side-by-side, we obtain an \(r \times s\) matrix \(MN.\)

That is, let \[N = 
\begin{pmatrix}
n_1^1 & n_2^1 & \cdots & n_s^1 \\
n_1^2 & n_2^2 & \cdots & n_s^2 \\
\multicolumn{1}{c}{\vdots} & \multicolumn{1}{c}{\vdots} &   & \multicolumn{1}{c}{\vdots} \\
n_1^k & n_2^k & \cdots & n_s^k \\
\end{pmatrix}
\]
and call the columns \(N_1\) through \(N_s\):
\[N_1 = \ccolvec{n_1^1\\n_1^2\\\vdots\\n_1^k}\, ,\:
N_2 = \ccolvec{n_2^1\\n_2^2\\\vdots\\n_2^k}\, ,\:
\ldots,\:
N_s = \ccolvec{n_s^1\\n_s^2\\\vdots\\n_s^k}.
\]
Then
%\[
%MN=M \rowvec{N_1 & N_2 & \cdots & N_s} = \rowvec{MN_1 & MN_2 & \cdots & MN_s}.
%\]
\[
MN=M
\begin{pmatrix}
\multicolumn{1}{c}{|} & \multicolumn{1}{c}{|} & & \multicolumn{1}{c}{|} \\
N_1 & N_2 & \cdots & N_s \\
\multicolumn{1}{c}{|} & \multicolumn{1}{c}{|} & & \multicolumn{1}{c}{|} \\
\end{pmatrix}
=
\begin{pmatrix}
\multicolumn{1}{c}{|} & \multicolumn{1}{c}{|} & & \multicolumn{1}{c}{|} \\
MN_1 & MN_2 & \cdots & MN_s \\
\multicolumn{1}{c}{|} & \multicolumn{1}{c}{|} & & \multicolumn{1}{c}{|} \\
\end{pmatrix}
\]

Concisely: If \(M=(m^i_j)\) for \(i=1, \ldots, r; j=1, \ldots, k\) and \(N=(n^i_j)\) for \(i=1, \ldots, k; j=1, \ldots, s,\) then \(MN=L\) where \(L=(\ell^i_j)\) for \(i=i, \ldots, r; j=1, \ldots, s\) is given by
\[
\ell^i_j = \sum_{p=1}^k m^i_p n^p_j.
\]
This rule obeys linearity.

Notice that in order for the multiplication to make sense, the columns and rows must match.  For an $r\times k$ matrix $M$ and an $s\times m$ matrix $N$, then to make the product $MN$ we must have $k=s$.  Likewise, for the product $NM$, it is required that $m=r$.  A common shorthand for keeping track of the sizes of the matrices involved in a given product is the following diagram.
\Shabox{1.05}{$\Big(r \times k\Big) { \text ~{\rm times}~} \Big(k\times m\Big) {\text ~{\rm is}~} \Big(r\times m\Big)$}

\Reading{Matrices}{2}


\begin{example}
Multiplying a $(3\times 1)$ matrix and a $(1\times 2)$ matrix yields a $(3\times 2)$ matrix.

\[
\colvec{1\\3\\2} \rowvec{2 & 3} = 
\begin{pmatrix}
1\cdot 2 & 1\cdot 3 \\
3\cdot 2 & 3\cdot 3 \\
2\cdot 2 & 2\cdot 3 \\
\end{pmatrix}
= \begin{pmatrix}
2 & 3 \\
6 & 9 \\
4 & 6 
\end{pmatrix} .
\]
\end{example}

Another way to view matrix multiplication is in terms of dot products:

\begin{center}
\shabox{The entries of $MN$ are made from the dot products of the rows of $M$ with the columns of $N$.}
\end{center}


\begin{example}
Let $$M=\begin{pmatrix}1&3\\3&5\\2&6\end{pmatrix}=:\ccolvec{u^T\\v^T\\w^T}
\mbox{ and }
N=\begin{pmatrix}2&3&1\\0&1&0\end{pmatrix}=:\rowvec{a & b & c}$$
where
$$
u=\colvec{1\\3}\, ,\quad
v=\colvec{3\\5}\, ,\quad 
w=\colvec{2\\6}\, ,\quad
a=\colvec{2\\0}\, ,\quad
b=\colvec{3\\1}\, ,\quad 
c=\colvec{1\\0}\, .
$$
Then 
$$
MN=\left(\!\begin{array}{ccc}
u\cdot a & u\cdot b & u\cdot c\\
v\cdot a & v\cdot b & v\cdot c\\
w\cdot a & w\cdot b & w\cdot c\\ 
\end{array}\!\right)
=
\begin{pmatrix}
2&6&1\\
6&14&3\\
4&12&2
\end{pmatrix}\, .
$$
\end{example}
This fact has an obvious yet important consequence:

\begin{theorem}
Let $M$ be a matrix and $x$ a column vector. If 
$$
Mx=0
$$
then the vector $x$ is \hyperlink{orthog}{orthogonal} to the rows of $M$.
\end{theorem}

\begin{remark}
Remember that the set of all vectors that can be obtained by adding up scalar multiples of the columns of a matrix is called its \hyperlink{column space}{column space}~\index{Column Space!concept of}. Similarly the {\bf row space}\index{Row Space} is the set of all row vectors obtained by adding up multiples of the rows of a matrix. The above theorem says that if $Mx=0$, then the vector $x$ is orthogonal to every vector in the row space of $M$.
\end{remark}


%\begin{center}\href{\webworkurl ReadingHomework8/1/}{Reading homework: problem 8.1}\end{center}

We know that $r\times k$ matrices can be used to represent linear transformations 
$ \Re^k \rightarrow \Re^r $
via $$(MV)^i = \sum_{j=1}^{k} m_j^iv^j , $$ which is the same rule used when we multiply an $r\times k$ matrix by a $k\times 1$ vector to produce an $r\times1$ vector.

Likewise, we can use a matrix $N=(n^i_j)$ to define a linear transformation of a vector space of matrices. For example
\[
L \colon M^s_k \stackrel{N}{\longrightarrow} M^r_k\, ,
\]
\[
L(M)=(l^i_k) \mbox{ where } l^i_k= \sum_{j=1}^{s} n_j^im^j_k.
\]
This is the same as the rule we use to multiply matrices. \hypertarget{leftmult}{In other words,} \(L(M)=NM\) is a linear transformation.

\begin{definition}[Matrix Terminology]  
Let $M=(m^i_j)$ be a matrix. The entries $m_i^i$ are called {\bf diagonal}, and the set $\{m_1^1$, $m_2^2$, $\ldots \}$~is called the {\bf diagonal} of the matrix\index{Matrix!diagonal of}.

Any $r\times r$ matrix is called a {\bf square matrix}\index{Square matrix}.  A square matrix that is zero for all non-diagonal entries is called a {\bf diagonal matrix}\index{Diagonal matrix}. An example of a square diagonal matrix is
$$\begin{pmatrix}
2 & 0 & 0\\
0 & 3 & 0\\
0 & 0 & 0\\
\end{pmatrix}\, .$$

The $r\times r$ diagonal matrix with all diagonal entries equal to $1$ is called the {\bf identity matrix}\index{Identity matrix}, $I_r$, or just $I$.  An identity matrix looks like \[ I=
\begin{pmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\multicolumn{1}{c}{\vdots} & \multicolumn{1}{c}{\vdots} & \multicolumn{1}{c}{\vdots} & \ddots & \multicolumn{1}{c}{\vdots} \\
0 & 0 & 0 & \cdots & 1
\end{pmatrix}.
\]
The identity matrix is special because $$I_rM=MI_k=M$$ for all $M$ of size $r\times k$.
\end{definition}



\begin{definition}
The {\bf transpose}\index{Transpose} of an $r\times k$ matrix $M = (m_j^i)$ is the $k\times r$ matrix 
\[
M^T = (\hat{m}_j^i)
\]
with entries that satisfy $\hat{m}_j^i = m_i^j$. \\

A matrix $M$ is \emph{symmetric}\index{Symmetric matrix} if $M=M^T$.
\end{definition}

\begin{example}
$$\begin{pmatrix}
2 & 5 & 6\\
1 & 3 & 4\\
\end{pmatrix}^T = 
\begin{pmatrix}
2 & 1 \\
5 & 3 \\
6 & 4 \\
\end{pmatrix}\, ,$$
and
$$
\begin{pmatrix}
2 & 5 & 6\\
1 & 3 & 4\\
\end{pmatrix}
\begin{pmatrix}
2 & 5 & 6\\
1 & 3 & 4\\
\end{pmatrix}^T = 
\begin{pmatrix}
65&43\\43&26
\end{pmatrix}\, ,$$
is symmetric.


\end{example}

%\begin{center}\href{\webworkurl ReadingHomework8/2/}{Reading homework: problem 8.2}\end{center}
\Reading{Matrices}{3}

\begin{remark}[Observations]

$\phantom{test}$

\begin{itemize}
\item Only square matrices can be symmetric.

\item The transpose of a column vector is a row vector, and vice-versa. 

\item Taking the transpose of a matrix twice does nothing.  \emph{i.e.,} $(M^T)^T=M$.
\end{itemize}
\end{remark}

\begin{theorem}[Transpose and Multiplication]
Let $M, N$ be matrices such that $MN$ makes sense.  Then 
\Shabox{1.05}{$(MN)^T = N^TM^T$.}
\end{theorem}
The proof of this theorem is left to \hyperref[MN=NTMT]{Review Question~\ref*{MN=NTMT}}.

%%%%%%%%%%%%%%%%%%%
\subsection{Associativity and Non-Commutativity }

Many properties of matrices following from the same property for real numbers. Here is an example.

\begin{example}
{\it Associativity of matrix multiplication.} We know for real numbers $x$, $y$ and $z$ that 
$$
x(yz)=(xy)z\, ,
$$
{\it i.e.}, the order of multiplications does not matter. The same property holds for matrix multiplication, let us show why.
Suppose $M=\big( m^i_j \big)$, $N=\big( n^j_k \big)$ and  $R=\big( r^k_l \big)$ are, 
respectively, $m\times n$, $n\times r$ and $r\times t$ matrices. Then from the rule for matrix
multiplication we have
$$
MN=\Big(\sum_{j=1}^n m^i_j n^j_k\Big)\mbox{ and } NR=\Big(\sum_{k=1}^r n^j_k r^k_l\Big)\, .
$$
So first we compute 
$$
(MN)R=\Big(\sum_{k=1}^r \Big[\sum_{j=1}^n m^i_j n^j_k\Big] r^k_l \Big) = 
\Big(\sum_{k=1}^r \sum_{j=1}^n \Big[ m^i_j n^j_k\Big] r^k_l \Big) =\Big(\sum_{k=1}^r \sum_{j=1}^n m^i_j n^j_k r^k_l \Big)\, .
$$
In the first step we just wrote out the definition for matrix multiplication, in the second step we
moved summation symbol outside the bracket (this is just the distributive
property $x(y+z)=xy+xz$ for numbers) and
in the last step we used the associativity property for real numbers to remove the square brackets. 
Exactly the same reasoning shows that
$$
M(NR)=\Big(\sum_{j=1}^n m^i_j\Big[\sum_{k=1}^r n^j_k r^k_l\Big]\Big) = 
\Big(\sum_{k=1}^r \sum_{j=1}^n  m^i_j \Big[n^j_kr^k_l \Big] \Big) =\Big(\sum_{k=1}^r \sum_{j=1}^n m^i_j n^j_k r^k_l \Big)\, .
$$
This is the same as above so we are done. \footnote{As a fun remark, note that Einstein would simply have written\\
$(MN)R=(m^i_j n^j_k) r^k_l= m^i_j n^j_k r^k_l = m^i_j (n^j_k r^k_l ) = M(NR)$.}
\end{example}
%%%%%%%%%%%%%% non-commute


Sometimes matrices do not share the properties of regular numbers. 
In particular, for {\it generic} $n\times n$ square matrices $M$ and $N$, 
\begin{center}
\shabox{\scalebox{1.1}{
$MN\neq NM\, .$ }}
\end{center}

\Videoscriptlink{matrices_commute.mp4}{Do Matrices Commute?}{script_matrices_commute}


\begin{example} (Matrix multiplication does \emph{not} commute.)
\[
\begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
1 & 1 \\
\end{pmatrix} =
\begin{pmatrix}
2 & 1 \\
1 & 1 \\
\end{pmatrix}
\]
while, on the other hand,
\[
\begin{pmatrix}
1 & 0 \\
1 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix} =
\begin{pmatrix}
1 & 1 \\
1 & 2 \\
\end{pmatrix}\, .
\]
\end{example}
Since $n\times n$ matrices are linear transformations $\Re^n \rightarrow \Re^n$, we can see that the order of successive linear transformations matters.  
%In general  for two linear transformations $K$ and $L$ taking $\Re^n \rightarrow \Re^n$, and $v\in \Re^n$,  one has
%$$K(L(v)) \neq L(K(v))\, .$$
%Finding matrices such that $MN=NM$ is an important problem in mathematics.
%We promise

Here is an example of matrices acting on objects in three dimensions that also shows matrices not commuting.
\begin{example}
In \hyperlink{rotationprob}{Review Problem}~\ref{rotate}, you learned that the matrix
$$M=\begin{pmatrix}\cos\theta & \sin\theta \\ -\sin \theta & \cos\theta\end{pmatrix}\, ,$$
rotates vectors in the plane by an angle~$\theta$. 
We can generalize this, using \hyperlink{blocks}{block matrices}, to three dimensions.
In fact the following matrices built from a $2\times 2$ rotation matrix, a $1\times 1$ identity matrix and zeroes everywhere else
$$
M=\begin{pmatrix}\cos\theta & \sin\theta &0\\ -\sin \theta & \cos\theta&0\\\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0}&1\end{pmatrix}\qquad\mbox{and}\qquad
N=\begin{pmatrix}1&\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0}\\0&\cos\theta & \sin\theta \\ 0&-\sin \theta & \cos\theta\end{pmatrix}\, ,
$$
perform rotations by an angle $\theta$ in the $xy$ and $yz$ planes, respectively. Because, they rotate single vectors, you can also use them to rotate objects built from a collection of vectors like pretty colored blocks! Here is a picture of $M$ and then $N$ acting on such a block, compared with the case of $N$ followed by $M$. The special case of $\theta=90^\circ$ is shown.
\begin{center}
\includegraphics[scale=.3]{MNNM.jpg}
\end{center}
Notice how the endproducts of $MN$ and $NM$ are different, so $MN\neq NM$ here.
\end{example}



\subsection{Block Matrices}

It is often convenient to partition a matrix $M$ into smaller matrices called \hypertarget{blocks}{\emph{blocks}}. For example 

\[
M=\begin{pmat}{ccc|c}
1 & 2 & 3 & 1 \\
4 & 5 & 6 & 0 \\
7 & 8 & 9 & 1 \\
\cline{1-4}
0 & 1 & 2 & 0 \\
\end{pmat}
=
\begin{pmat}{c|c}
A & B \\
\cline{1-2}
C & D \\
\end{pmat}
\]
Where $A = \begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{pmatrix}$, $B=\colvec{1\\0\\1}$, $C=\rowvec{0 & 1 & 2 }$, $D=(0)$.

\begin{itemize}
\item The blocks of a block matrix\index{Block matrix}  must fit together to form a rectangle.  So 
$\begin{pmat}{c|c}
B & A \\
\cline{1-2}
D & C \\
\end{pmat}
$ makes sense, but 
$\begin{pmat}{c|c}
C & B \\
\cline{1-2}
D & A \\
\end{pmat}
$ does not.

%\href{\webworkurl ReadingHomework9/1/}{Reading homework: problem 9.1}
\Reading{Matrices}{4}

\item There are many ways to cut up an $n\times n$ matrix into blocks.  Often context or the entries of the matrix will suggest a useful way to divide the matrix into blocks.  For example, if there are large blocks of zeros in a matrix, or blocks that look like an identity matrix, it can be useful to partition the matrix accordingly.

\item Matrix operations on block matrices can be carried out by treating the blocks as matrix entries.  In the example above,
\begin{eqnarray*}
M^2 & = & \begin{pmat}{c|c}
A & B \\
\cline{1-2}
C & D \\
\end{pmat}
\begin{pmat}{c|c}
A & B \\
\cline{1-2}
C & D \\
\end{pmat} \\[2mm]
& = & \begin{pmat}{c|c}
A^2+BC & AB+BD \\
\cline{1-2}
CA+DC & CB+D^2 \\
\end{pmat} \\
\end{eqnarray*}

Computing the individual blocks, we get:
\begin{eqnarray*}
A^2+BC &=& \begin{pmatrix}
	30 & 37 & 44 \\
	66 & 81 & 96 \\
	102&127 &152 \\
	\end{pmatrix} \\
AB+BD  &=& \colvec{ 4 \\ 10 \\ 16 } \\
CA+DC  &=& \rowvec{ 4 & 10 & 16 } \\
CB+D^2 &=& (2) 
\end{eqnarray*}

Assembling these pieces into a block matrix gives:
\[
\begin{pmat}{ccc|c}
30 & 37 & 44 & 4 \\
66 & 81 & 96 & 10 \\
102 & 127 & 152 & 16 \\
\cline{1-4}
4 & 10 & 16 & 2 \\
\end{pmat}
\]

This is exactly $M^2$.
\end{itemize}

\subsection{The Algebra of Square Matrices }\index{Square matrices}

Not every pair of matrices can be multiplied.  When multiplying two matrices, the number of rows in the left matrix must equal the number of columns in the right.  For an $r\times k$ matrix $M$ and an $s\times l$ matrix $N$, then we must have~$k=s$.

This is not a problem for square matrices of the same size, though.  Two~$n\times n$ matrices can be multiplied in either order.  For a single matrix $M \in \mathbb{M}^n_n$, we can form $M^2=MM$, $M^3=MMM$, and so on. It is useful to define $$M^0=I\, ,$$ the identity matrix, just like $x^0=1$ for numbers.

As a result, any polynomial  can be have square matrices in it's domain. 

\begin{example}
Let $f(x) = x - 2x^2 + 3x^3$
and $$M=\begin{pmatrix}
1 & t \\
0 & 1 \\
\end{pmatrix}\, .$$  
Then 
\[
M^2 = \begin{pmatrix}
1 & 2t \\
0 & 1 \\
\end{pmatrix}\, ,\:\:
M^3 = \begin{pmatrix}
1 & 3t \\
0 & 1 \\
\end{pmatrix}\, ,\: \ldots
\]
and so 
\begin{eqnarray*}
f(M) &=& \begin{pmatrix}
	1 & t \\
	0 & 1 \\
	\end{pmatrix} 
- 2 \begin{pmatrix}
	1 & 2t \\
	0 & 1 \\
	\end{pmatrix} 
+ 3 \begin{pmatrix}
	1 & 3t \\
	0 & 1 \\
	\end{pmatrix} \\
&=& \begin{pmatrix}
	2 & 6t \\
	0 & 2 \\
	\end{pmatrix}.
\end{eqnarray*}
\end{example}

Suppose $f(x)$ is any function defined by a convergent Taylor Series:
\[
f(x) = f(0) + f'(0)x + \frac{1}{2!}f''(0)x^2 + \cdots\, .
\]
Then we can define the matrix function by just plugging in $M$:
\[
f(M) = f(0) + f'(0)M + \frac{1}{2!}f''(0)M^2 + \cdots\, .
\]
There are additional techniques to determine the convergence of Taylor Series of matrices, based on the fact that the convergence problem is simple for diagonal matrices.  It also turns out that the matrix exponential\index{Matrix exponential}
$$\exp (M) = I + M + \frac{1}{2}M^2 + \frac{1}{3!}M^3 + \cdots\, ,$$ always converges.

\Videoscriptlink{properties_of_matrices_example.mp4}{Matrix Exponential Example}{properties_of_matrices_example}


\subsection{Trace}\index{Trace}

A large matrix contains a great deal of information, some of which often reflects the fact that you have not set up your problem efficiently. For example, a clever choice of basis can often make the matrix of a linear transformation very simple. Therefore, finding ways to extract the essential information of a matrix is useful. Here we need to assume that $n < \infty$ otherwise there are subtleties with convergence that we'd have to address.

\begin{definition}
The \hypertarget{TRACE}{{\bf trace}} of a square matrix $M=(m_j^i)$ is the sum of its diagonal entries:
\[
\tr M = \sum_{i=1}^{n}m_i^i\, .
\]
\end{definition}

\begin{example}
\[
\tr \begin{pmatrix}
2 & 7 & 6\\
9 & 5 & 1\\
4 & 3 & 8\\
\end{pmatrix} = 2+5+8 = 15\, .
\]
\end{example}
While matrix multiplication does not commute, the trace of a product of matrices does not depend on the order of multiplication:

\begin{eqnarray*}
\tr(MN) & = & \tr( \sum_l M_l^i N_j^l ) \\
& = & \sum_i \sum_l M_l^i N_i^l \\
& = & \sum_l \sum_i N_i^l M_l^i \\
& = & \tr( \sum_i N_i^l M_l^i ) \\
& = & \tr( NM ).
\end{eqnarray*}
\Videoscriptlink{properties_of_matrices_trace_proof.mp4}{Proof Explanation}{scripts_properties_of_matrices_trace_proof}
Thus we have a Theorem:
\begin{theorem} For any square matrices $M$ and $N$ $$\tr(MN)=\tr(NM).$$
\end{theorem}

\begin{example}
Continuing from the previous example, 

\[
M= \begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix}, N=
\begin{pmatrix}
1 & 0 \\
1 & 1 \\
\end{pmatrix}.
\]
so
\[
MN = \begin{pmatrix}
2 & 1 \\
1 & 1 \\
\end{pmatrix} \neq
NM = \begin{pmatrix}
1 & 1 \\
1 & 2 \\
\end{pmatrix}.
\]
However, $\tr(MN) = 2+1 = 3 = 1+2 = \tr(NM)$.
\end{example}

Another useful property of the trace is that:
\[\tr M = \tr M^T\] 
This is true because the trace only uses the diagonal entries, which are fixed by the transpose.  For example, 
$$\tr \begin{pmatrix}
1 & 1 \\
2 & 3 \\
\end{pmatrix} = 4 = \tr \begin{pmatrix}
1 & 2 \\
1 & 3 \\
\end{pmatrix} = \tr \begin{pmatrix}
1 & 2 \\
1 & 3 \\
\end{pmatrix}^T\, .
$$
Finally, trace is a linear transformation from matrices to the real numbers.  This is easy to check.

%\Videoscriptlink{properties_of_matrices_trace.mp4}{More on the trace function}{scripts_properties_of_matrices_trace}

%\begin{remark}[Linear Systems Redux]
%
%Recall that we can view a linear system as a matrix equation
%\[MX=V,\] 
%with $M$ an $r\times k$ matrix of coefficients, $X$ a $k\times 1$ matrix of unknowns, and $V$ an $r\times 1$ matrix of constants.  If $M$ is a square matrix, then the number of equations~$r$ is the same as the number of unknowns~$k$, so we have hope of finding a single solution.
%
%Above we discussed functions of matrices.  An extremely useful function would be $f(M)=\frac{1}{M}$, where $M\frac{1}{M}=I$.  If we could compute $\frac{1}{M}$, then we would multiply both sides of the equation $MX=V$ by $\frac{1}{M}$ to obtain the solution immediately: $X=\frac{1}{M}V$.
%
%Clearly, if the linear system has no solution, then there can be no hope of finding $\frac{1}{M}$, since if it existed we could find a solution.  On the other hand, if the system has more than one solution, it also seems unlikely that $\frac{1}{M}$ would exist, since $X=\frac{1}{M}V$ yields only a single solution.  
%
%Therefore $\frac{1}{M}$ only sometimes exists.  It is called the \emph{inverse}\index{Inverse matrix!concept of} of $M$, and is usually written $M^{-1}$.
%\end{remark}
%

%\section*{References}
%Beezer: Part T, Section T
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Trace_(linear_algebra)}{Trace (Linear Algebra)}
%\item \href{http://en.wikipedia.org/wiki/Block_matrix}{Block Matrix}
%\end{itemize}
%

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & \hwrref{Matrices}{2}, \hwrref{Matrices}{3},
 \hwrref{Matrices}{4}\\\hline
\end{tabular}

\input{\propMatricesPath/problems}

\section{\inverseMatTitle}
\label{inverse_matrix}


\begin{definition}
A square matrix $M$ is {\bf invertible} (or {\bf nonsingular})\index{Invertible}\index{Nonsingular} if there exists a matrix $M^{-1}$ such that
\[
M^{-1}M=I=MM^{-1}.
\]
If $M$ has no inverse, we say $M$ is {\bf singular}\index{singular} or {\bf non-invertible}\index{Non-invertible}.
\end{definition}

%\begin{figure}
%\begin{center}
%\includegraphics[scale=.27]{\inverseMatPath/defn_inverse_matrix.jpg}
%\end{center}
%\end{figure}


\begin{remark}[Inverse of a $2\times 2$ Matrix] Let $M$ and $N$ be the matrices:
\[
M=\begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix},\qquad N=\begin{pmatrix}
d & -b \\
-c & a \\
\end{pmatrix}
\]
Multiplying these matrices gives:
\[
MN=\begin{pmatrix}
ad-bc & 0 \\
0 & ad-bc \\
\end{pmatrix}=(ad-bc)I\, .
\]
Then $M^{-1}=\frac{1}{ad-bc}\begin{pmatrix}
d & -b \\
-c & a \\
\end{pmatrix}$, so long as $ad-bc\neq 0$.    
\end{remark}

\begin{figure}
\begin{center}
\includegraphics[scale=.24]{\inverseMatPath/2x2_inverse.jpg}
\end{center}
\caption{The formula for the inverse of a 2$\times$2 matrix is worth memorizing!}
\end{figure}



\subsection{Three Properties of the Inverse}

\begin{enumerate}
\item If $A$ is a square matrix and $B$ is the inverse of $A$, then $A$ is the inverse of $B$, since $AB=I=BA$.  So we have the identity
\[
(A^{-1})^{-1}=A.
\]

\item Notice that $B^{-1}A^{-1}AB=B^{-1}IB=I=ABB^{-1}A^{-1}$ so
\Shabox{1}{$
(AB)^{-1}=B^{-1}A^{-1}
$}
Thus, much like the transpose, taking the inverse of a product \emph{reverses} the order of the product.


%\begin{center}
%\includegraphics[scale=.26]{\inverseMatPath/inverse_inverse.jpg}
%\end{center}

\item Finally, recall that $(AB)^T=B^TA^T$.  Since $I^T=I$, then $(A^{-1}A)^T=A^T(A^{-1})^T=I$.  Similarly, $(AA^{-1})^T=(A^{-1})^TA^T=I$.  Then:
\[
(A^{-1})^T=(A^T)^{-1}
\]
%As such, we could even write $A^{-T}$ for the inverse of the transpose of $A$ (or equivalently the transpose of the inverse).
%\begin{center}
%\includegraphics[scale=.20]{\inverseMatPath/transpose_inverse.jpg}
%\end{center}
\end{enumerate}

\Videoscriptlink{inverse_matrix_2by2_example.mp4}{$2\times 2$ Example}{scripts_inverse_matrix_2by2_example}




\subsection{Finding Inverses (Redux)}

Gaussian elimination can be used to find inverse matrices. This concept is covered in chapter~\ref{systems}, section~\ref{EROinverse},
but is presented here again as review in more sophisticated terms.


Suppose $M$ is a square invertible matrix and $MX=V$ is a linear system. The   solution
must be unique because it can be found by multiplying the equation on both sides by $M^{-1}$
yielding
 $X=M^{-1}V$. Thus,
the reduced row echelon form of the linear system has an identity matrix on the left:
\[
\begin{amatrix}{1}
M & V
\end{amatrix}
\sim
\begin{amatrix}{1}
I & M^{-1}V
\end{amatrix}
\]
Solving the linear system $MX=V$ then tells us what $M^{-1}V$ is.  

To solve many linear systems with the same matrix at once, 
$$MX=V_1,~MX=V_2$$
we can consider augmented matrices with 
%a matrix on the right side instead of a column vector, 
many columns on the right 
 and then apply Gaussian row reduction to the left side of the matrix.  Once the identity matrix is on the left side of the augmented matrix, then the solution of each of the individual linear systems is on the right.
 \[
\left(\begin{array}{c|cc}
\!M & V_1&V_2\!
\end{array}\right)
\sim
\left(\begin{array}{c|cc}
\!I & M^{-1}V_1 & M^{-1}V_2\!
\end{array}\right)
\]


To compute $M^{-1}$, we would like $M^{-1}$, rather than $M^{-1}V$ to appear on the right side of our augmented matrix.
This is achieved by  solving the collection of systems $MX=e_k$, where $e_k$ is the column vector of zeroes with a $1$ in the $k$th entry.  
{\it I.e.,} the $n\times n$ identity matrix can be viewed as a bunch of column vectors $I_n=(e_1 \ e_2 \ \cdots e_n)$. So, putting the $e_k$'s together into an identity matrix, we get:
\[
\begin{amatrix}{1}
M & I
\end{amatrix}
\sim
\begin{amatrix}{1}
I & M^{-1}I
\end{amatrix}
=\begin{amatrix}{1}
I & M^{-1}
\end{amatrix}
\]


\begin{example}
Find $\begin{pmatrix}
-1 & 2 & -3 \\
2 & 1 & 0 \\
4 & -2 & 5 \\
\end{pmatrix}^{-1}
$.

\noindent
We start by writing the augmented matrix, then apply row reduction to the left side.

\begin{eqnarray*}
\begin{pmat}{rrr|ccc}
-1 & 2 & -3 & 1 & 0 & 0 \\[1mm]
2  & 1 &  0 & 0 & 1 & 0 \\[1mm]
 4 & -2 & 5 & 0 & 0 & 1 \\[1mm]
\end{pmat} & \sim & \begin{pmat}{crr|ccc}
1  & -2&  3  & 1 & 0 & 0 \\[1mm]
0  & 5 &  -6 & 2 & 1 & 0 \\[1mm]
 0 & 6 & -7  & 4 & 0 & 1 \\[1mm]
\end{pmat} \\[2mm]
& \sim & \begin{pmat}{ccr|rrc}
1  & 0 &  \frac{3}{5}  & -\frac{1}{4} & \frac{2}{5} & 0 \\[1mm]
0  & 1 &  -\frac{6}{5} & \frac{2}{5} & \frac{1}{5}  & 0 \\[1mm]
 0 & 0 &  \frac{1}{5}  & \frac{4}{5} & -\frac{6}{5} & 1 \\[1mm]
\end{pmat} \\[2mm]
& \sim & \begin{pmat}{ccc|rrr}
1  & 0 &  0  & -5 & 4 & -3 \\[1mm]
0  & 1 &  0  & 10 & -7 & 6 \\[1mm]
 0 & 0 &  1  & 8 & -6 & 5 \\[1mm]
\end{pmat} \\
\end{eqnarray*}
At this point, we know $M^{-1}$ assuming we didn't goof up\index{Goofing up}.  However, row reduction is a lengthy and  involved process with lots of room for arithmetic errors, so we should~\emph{check our answer,} by confirming that $MM^{-1}=I$ (or if you prefer $M^{-1}M=I$):
\[MM^{-1} = 
\begin{pmatrix}
-1 & 2 & -3 \\
2 & 1 & 0 \\
4 & -2 & 5 \\
\end{pmatrix}\begin{pmatrix}
-5 & 4 & -3 \\
10 & -7 & 6 \\
 8 & -6 & 5 \\
\end{pmatrix}
=\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
\]  
The product of the two matrices is indeed the identity matrix, so we're done.
\end{example}

%\href{\webworkurl ReadingHomework10/1/}{Reading homework: problem 10.1}
\Reading{Matrices}{5}
\subsection{Linear Systems and Inverses}

If $M^{-1}$ exists and is known, then we can immediately solve linear systems associated to $M$.

\begin{example}
Consider the linear system:

\[
      \begin{linsys}{2}
            -x & +2y & -3z         &=& 1  \\[1mm]
            2x & +\ y\,   &             &=& 2 \\[1mm]
            4x & -2y & +5z         &=& 0  
      \end{linsys}
\]
The associated matrix equation is $MX=\colvec{1\\2\\0},$ where \(M\) is the same as in the previous section, so the system above is equivalent to the matrix equation

\[
\colvec{x\\y\\z}=\begin{pmatrix}
-1 & 2 & -3 \\
2 & 1 & 0 \\
4 & -2 & 5 \\
\end{pmatrix}^{-1}\colvec{1\\2\\0}
=\begin{pmatrix}
-5 & 4 & -3 \\
10 & -7 & 6 \\
 8 & -6 & 5 \\
\end{pmatrix}\colvec{1\\2\\0}
=\colvec{3\\-4\\-4}.
\]
That is, the system is equivalent to the equation $\colvec{x\\y\\z}=\colvec{3\\-4\\-4}$, and it is easy to see what the solution(s) to this equation are. 
\end{example}
In summary, when $M^{-1}$ exists 

\begin{center}
\shabox{$Mx=v \Leftrightarrow x=M^{-1}v\, .$}
\end{center}


%\href{\webworkurl ReadingHomework10/2/}{Reading homework: problem 10.2}
\Reading{Matrices}{5}

\subsection{Homogeneous Systems}

\begin{theorem}
A square matrix $M$ is invertible if and only if the homogeneous system $$Mx=0$$ has no non-zero solutions.
\end{theorem}

\begin{proof}
First, suppose that $M^{-1}$ exists.  Then $Mx=0 \Rightarrow x=M^{-1}0=0$.  Thus, if $M$ is invertible, then $Mx=0$ has no non-zero solutions.

On the other hand, $Mx=0$ always has the solution $x=0$.  If no other solutions exist, then $M$ can be put into reduced row echelon form with every variable a pivot.  In this case, $M^{-1}$ can be computed using the process in the previous section.
\end{proof}

%\begin{figure}
\begin{center}
\includegraphics[scale=.26]{\inverseMatPath/inverse_theorem.jpg}
\end{center}
%\end{figure}

%A great test of your linear algebra knowledge is to make a list of conditions for a matrix to be singular.
%You will learn more of these as the course goes by, but can also skip straight to the list in Section~\ref{thelist}. 

\subsection{Bit Matrices}\index{Bit matrices}
In computer science, information is recorded using binary strings of data.  For example, the following string contains an English word:
\[
011011000110100101101110011001010110000101110010
\]
A \hypertarget{bits}{\emph{bit}} is the basic unit of information, keeping track of a single one or zero.  Computers can add and multiply individual bits very quickly.

In chapter~\ref{vectorSpaces}, section~\ref{otherfields} it is explained how to formulate vector spaces over \hyperref[fields]{fields} other than real numbers.
In particular, al of the properties of a 
vector space make sense with numbers $\Z_2=\{0,1 \}$ with addition and multiplication given by the following tables. 
\label{Z2}
\[
\begin{array}{c|cc}
+ & 0 & 1 \\ \hline
0 & 0 & 1 \\
1 & 1 & 0 \\
\end{array}
\qquad
\begin{array}{c|cc}
\times& 0 & 1 \\ \hline
0 & 0 & 0 \\
1 & 0 & 1 \\
\end{array}
\]
Notice that $-1=1$, since $1+1=0$.
Therefore,  we can apply all of the linear algebra we have learned thus far to matrices with $\Z_2$ entries.  A matrix with entries in $\Z_2$ is sometimes called a \emph{bit matrix}\index{Bit Matrix}.

\begin{example}
$\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}$ is an invertible matrix over $\Z_2$;

\[\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}^{-1}=\begin{pmatrix}
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}.
\]

This can be easily verified by multiplying:

\[\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}\begin{pmatrix}
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}=\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
\]
\end{example}

\begin{remark}[Application: Cryptography]  
A very simple way to hide information is to use a substitution cipher, in which the alphabet is permuted and each letter in a message is systematically exchanged for another.  For example, the ROT-13 cypher just exchanges a letter with the letter thirteen places before or after it in the alphabet.  For example, HELLO becomes URYYB.  Applying the algorithm again decodes the message, turning URYYB back into HELLO.  Substitution ciphers are easy to break, but the basic idea can be extended to create cryptographic systems that are practically uncrackable.  For example, a \emph{one-time pad} is a system that uses a different substitution for each letter in the message.  So long as a particular set of substitutions is not used on more than one message, the one-time pad is unbreakable.

English characters are often stored in computers in the ASCII format.  In ASCII, a single character is represented by a string of eight bits, which we can consider as a vector in $\Z_2^8$ (which is like vectors in $\Re^8$, where the entries are zeros and ones).  One way to create a substitution cipher, then, is to choose an $8\times 8$ invertible bit matrix $M$, and multiply each letter of the message by $M$.  Then to decode the message, each string of eight characters would be multiplied by $M^{-1}$.  

To make the message a bit tougher to decode, one could consider pairs (or longer sequences) of letters as a single vector in $\Z_2^{16}$ (or a higher-dimensional space), and then use an appropriately-sized invertible matrix.
For more on cryptography, see ``The Code Book,'' by Simon Singh (1999, Doubleday).
\end{remark}

%{\it You should now be ready to attempt the \hyperref[sample1]{first sample midterm}.}
%\begin{center}
%\shabox{
%\begin{tabular}{c}
%\it \hyperref[sample1]{You are now ready to attempt the first sample midterm}. 
%\\
%\hyperref[sample1]{\includegraphics[scale=.15]{midterm.jpg}}
%\end{tabular}
%}
%\end{center}
%
%\section*{References}
%
%Hefferon: Chapter Three, Section IV.2
%\\
%Beezer: Chapter M, Section MISLE
%\\
%Wikipedia:
%\href{http://en.wikipedia.org/wiki/Invertible_matrix}{Invertible Matrix}
%
%
\section{Review Problems}
{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & \hwrref{Matrices}{6}, \hwrref{Matrices}{7}
 \\
 \hline
\end{tabular}


\input{\inverseMatPath/problems}

\newpage

\section{LU Redux}
\label{LUdecomp}

Certain matrices are easier to work with than others.  In this section, we will see how to write any square\footnote{The case where $M$ is not square is dealt with at the end of the section.} matrix $M$ as the product of two simpler matrices.  We will write $$M=LU\, ,$$ where:
\begin{itemize}
\item $L$ is \emph{lower triangular}\index{Lower triangular matrix}.  This means that all entries above the main diagonal are zero.  In notation,
$L=(l^i_j)$ with $l^i_j=0$ for all $j>i$.
\[L=\begin{pmatrix}
l^1_1 & 0 & 0 & \cdots \\
l^2_1 & l^2_2 & 0 & \cdots \\
l^3_1 & l^3_2 & l^3_3 & \cdots \\
\vdots & \vdots & \vdots & \ddots \\
\end{pmatrix}
\]

\item $U$ is \emph{upper triangular}\index{Upper triangular matrix}.  This means that all entries below the main diagonal are zero.  In notation,
$U=(u^i_j)$ with $u^i_j=0$ for all $j<i$.
\[U=\begin{pmatrix}
u^1_1 & u^1_2 & u^1_3 & \cdots \\
0 & u^2_2 & u^2_3 & \cdots \\
0 & 0 & u^3_3 & \cdots \\
\vdots & \vdots & \vdots & \ddots \\
\end{pmatrix}
\]
\end{itemize}
$M=LU$ is called an \emph{$LU$ decomposition}\index{LU@$LU$ decomposition} of $M$.

This is a useful trick for  computational reasons; it is much easier to compute the inverse of an upper or lower triangular matrix than general matrices.  Since inverses are useful for solving linear systems, this makes solving any linear system associated to the matrix much faster as well.  The determinant---a very important quantity associated with any square matrix---is very easy to compute for triangular matrices.

\begin{example}
Linear systems associated to upper triangular matrices are very easy to solve by back substitution.
\[
\begin{amatrix}{2}
a & b & 1 \\
0 & c & e \\
\end{amatrix} \ \Rightarrow \ y=\frac{e}{c}\, , \quad x=\frac{1}{a}\left(1-\frac{be}{c}\right)
\]

$$
\begin{amatrix}{3}
1 & 0 & 0 & d \\
a & 1 & 0 & e \\
b & c & 1 & f \\
\end{amatrix} 
\Rightarrow 
\left\{    \begin{array}{l} x=d\\   y=e-ax\\  z=f-bx-cy \end{array} \right\}
\Rightarrow 
\left\{    \begin{array}{l} x=d\\   y=e-ad\\  z=f-bd-c(e-ad) \end{array} \right. .
$$
For lower triangular matrices, 
\emph{forward} substitution\index{Forward substitution} 
gives a quick solution; for upper triangular matrices, 
\emph{back} substitution\index{Back substitution} 
gives the solution.
\end{example}





\subsection{Using $LU$ Decomposition to Solve Linear Systems}

Suppose we have $M=LU$ and want to solve the system
\[
MX=LUX=V.
\]

\begin{itemize}
\item{Step 1:} Set $W=\colvec{u\\v\\w}=UX$.  

\item{Step 2:} Solve the system $LW=V$.  This should be simple by forward substitution since $L$ is lower triangular.  Suppose the solution to $LW=V$ is $W_0$.  

\item{Step 3:} Now solve the system $UX=W_0$.  This should be easy by backward substitution, since $U$ is upper triangular.  The solution to this system is the solution to the original system.
\end{itemize}
We can think of this as using the matrix $L$ to perform row operations on the matrix $U$ in order to solve the system; this idea also appears in the  study of determinants.

%\href{\webworkurl ReadingHomework11/1/}{Reading homework: problem 11.1}
\Reading{Matrices}{7}

\begin{example}
Consider the linear system:
\[
      \begin{linsys}{4}
            6x & +&18y & +&3z         &=& 3  \\[1mm]
            2x & +&12y & +&z	    &=& 19 \\[1mm]
            4x & +&15y & +&3z         &=& 0  
      \end{linsys}
\]

An $LU$ decomposition for the associated matrix $M$ is
\[
\begin{pmatrix}
6 & 18 & 3 \\
2 & 12 & 1 \\
4 & 15 & 3 
\end{pmatrix} =
\begin{pmatrix}
3 & 0 & 0 \\
1 & 6 & 0 \\
2 & 3 & 1 
\end{pmatrix}
\begin{pmatrix}
2 & 6 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{pmatrix}.
\]

\begin{itemize}
\item{Step 1:} \hypertarget{LUproc}{Set} $W=\colvec{u\\v\\w}=UX$.  

\item{Step 2:} Solve the system $LW=V$:

\[
\begin{pmatrix}
3 & 0 & 0 \\
1 & 6 & 0 \\
2 & 3 & 1 
\end{pmatrix}
\colvec{u\\v\\w} =
\colvec{3\\19\\0}
\]

By substitution, we get $u=1$, $v=3$, and $w=-11$.  Then 
\[W_0=\colvec{1\\3\\-11}\]

\item{Step 3:} Solve the system $UX=W_0$.  
\[
\begin{pmatrix}
2 & 6 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{pmatrix}
\colvec{x\\y\\z} =
\colvec{1\\3\\-11}
\]
Back substitution gives $z=-11, y=3$, and $x=-3$.  

Then $X=\colvec{-3\\3\\-11}$, and we're done.
\end{itemize}
\end{example}

\Videoscriptlink{lu_decomposition_using_lu_decomp.mp4}{Using an $LU$ decomposition}{scripts_lu_decomposition_using_lu_example}

%\begin{figure}
\begin{center}
\includegraphics[scale=.3]{\luDecompPath/LU_solution.jpg}
\end{center}
%\end{figure}

\subsection{Finding an $LU$ Decomposition.}
\label{finding_LU_decomp}

In chapter~\ref{systems}, section~\ref{LUtake1}, Gaussian elimination was used to find~$LU$ matrix decompositions.  
These ideas are presented here again as review.
 
For any given matrix, there are actually many different $LU$ decompositions.  However, there is a unique $LU$ decomposition in which the $L$ matrix has ones on the diagonal. In that case $L$ is called a \emph{lower unit triangular matrix}\index{Lower unit triangular matrix}.

To find the $LU$ decomposition, we'll create two sequences of matrices $L_1, L_2,\ldots$ and $U_1, U_2, \ldots$ such that at each step, $L_iU_i=M$.  Each of the $L_i$ will be lower triangular, but only the last $U_i$ will be upper triangular.
 The main trick for this calculation is captured by the following example:

\begin{example} (An Elementary Matrix)

\noindent
Consider $$E=\begin{pmatrix}1&0\\\lambda&1\end{pmatrix}\, ,\qquad M=\begin{pmatrix}a&b&c&\cdots\\d&e&f&\cdots\end{pmatrix}\, .$$
Lets compute $EM$
$$
EM=\begin{pmatrix}\multicolumn{1}{c}{a}&\mc{b}&\mc{c}&\cdots\\d+\lambda a&e+\lambda b&f+\lambda c&\cdots\end{pmatrix}\, .
$$
Something neat happened here: multiplying $M$ by $E$ performed the row operation $R_2\to R_2+\lambda R_1$ on $M$.
Another interesting fact:
$$
E^{-1}:=\begin{pmatrix}1&0\\-\lambda&1\end{pmatrix}
$$ 
obeys (check this yourself...)
$$
E^{-1} E = 1\, .
$$
Hence $M=E^{-1} E M$ or, writing this out
$$
\begin{pmatrix}a&b&c&\cdots\\d&e&f&\cdots\end{pmatrix}=\begin{pmatrix}1&0\\-\lambda&1\end{pmatrix} \begin{pmatrix}\mc{a}&\mc{b}&\mc{c}&\cdots\\d+\lambda a&e+\lambda b&f+\lambda c&\cdots\end{pmatrix}\, .
$$
Here the matrix on the left is lower triangular, while the matrix on the right has had a row operation performed on it.
\end{example}




\vspace{2mm}
We would like to  use the first row of $M$ to zero out the first entry of every row below it.  For our running example, $$M=\begin{pmatrix}
6 & 18 & 3 \\
2 & 12 & 1 \\
4 & 15 & 3 
\end{pmatrix}\, ,$$ so we would like to perform the row operations $$R_2\to R_2 -\frac 13 R_1 \mbox{ and } R_3\to R_3-\frac 23R_1\, .$$
%so the second row minus $\frac{1}{3}$ of the first row will zero out the first entry in the second row.  Likewise, the third row minus $\frac{2}{3}$ of the first row will zero out the first entry in the third row.
If we perform these row operations on $M$ to produce 
$$U_1=\begin{pmatrix}
6 & 18 & 3 \\
0 & 6 & 0 \\
0 & 3 & 1 
\end{pmatrix}\, ,$$
we need to multiply this on the left by a lower triangular matrix $L_1$ so that the product $L_1U_1=M$ still.
The above example shows how to do this:
Set $L_1$ to be the lower triangular matrix whose first column is filled with  {\bf minus} the constants used to zero out the first column of $M$.  Then $$L_1 = \begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & 0 & 1 
\end{pmatrix}\, .$$  
%Set $U_1$ to be the matrix obtained by zeroing out the first column of $M$.  Then $U_1=\begin{pmatrix}
%6 & 18 & 3 \\
%0 & 6 & 0 \\
%0 & 3 & 1 
%\end{pmatrix}$.
By construction $L_1 U_1=M$, but you should compute this yourself as a double check.

Now repeat the process by zeroing the second column of $U_1$ below the diagonal using the second row of $U_1$ using the row operation
$R_3\to R_3-\frac 12 R_2$ to produce
$$U_2=\begin{pmatrix}6&18&3\\0&6&0\\0&0&1\end{pmatrix}\, .$$
The matrix that undoes this row operation is obtained in the same way we found $L_1$ above and is:
$$
\begin{pmatrix}
1&0&0\\
0&1&0\\
0&\frac 12& 1
\end{pmatrix}\, .
$$
Thus our answer for $L_2$ is the product of this matrix with $L_1$, namely
$$
L_2=
\begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & 0 & 1 
\end{pmatrix}\begin{pmatrix}
1&0&0\\
0&1&0\\
0&\frac 12& 1
\end{pmatrix}
=\begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}\, .
$$
Notice that it is lower triangular because 

\begin{center}
\shabox{The product of lower triangular matrices is always lower triangular!}
\end{center}

\noindent
Moreover it is obtained by recording minus the constants used for all our row operations in the appropriate columns (this always works this way).
Moreover, $U_2$ is upper triangular and $M=L_2U_2$, we are done!
Putting this all together we have
$$M=\begin{pmatrix}
6 & 18 & 3 \\
2 & 12 & 1 \\
4 & 15 & 3 
\end{pmatrix}= \begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}\begin{pmatrix}
6 & 18 & 3 \\[1mm]
0 & 6 & 0 \\[1mm]
0 & 0 & 1 
\end{pmatrix}\, .$$  
%Since $U_2$ is upper-triangular, we're done.  Inserting the new number into $L_1$ to get $L_2$ really is safe: the numbers in the first column don't affect the second column of $U_1$, since the first column of $U_1$ is already zeroed out.

If the matrix you're working with has more than three rows, just continue this process by zeroing out the next column below the diagonal, and repeat until there's nothing left to do.

\Videoscriptlink{lu_decomposition_example.mp4}{Another $LU$ decomposition example}{scripts_lu_decomposition_example}

The fractions in the $L$ matrix are admittedly ugly.  For two matrices $LU$, we can multiply one entire column of $L$ by a constant $\lambda$ and divide the corresponding row of $U$ by the same constant without changing the product of the two matrices.  Then:

\begin{eqnarray*}
LU &=& \begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}
I
\begin{pmatrix}
6 & 18 & 3 \\[1mm]
0 & 6 & 0 \\[1mm]
0 & 0 & 1 
\end{pmatrix} \\
&=&
\begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}
\begin{pmatrix}
3 & 0 & 0 \\
0 & 6 & 0 \\
0 & 0 & 1 
\end{pmatrix}
\begin{pmatrix}
\frac{1}{3} & 0 & 0 \\[1mm]
0 & \frac{1}{6} & 0 \\[1mm]
0 & 0 & 1 
\end{pmatrix}
\begin{pmatrix}
6 & 18 & 3 \\
0 & 6 & 0 \\
0 & 0 & 1 
\end{pmatrix} \\
&=&
\begin{pmatrix}
3 & 0 & 0 \\
1 & 6 & 0 \\
2 & 3 & 1 
\end{pmatrix}\begin{pmatrix}
2 & 6 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{pmatrix}.
\end{eqnarray*}
The resulting matrix looks nicer, but isn't in standard (lower unit triangular matrix) form.

\Reading{Matrices}{7}
%\href{\webworkurl ReadingHomework11/2/}{Reading homework: problem 11.2}

For matrices that are not square, $LU$ decomposition still makes sense.  Given an $m\times n$ matrix $M$, for example we could write $M=LU$ with $L$ a square lower unit triangular matrix, and $U$ a rectangular matrix.  Then $L$ will be an $m\times m$ matrix, and $U$ will be an $m\times n$ matrix (of the same shape as $M$).  From here, the process is exactly the same as for a square matrix.  We create a sequence of matrices $L_i$ and $U_i$ that is eventually the $LU$ decomposition.  Again, we start with $L_0=I$ and $U_0=M$.

\begin{example}
Let's find the $LU$ decomposition of $M=U_0=\begin{pmatrix}
-2 & 1 & 3 \\
-4 & 4 & 1 
\end{pmatrix}$.  Since $M$ is a $2\times 3$ matrix, our decomposition will consist of a $2\times 2$ matrix and a $2\times 3$ matrix.  Then we start with $L_0=I_2=\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}$.

The next step is to zero-out the first column of $M$ below the diagonal.  There is only one row to cancel, then, and it can be removed by subtracting $2$ times the first row of $M$ to the second row of $M$.  Then:

\[
L_1=\begin{pmatrix}
1 & 0 \\
2 & 1
\end{pmatrix}, \qquad 
U_1 = \begin{pmatrix}
-2 & 1 & 3 \\
0 & 2 & -5 
\end{pmatrix}
\]
Since $U_1$ is upper triangular, we're done.  With a larger matrix, we would just continue the process.
\end{example}





\subsection{Block $LDU$ Decomposition}

Let $M$ be a square block matrix with square blocks $X,Y,Z,W$ such that $X^{-1}$ exists.  Then $M$ can be decomposed as a block $LDU$ decomposition, where $D$ is block diagonal, as follows:
\[
M=\begin{pmatrix}
X & Y \\
Z & W
\end{pmatrix}
\]
Then: 
\begin{center}
\shabox{
$M=\left(\begin{array}{cc}
I &  0 \\[1mm]
ZX^{-1} & I
\end{array}\right)
\left(\begin{array}{cc}
X & 0 \\[1mm]
0 & W-ZX^{-1}Y
\end{array}\right)\left(\begin{array}{cc}
I & X^{-1}Y \\[1mm]
0 & I
\end{array}\right)\, .$}\end{center}
This can be checked explicitly simply by block-multiplying these three matrices.

\Videoscriptlink{lu_decomposition_blocks.mp4}{Block $LDU$ Explanation}{scripts_lu_decomposition_blocks}

\begin{example}
For a $2\times 2$ matrix, we can regard each entry as a $1\times1$ block.
\[
\begin{pmatrix}
1 & 2 \\
3 & 4
\end{pmatrix}=
\begin{pmatrix}
1 & 0 \\
3 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
0 & -2
\end{pmatrix}
\begin{pmatrix}
1 & 2 \\
0 & 1
\end{pmatrix}
\]
By multiplying the diagonal matrix by the upper triangular matrix, we get the standard $LU$ decomposition of the matrix.
\end{example}


%{\bf You should now be ready to attempt the \hyperref[sample1]{first sample midterm}.}
\begin{center}
\shabox{
\begin{tabular}{c}
\bf \hyperref[sample1]{You are now ready to attempt the first sample midterm}. 
\\
\hyperref[sample1]{\includegraphics[scale=.15]{midterm.jpg}}
\end{tabular}
}
\end{center}

%\section*{References}
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/LU_decomposition}{$LU$ Decomposition}
%\item \href{http://en.wikipedia.org/wiki/Block_LU_decomposition}{Block $LU$ Decomposition}
%\end{itemize}

\section{Review Problems}
{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{Matrices}{7},\hwrref{Matrices}{8}\\
 $LU$ Decomposition & \hwref{Matrices}{14}\\
 \hline
\end{tabular}
\input{\luDecompPath/problems}

\newpage




","\chapter{\matricesTitle}
\label{Matrices}

Matrices are a powerful tool for calculations involving linear transformations.
It is important to understand how to find the matrix of a linear transformation 
and the properties of matrices.

\section{Linear Transformations and Matrices}

Ordered, finite-dimensional, bases for  vector spaces allows us to express linear operators as matrices.
%We represent vectors from $\Re^n$ as matrices with one column, so lets begin there.

\subsection{Basis Notation}
\hypertarget{Basis notation}{~}

\noindent
A basis allows us to efficiently label arbitrary vectors in terms of column vectors. Here is an  example.
\begin{example}
Let $$V=\left\{\begin{pmatrix}a&b\\c&d\end{pmatrix}\middle| \,a,b,c,d\in {\mathbb R}\right\}$$ be the vector space of $2\times 2$ real matrices,
with addition and scalar multiplication defined componentwise. 
One choice of basis is the ordered set (or list) of matrices
$$B=\left(\begin{pmatrix}1&0\\0&0\end{pmatrix},\begin{pmatrix}0&1\\0&0\end{pmatrix},\begin{pmatrix}0&0\\ 1&0\end{pmatrix},\begin{pmatrix}0&0\\ 0&1\end{pmatrix}\right)=:(e_1^1,e_2^1,e^2_1,e^2_2)\, .$$ 
Given a particular vector and a basis, your job is to write that vector as a sum of multiples of basis elements. Here
an arbitrary vector $v\in V$ is just a matrix, so we write
\begin{eqnarray*}
v\ =\ \begin{pmatrix}a&b\\c&d\end{pmatrix}&=&\quad\!\! \begin{pmatrix}a&0\\0&0\end{pmatrix}+\begin{pmatrix}0&b\\0&0\end{pmatrix}+\begin{pmatrix}0&0\\ c&0\end{pmatrix}+\begin{pmatrix}0&0\\0&d\end{pmatrix}\\[1mm]
&=&a\begin{pmatrix}1&0\\0&0\end{pmatrix}+b\begin{pmatrix}0&1\\0&0\end{pmatrix}+c\begin{pmatrix}0&0\\ 1&0\end{pmatrix}
+d\begin{pmatrix}0&0\\0&1\end{pmatrix}\\[1mm]
&=&a\,  e^1_1+b \, e^1_2+c \, e^2_1+d \, e^2_2\, .
\end{eqnarray*}
The coefficients $(a,b,c,d)$ of the basis vectors $(e^1_1,e^1_2,e^2_1,e^2_2)$ encode the information of which matrix the vector $v$ is.
We store them in column vector by writing
$$
v=a \, e^1_1+b \, e^1_2+c\,  e^2_1+d \, e^2_2=: (e^1_1,e^1_2,e^2_1,e^2_2)\colvec{a\\b\\c\\d}=:\colvec{a \\ b\\c\\d}_B\, .
$$
The 4-vector $\colvec{a\\b\\c\\d} \in \mathbb{R}^4$ encodes the vector $\begin{pmatrix}a&b\\c&d\end{pmatrix}\in V$ but is NOT equal to it! \\(After all, $v$ is a matrix so could not equal a column vector.) Both notations on the right hand side of the above equation really stand for the vector obtained by multiplying the coefficients stored in the column vector by the corresponding basis element and then summing over them.
\end{example}
 
Next, lets consider a tautological example showing how to label column vectors in terms of column vectors:

\begin{example} (Standard Basis of $\Re^2$)\index{Standard basis!for ${\mathbb R}^2$}\\
The vectors 
$$
e_1=\colvec{1\\0},~~~
e_2=\colvec{0\\1}
$$
are called the standard basis vectors of $\Re^2=\Re^{\{1,2\}}$. 
Their description as functions of~$\{1,2\}$ are 
\begin{displaymath}
   e_1 (k)= \left\{
     \begin{array}{lr}
       1 & {\rm ~if~} k=1\\
       0 & {\rm ~if~} k=2
     \end{array}
   \right. \, ,~~
   e_2 (k)= \left\{
     \begin{array}{ll}
       0 & {\rm ~if~} k=1\\
       1 & {\rm ~if~} k=2\, .
     \end{array}
   \right.
\end{displaymath} 
It is  natural to assign these  the order: $e_1$ is first and $e_2$ is second.
An arbitrary vector~$v$ of $\Re^2$ can be written as  
$$
v=\colvec{x\\y}=x e_1+ ye_2 .
$$
To emphasize that we are using the standard basis we define the list (or ordered set) $$E=(e_1,e_2)\, ,$$ and write 
$$
\colvec{x\\y}_E:=(e_1,e_2)\colvec{x\\y}:=x e_1+ ye_2=v .
$$
You should read this equation by saying:
\begin{quote}
``The column vector of the vector $v$ in the basis $E$ is $\colvec{x\\y}$.''
\end{quote}

Again, the first notation of a column vector with a subscript~$E$ refers to the vector obtained by multiplying each basis vector
by the corresponding scalar listed in the column and then  summing these, {\it i.e.} $xe_1+y e_2$. 
The second notation denotes exactly the same thing but we first list the basis elements and then the column vector; a useful trick
because this can be read in the same way as matrix multiplication of a row vector times a column vector--except that the entries of the row vector are themselves vectors! 
\end{example}

You should already try to write down the standard basis vectors for $\Re^n$ for other values of $n$ and express an arbitrary vector in $\Re^n$  in terms of them.%webwork problem? 

The last example probably seems pedantic because column vectors are already just ordered lists of numbers and the basis notation 
has simply allowed us to ``re-express'' these as lists of numbers. Of course, this objection does not apply to more complicated vector spaces like our first matrix example. Moreover, as we saw \hypertarget{nonstandard r2 basis}{earlier}, there are infinitely many other pairs of vectors in $\Re^2$ that form a basis.

\begin{example}(A Non-Standard Basis of $\Re^2=\Re^{\{1,2\}}$)\\
$$
b=\colvec{1\\1}\, ,\quad \beta=\colvec{1\\-1}.
$$
As functions of $\{1,2\}$ they read
\begin{displaymath}
   b (k)= \left\{
     \begin{array}{lr}
       1 & {\rm ~if~} k=1\\
       1 & {\rm ~if~} k=2
     \end{array}
   \right. \, ,~~
   \beta (k)= \left\{
     \begin{array}{rl}
       1 & {\rm ~if~} k=1\\
       -1 & {\rm ~if~} k=2\, .
     \end{array}
   \right.
\end{displaymath} 

Notice something important: there is no reason to say that $\beta$ comes before $b$ or vice versa. That is, there is no {\it a priori} reason to give these basis elements one order or the other. 
However, it will be necessary to give the basis elements an order if we want to use them to encode other vectors.  We choose one arbitrarily; let $$B=(b,\beta)$$ be the ordered basis. Note that for an unordered set we use the $\{\}$ parentheses while for lists or ordered sets we use $()$. 

As before we define 
$$
\colvec{x\\y}_B :=(b,\beta)\colvec{x \\ y}:= xb+ y \beta\, . 
$$
You might think that the numbers $x$ and $y$ denote exactly the same vector as in the previous example. However, they do not. Inserting
the actual vectors that $b$ and $\beta$ represent we have
$$
xb+ y \beta=x\colvec{1\\1}+y\colvec{1\\-1}=\colvec{x+y\\x-y}\, .
$$
Thus, to contrast, we have
$$
\colvec{x\\y}_B=\colvec{x+y\\x-y} \mbox{ and } \colvec{x\\y}_E=\colvec{x\\y}
$$
Only in the standard basis $E$ does the column vector of $v$ agree with the column vector that $v$ actually is!

%This equation shows how a pair of numbers in ordered basis $B$ gives a pair of numbers in the standard basis $E$. The stacks of numbers we have dealt with in the past were always specifying vectors in the standard basis. 

%The other order for the basis, $B'=(\beta,b)$ yields
%$$
%\colvec{x\\y}_B' := x\beta+ y b =x\colvec{1\\-1}+y \colvec{1\\1}=\colvec{x-y\\x+y}_E.
%$$
%Again, order matters. 
\end{example}

Based on the above example, you might think that our aim would be to find the ``standard basis'' for any problem. 
In fact, this is far from the truth. Notice, for example that the vector $$v=\colvec{1\\1}=e_1+e_2=b$$ written in the standard basis~$E$ is just
$$
v=\colvec{1\\1}_E\, ,
$$
which was easy to calculate. But in the basis~$B$ we find
$$
v=\colvec{1\\0}_B\, ,
$$
which is actually a simpler column vector!
The fact that there are many bases for any given vector space allows us to choose a basis in which our computation is easiest. 
In any case, the standard basis only makes sense for $\Re^n$. Suppose your vector space was the set of solutions to a differential equation--what would a standard basis then be?



\begin{example} (A Basis For a Hyperplane)\\
Lets again consider the hyperplane
$$
V=\left\{  c_1\colvec{1\\1\\0} +c_2\colvec{0\\1\\1} \middle| c_1,c_2\in \Re \right\} 
$$
One possible choice of ordered basis is 
$$b_1=\colvec{1\\1\\0},~~~b_2=\colvec{0\\1\\1}, ~~~
B=(b_1,b_2).
$$
With this choice
$$
\colvec{x\\y}_B := xb_1+ y b_2 
=x\colvec{1\\1\\0}+y \colvec{0\\1\\1}=\ccolvec{x\\x+y\\y}_E.
$$
With the other choice of order $B'=(b_2, b_1)$ 
$$
\colvec{x\\y}_{\!B'} := xb_2+ y b_1 
=x\colvec{0\\1\\1}+y \colvec{1\\1\\0}=\ccolvec{y\\x+y\\x}_E.
$$
We see that the  order of basis elements matters. 
\end{example}

Finding the column vector of a given vector in a given basis usually amounts to a linear systems problem:

\begin{example} (Pauli Matrices)\\
Let $$V=\left\{\begin{pmatrix}z&u\\v&-z\end{pmatrix}\middle| z,u,v\in{\mathbb C}\right\}$$\
be the vector space of \hyperlink{TRACE}{trace}-free complex-valued matrices (over~${\mathbb C}$) with basis
$$
B=(\sigma_x,\sigma_y,\sigma_z)\, ,
$$ where
$$
\sigma_x=\begin{pmatrix}0&1\\1&0\end{pmatrix}\, ,\quad
\sigma_y=\begin{pmatrix}0&-i\\i&0\end{pmatrix}\, ,\quad
\sigma_z=\begin{pmatrix}1&0\\0&-1\end{pmatrix}\, .
$$
These three matrices are the famous {\it Pauli matrices}\index{Pauli Matrices}; they are used to describe electrons in quantum theory, or qubits in quantum computation.
Let 
$$
v=\begin{pmatrix}
-2+i&1+i\\3-i&2-i
\end{pmatrix}\, .
$$
Find the column vector of $v$ in the basis~$B$.

For this we must solve the equation
$$
\begin{pmatrix}
-2+i&1+i\\3-i&2-i
\end{pmatrix}
=\alpha^x \begin{pmatrix}0&1\\1&0\end{pmatrix}+\alpha^y
\begin{pmatrix}0&-i\\i&0\end{pmatrix}+\alpha^z
\begin{pmatrix}1&0\\0&-1\end{pmatrix}\, .
$$
This gives four equations, {\it i.e.} a linear systems problem, for the $\alpha$'s
$$
\left\{
\begin{array}{rrrrr}
\alpha^x&\!\!-\ i\alpha^y&&=&1+i\\
\alpha^x&\!\!+\ i\alpha^y&&=&3-i\\
&&\alpha^z&=&-2+i\\
&&-\alpha^z&=&2-i\\
\end{array}
\right.
$$
with solution
$$
\alpha^x=2\, ,\quad \alpha^y=2-2i\, ,\quad \alpha^z=-2+i\, .
$$
Thus
$$
v=\begin{pmatrix}
-2+i&1+i\\3-i&2-i
\end{pmatrix}=\ccolvec{2\\\ 2-i\\\!\!-2+i }_B\, .
$$
\end{example}

To summarize, the {\it column vector 
of a vector}\index{Column vector! of a vector} $v$ in an ordered basis $B=(b_1,b_2,\ldots,b_n)$,
$$\ccolvec{\alpha^1\\\alpha^2\\\vdots\\\alpha^n}\, ,$$ is defined by solving the linear systems problem
$$
v=\alpha^1 b_1 + \alpha^2b_2 +\cdots + \alpha^n b_n = \sum_{i=1}^n \alpha^i b_i\, .
$$
The numbers $(\alpha^1,\alpha^2,\ldots,\alpha^n)$ are called the {\it components of the vector}\index{Components of a vector}~$v$.
Two useful shorthand notations for this are
$$
v=\ccolvec{\alpha^1\\\alpha^2\\\vdots\\\alpha^n}_B = (b_1,b_2,\ldots,b_n)\ccolvec{\alpha^1\\\alpha^2\\\vdots\\\alpha^n}\, .
$$

\subsection{From Linear Operators to Matrices}

Chapter~\ref{sec:linearTransformation} showed that linear functions are very special kinds of functions; 
they are fully specified by their values on any basis for their domain. 
A matrix  records how a linear operator maps an element of the basis to a sum of multiples in the target space basis.

More carefully, if $L$ is a linear operator from $V$ to $W$ 
then the matrix for~$L$ in the ordered bases $B=(b_1,b_2,\dots)$ for $V$ and $B'=(\beta_1,\beta_2,\dots)$ for~$W$, is the
 array of numbers %$M$ specified by 
% $$
% Lv_B=(Mv)_{B'}
% $$
%
 $m_{i}^j$ specified by 
$$L(b_i)= m_{i}^1\beta_1^{\phantom{1}}+\dots +m_{i}^j \beta_j^{\phantom{1}}+\cdots$$

\begin{remark}
To calculate the matrix of a linear transformation you must compute what the linear transformation does to every input basis vector and then write the answers in terms of the output basis vectors:
\begin{equation*}
\begin{split}
\big(L(b_1)&,L(b_2),\ldots,L(b_j),\ldots\big)\\[3mm]
&
=\scalebox{.76}{$
\Big((\beta_1,\beta_2,\ldots,\beta_j,\ldots)\ccolvec{m^1_1\\[.5mm]m^2_2\\\vdots\\m^j_1\\\vdots},
(\beta_1,\beta_2,\ldots,\beta_j,\ldots)\ccolvec{m^1_2\\[.5mm]m^2_2\\\vdots\\m^j_2\\\vdots},\cdots,
(\beta_1,\beta_2,\ldots,\beta_j,\ldots)\ccolvec{m^1_i\\[.5mm]m^2_i\\\vdots\\m^j_i\\\vdots},\cdots\Big)$}\\[3mm]
&
=(\beta_1,\beta_2,\ldots,\beta_j,\ldots)\begin{pmatrix}
m_1^1 & m_2^1 & \cdots & m_i^1 &\cdots\\[.5mm]
m_1^2 & m_2^2 & \cdots & m_i^2 &\cdots\\
\multicolumn{1}{c}{\vdots}& \multicolumn{1}{c}{\vdots} &   & \multicolumn{1}{c}{\vdots} &\\
m_1^j & m_2^j & \cdots & m_i^j& \cdots \\
\multicolumn{1}{c}{\vdots}&\multicolumn{1}{c}{\vdots}&&\multicolumn{1}{c}{\vdots}
\end{pmatrix}
\end{split}
\end{equation*}
\end{remark}


\begin{example}
Consider 
$L:V\to \Re^3$ (as in example~\ref{Vdef}) defined by 
$$
L\colvec{1\\1\\0 } = \colvec{0\\1\\0}\, ,\quad
L\colvec{0\\1\\1 } = \colvec{0\\1\\0}\, .
$$
By linearity this specifies the action of $L$ on any vector from $V$ as
$$
L\left[ c_1\colvec{1\\1\\0 } + c_2 \colvec{0\\1\\1} \right]= (c_1+c_2)\colvec{0\\1\\0}.
$$
We had trouble expressing this linear operator as a matrix. Lets take input basis
$$
B=\left(\colvec{1\\1\\0 } ,\colvec{0\\1\\1 }\right)=:(b_1,b_2)\, ,
$$
and output basis 
$$
E=\left(\colvec{1\\0\\0 } ,\colvec{0\\1\\0 } ,\colvec{0\\0\\1 }\right)\, .
$$
Then
$$
L b_1 = 0 e_1 + 1e_2+ 0  e_3 \, ,
$$
$$
L b_2 = 0 e_1 + 1e_2+ 0  e_3 \, ,
$$
or 
$$
\big(Lb_1, L b_2) = \big( (e_1,e_2,e_3)\colvec{0\\1\\0}, (e_1,e_2,e_3)\colvec{0\\1\\0}\big)=(e_1,e_2,e_3)\begin{pmatrix}0&0\\1&1\\0&0
\end{pmatrix}\, .
$$
The matrix on the right is the matrix of $L$ in these bases.
More succinctly we could write
$$
L\colvec{x\\y}_B=(x+y) \colvec{0\\1\\0}_E
$$
and thus see that $L$ acts like the matrix 
$
\begin{pmatrix}
0&0\\
1&1\\
0&0
\end{pmatrix}
$.

Hence
$$L\colvec{x\\y}_B
=\left(  \begin{pmatrix}
0&0\\
1&1\\
0&0
\end{pmatrix}
\colvec{x\\y} \right)_E\, ;
$$
given input and output bases, the linear operator is now encoded by a matrix.
\end{example}
This is the general rule for this chapter:



\begin{center}
\shabox{
Linear operators become matrices when given ordered~input~and~output~bases. }
\end{center}

\Reading{Matrices}{1}

\begin{example} Lets compute a matrix for the derivative operator acting on the vector space of  polynomials of degree 2 or less:
\[
V = \{a_01 + a_1x + a_2 x^2 \,|\,  a_0,a_1,a_2 \in \Re \}\, .
\]
In the ordered basis $B=( 1,x,x^2)$ we write 
\[
\colvec{a\\b\\c}_{\!B}= a\cdot 1 + b x + c x^2
\]
and
\[
\frac{d}{dx}  \colvec{a\\b\\c}_{\!B}=b\cdot 1 +2c x +0x^2=\ccolvec{b\\2c\\0}_B
\]
In the ordered basis $B$ for both domain and range
\[
\frac{d}{dx} 
\stackrel{B}{\mapsto}
\begin{pmatrix}
0&1&0\\
0&0&2\\
0&0&0
\end{pmatrix}\]
Notice this last line makes no sense without explaining which bases we are using!
\end{example}







%\section*{References}
%Hefferon, Chapter Three, Section IV, parts 1-3.
%\\
%Beezer, Chapter M, Section MM.
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Matrix_multiplication}{Matrix Multiplication}
%\end{itemize}


\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}\hline
Reading problem&\hwrref{Matrices}{1}
\\
Matrix of a Linear Transformation & \hwref{Matrices}{9}, \hwref{Matrices}{10},
 \hwref{Matrices}{11},
 \hwref{Matrices}{12},
 \hwref{Matrices}{13}\\\hline
\end{tabular}


\input{\matricesPath/problems}


\section{\propMatricesTitle} \label{properties_matrices}

The objects of study in linear algebra are linear operators. 
We have seen that linear operators can be represented as matrices through choices of ordered bases, and that matrices provide a means of efficient computation. 

We now begin an in depth study of matrices.

\begin{definition}
An $r\times k$ {\bf matrix}\index{Matrix} $M=(m^i_j)$ for $i=1, \ldots, r; j=1, \ldots, k$ is a rectangular array of real (or complex) numbers:
\label{matrixnotation}
\[M = 
\begin{pmatrix}
m_1^1 & m_2^1 & \cdots & m_k^1 \\[1mm]
m_1^2 & m_2^2 & \cdots & m_k^2 \\
\multicolumn{1}{c}{\vdots} & \multicolumn{1}{c}{\vdots} &   & \multicolumn{1}{c}{\vdots} \\
m_1^r & m_2^r & \cdots & m_k^r \\
\end{pmatrix}\, .
\]
The numbers $m^i_j$ are called {\bf entries}\index{Matrix!entries of}.  The superscript indexes the row of the matrix and the subscript indexes the column of the matrix in which $m_j^i$ appears.
\end{definition}

%It is often useful to consider matrices whose entries are more general than the real numbers, so we allow that possibility.

An $r\times 1$ matrix $v = (v^r_1) = (v^r)$ is called a \emph{column vector}\index{Column vector}, written $$v = \ccolvec{v^1\\v^2\\ \vdots \\ v^r }\, .$$  A $1\times k$ matrix $v = (v^1_k) = (v_k)$ is called a \emph{row vector}\index{Row vector}, written $$v = \rowvec{v_1 & v_2 & \cdots & v_k }\, .$$  
The {\it transpose of a column vector}\index{Transpose!of a column vector} is the corresponding row vector and vice versa:

\begin{example}
Let 
$$
v=\colvec{1\\2\\3}\, .
$$
Then
$$
v^T=\rowvec{1 &2 &3}\, ,
$$
and $(v^T)^T=v$. This is an example of an {\it involution}\index{Involution}, namely an operation which when performed twice does nothing.
\end{example}

A matrix is  an efficient way to store information.

\begin{example}
In computer graphics, you may have encountered image files with a .gif extension.  These files are actually just matrices: at the start of the file the size of the matrix is given, after which each number is a matrix entry  indicating the color of a particular pixel in the image.

This matrix  then has its rows shuffled a bit: by listing, say, every eighth row, a web browser downloading the file can start displaying an incomplete version of the picture before the download is complete.

Finally, a compression algorithm is applied to the matrix to reduce the file size.
\end{example}



\begin{example}
Graphs occur in many applications, ranging from telephone networks to airline routes.  In the subject of \emph{graph theory}\index{Graph theory}, a graph is just a collection of vertices and some edges connecting vertices.  A matrix can be used to indicate how many edges attach one vertex to another.

\begin{center}
\includegraphics[width=10cm]{notes8-0.png}
\end{center}
For example, the graph pictured above would have the following matrix, where $m^i_j$ indicates the number of edges between the vertices labeled $i$~and~$j$:

\[
M = \begin{pmatrix}
1 & 2 & 1 & 1 \\
2 & 0 & 1 & 0 \\
1 & 1 & 0 & 1 \\
1 & 0 & 1 & 3 \\
\end{pmatrix}
\]
This is an example of a \emph{symmetric matrix}, since $m_j^i = m_i^j$.

\Videoscriptlink{matrices_example.mp4}{Adjacency Matrix Example}{scripts_matrices_example}
\end{example}

The set of all $r\times k$ matrices 
$${\mathbb M}_k^r:=\{(m^i_j)|m^i_j\in {\mathbb R};\,  
i\in \{1,\dots,r\} ;\, 
j\in \{1\dots k\} \}\, ,$$ 
is itself  a vector space with  addition and scalar multiplication defined as follows:

\Shabox{1}{\begin{tabular}{c}
$M+N = (m_j^i) + (n_j^i) = ( m_j^i + n_j^i )$\\[2mm]
$rM = r(m_j^i) = (rm_j^i)$\end{tabular}}

In other words, addition just adds corresponding entries in two matrices, and scalar multiplication multiplies every entry.
Notice that $M_1^n = \Re^n$ is just the vector space of column vectors.

Recall that we can multiply an \(r \times k\) matrix by a \(k \times 1\) column vector to produce a \(r \times 1\) column vector using the rule
\[MV = \big(\sum_{j=1}^k m_j^i v^j\big)\, .\]

This suggests the rule for multiplying an \(r \times k\) matrix \(M\) by a \(k \times s\) matrix~\(N\): our \(k \times s\) matrix \(N\) consists of \(s\) column vectors side-by-side, each of dimension \(k \times 1.\) We can multiply our \(r \times k\) matrix \(M\) by each of these \(s\) column vectors using the rule we already know, obtaining \(s\) column vectors each of dimension \(r \times 1.\) If we place these \(s\) column vectors side-by-side, we obtain an \(r \times s\) matrix \(MN.\)

That is, let \[N = 
\begin{pmatrix}
n_1^1 & n_2^1 & \cdots & n_s^1 \\
n_1^2 & n_2^2 & \cdots & n_s^2 \\
\multicolumn{1}{c}{\vdots} & \multicolumn{1}{c}{\vdots} &   & \multicolumn{1}{c}{\vdots} \\
n_1^k & n_2^k & \cdots & n_s^k \\
\end{pmatrix}
\]
and call the columns \(N_1\) through \(N_s\):
\[N_1 = \ccolvec{n_1^1\\n_1^2\\\vdots\\n_1^k}\, ,\:
N_2 = \ccolvec{n_2^1\\n_2^2\\\vdots\\n_2^k}\, ,\:
\ldots,\:
N_s = \ccolvec{n_s^1\\n_s^2\\\vdots\\n_s^k}.
\]
Then
%\[
%MN=M \rowvec{N_1 & N_2 & \cdots & N_s} = \rowvec{MN_1 & MN_2 & \cdots & MN_s}.
%\]
\[
MN=M
\begin{pmatrix}
\multicolumn{1}{c}{|} & \multicolumn{1}{c}{|} & & \multicolumn{1}{c}{|} \\
N_1 & N_2 & \cdots & N_s \\
\multicolumn{1}{c}{|} & \multicolumn{1}{c}{|} & & \multicolumn{1}{c}{|} \\
\end{pmatrix}
=
\begin{pmatrix}
\multicolumn{1}{c}{|} & \multicolumn{1}{c}{|} & & \multicolumn{1}{c}{|} \\
MN_1 & MN_2 & \cdots & MN_s \\
\multicolumn{1}{c}{|} & \multicolumn{1}{c}{|} & & \multicolumn{1}{c}{|} \\
\end{pmatrix}
\]

Concisely: If \(M=(m^i_j)\) for \(i=1, \ldots, r; j=1, \ldots, k\) and \(N=(n^i_j)\) for \(i=1, \ldots, k; j=1, \ldots, s,\) then \(MN=L\) where \(L=(\ell^i_j)\) for \(i=i, \ldots, r; j=1, \ldots, s\) is given by
\[
\ell^i_j = \sum_{p=1}^k m^i_p n^p_j.
\]
This rule obeys linearity.

Notice that in order for the multiplication to make sense, the columns and rows must match.  For an $r\times k$ matrix $M$ and an $s\times m$ matrix $N$, then to make the product $MN$ we must have $k=s$.  Likewise, for the product $NM$, it is required that $m=r$.  A common shorthand for keeping track of the sizes of the matrices involved in a given product is the following diagram.
\Shabox{1.05}{$\Big(r \times k\Big) { \text ~{\rm times}~} \Big(k\times m\Big) {\text ~{\rm is}~} \Big(r\times m\Big)$}

\Reading{Matrices}{2}


\begin{example}
Multiplying a $(3\times 1)$ matrix and a $(1\times 2)$ matrix yields a $(3\times 2)$ matrix.

\[
\colvec{1\\3\\2} \rowvec{2 & 3} = 
\begin{pmatrix}
1\cdot 2 & 1\cdot 3 \\
3\cdot 2 & 3\cdot 3 \\
2\cdot 2 & 2\cdot 3 \\
\end{pmatrix}
= \begin{pmatrix}
2 & 3 \\
6 & 9 \\
4 & 6 
\end{pmatrix} .
\]
\end{example}

Another way to view matrix multiplication is in terms of dot products:

\begin{center}
\shabox{The entries of $MN$ are made from the dot products of the rows of $M$ with the columns of $N$.}
\end{center}


\begin{example}
Let $$M=\begin{pmatrix}1&3\\3&5\\2&6\end{pmatrix}=:\ccolvec{u^T\\v^T\\w^T}
\mbox{ and }
N=\begin{pmatrix}2&3&1\\0&1&0\end{pmatrix}=:\rowvec{a & b & c}$$
where
$$
u=\colvec{1\\3}\, ,\quad
v=\colvec{3\\5}\, ,\quad 
w=\colvec{2\\6}\, ,\quad
a=\colvec{2\\0}\, ,\quad
b=\colvec{3\\1}\, ,\quad 
c=\colvec{1\\0}\, .
$$
Then 
$$
MN=\left(\!\begin{array}{ccc}
u\cdot a & u\cdot b & u\cdot c\\
v\cdot a & v\cdot b & v\cdot c\\
w\cdot a & w\cdot b & w\cdot c\\ 
\end{array}\!\right)
=
\begin{pmatrix}
2&6&1\\
6&14&3\\
4&12&2
\end{pmatrix}\, .
$$
\end{example}
This fact has an obvious yet important consequence:

\begin{theorem}
Let $M$ be a matrix and $x$ a column vector. If 
$$
Mx=0
$$
then the vector $x$ is \hyperlink{orthog}{orthogonal} to the rows of $M$.
\end{theorem}

\begin{remark}
Remember that the set of all vectors that can be obtained by adding up scalar multiples of the columns of a matrix is called its \hyperlink{column space}{column space}~\index{Column Space!concept of}. Similarly the {\bf row space}\index{Row Space} is the set of all row vectors obtained by adding up multiples of the rows of a matrix. The above theorem says that if $Mx=0$, then the vector $x$ is orthogonal to every vector in the row space of $M$.
\end{remark}


%\begin{center}\href{\webworkurl ReadingHomework8/1/}{Reading homework: problem 8.1}\end{center}

We know that $r\times k$ matrices can be used to represent linear transformations 
$ \Re^k \rightarrow \Re^r $
via $$(MV)^i = \sum_{j=1}^{k} m_j^iv^j , $$ which is the same rule used when we multiply an $r\times k$ matrix by a $k\times 1$ vector to produce an $r\times1$ vector.

Likewise, we can use a matrix $N=(n^i_j)$ to define a linear transformation of a vector space of matrices. For example
\[
L \colon M^s_k \stackrel{N}{\longrightarrow} M^r_k\, ,
\]
\[
L(M)=(l^i_k) \mbox{ where } l^i_k= \sum_{j=1}^{s} n_j^im^j_k.
\]
This is the same as the rule we use to multiply matrices. \hypertarget{leftmult}{In other words,} \(L(M)=NM\) is a linear transformation.

\begin{definition}[Matrix Terminology]  
Let $M=(m^i_j)$ be a matrix. The entries $m_i^i$ are called {\bf diagonal}, and the set $\{m_1^1$, $m_2^2$, $\ldots \}$~is called the {\bf diagonal} of the matrix\index{Matrix!diagonal of}.

Any $r\times r$ matrix is called a {\bf square matrix}\index{Square matrix}.  A square matrix that is zero for all non-diagonal entries is called a {\bf diagonal matrix}\index{Diagonal matrix}. An example of a square diagonal matrix is
$$\begin{pmatrix}
2 & 0 & 0\\
0 & 3 & 0\\
0 & 0 & 0\\
\end{pmatrix}\, .$$

The $r\times r$ diagonal matrix with all diagonal entries equal to $1$ is called the {\bf identity matrix}\index{Identity matrix}, $I_r$, or just $I$.  An identity matrix looks like \[ I=
\begin{pmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\multicolumn{1}{c}{\vdots} & \multicolumn{1}{c}{\vdots} & \multicolumn{1}{c}{\vdots} & \ddots & \multicolumn{1}{c}{\vdots} \\
0 & 0 & 0 & \cdots & 1
\end{pmatrix}.
\]
The identity matrix is special because $$I_rM=MI_k=M$$ for all $M$ of size $r\times k$.
\end{definition}



\begin{definition}
The {\bf transpose}\index{Transpose} of an $r\times k$ matrix $M = (m_j^i)$ is the $k\times r$ matrix 
\[
M^T = (\hat{m}_j^i)
\]
with entries that satisfy $\hat{m}_j^i = m_i^j$. \\

A matrix $M$ is \emph{symmetric}\index{Symmetric matrix} if $M=M^T$.
\end{definition}

\begin{example}
$$\begin{pmatrix}
2 & 5 & 6\\
1 & 3 & 4\\
\end{pmatrix}^T = 
\begin{pmatrix}
2 & 1 \\
5 & 3 \\
6 & 4 \\
\end{pmatrix}\, ,$$
and
$$
\begin{pmatrix}
2 & 5 & 6\\
1 & 3 & 4\\
\end{pmatrix}
\begin{pmatrix}
2 & 5 & 6\\
1 & 3 & 4\\
\end{pmatrix}^T = 
\begin{pmatrix}
65&43\\43&26
\end{pmatrix}\, ,$$
is symmetric.


\end{example}

%\begin{center}\href{\webworkurl ReadingHomework8/2/}{Reading homework: problem 8.2}\end{center}
\Reading{Matrices}{3}

\begin{remark}[Observations]

$\phantom{test}$

\begin{itemize}
\item Only square matrices can be symmetric.

\item The transpose of a column vector is a row vector, and vice-versa. 

\item Taking the transpose of a matrix twice does nothing.  \emph{i.e.,} $(M^T)^T=M$.
\end{itemize}
\end{remark}

\begin{theorem}[Transpose and Multiplication]
Let $M, N$ be matrices such that $MN$ makes sense.  Then 
\Shabox{1.05}{$(MN)^T = N^TM^T$.}
\end{theorem}
The proof of this theorem is left to \hyperref[MN=NTMT]{Review Question~\ref*{MN=NTMT}}.

%%%%%%%%%%%%%%%%%%%
\subsection{Associativity and Non-Commutativity }

Many properties of matrices following from the same property for real numbers. Here is an example.

\begin{example}
{\it Associativity of matrix multiplication.} We know for real numbers $x$, $y$ and $z$ that 
$$
x(yz)=(xy)z\, ,
$$
{\it i.e.}, the order of multiplications does not matter. The same property holds for matrix multiplication, let us show why.
Suppose $M=\big( m^i_j \big)$, $N=\big( n^j_k \big)$ and  $R=\big( r^k_l \big)$ are, 
respectively, $m\times n$, $n\times r$ and $r\times t$ matrices. Then from the rule for matrix
multiplication we have
$$
MN=\Big(\sum_{j=1}^n m^i_j n^j_k\Big)\mbox{ and } NR=\Big(\sum_{k=1}^r n^j_k r^k_l\Big)\, .
$$
So first we compute 
$$
(MN)R=\Big(\sum_{k=1}^r \Big[\sum_{j=1}^n m^i_j n^j_k\Big] r^k_l \Big) = 
\Big(\sum_{k=1}^r \sum_{j=1}^n \Big[ m^i_j n^j_k\Big] r^k_l \Big) =\Big(\sum_{k=1}^r \sum_{j=1}^n m^i_j n^j_k r^k_l \Big)\, .
$$
In the first step we just wrote out the definition for matrix multiplication, in the second step we
moved summation symbol outside the bracket (this is just the distributive
property $x(y+z)=xy+xz$ for numbers) and
in the last step we used the associativity property for real numbers to remove the square brackets. 
Exactly the same reasoning shows that
$$
M(NR)=\Big(\sum_{j=1}^n m^i_j\Big[\sum_{k=1}^r n^j_k r^k_l\Big]\Big) = 
\Big(\sum_{k=1}^r \sum_{j=1}^n  m^i_j \Big[n^j_kr^k_l \Big] \Big) =\Big(\sum_{k=1}^r \sum_{j=1}^n m^i_j n^j_k r^k_l \Big)\, .
$$
This is the same as above so we are done. \footnote{As a fun remark, note that Einstein would simply have written\\
$(MN)R=(m^i_j n^j_k) r^k_l= m^i_j n^j_k r^k_l = m^i_j (n^j_k r^k_l ) = M(NR)$.}
\end{example}
%%%%%%%%%%%%%% non-commute


Sometimes matrices do not share the properties of regular numbers. 
In particular, for {\it generic} $n\times n$ square matrices $M$ and $N$, 
\begin{center}
\shabox{\scalebox{1.1}{
$MN\neq NM\, .$ }}
\end{center}

\Videoscriptlink{matrices_commute.mp4}{Do Matrices Commute?}{script_matrices_commute}


\begin{example} (Matrix multiplication does \emph{not} commute.)
\[
\begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
1 & 1 \\
\end{pmatrix} =
\begin{pmatrix}
2 & 1 \\
1 & 1 \\
\end{pmatrix}
\]
while, on the other hand,
\[
\begin{pmatrix}
1 & 0 \\
1 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix} =
\begin{pmatrix}
1 & 1 \\
1 & 2 \\
\end{pmatrix}\, .
\]
\end{example}
Since $n\times n$ matrices are linear transformations $\Re^n \rightarrow \Re^n$, we can see that the order of successive linear transformations matters.  
%In general  for two linear transformations $K$ and $L$ taking $\Re^n \rightarrow \Re^n$, and $v\in \Re^n$,  one has
%$$K(L(v)) \neq L(K(v))\, .$$
%Finding matrices such that $MN=NM$ is an important problem in mathematics.
%We promise

Here is an example of matrices acting on objects in three dimensions that also shows matrices not commuting.
\begin{example}
In \hyperlink{rotationprob}{Review Problem}~\ref{rotate}, you learned that the matrix
$$M=\begin{pmatrix}\cos\theta & \sin\theta \\ -\sin \theta & \cos\theta\end{pmatrix}\, ,$$
rotates vectors in the plane by an angle~$\theta$. 
We can generalize this, using \hyperlink{blocks}{block matrices}, to three dimensions.
In fact the following matrices built from a $2\times 2$ rotation matrix, a $1\times 1$ identity matrix and zeroes everywhere else
$$
M=\begin{pmatrix}\cos\theta & \sin\theta &0\\ -\sin \theta & \cos\theta&0\\\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0}&1\end{pmatrix}\qquad\mbox{and}\qquad
N=\begin{pmatrix}1&\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0}\\0&\cos\theta & \sin\theta \\ 0&-\sin \theta & \cos\theta\end{pmatrix}\, ,
$$
perform rotations by an angle $\theta$ in the $xy$ and $yz$ planes, respectively. Because, they rotate single vectors, you can also use them to rotate objects built from a collection of vectors like pretty colored blocks! Here is a picture of $M$ and then $N$ acting on such a block, compared with the case of $N$ followed by $M$. The special case of $\theta=90^\circ$ is shown.
\begin{center}
\includegraphics[scale=.3]{MNNM.jpg}
\end{center}
Notice how the endproducts of $MN$ and $NM$ are different, so $MN\neq NM$ here.
\end{example}



\subsection{Block Matrices}

It is often convenient to partition a matrix $M$ into smaller matrices called \hypertarget{blocks}{\emph{blocks}}. For example 

\[
M=\begin{pmat}{ccc|c}
1 & 2 & 3 & 1 \\
4 & 5 & 6 & 0 \\
7 & 8 & 9 & 1 \\
\cline{1-4}
0 & 1 & 2 & 0 \\
\end{pmat}
=
\begin{pmat}{c|c}
A & B \\
\cline{1-2}
C & D \\
\end{pmat}
\]
Where $A = \begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{pmatrix}$, $B=\colvec{1\\0\\1}$, $C=\rowvec{0 & 1 & 2 }$, $D=(0)$.

\begin{itemize}
\item The blocks of a block matrix\index{Block matrix}  must fit together to form a rectangle.  So 
$\begin{pmat}{c|c}
B & A \\
\cline{1-2}
D & C \\
\end{pmat}
$ makes sense, but 
$\begin{pmat}{c|c}
C & B \\
\cline{1-2}
D & A \\
\end{pmat}
$ does not.

%\href{\webworkurl ReadingHomework9/1/}{Reading homework: problem 9.1}
\Reading{Matrices}{4}

\item There are many ways to cut up an $n\times n$ matrix into blocks.  Often context or the entries of the matrix will suggest a useful way to divide the matrix into blocks.  For example, if there are large blocks of zeros in a matrix, or blocks that look like an identity matrix, it can be useful to partition the matrix accordingly.

\item Matrix operations on block matrices can be carried out by treating the blocks as matrix entries.  In the example above,
\begin{eqnarray*}
M^2 & = & \begin{pmat}{c|c}
A & B \\
\cline{1-2}
C & D \\
\end{pmat}
\begin{pmat}{c|c}
A & B \\
\cline{1-2}
C & D \\
\end{pmat} \\[2mm]
& = & \begin{pmat}{c|c}
A^2+BC & AB+BD \\
\cline{1-2}
CA+DC & CB+D^2 \\
\end{pmat} \\
\end{eqnarray*}

Computing the individual blocks, we get:
\begin{eqnarray*}
A^2+BC &=& \begin{pmatrix}
	30 & 37 & 44 \\
	66 & 81 & 96 \\
	102&127 &152 \\
	\end{pmatrix} \\
AB+BD  &=& \colvec{ 4 \\ 10 \\ 16 } \\
CA+DC  &=& \rowvec{ 4 & 10 & 16 } \\
CB+D^2 &=& (2) 
\end{eqnarray*}

Assembling these pieces into a block matrix gives:
\[
\begin{pmat}{ccc|c}
30 & 37 & 44 & 4 \\
66 & 81 & 96 & 10 \\
102 & 127 & 152 & 16 \\
\cline{1-4}
4 & 10 & 16 & 2 \\
\end{pmat}
\]

This is exactly $M^2$.
\end{itemize}

\subsection{The Algebra of Square Matrices }\index{Square matrices}

Not every pair of matrices can be multiplied.  When multiplying two matrices, the number of rows in the left matrix must equal the number of columns in the right.  For an $r\times k$ matrix $M$ and an $s\times l$ matrix $N$, then we must have~$k=s$.

This is not a problem for square matrices of the same size, though.  Two~$n\times n$ matrices can be multiplied in either order.  For a single matrix $M \in \mathbb{M}^n_n$, we can form $M^2=MM$, $M^3=MMM$, and so on. It is useful to define $$M^0=I\, ,$$ the identity matrix, just like $x^0=1$ for numbers.

As a result, any polynomial  can be have square matrices in it's domain. 

\begin{example}
Let $f(x) = x - 2x^2 + 3x^3$
and $$M=\begin{pmatrix}
1 & t \\
0 & 1 \\
\end{pmatrix}\, .$$  
Then 
\[
M^2 = \begin{pmatrix}
1 & 2t \\
0 & 1 \\
\end{pmatrix}\, ,\:\:
M^3 = \begin{pmatrix}
1 & 3t \\
0 & 1 \\
\end{pmatrix}\, ,\: \ldots
\]
and so 
\begin{eqnarray*}
f(M) &=& \begin{pmatrix}
	1 & t \\
	0 & 1 \\
	\end{pmatrix} 
- 2 \begin{pmatrix}
	1 & 2t \\
	0 & 1 \\
	\end{pmatrix} 
+ 3 \begin{pmatrix}
	1 & 3t \\
	0 & 1 \\
	\end{pmatrix} \\
&=& \begin{pmatrix}
	2 & 6t \\
	0 & 2 \\
	\end{pmatrix}.
\end{eqnarray*}
\end{example}

Suppose $f(x)$ is any function defined by a convergent Taylor Series:
\[
f(x) = f(0) + f'(0)x + \frac{1}{2!}f''(0)x^2 + \cdots\, .
\]
Then we can define the matrix function by just plugging in $M$:
\[
f(M) = f(0) + f'(0)M + \frac{1}{2!}f''(0)M^2 + \cdots\, .
\]
There are additional techniques to determine the convergence of Taylor Series of matrices, based on the fact that the convergence problem is simple for diagonal matrices.  It also turns out that the matrix exponential\index{Matrix exponential}
$$\exp (M) = I + M + \frac{1}{2}M^2 + \frac{1}{3!}M^3 + \cdots\, ,$$ always converges.

\Videoscriptlink{properties_of_matrices_example.mp4}{Matrix Exponential Example}{properties_of_matrices_example}


\subsection{Trace}\index{Trace}

A large matrix contains a great deal of information, some of which often reflects the fact that you have not set up your problem efficiently. For example, a clever choice of basis can often make the matrix of a linear transformation very simple. Therefore, finding ways to extract the essential information of a matrix is useful. Here we need to assume that $n < \infty$ otherwise there are subtleties with convergence that we'd have to address.

\begin{definition}
The \hypertarget{TRACE}{{\bf trace}} of a square matrix $M=(m_j^i)$ is the sum of its diagonal entries:
\[
\tr M = \sum_{i=1}^{n}m_i^i\, .
\]
\end{definition}

\begin{example}
\[
\tr \begin{pmatrix}
2 & 7 & 6\\
9 & 5 & 1\\
4 & 3 & 8\\
\end{pmatrix} = 2+5+8 = 15\, .
\]
\end{example}
While matrix multiplication does not commute, the trace of a product of matrices does not depend on the order of multiplication:

\begin{eqnarray*}
\tr(MN) & = & \tr( \sum_l M_l^i N_j^l ) \\
& = & \sum_i \sum_l M_l^i N_i^l \\
& = & \sum_l \sum_i N_i^l M_l^i \\
& = & \tr( \sum_i N_i^l M_l^i ) \\
& = & \tr( NM ).
\end{eqnarray*}
\Videoscriptlink{properties_of_matrices_trace_proof.mp4}{Proof Explanation}{scripts_properties_of_matrices_trace_proof}
Thus we have a Theorem:
\begin{theorem} For any square matrices $M$ and $N$ $$\tr(MN)=\tr(NM).$$
\end{theorem}

\begin{example}
Continuing from the previous example, 

\[
M= \begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix}, N=
\begin{pmatrix}
1 & 0 \\
1 & 1 \\
\end{pmatrix}.
\]
so
\[
MN = \begin{pmatrix}
2 & 1 \\
1 & 1 \\
\end{pmatrix} \neq
NM = \begin{pmatrix}
1 & 1 \\
1 & 2 \\
\end{pmatrix}.
\]
However, $\tr(MN) = 2+1 = 3 = 1+2 = \tr(NM)$.
\end{example}

Another useful property of the trace is that:
\[\tr M = \tr M^T\] 
This is true because the trace only uses the diagonal entries, which are fixed by the transpose.  For example, 
$$\tr \begin{pmatrix}
1 & 1 \\
2 & 3 \\
\end{pmatrix} = 4 = \tr \begin{pmatrix}
1 & 2 \\
1 & 3 \\
\end{pmatrix} = \tr \begin{pmatrix}
1 & 2 \\
1 & 3 \\
\end{pmatrix}^T\, .
$$
Finally, trace is a linear transformation from matrices to the real numbers.  This is easy to check.

%\Videoscriptlink{properties_of_matrices_trace.mp4}{More on the trace function}{scripts_properties_of_matrices_trace}

%\begin{remark}[Linear Systems Redux]
%
%Recall that we can view a linear system as a matrix equation
%\[MX=V,\] 
%with $M$ an $r\times k$ matrix of coefficients, $X$ a $k\times 1$ matrix of unknowns, and $V$ an $r\times 1$ matrix of constants.  If $M$ is a square matrix, then the number of equations~$r$ is the same as the number of unknowns~$k$, so we have hope of finding a single solution.
%
%Above we discussed functions of matrices.  An extremely useful function would be $f(M)=\frac{1}{M}$, where $M\frac{1}{M}=I$.  If we could compute $\frac{1}{M}$, then we would multiply both sides of the equation $MX=V$ by $\frac{1}{M}$ to obtain the solution immediately: $X=\frac{1}{M}V$.
%
%Clearly, if the linear system has no solution, then there can be no hope of finding $\frac{1}{M}$, since if it existed we could find a solution.  On the other hand, if the system has more than one solution, it also seems unlikely that $\frac{1}{M}$ would exist, since $X=\frac{1}{M}V$ yields only a single solution.  
%
%Therefore $\frac{1}{M}$ only sometimes exists.  It is called the \emph{inverse}\index{Inverse matrix!concept of} of $M$, and is usually written $M^{-1}$.
%\end{remark}
%

%\section*{References}
%Beezer: Part T, Section T
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Trace_(linear_algebra)}{Trace (Linear Algebra)}
%\item \href{http://en.wikipedia.org/wiki/Block_matrix}{Block Matrix}
%\end{itemize}
%

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & \hwrref{Matrices}{2}, \hwrref{Matrices}{3},
 \hwrref{Matrices}{4}\\\hline
\end{tabular}

\input{\propMatricesPath/problems}

\section{\inverseMatTitle}
\label{inverse_matrix}


\begin{definition}
A square matrix $M$ is {\bf invertible} (or {\bf nonsingular})\index{Invertible}\index{Nonsingular} if there exists a matrix $M^{-1}$ such that
\[
M^{-1}M=I=MM^{-1}.
\]
If $M$ has no inverse, we say $M$ is {\bf singular}\index{singular} or {\bf non-invertible}\index{Non-invertible}.
\end{definition}

%\begin{figure}
%\begin{center}
%\includegraphics[scale=.27]{\inverseMatPath/defn_inverse_matrix.jpg}
%\end{center}
%\end{figure}


\begin{remark}[Inverse of a $2\times 2$ Matrix] Let $M$ and $N$ be the matrices:
\[
M=\begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix},\qquad N=\begin{pmatrix}
d & -b \\
-c & a \\
\end{pmatrix}
\]
Multiplying these matrices gives:
\[
MN=\begin{pmatrix}
ad-bc & 0 \\
0 & ad-bc \\
\end{pmatrix}=(ad-bc)I\, .
\]
Then $M^{-1}=\frac{1}{ad-bc}\begin{pmatrix}
d & -b \\
-c & a \\
\end{pmatrix}$, so long as $ad-bc\neq 0$.    
\end{remark}

\begin{figure}
\begin{center}
\includegraphics[scale=.24]{\inverseMatPath/2x2_inverse.jpg}
\end{center}
\caption{The formula for the inverse of a 2$\times$2 matrix is worth memorizing!}
\end{figure}



\subsection{Three Properties of the Inverse}

\begin{enumerate}
\item If $A$ is a square matrix and $B$ is the inverse of $A$, then $A$ is the inverse of $B$, since $AB=I=BA$.  So we have the identity
\[
(A^{-1})^{-1}=A.
\]

\item Notice that $B^{-1}A^{-1}AB=B^{-1}IB=I=ABB^{-1}A^{-1}$ so
\Shabox{1}{$
(AB)^{-1}=B^{-1}A^{-1}
$}
Thus, much like the transpose, taking the inverse of a product \emph{reverses} the order of the product.


%\begin{center}
%\includegraphics[scale=.26]{\inverseMatPath/inverse_inverse.jpg}
%\end{center}

\item Finally, recall that $(AB)^T=B^TA^T$.  Since $I^T=I$, then $(A^{-1}A)^T=A^T(A^{-1})^T=I$.  Similarly, $(AA^{-1})^T=(A^{-1})^TA^T=I$.  Then:
\[
(A^{-1})^T=(A^T)^{-1}
\]
%As such, we could even write $A^{-T}$ for the inverse of the transpose of $A$ (or equivalently the transpose of the inverse).
%\begin{center}
%\includegraphics[scale=.20]{\inverseMatPath/transpose_inverse.jpg}
%\end{center}
\end{enumerate}

\Videoscriptlink{inverse_matrix_2by2_example.mp4}{$2\times 2$ Example}{scripts_inverse_matrix_2by2_example}




\subsection{Finding Inverses (Redux)}

Gaussian elimination can be used to find inverse matrices. This concept is covered in chapter~\ref{systems}, section~\ref{EROinverse},
but is presented here again as review in more sophisticated terms.


Suppose $M$ is a square invertible matrix and $MX=V$ is a linear system. The   solution
must be unique because it can be found by multiplying the equation on both sides by $M^{-1}$
yielding
 $X=M^{-1}V$. Thus,
the reduced row echelon form of the linear system has an identity matrix on the left:
\[
\begin{amatrix}{1}
M & V
\end{amatrix}
\sim
\begin{amatrix}{1}
I & M^{-1}V
\end{amatrix}
\]
Solving the linear system $MX=V$ then tells us what $M^{-1}V$ is.  

To solve many linear systems with the same matrix at once, 
$$MX=V_1,~MX=V_2$$
we can consider augmented matrices with 
%a matrix on the right side instead of a column vector, 
many columns on the right 
 and then apply Gaussian row reduction to the left side of the matrix.  Once the identity matrix is on the left side of the augmented matrix, then the solution of each of the individual linear systems is on the right.
 \[
\left(\begin{array}{c|cc}
\!M & V_1&V_2\!
\end{array}\right)
\sim
\left(\begin{array}{c|cc}
\!I & M^{-1}V_1 & M^{-1}V_2\!
\end{array}\right)
\]


To compute $M^{-1}$, we would like $M^{-1}$, rather than $M^{-1}V$ to appear on the right side of our augmented matrix.
This is achieved by  solving the collection of systems $MX=e_k$, where $e_k$ is the column vector of zeroes with a $1$ in the $k$th entry.  
{\it I.e.,} the $n\times n$ identity matrix can be viewed as a bunch of column vectors $I_n=(e_1 \ e_2 \ \cdots e_n)$. So, putting the $e_k$'s together into an identity matrix, we get:
\[
\begin{amatrix}{1}
M & I
\end{amatrix}
\sim
\begin{amatrix}{1}
I & M^{-1}I
\end{amatrix}
=\begin{amatrix}{1}
I & M^{-1}
\end{amatrix}
\]


\begin{example}
Find $\begin{pmatrix}
-1 & 2 & -3 \\
2 & 1 & 0 \\
4 & -2 & 5 \\
\end{pmatrix}^{-1}
$.

\noindent
We start by writing the augmented matrix, then apply row reduction to the left side.

\begin{eqnarray*}
\begin{pmat}{rrr|ccc}
-1 & 2 & -3 & 1 & 0 & 0 \\[1mm]
2  & 1 &  0 & 0 & 1 & 0 \\[1mm]
 4 & -2 & 5 & 0 & 0 & 1 \\[1mm]
\end{pmat} & \sim & \begin{pmat}{crr|ccc}
1  & -2&  3  & 1 & 0 & 0 \\[1mm]
0  & 5 &  -6 & 2 & 1 & 0 \\[1mm]
 0 & 6 & -7  & 4 & 0 & 1 \\[1mm]
\end{pmat} \\[2mm]
& \sim & \begin{pmat}{ccr|rrc}
1  & 0 &  \frac{3}{5}  & -\frac{1}{4} & \frac{2}{5} & 0 \\[1mm]
0  & 1 &  -\frac{6}{5} & \frac{2}{5} & \frac{1}{5}  & 0 \\[1mm]
 0 & 0 &  \frac{1}{5}  & \frac{4}{5} & -\frac{6}{5} & 1 \\[1mm]
\end{pmat} \\[2mm]
& \sim & \begin{pmat}{ccc|rrr}
1  & 0 &  0  & -5 & 4 & -3 \\[1mm]
0  & 1 &  0  & 10 & -7 & 6 \\[1mm]
 0 & 0 &  1  & 8 & -6 & 5 \\[1mm]
\end{pmat} \\
\end{eqnarray*}
At this point, we know $M^{-1}$ assuming we didn't goof up\index{Goofing up}.  However, row reduction is a lengthy and  involved process with lots of room for arithmetic errors, so we should~\emph{check our answer,} by confirming that $MM^{-1}=I$ (or if you prefer $M^{-1}M=I$):
\[MM^{-1} = 
\begin{pmatrix}
-1 & 2 & -3 \\
2 & 1 & 0 \\
4 & -2 & 5 \\
\end{pmatrix}\begin{pmatrix}
-5 & 4 & -3 \\
10 & -7 & 6 \\
 8 & -6 & 5 \\
\end{pmatrix}
=\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
\]  
The product of the two matrices is indeed the identity matrix, so we're done.
\end{example}

%\href{\webworkurl ReadingHomework10/1/}{Reading homework: problem 10.1}
\Reading{Matrices}{5}
\subsection{Linear Systems and Inverses}

If $M^{-1}$ exists and is known, then we can immediately solve linear systems associated to $M$.

\begin{example}
Consider the linear system:

\[
      \begin{linsys}{2}
            -x & +2y & -3z         &=& 1  \\[1mm]
            2x & +\ y\,   &             &=& 2 \\[1mm]
            4x & -2y & +5z         &=& 0  
      \end{linsys}
\]
The associated matrix equation is $MX=\colvec{1\\2\\0},$ where \(M\) is the same as in the previous section, so the system above is equivalent to the matrix equation

\[
\colvec{x\\y\\z}=\begin{pmatrix}
-1 & 2 & -3 \\
2 & 1 & 0 \\
4 & -2 & 5 \\
\end{pmatrix}^{-1}\colvec{1\\2\\0}
=\begin{pmatrix}
-5 & 4 & -3 \\
10 & -7 & 6 \\
 8 & -6 & 5 \\
\end{pmatrix}\colvec{1\\2\\0}
=\colvec{3\\-4\\-4}.
\]
That is, the system is equivalent to the equation $\colvec{x\\y\\z}=\colvec{3\\-4\\-4}$, and it is easy to see what the solution(s) to this equation are. 
\end{example}
In summary, when $M^{-1}$ exists 

\begin{center}
\shabox{$Mx=v \Leftrightarrow x=M^{-1}v\, .$}
\end{center}


%\href{\webworkurl ReadingHomework10/2/}{Reading homework: problem 10.2}
\Reading{Matrices}{5}

\subsection{Homogeneous Systems}

\begin{theorem}
A square matrix $M$ is invertible if and only if the homogeneous system $$Mx=0$$ has no non-zero solutions.
\end{theorem}

\begin{proof}
First, suppose that $M^{-1}$ exists.  Then $Mx=0 \Rightarrow x=M^{-1}0=0$.  Thus, if $M$ is invertible, then $Mx=0$ has no non-zero solutions.

On the other hand, $Mx=0$ always has the solution $x=0$.  If no other solutions exist, then $M$ can be put into reduced row echelon form with every variable a pivot.  In this case, $M^{-1}$ can be computed using the process in the previous section.
\end{proof}

%\begin{figure}
\begin{center}
\includegraphics[scale=.26]{\inverseMatPath/inverse_theorem.jpg}
\end{center}
%\end{figure}

%A great test of your linear algebra knowledge is to make a list of conditions for a matrix to be singular.
%You will learn more of these as the course goes by, but can also skip straight to the list in Section~\ref{thelist}. 

\subsection{Bit Matrices}\index{Bit matrices}
In computer science, information is recorded using binary strings of data.  For example, the following string contains an English word:
\[
011011000110100101101110011001010110000101110010
\]
A \hypertarget{bits}{\emph{bit}} is the basic unit of information, keeping track of a single one or zero.  Computers can add and multiply individual bits very quickly.

In chapter~\ref{vectorSpaces}, section~\ref{otherfields} it is explained how to formulate vector spaces over \hyperref[fields]{fields} other than real numbers.
In particular, al of the properties of a 
vector space make sense with numbers $\Z_2=\{0,1 \}$ with addition and multiplication given by the following tables. 
\label{Z2}
\[
\begin{array}{c|cc}
+ & 0 & 1 \\ \hline
0 & 0 & 1 \\
1 & 1 & 0 \\
\end{array}
\qquad
\begin{array}{c|cc}
\times& 0 & 1 \\ \hline
0 & 0 & 0 \\
1 & 0 & 1 \\
\end{array}
\]
Notice that $-1=1$, since $1+1=0$.
Therefore,  we can apply all of the linear algebra we have learned thus far to matrices with $\Z_2$ entries.  A matrix with entries in $\Z_2$ is sometimes called a \emph{bit matrix}\index{Bit Matrix}.

\begin{example}
$\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}$ is an invertible matrix over $\Z_2$;

\[\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}^{-1}=\begin{pmatrix}
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}.
\]

This can be easily verified by multiplying:

\[\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}\begin{pmatrix}
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}=\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
\]
\end{example}

\begin{remark}[Application: Cryptography]  
A very simple way to hide information is to use a substitution cipher, in which the alphabet is permuted and each letter in a message is systematically exchanged for another.  For example, the ROT-13 cypher just exchanges a letter with the letter thirteen places before or after it in the alphabet.  For example, HELLO becomes URYYB.  Applying the algorithm again decodes the message, turning URYYB back into HELLO.  Substitution ciphers are easy to break, but the basic idea can be extended to create cryptographic systems that are practically uncrackable.  For example, a \emph{one-time pad} is a system that uses a different substitution for each letter in the message.  So long as a particular set of substitutions is not used on more than one message, the one-time pad is unbreakable.

English characters are often stored in computers in the ASCII format.  In ASCII, a single character is represented by a string of eight bits, which we can consider as a vector in $\Z_2^8$ (which is like vectors in $\Re^8$, where the entries are zeros and ones).  One way to create a substitution cipher, then, is to choose an $8\times 8$ invertible bit matrix $M$, and multiply each letter of the message by $M$.  Then to decode the message, each string of eight characters would be multiplied by $M^{-1}$.  

To make the message a bit tougher to decode, one could consider pairs (or longer sequences) of letters as a single vector in $\Z_2^{16}$ (or a higher-dimensional space), and then use an appropriately-sized invertible matrix.
For more on cryptography, see ``The Code Book,'' by Simon Singh (1999, Doubleday).
\end{remark}

%{\it You should now be ready to attempt the \hyperref[sample1]{first sample midterm}.}
%\begin{center}
%\shabox{
%\begin{tabular}{c}
%\it \hyperref[sample1]{You are now ready to attempt the first sample midterm}. 
%\\
%\hyperref[sample1]{\includegraphics[scale=.15]{midterm.jpg}}
%\end{tabular}
%}
%\end{center}
%
%\section*{References}
%
%Hefferon: Chapter Three, Section IV.2
%\\
%Beezer: Chapter M, Section MISLE
%\\
%Wikipedia:
%\href{http://en.wikipedia.org/wiki/Invertible_matrix}{Invertible Matrix}
%
%
\section{Review Problems}
{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & \hwrref{Matrices}{6}, \hwrref{Matrices}{7}
 \\
 \hline
\end{tabular}


\input{\inverseMatPath/problems}

\newpage

\section{LU Redux}
\label{LUdecomp}

Certain matrices are easier to work with than others.  In this section, we will see how to write any square\footnote{The case where $M$ is not square is dealt with at the end of the section.} matrix $M$ as the product of two simpler matrices.  We will write $$M=LU\, ,$$ where:
\begin{itemize}
\item $L$ is \emph{lower triangular}\index{Lower triangular matrix}.  This means that all entries above the main diagonal are zero.  In notation,
$L=(l^i_j)$ with $l^i_j=0$ for all $j>i$.
\[L=\begin{pmatrix}
l^1_1 & 0 & 0 & \cdots \\
l^2_1 & l^2_2 & 0 & \cdots \\
l^3_1 & l^3_2 & l^3_3 & \cdots \\
\vdots & \vdots & \vdots & \ddots \\
\end{pmatrix}
\]

\item $U$ is \emph{upper triangular}\index{Upper triangular matrix}.  This means that all entries below the main diagonal are zero.  In notation,
$U=(u^i_j)$ with $u^i_j=0$ for all $j<i$.
\[U=\begin{pmatrix}
u^1_1 & u^1_2 & u^1_3 & \cdots \\
0 & u^2_2 & u^2_3 & \cdots \\
0 & 0 & u^3_3 & \cdots \\
\vdots & \vdots & \vdots & \ddots \\
\end{pmatrix}
\]
\end{itemize}
$M=LU$ is called an \emph{$LU$ decomposition}\index{LU@$LU$ decomposition} of $M$.

This is a useful trick for  computational reasons; it is much easier to compute the inverse of an upper or lower triangular matrix than general matrices.  Since inverses are useful for solving linear systems, this makes solving any linear system associated to the matrix much faster as well.  The determinant---a very important quantity associated with any square matrix---is very easy to compute for triangular matrices.

\begin{example}
Linear systems associated to upper triangular matrices are very easy to solve by back substitution.
\[
\begin{amatrix}{2}
a & b & 1 \\
0 & c & e \\
\end{amatrix} \ \Rightarrow \ y=\frac{e}{c}\, , \quad x=\frac{1}{a}\left(1-\frac{be}{c}\right)
\]

$$
\begin{amatrix}{3}
1 & 0 & 0 & d \\
a & 1 & 0 & e \\
b & c & 1 & f \\
\end{amatrix} 
\Rightarrow 
\left\{    \begin{array}{l} x=d\\   y=e-ax\\  z=f-bx-cy \end{array} \right\}
\Rightarrow 
\left\{    \begin{array}{l} x=d\\   y=e-ad\\  z=f-bd-c(e-ad) \end{array} \right. .
$$
For lower triangular matrices, 
\emph{forward} substitution\index{Forward substitution} 
gives a quick solution; for upper triangular matrices, 
\emph{back} substitution\index{Back substitution} 
gives the solution.
\end{example}





\subsection{Using $LU$ Decomposition to Solve Linear Systems}

Suppose we have $M=LU$ and want to solve the system
\[
MX=LUX=V.
\]

\begin{itemize}
\item{Step 1:} Set $W=\colvec{u\\v\\w}=UX$.  

\item{Step 2:} Solve the system $LW=V$.  This should be simple by forward substitution since $L$ is lower triangular.  Suppose the solution to $LW=V$ is $W_0$.  

\item{Step 3:} Now solve the system $UX=W_0$.  This should be easy by backward substitution, since $U$ is upper triangular.  The solution to this system is the solution to the original system.
\end{itemize}
We can think of this as using the matrix $L$ to perform row operations on the matrix $U$ in order to solve the system; this idea also appears in the  study of determinants.

%\href{\webworkurl ReadingHomework11/1/}{Reading homework: problem 11.1}
\Reading{Matrices}{7}

\begin{example}
Consider the linear system:
\[
      \begin{linsys}{4}
            6x & +&18y & +&3z         &=& 3  \\[1mm]
            2x & +&12y & +&z	    &=& 19 \\[1mm]
            4x & +&15y & +&3z         &=& 0  
      \end{linsys}
\]

An $LU$ decomposition for the associated matrix $M$ is
\[
\begin{pmatrix}
6 & 18 & 3 \\
2 & 12 & 1 \\
4 & 15 & 3 
\end{pmatrix} =
\begin{pmatrix}
3 & 0 & 0 \\
1 & 6 & 0 \\
2 & 3 & 1 
\end{pmatrix}
\begin{pmatrix}
2 & 6 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{pmatrix}.
\]

\begin{itemize}
\item{Step 1:} \hypertarget{LUproc}{Set} $W=\colvec{u\\v\\w}=UX$.  

\item{Step 2:} Solve the system $LW=V$:

\[
\begin{pmatrix}
3 & 0 & 0 \\
1 & 6 & 0 \\
2 & 3 & 1 
\end{pmatrix}
\colvec{u\\v\\w} =
\colvec{3\\19\\0}
\]

By substitution, we get $u=1$, $v=3$, and $w=-11$.  Then 
\[W_0=\colvec{1\\3\\-11}\]

\item{Step 3:} Solve the system $UX=W_0$.  
\[
\begin{pmatrix}
2 & 6 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{pmatrix}
\colvec{x\\y\\z} =
\colvec{1\\3\\-11}
\]
Back substitution gives $z=-11, y=3$, and $x=-3$.  

Then $X=\colvec{-3\\3\\-11}$, and we're done.
\end{itemize}
\end{example}

\Videoscriptlink{lu_decomposition_using_lu_decomp.mp4}{Using an $LU$ decomposition}{scripts_lu_decomposition_using_lu_example}

%\begin{figure}
\begin{center}
\includegraphics[scale=.3]{\luDecompPath/LU_solution.jpg}
\end{center}
%\end{figure}

\subsection{Finding an $LU$ Decomposition.}
\label{finding_LU_decomp}

In chapter~\ref{systems}, section~\ref{LUtake1}, Gaussian elimination was used to find~$LU$ matrix decompositions.  
These ideas are presented here again as review.
 
For any given matrix, there are actually many different $LU$ decompositions.  However, there is a unique $LU$ decomposition in which the $L$ matrix has ones on the diagonal. In that case $L$ is called a \emph{lower unit triangular matrix}\index{Lower unit triangular matrix}.

To find the $LU$ decomposition, we'll create two sequences of matrices $L_1, L_2,\ldots$ and $U_1, U_2, \ldots$ such that at each step, $L_iU_i=M$.  Each of the $L_i$ will be lower triangular, but only the last $U_i$ will be upper triangular.
 The main trick for this calculation is captured by the following example:

\begin{example} (An Elementary Matrix)

\noindent
Consider $$E=\begin{pmatrix}1&0\\\lambda&1\end{pmatrix}\, ,\qquad M=\begin{pmatrix}a&b&c&\cdots\\d&e&f&\cdots\end{pmatrix}\, .$$
Lets compute $EM$
$$
EM=\begin{pmatrix}\multicolumn{1}{c}{a}&\mc{b}&\mc{c}&\cdots\\d+\lambda a&e+\lambda b&f+\lambda c&\cdots\end{pmatrix}\, .
$$
Something neat happened here: multiplying $M$ by $E$ performed the row operation $R_2\to R_2+\lambda R_1$ on $M$.
Another interesting fact:
$$
E^{-1}:=\begin{pmatrix}1&0\\-\lambda&1\end{pmatrix}
$$ 
obeys (check this yourself...)
$$
E^{-1} E = 1\, .
$$
Hence $M=E^{-1} E M$ or, writing this out
$$
\begin{pmatrix}a&b&c&\cdots\\d&e&f&\cdots\end{pmatrix}=\begin{pmatrix}1&0\\-\lambda&1\end{pmatrix} \begin{pmatrix}\mc{a}&\mc{b}&\mc{c}&\cdots\\d+\lambda a&e+\lambda b&f+\lambda c&\cdots\end{pmatrix}\, .
$$
Here the matrix on the left is lower triangular, while the matrix on the right has had a row operation performed on it.
\end{example}




\vspace{2mm}
We would like to  use the first row of $M$ to zero out the first entry of every row below it.  For our running example, $$M=\begin{pmatrix}
6 & 18 & 3 \\
2 & 12 & 1 \\
4 & 15 & 3 
\end{pmatrix}\, ,$$ so we would like to perform the row operations $$R_2\to R_2 -\frac 13 R_1 \mbox{ and } R_3\to R_3-\frac 23R_1\, .$$
%so the second row minus $\frac{1}{3}$ of the first row will zero out the first entry in the second row.  Likewise, the third row minus $\frac{2}{3}$ of the first row will zero out the first entry in the third row.
If we perform these row operations on $M$ to produce 
$$U_1=\begin{pmatrix}
6 & 18 & 3 \\
0 & 6 & 0 \\
0 & 3 & 1 
\end{pmatrix}\, ,$$
we need to multiply this on the left by a lower triangular matrix $L_1$ so that the product $L_1U_1=M$ still.
The above example shows how to do this:
Set $L_1$ to be the lower triangular matrix whose first column is filled with  {\bf minus} the constants used to zero out the first column of $M$.  Then $$L_1 = \begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & 0 & 1 
\end{pmatrix}\, .$$  
%Set $U_1$ to be the matrix obtained by zeroing out the first column of $M$.  Then $U_1=\begin{pmatrix}
%6 & 18 & 3 \\
%0 & 6 & 0 \\
%0 & 3 & 1 
%\end{pmatrix}$.
By construction $L_1 U_1=M$, but you should compute this yourself as a double check.

Now repeat the process by zeroing the second column of $U_1$ below the diagonal using the second row of $U_1$ using the row operation
$R_3\to R_3-\frac 12 R_2$ to produce
$$U_2=\begin{pmatrix}6&18&3\\0&6&0\\0&0&1\end{pmatrix}\, .$$
The matrix that undoes this row operation is obtained in the same way we found $L_1$ above and is:
$$
\begin{pmatrix}
1&0&0\\
0&1&0\\
0&\frac 12& 1
\end{pmatrix}\, .
$$
Thus our answer for $L_2$ is the product of this matrix with $L_1$, namely
$$
L_2=
\begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & 0 & 1 
\end{pmatrix}\begin{pmatrix}
1&0&0\\
0&1&0\\
0&\frac 12& 1
\end{pmatrix}
=\begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}\, .
$$
Notice that it is lower triangular because 

\begin{center}
\shabox{The product of lower triangular matrices is always lower triangular!}
\end{center}

\noindent
Moreover it is obtained by recording minus the constants used for all our row operations in the appropriate columns (this always works this way).
Moreover, $U_2$ is upper triangular and $M=L_2U_2$, we are done!
Putting this all together we have
$$M=\begin{pmatrix}
6 & 18 & 3 \\
2 & 12 & 1 \\
4 & 15 & 3 
\end{pmatrix}= \begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}\begin{pmatrix}
6 & 18 & 3 \\[1mm]
0 & 6 & 0 \\[1mm]
0 & 0 & 1 
\end{pmatrix}\, .$$  
%Since $U_2$ is upper-triangular, we're done.  Inserting the new number into $L_1$ to get $L_2$ really is safe: the numbers in the first column don't affect the second column of $U_1$, since the first column of $U_1$ is already zeroed out.

If the matrix you're working with has more than three rows, just continue this process by zeroing out the next column below the diagonal, and repeat until there's nothing left to do.

\Videoscriptlink{lu_decomposition_example.mp4}{Another $LU$ decomposition example}{scripts_lu_decomposition_example}

The fractions in the $L$ matrix are admittedly ugly.  For two matrices $LU$, we can multiply one entire column of $L$ by a constant $\lambda$ and divide the corresponding row of $U$ by the same constant without changing the product of the two matrices.  Then:

\begin{eqnarray*}
LU &=& \begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}
I
\begin{pmatrix}
6 & 18 & 3 \\[1mm]
0 & 6 & 0 \\[1mm]
0 & 0 & 1 
\end{pmatrix} \\
&=&
\begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}
\begin{pmatrix}
3 & 0 & 0 \\
0 & 6 & 0 \\
0 & 0 & 1 
\end{pmatrix}
\begin{pmatrix}
\frac{1}{3} & 0 & 0 \\[1mm]
0 & \frac{1}{6} & 0 \\[1mm]
0 & 0 & 1 
\end{pmatrix}
\begin{pmatrix}
6 & 18 & 3 \\
0 & 6 & 0 \\
0 & 0 & 1 
\end{pmatrix} \\
&=&
\begin{pmatrix}
3 & 0 & 0 \\
1 & 6 & 0 \\
2 & 3 & 1 
\end{pmatrix}\begin{pmatrix}
2 & 6 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{pmatrix}.
\end{eqnarray*}
The resulting matrix looks nicer, but isn't in standard (lower unit triangular matrix) form.

\Reading{Matrices}{7}
%\href{\webworkurl ReadingHomework11/2/}{Reading homework: problem 11.2}

For matrices that are not square, $LU$ decomposition still makes sense.  Given an $m\times n$ matrix $M$, for example we could write $M=LU$ with $L$ a square lower unit triangular matrix, and $U$ a rectangular matrix.  Then $L$ will be an $m\times m$ matrix, and $U$ will be an $m\times n$ matrix (of the same shape as $M$).  From here, the process is exactly the same as for a square matrix.  We create a sequence of matrices $L_i$ and $U_i$ that is eventually the $LU$ decomposition.  Again, we start with $L_0=I$ and $U_0=M$.

\begin{example}
Let's find the $LU$ decomposition of $M=U_0=\begin{pmatrix}
-2 & 1 & 3 \\
-4 & 4 & 1 
\end{pmatrix}$.  Since $M$ is a $2\times 3$ matrix, our decomposition will consist of a $2\times 2$ matrix and a $2\times 3$ matrix.  Then we start with $L_0=I_2=\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}$.

The next step is to zero-out the first column of $M$ below the diagonal.  There is only one row to cancel, then, and it can be removed by subtracting $2$ times the first row of $M$ to the second row of $M$.  Then:

\[
L_1=\begin{pmatrix}
1 & 0 \\
2 & 1
\end{pmatrix}, \qquad 
U_1 = \begin{pmatrix}
-2 & 1 & 3 \\
0 & 2 & -5 
\end{pmatrix}
\]
Since $U_1$ is upper triangular, we're done.  With a larger matrix, we would just continue the process.
\end{example}





\subsection{Block $LDU$ Decomposition}

Let $M$ be a square block matrix with square blocks $X,Y,Z,W$ such that $X^{-1}$ exists.  Then $M$ can be decomposed as a block $LDU$ decomposition, where $D$ is block diagonal, as follows:
\[
M=\begin{pmatrix}
X & Y \\
Z & W
\end{pmatrix}
\]
Then: 
\begin{center}
\shabox{
$M=\left(\begin{array}{cc}
I &  0 \\[1mm]
ZX^{-1} & I
\end{array}\right)
\left(\begin{array}{cc}
X & 0 \\[1mm]
0 & W-ZX^{-1}Y
\end{array}\right)\left(\begin{array}{cc}
I & X^{-1}Y \\[1mm]
0 & I
\end{array}\right)\, .$}\end{center}
This can be checked explicitly simply by block-multiplying these three matrices.

\Videoscriptlink{lu_decomposition_blocks.mp4}{Block $LDU$ Explanation}{scripts_lu_decomposition_blocks}

\begin{example}
For a $2\times 2$ matrix, we can regard each entry as a $1\times1$ block.
\[
\begin{pmatrix}
1 & 2 \\
3 & 4
\end{pmatrix}=
\begin{pmatrix}
1 & 0 \\
3 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
0 & -2
\end{pmatrix}
\begin{pmatrix}
1 & 2 \\
0 & 1
\end{pmatrix}
\]
By multiplying the diagonal matrix by the upper triangular matrix, we get the standard $LU$ decomposition of the matrix.
\end{example}


%{\bf You should now be ready to attempt the \hyperref[sample1]{first sample midterm}.}
\begin{center}
\shabox{
\begin{tabular}{c}
\bf \hyperref[sample1]{You are now ready to attempt the first sample midterm}. 
\\
\hyperref[sample1]{\includegraphics[scale=.15]{midterm.jpg}}
\end{tabular}
}
\end{center}

%\section*{References}
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/LU_decomposition}{$LU$ Decomposition}
%\item \href{http://en.wikipedia.org/wiki/Block_LU_decomposition}{Block $LU$ Decomposition}
%\end{itemize}

\section{Review Problems}
{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{Matrices}{7},\hwrref{Matrices}{8}\\
 $LU$ Decomposition & \hwref{Matrices}{14}\\
 \hline
\end{tabular}
\input{\luDecompPath/problems}

\newpage




",lesson
10,Properties Of Matrices,"\chapter{\propMatricesTitle} \label{properties_matrices}

The objects of study in linear algebra are linear operators. 
We have seen that linear operators can be represented as matrices through choices of ordered bases, and that matrices provide a means of efficient computation. 

We now begin an in depth study of matrices.

\begin{definition}
An $r\times k$ matrix\index{Matrix} $M=(m^i_j)$ for $i=1, \ldots, r; j=1, \ldots, k$ is a rectangular array of real (or complex) numbers:
\label{matrixnotation}
\[M = 
\begin{pmatrix}
m_1^1 & m_2^1 & \cdots & m_k^1 \\
m_1^2 & m_2^2 & \cdots & m_k^2 \\
\vdots & \vdots &   & \vdots \\
m_1^r & m_2^r & \cdots & m_k^r \\
\end{pmatrix}
\]
The numbers $m^i_j$ are called \emph{entries}\index{Matrix!entries of}.  The superscript indexes the row of the matrix and the subscript indexes the column of the matrix in which $m_j^i$ appears\footnote{This notation was first introduced by Albert Einstein.}.
\end{definition}

%It is often useful to consider matrices whose entries are more general than the real numbers, so we allow that possibility.

An $r\times 1$ matrix $v = (v^r_1) = (v^r)$ is called a \emph{column vector}\index{Column vector}, written $$v = \colvec{v^1\\v^2\\ \vdots \\ v^r }\, .$$  A $1\times k$ matrix $v = (v^1_k) = (v_k)$ is called a \emph{row vector}\index{Row vector}, written $$v = \rowvec{v_1 & v_2 & \cdots & v_k }\, .$$  


A matrix is a very useful and efficient way to store information:

\begin{example}
In computer graphics, you may have encountered image files with a .gif extension.  These files are actually just matrices: at the start of the file the size of the matrix is given, and then each entry of the matrix is a number indicating the color of a particular pixel in the image.

The resulting matrix  then has its rows shuffled a bit: by listing, say, every eighth row, then a web browser downloading the file can start displaying an incomplete version of the picture before the download is complete.

Finally, a compression algorithm is applied to the matrix to reduce the size of the file.
\end{example}

\videoscriptlink{matrices_example.mp4}{Adjacency Matrix Example}{scripts_matrices_example}

\begin{example}
Graphs occur in many applications, ranging from telephone networks to airline routes.  In the subject of \emph{graph theory}\index{Graph theory}, a graph is just a collection of vertices and some edges connecting vertices.  A matrix can be used to indicate how many edges attach one vertex to another.

\begin{center}
\includegraphics[width=10cm]{notes8-0.png}
\end{center}
For example, the graph pictured above would have the following matrix, where $m^i_j$ indicates the number of edges between the vertices labeled $i$~and~$j$:

\[
M = \begin{pmatrix}
1 & 2 & 1 & 1 \\
2 & 0 & 1 & 0 \\
1 & 1 & 0 & 1 \\
1 & 0 & 1 & 3 \\
\end{pmatrix}
\]
This is an example of a \emph{symmetric matrix}, since $m_j^i = m_i^j$.
\end{example}

The space of $r\times k$ matrices $M_k^r$ is a vector space with the addition and scalar multiplication defined as follows:

\begin{align*}
&M+N = (m_j^i) + (n_j^i) = ( m_j^i + n_j^i ) \\
&rM = r(m_j^i) = (rm_j^i)
\end{align*}
In other words, addition just adds corresponding entries in two matrices, and scalar multiplication multiplies every entry.
Notice that $M_1^n = \Re^n$ is just the vector space of column vectors.

Recall that we can multiply an \(r \times k\) matrix by a \(k \times 1\) column vector to produce a \(r \times 1\) column vector using the rule
\[MV = \big(\sum_{j=1}^k m_j^i v^j\big)\, .\]

This suggests a rule for multiplying an \(r \times k\) matrix \(M\) by a \(k \times s\) matrix~\(N\): our \(k \times s\) matrix \(N\) consists of \(s\) column vectors side-by-side, each of dimension \(k \times 1.\) We can multiply our \(r \times k\) matrix \(M\) by each of these \(s\) column vectors using the rule we already know, obtaining \(s\) column vectors each of dimension \(r \times 1.\) If we place these \(s\) column vectors side-by-side, we obtain an \(r \times s\) matrix \(MN.\)

That is, let \[N = 
\begin{pmatrix}
n_1^1 & n_2^1 & \cdots & n_s^1 \\
n_1^2 & n_2^2 & \cdots & n_s^2 \\
\vdots & \vdots &   & \vdots \\
n_1^k & n_2^k & \cdots & n_s^k \\
\end{pmatrix}
\]
and call the columns \(N_1\) through \(N_s\):
\[N_1 = \colvec{n_1^1\\n_1^2\\\vdots\\n_1^k},
N_2 = \colvec{n_2^1\\n_2^2\\\vdots\\n_2^k},
\ldots,
N_s = \colvec{n_s^1\\n_s^2\\\vdots\\n_s^k}.
\]
Then
%\[
%MN=M \rowvec{N_1 & N_2 & \cdots & N_s} = \rowvec{MN_1 & MN_2 & \cdots & MN_s}.
%\]
\[
MN=M
\begin{pmatrix}
| & | & & | \\
N_1 & N_2 & \cdots & N_s \\
| & | & & | \\
\end{pmatrix}
=
\begin{pmatrix}
| & | & & | \\
MN_1 & MN_2 & \cdots & MN_s \\
| & | & & | \\
\end{pmatrix}
\]

A more concise way to write this rule is: If \(M=(m^i_j)\) for \(i=1, \ldots, r; j=1, \ldots, k\) and \(N=(n^i_j)\) for \(i=1, \ldots, k; j=1, \ldots, s,\) then \(MN=L\) where \(L=(\ell^i_j)\) for \(i=i, \ldots, r; j=1, \ldots, s\) is given by
\[
\ell^i_j = \sum_{p=1}^k m^i_p n^p_j.
\]
This rule obeys linearity.

Notice that in order for the multiplication to make sense, the columns and rows must match.  For an $r\times k$ matrix $M$ and an $s\times m$ matrix $N$, then to make the product $MN$ we must have $k=s$.  Likewise, for the product $NM$, it is required that $m=r$.  A common shorthand for keeping track of the sizes of the matrices involved in a given product is: 
\[\Big(r \times k\Big)\times \Big(k\times m\Big) = \Big(r\times m\Big)\]

\begin{example}
Multiplying a $(3\times 1)$ matrix and a $(1\times 2)$ matrix yields a $(3\times 2)$ matrix.

\[
\colvec{1\\3\\2} \rowvec{2 & 3} = 
\begin{pmatrix}
1\cdot 2 & 1\cdot 3 \\
3\cdot 2 & 3\cdot 3 \\
2\cdot 2 & 2\cdot 3 \\
\end{pmatrix}
= \begin{pmatrix}
2 & 3 \\
6 & 9 \\
4 & 6 \\
\end{pmatrix}
\]
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework8/1/}{Reading homework: problem 8.1}\end{center}

Recall that $r\times k$ matrices can be used to represent linear transformations 
$ \Re^k \rightarrow \Re^r $
via $$MV = \sum_{j=1}^{k} m_j^iv^j , $$ which is the same rule we use when we multiply an $r\times k$ matrix by a $k\times 1$ vector to produce an $r\times1$ vector.

Likewise, we can use a matrix $N=(n^i_j)$ to define a linear transformation of a vector space of matrices. For example
\[
L \colon M^s_k \stackrel{N}{\longrightarrow} M^r_k
\]
\[
L(M)^i_l = \sum_{j=1}^{s} n_j^im^j_l.
\]
This is the same as the rule we use to multiply matrices. \hypertarget{leftmult}{In other words,} \(L(M)=NM\) is a linear transformation.

\begin{definition}[Matrix Terminology]
The entries $m_i^i$ are called \emph{diagonal}, and the set $\{m_1^1$, $m_2^2$, $\ldots \}$~is called the \emph{diagonal of the matrix}\index{Matrix!diagonal of}.

Any $r\times r$ matrix is called a \emph{square matrix}\index{Square matrix}.  A square matrix that is zero for all non-diagonal entries is called a diagonal matrix\index{Diagonal matrix}.

The $r\times r$ diagonal matrix with all diagonal entries equal to $1$ is called the \emph{identity matrix}\index{Identity matrix}, $I_r$, or just $\1$.  An identity matrix looks like \[
\begin{pmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{pmatrix}.
\]
The identity matrix is special because $$I_rM=MI_k=M$$ for all $M$ of size $r\times k$.
\end{definition}

In the matrix given by the product of matrices  above, the diagonal entries are $2$ and $9$.
An example of a diagonal matrix is 
$$\begin{pmatrix}
2 & 0 & 0\\
0 & 3 & 0\\
0 & 0 & 0\\
\end{pmatrix}\, .$$

\begin{definition}
The \emph{transpose}\index{Transpose} of an $r\times k$ matrix $M = (m_j^i)$ is the $k\times r$ matrix with entries
\[
M^T = (\hat{m}_j^i)
\]
with $\hat{m}_j^i = m_i^j$. 

A matrix $M$ is \emph{symmetric}\index{Symmetric matrix} if $M=M^T$.
\end{definition}

\begin{example}
$\begin{pmatrix}
2 & 5 & 6\\
1 & 3 & 4\\
\end{pmatrix}^T = 
\begin{pmatrix}
2 & 1 \\
5 & 3 \\
6 & 4 \\
\end{pmatrix}$
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework8/2/}{Reading homework: problem 8.2}\end{center}

\begin{remark}[Observations]

$\phantom{test}$

\begin{itemize}
\item Only square matrices can be symmetric.

\item The transpose of a column vector is a row vector, and vice-versa. 

\item Taking the transpose of a matrix twice does nothing.  \emph{i.e.,} $(M^T)^T=M$.
\end{itemize}
\end{remark}

\begin{theorem}[Transpose and Multiplication]
Let $M, N$ be matrices such that $MN$ makes sense.  Then $(MN)^T = N^TM^T$.
\end{theorem}
The proof of this theorem is left to \hyperref[MN=NTMT]{Review Question~\ref*{MN=NTMT}}.

%%%%%%%%%%%%%%%%%%%
\section{Associativity and Non-Commutativity }

Many properties of matrices following from the same property for real numbers. Here is an example.

\begin{example}
{\it Associativity of matrix multiplication.} We know for real numbers $x$, $y$ and $z$ that 
$$
x(yz)=(xy)z\, ,
$$
{\it i.e.} the order of bracketing does not matter. The same property holds for matrix multiplication, let us show why.
Suppose $M=\big( m^i_j \big)$, $N=\big( n^j_k \big)$ and  $R=\big( r^k_l \big)$ are, 
respectively, $m\times n$, $n\times r$ and $r\times t$ matrices. Then from the rule for matrix
multiplication we have
$$
MN=\Big(\sum_{j=1}^n m^i_j n^j_k\Big)\mbox{ and } NR=\Big(\sum_{k=1}^r n^j_k r^k_l\Big)\, .
$$
So first we compute 
$$
(MN)R=\Big(\sum_{k=1}^r \Big[\sum_{j=1}^n m^i_j n^j_k\Big] r^k_l \Big) = 
\Big(\sum_{k=1}^r \sum_{j=1}^n \Big[ m^i_j n^j_k\Big] r^k_l \Big) =\Big(\sum_{k=1}^r \sum_{j=1}^n m^i_j n^j_k r^k_l \Big)\, .
$$
In the first step we just wrote out the definition for matrix multiplication, in the second step we
moved summation symbol outside the bracket (this is just the distributive
property $x(y+z)=xy+xz$ for numbers) and
in the last step we used the associativity property for real numbers to remove the square brackets. 
Exactly the same reasoning shows that
$$
M(NR)=\Big(\sum_{j=1}^n m^i_j\Big[\sum_{k=1}^r n^j_k r^k_l\Big]\Big) = 
\Big(\sum_{k=1}^r \sum_{j=1}^n  m^i_j \Big[n^j_kr^k_l \Big] \Big) =\Big(\sum_{k=1}^r \sum_{j=1}^n m^i_j n^j_k r^k_l \Big)\, .
$$
This is the same as above so we are done. {\it As a fun remark, note that Einstein would simply have written
$(MN)R=(m^i_j n^j_k) r^k_l= m^i_j n^j_k r^k_l = m^i_j (n^j_k r^k_l ) = M(NR)$.}
\end{example}
%%%%%%%%%%%%%% non-commute


Sometimes matrices do not share the properties of regular numbers. 

\videoscriptlink{matrices_commute.mp4}{Matrices do not Commute}{script_matrices_commute}

That is, for {\it generic} $n\times n$ square matrices $M$ and $N$, then $MN\neq NM$. 

\begin{example} (Matrix multiplication does \emph{not} commute.)
\[
\begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
1 & 1 \\
\end{pmatrix} =
\begin{pmatrix}
2 & 1 \\
1 & 1 \\
\end{pmatrix}
\]
On the other hand:
\[
\begin{pmatrix}
1 & 0 \\
1 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix} =
\begin{pmatrix}
1 & 1 \\
1 & 2 \\
\end{pmatrix}
\]
\end{example}
Since $n\times n$ matrices are linear transformations $\Re^n \rightarrow \Re^n$, we can see that the order of successive linear transformations matters.  
%In general  for two linear transformations $K$ and $L$ taking $\Re^n \rightarrow \Re^n$, and $v\in \Re^n$,  one has
%$$K(L(v)) \neq L(K(v))\, .$$
%Finding matrices such that $MN=NM$ is an important problem in mathematics.
%We promise

Here is an example of matrices acting on objects in three dimensions that also shows matrices not commuting.
\begin{example}
You learned in a \hyperlink{rotationprob}{Review Problem} that the matrix
$$M=\begin{pmatrix}\cos\theta & \sin\theta \\ -\sin \theta & \cos\theta\end{pmatrix}\, ,$$
rotates vectors in the plane by an angle~$\theta$. 
We can generalize this, using block matrices, to three dimensions.
In fact the following matrices built from a $2\times 2$ rotation matrix, a $1\times 1$ identity matrix and zeroes everywhere else
$$
M=\begin{pmatrix}\cos\theta & \sin\theta &0\\ -\sin \theta & \cos\theta&0\\0&0&1\end{pmatrix}\qquad\mbox{and}\qquad
N=\begin{pmatrix}1&0&0\\0&\cos\theta & \sin\theta \\ 0&-\sin \theta & \cos\theta\end{pmatrix}\, ,
$$
perform rotations by an angle $\theta$ in the $xy$ and $yz$ planes, respectively. Because, they rotate single vectors, you can also use them to rotate objects built from a collection of vectors like pretty colored blocks! Here is a picture of $M$ and then $N$ acting on such a block, compared with the case of $N$ followed by $M$. The special case of $\theta=90^\circ$ is shown.
\begin{center}
\includegraphics[scale=.3]{MNNM.jpg}
\end{center}
Notice how the end product of $MN$ and $NM$ are different, so $MN\neq NM$ here.
\end{example}



\section{Block Matrices}

It is often convenient to partition a matrix $M$ into smaller matrices called \emph{blocks}, like so:

\[
M=\begin{pmat}{ccc|c}
1 & 2 & 3 & 1 \\
4 & 5 & 6 & 0 \\
7 & 8 & 9 & 1 \\
\cline{1-4}
0 & 1 & 2 & 0 \\
\end{pmat}
=
\begin{pmat}{c|c}
A & B \\
\cline{1-2}
C & D \\
\end{pmat}
\]
Here $A = \begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{pmatrix}$, $B=\colvec{1\\0\\1}$, $C=\rowvec{0 & 1 & 2 }$, $D=(0)$.

\begin{itemize}
\item The blocks of a block matrix\index{Block matrix}  must fit together to form a rectangle.  So 
$\begin{pmat}{c|c}
B & A \\
\cline{1-2}
D & C \\
\end{pmat}
$ makes sense, but 
$\begin{pmat}{c|c}
C & B \\
\cline{1-2}
D & A \\
\end{pmat}
$ does not.

%\href{\webworkurl ReadingHomework9/1/}{Reading homework: problem 9.1}
\reading{9}{1}

\item There are many ways to cut up an $n\times n$ matrix into blocks.  Often context or the entries of the matrix will suggest a useful way to divide the matrix into blocks.  For example, if there are large blocks of zeros in a matrix, or blocks that look like an identity matrix, it can be useful to partition the matrix accordingly.

\item Matrix operations on block matrices can be carried out by treating the blocks as matrix entries.  In the example above,
\begin{eqnarray*}
M^2 & = & \begin{pmat}{c|c}
A & B \\
\cline{1-2}
C & D \\
\end{pmat}
\begin{pmat}{c|c}
A & B \\
\cline{1-2}
C & D \\
\end{pmat} \\
& = & \begin{pmat}{c|c}
A^2+BC & AB+BD \\
\cline{1-2}
CA+DC & CB+D^2 \\
\end{pmat} \\
\end{eqnarray*}

Computing the individual blocks, we get:
\begin{eqnarray*}
A^2+BC &=& \begin{pmatrix}
	30 & 37 & 44 \\
	66 & 81 & 96 \\
	102&127 &152 \\
	\end{pmatrix} \\
AB+BD  &=& \colvec{ 4 \\ 10 \\ 16 } \\
CA+DC  &=& \rowvec{ 18 \\ 21 \\ 24 } \\
CB+D^2 &=& (2) 
\end{eqnarray*}

Assembling these pieces into a block matrix gives:
\[
\begin{pmat}{ccc|c}
30 & 37 & 44 & 4 \\
66 & 81 & 96 & 10 \\
102 & 127 & 152 & 16 \\
\cline{1-4}
4 & 10 & 16 & 2 \\
\end{pmat}
\]

This is exactly $M^2$.
\end{itemize}

\section{The Algebra of Square Matrices }\index{Square matrices}

Not every pair of matrices can be multiplied.  When multiplying two matrices, the number of rows in the left matrix must equal the number of columns in the right.  For an $r\times k$ matrix $M$ and an $s\times l$ matrix $N$, then we must have~$k=s$.

This is not a problem for square matrices of the same size, though.  Two $n\times n$ matrices can be multiplied in either order.  For a single matrix $M \in M^n_n$, we can form $M^2=MM$, $M^3=MMM$, and so on, and define $M^0=I_n$, the identity matrix.

As a result, any polynomial equation can be evaluated on a matrix.

\begin{example}
Let $f(x) = x - 2x^2 + 3x^3$.

Let $M=\begin{pmatrix}
1 & t \\
0 & 1 \\
\end{pmatrix}$.  Then:
\[
M^2 = \begin{pmatrix}
1 & 2t \\
0 & 1 \\
\end{pmatrix},
M^3 = \begin{pmatrix}
1 & 3t \\
0 & 1 \\
\end{pmatrix}, \ldots
\]

Hence:
\begin{eqnarray*}
f(M) &=& \begin{pmatrix}
	1 & t \\
	0 & 1 \\
	\end{pmatrix} 
- 2 \begin{pmatrix}
	1 & 2t \\
	0 & 1 \\
	\end{pmatrix} 
+ 3 \begin{pmatrix}
	1 & 3t \\
	0 & 1 \\
	\end{pmatrix} \\
&=& \begin{pmatrix}
	2 & 6t \\
	0 & 2 \\
	\end{pmatrix}
\end{eqnarray*}
\end{example}

Suppose $f(x)$ is any function defined by a convergent Taylor Series:
\[
f(x) = f(0) + f'(0)x + \frac{1}{2!}f''(0)x^2 + \cdots
\]
Then we can define the matrix function by just plugging in $M$:
\[
f(M) = f(0) + f'(0)M + \frac{1}{2!}f''(0)M^2 + \cdots
\]
There are additional techniques to determine the convergence of Taylor Series of matrices, based on the fact that the convergence problem is simple for diagonal matrices.  It also turns out that $\exp (M) = 1 + M + \frac{1}{2}M^2 + \frac{1}{3!}M^3 + \cdots$ always converges.

\videoscriptlink{properties_of_matrices_example.mp4}{Matrix Exponential Example}{properties_of_matrices_example}


\section{Trace}\index{Trace}

Matrices contain a great deal of information, so finding ways to extract essential information is useful. Here we need to assume that $n < \infty$ otherwise there are subtleties with convergence that we'd have to address.

\begin{definition}
The \emph{trace} of a square matrix $M=(m_j^i)$ is the sum of its diagonal entries.
\[
\tr M = \sum_{i=1}^{n}m_i^i\, .
\]
\end{definition}

\begin{example}
\[
\tr \begin{pmatrix}
2 & 7 & 6\\
9 & 5 & 1\\
4 & 3 & 8\\
\end{pmatrix} = 2+5+8 = 15
\]
\end{example}
While matrix multiplication does not commute, the trace of a product of matrices does not depend on the order of multiplication:

\begin{eqnarray*}
\tr(MN) & = & \tr( \sum_l M_l^i N_j^l ) \\
& = & \sum_i \sum_l M_l^i N_i^l \\
& = & \sum_l \sum_i N_i^l M_l^i \\
& = & \tr( \sum_i N_i^l M_l^i ) \\
& = & \tr( NM ).
\end{eqnarray*}
\videoscriptlink{properties_of_matrices_trace_proof.mp4}{Explanation of this Proof}{scripts_properties_of_matrices_trace_proof}
Thus we have a Theorem:
\begin{theorem} $$\tr(MN)=\tr(NM)$$ for any square matrices $M$ and $N$.
\end{theorem}

\begin{example}
Continuing from the previous example, 

\[
M= \begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix}, N=
\begin{pmatrix}
1 & 0 \\
1 & 1 \\
\end{pmatrix}.
\]
so
\[
MN = \begin{pmatrix}
2 & 1 \\
1 & 1 \\
\end{pmatrix} \neq
NM = \begin{pmatrix}
1 & 1 \\
1 & 2 \\
\end{pmatrix}.
\]
However, $\tr(MN) = 2+1 = 3 = 1+2 = \tr(NM)$.
\end{example}

Another useful property of the trace is that:
\[\tr M = \tr M^T\] 
This is true because the trace only uses the diagonal entries, which are fixed by the transpose.  For example:
$\tr \begin{pmatrix}
1 & 1 \\
2 & 3 \\
\end{pmatrix} = 4 = \tr \begin{pmatrix}
1 & 2 \\
1 & 3 \\
\end{pmatrix} = \tr \begin{pmatrix}
1 & 2 \\
1 & 3 \\
\end{pmatrix}^T
$

Finally, trace is a linear transformation from matrices to the real numbers.  This is easy to check.

\videoscriptlink{properties_of_matrices_trace.mp4}{More on the trace function}{scripts_properties_of_matrices_trace}

%\begin{remark}[Linear Systems Redux]
%
%Recall that we can view a linear system as a matrix equation
%\[MX=V,\] 
%with $M$ an $r\times k$ matrix of coefficients, $X$ a $k\times 1$ matrix of unknowns, and $V$ an $r\times 1$ matrix of constants.  If $M$ is a square matrix, then the number of equations~$r$ is the same as the number of unknowns~$k$, so we have hope of finding a single solution.
%
%Above we discussed functions of matrices.  An extremely useful function would be $f(M)=\frac{1}{M}$, where $M\frac{1}{M}=I$.  If we could compute $\frac{1}{M}$, then we would multiply both sides of the equation $MX=V$ by $\frac{1}{M}$ to obtain the solution immediately: $X=\frac{1}{M}V$.
%
%Clearly, if the linear system has no solution, then there can be no hope of finding $\frac{1}{M}$, since if it existed we could find a solution.  On the other hand, if the system has more than one solution, it also seems unlikely that $\frac{1}{M}$ would exist, since $X=\frac{1}{M}V$ yields only a single solution.  
%
%Therefore $\frac{1}{M}$ only sometimes exists.  It is called the \emph{inverse}\index{Inverse matrix!concept of} of $M$, and is usually written $M^{-1}$.
%\end{remark}
%

%\section*{References}
%Beezer: Part T, Section T
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Trace_(linear_algebra)}{Trace (Linear Algebra)}
%\item \href{http://en.wikipedia.org/wiki/Block_matrix}{Block Matrix}
%\end{itemize}
%

\section{Review Problems}
\input{\propMatricesPath/problems}

","\chapter{\propMatricesTitle} \label{properties_matrices}

The objects of study in linear algebra are linear operators. 
We have seen that linear operators can be represented as matrices through choices of ordered bases, and that matrices provide a means of efficient computation. 

We now begin an in depth study of matrices.

\begin{definition}
An $r\times k$ matrix\index{Matrix} $M=(m^i_j)$ for $i=1, \ldots, r; j=1, \ldots, k$ is a rectangular array of real (or complex) numbers:
\label{matrixnotation}
\[M = 
\begin{pmatrix}
m_1^1 & m_2^1 & \cdots & m_k^1 \\
m_1^2 & m_2^2 & \cdots & m_k^2 \\
\vdots & \vdots &   & \vdots \\
m_1^r & m_2^r & \cdots & m_k^r \\
\end{pmatrix}
\]
The numbers $m^i_j$ are called \emph{entries}\index{Matrix!entries of}.  The superscript indexes the row of the matrix and the subscript indexes the column of the matrix in which $m_j^i$ appears\footnote{This notation was first introduced by Albert Einstein.}.
\end{definition}

%It is often useful to consider matrices whose entries are more general than the real numbers, so we allow that possibility.

An $r\times 1$ matrix $v = (v^r_1) = (v^r)$ is called a \emph{column vector}\index{Column vector}, written $$v = \colvec{v^1\\v^2\\ \vdots \\ v^r }\, .$$  A $1\times k$ matrix $v = (v^1_k) = (v_k)$ is called a \emph{row vector}\index{Row vector}, written $$v = \rowvec{v_1 & v_2 & \cdots & v_k }\, .$$  


A matrix is a very useful and efficient way to store information:

\begin{example}
In computer graphics, you may have encountered image files with a .gif extension.  These files are actually just matrices: at the start of the file the size of the matrix is given, and then each entry of the matrix is a number indicating the color of a particular pixel in the image.

The resulting matrix  then has its rows shuffled a bit: by listing, say, every eighth row, then a web browser downloading the file can start displaying an incomplete version of the picture before the download is complete.

Finally, a compression algorithm is applied to the matrix to reduce the size of the file.
\end{example}

\videoscriptlink{matrices_example.mp4}{Adjacency Matrix Example}{scripts_matrices_example}

\begin{example}
Graphs occur in many applications, ranging from telephone networks to airline routes.  In the subject of \emph{graph theory}\index{Graph theory}, a graph is just a collection of vertices and some edges connecting vertices.  A matrix can be used to indicate how many edges attach one vertex to another.

\begin{center}
\includegraphics[width=10cm]{notes8-0.png}
\end{center}
For example, the graph pictured above would have the following matrix, where $m^i_j$ indicates the number of edges between the vertices labeled $i$~and~$j$:

\[
M = \begin{pmatrix}
1 & 2 & 1 & 1 \\
2 & 0 & 1 & 0 \\
1 & 1 & 0 & 1 \\
1 & 0 & 1 & 3 \\
\end{pmatrix}
\]
This is an example of a \emph{symmetric matrix}, since $m_j^i = m_i^j$.
\end{example}

The space of $r\times k$ matrices $M_k^r$ is a vector space with the addition and scalar multiplication defined as follows:

\begin{align*}
&M+N = (m_j^i) + (n_j^i) = ( m_j^i + n_j^i ) \\
&rM = r(m_j^i) = (rm_j^i)
\end{align*}
In other words, addition just adds corresponding entries in two matrices, and scalar multiplication multiplies every entry.
Notice that $M_1^n = \Re^n$ is just the vector space of column vectors.

Recall that we can multiply an \(r \times k\) matrix by a \(k \times 1\) column vector to produce a \(r \times 1\) column vector using the rule
\[MV = \big(\sum_{j=1}^k m_j^i v^j\big)\, .\]

This suggests a rule for multiplying an \(r \times k\) matrix \(M\) by a \(k \times s\) matrix~\(N\): our \(k \times s\) matrix \(N\) consists of \(s\) column vectors side-by-side, each of dimension \(k \times 1.\) We can multiply our \(r \times k\) matrix \(M\) by each of these \(s\) column vectors using the rule we already know, obtaining \(s\) column vectors each of dimension \(r \times 1.\) If we place these \(s\) column vectors side-by-side, we obtain an \(r \times s\) matrix \(MN.\)

That is, let \[N = 
\begin{pmatrix}
n_1^1 & n_2^1 & \cdots & n_s^1 \\
n_1^2 & n_2^2 & \cdots & n_s^2 \\
\vdots & \vdots &   & \vdots \\
n_1^k & n_2^k & \cdots & n_s^k \\
\end{pmatrix}
\]
and call the columns \(N_1\) through \(N_s\):
\[N_1 = \colvec{n_1^1\\n_1^2\\\vdots\\n_1^k},
N_2 = \colvec{n_2^1\\n_2^2\\\vdots\\n_2^k},
\ldots,
N_s = \colvec{n_s^1\\n_s^2\\\vdots\\n_s^k}.
\]
Then
%\[
%MN=M \rowvec{N_1 & N_2 & \cdots & N_s} = \rowvec{MN_1 & MN_2 & \cdots & MN_s}.
%\]
\[
MN=M
\begin{pmatrix}
| & | & & | \\
N_1 & N_2 & \cdots & N_s \\
| & | & & | \\
\end{pmatrix}
=
\begin{pmatrix}
| & | & & | \\
MN_1 & MN_2 & \cdots & MN_s \\
| & | & & | \\
\end{pmatrix}
\]

A more concise way to write this rule is: If \(M=(m^i_j)\) for \(i=1, \ldots, r; j=1, \ldots, k\) and \(N=(n^i_j)\) for \(i=1, \ldots, k; j=1, \ldots, s,\) then \(MN=L\) where \(L=(\ell^i_j)\) for \(i=i, \ldots, r; j=1, \ldots, s\) is given by
\[
\ell^i_j = \sum_{p=1}^k m^i_p n^p_j.
\]
This rule obeys linearity.

Notice that in order for the multiplication to make sense, the columns and rows must match.  For an $r\times k$ matrix $M$ and an $s\times m$ matrix $N$, then to make the product $MN$ we must have $k=s$.  Likewise, for the product $NM$, it is required that $m=r$.  A common shorthand for keeping track of the sizes of the matrices involved in a given product is: 
\[\Big(r \times k\Big)\times \Big(k\times m\Big) = \Big(r\times m\Big)\]

\begin{example}
Multiplying a $(3\times 1)$ matrix and a $(1\times 2)$ matrix yields a $(3\times 2)$ matrix.

\[
\colvec{1\\3\\2} \rowvec{2 & 3} = 
\begin{pmatrix}
1\cdot 2 & 1\cdot 3 \\
3\cdot 2 & 3\cdot 3 \\
2\cdot 2 & 2\cdot 3 \\
\end{pmatrix}
= \begin{pmatrix}
2 & 3 \\
6 & 9 \\
4 & 6 \\
\end{pmatrix}
\]
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework8/1/}{Reading homework: problem 8.1}\end{center}

Recall that $r\times k$ matrices can be used to represent linear transformations 
$ \Re^k \rightarrow \Re^r $
via $$MV = \sum_{j=1}^{k} m_j^iv^j , $$ which is the same rule we use when we multiply an $r\times k$ matrix by a $k\times 1$ vector to produce an $r\times1$ vector.

Likewise, we can use a matrix $N=(n^i_j)$ to define a linear transformation of a vector space of matrices. For example
\[
L \colon M^s_k \stackrel{N}{\longrightarrow} M^r_k
\]
\[
L(M)^i_l = \sum_{j=1}^{s} n_j^im^j_l.
\]
This is the same as the rule we use to multiply matrices. \hypertarget{leftmult}{In other words,} \(L(M)=NM\) is a linear transformation.

\begin{definition}[Matrix Terminology]
The entries $m_i^i$ are called \emph{diagonal}, and the set $\{m_1^1$, $m_2^2$, $\ldots \}$~is called the \emph{diagonal of the matrix}\index{Matrix!diagonal of}.

Any $r\times r$ matrix is called a \emph{square matrix}\index{Square matrix}.  A square matrix that is zero for all non-diagonal entries is called a diagonal matrix\index{Diagonal matrix}.

The $r\times r$ diagonal matrix with all diagonal entries equal to $1$ is called the \emph{identity matrix}\index{Identity matrix}, $I_r$, or just $\1$.  An identity matrix looks like \[
\begin{pmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{pmatrix}.
\]
The identity matrix is special because $$I_rM=MI_k=M$$ for all $M$ of size $r\times k$.
\end{definition}

In the matrix given by the product of matrices  above, the diagonal entries are $2$ and $9$.
An example of a diagonal matrix is 
$$\begin{pmatrix}
2 & 0 & 0\\
0 & 3 & 0\\
0 & 0 & 0\\
\end{pmatrix}\, .$$

\begin{definition}
The \emph{transpose}\index{Transpose} of an $r\times k$ matrix $M = (m_j^i)$ is the $k\times r$ matrix with entries
\[
M^T = (\hat{m}_j^i)
\]
with $\hat{m}_j^i = m_i^j$. 

A matrix $M$ is \emph{symmetric}\index{Symmetric matrix} if $M=M^T$.
\end{definition}

\begin{example}
$\begin{pmatrix}
2 & 5 & 6\\
1 & 3 & 4\\
\end{pmatrix}^T = 
\begin{pmatrix}
2 & 1 \\
5 & 3 \\
6 & 4 \\
\end{pmatrix}$
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework8/2/}{Reading homework: problem 8.2}\end{center}

\begin{remark}[Observations]

$\phantom{test}$

\begin{itemize}
\item Only square matrices can be symmetric.

\item The transpose of a column vector is a row vector, and vice-versa. 

\item Taking the transpose of a matrix twice does nothing.  \emph{i.e.,} $(M^T)^T=M$.
\end{itemize}
\end{remark}

\begin{theorem}[Transpose and Multiplication]
Let $M, N$ be matrices such that $MN$ makes sense.  Then $(MN)^T = N^TM^T$.
\end{theorem}
The proof of this theorem is left to \hyperref[MN=NTMT]{Review Question~\ref*{MN=NTMT}}.

%%%%%%%%%%%%%%%%%%%
\section{Associativity and Non-Commutativity }

Many properties of matrices following from the same property for real numbers. Here is an example.

\begin{example}
{\it Associativity of matrix multiplication.} We know for real numbers $x$, $y$ and $z$ that 
$$
x(yz)=(xy)z\, ,
$$
{\it i.e.} the order of bracketing does not matter. The same property holds for matrix multiplication, let us show why.
Suppose $M=\big( m^i_j \big)$, $N=\big( n^j_k \big)$ and  $R=\big( r^k_l \big)$ are, 
respectively, $m\times n$, $n\times r$ and $r\times t$ matrices. Then from the rule for matrix
multiplication we have
$$
MN=\Big(\sum_{j=1}^n m^i_j n^j_k\Big)\mbox{ and } NR=\Big(\sum_{k=1}^r n^j_k r^k_l\Big)\, .
$$
So first we compute 
$$
(MN)R=\Big(\sum_{k=1}^r \Big[\sum_{j=1}^n m^i_j n^j_k\Big] r^k_l \Big) = 
\Big(\sum_{k=1}^r \sum_{j=1}^n \Big[ m^i_j n^j_k\Big] r^k_l \Big) =\Big(\sum_{k=1}^r \sum_{j=1}^n m^i_j n^j_k r^k_l \Big)\, .
$$
In the first step we just wrote out the definition for matrix multiplication, in the second step we
moved summation symbol outside the bracket (this is just the distributive
property $x(y+z)=xy+xz$ for numbers) and
in the last step we used the associativity property for real numbers to remove the square brackets. 
Exactly the same reasoning shows that
$$
M(NR)=\Big(\sum_{j=1}^n m^i_j\Big[\sum_{k=1}^r n^j_k r^k_l\Big]\Big) = 
\Big(\sum_{k=1}^r \sum_{j=1}^n  m^i_j \Big[n^j_kr^k_l \Big] \Big) =\Big(\sum_{k=1}^r \sum_{j=1}^n m^i_j n^j_k r^k_l \Big)\, .
$$
This is the same as above so we are done. {\it As a fun remark, note that Einstein would simply have written
$(MN)R=(m^i_j n^j_k) r^k_l= m^i_j n^j_k r^k_l = m^i_j (n^j_k r^k_l ) = M(NR)$.}
\end{example}
%%%%%%%%%%%%%% non-commute


Sometimes matrices do not share the properties of regular numbers. 

\videoscriptlink{matrices_commute.mp4}{Matrices do not Commute}{script_matrices_commute}

That is, for {\it generic} $n\times n$ square matrices $M$ and $N$, then $MN\neq NM$. 

\begin{example} (Matrix multiplication does \emph{not} commute.)
\[
\begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
1 & 1 \\
\end{pmatrix} =
\begin{pmatrix}
2 & 1 \\
1 & 1 \\
\end{pmatrix}
\]
On the other hand:
\[
\begin{pmatrix}
1 & 0 \\
1 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix} =
\begin{pmatrix}
1 & 1 \\
1 & 2 \\
\end{pmatrix}
\]
\end{example}
Since $n\times n$ matrices are linear transformations $\Re^n \rightarrow \Re^n$, we can see that the order of successive linear transformations matters.  
%In general  for two linear transformations $K$ and $L$ taking $\Re^n \rightarrow \Re^n$, and $v\in \Re^n$,  one has
%$$K(L(v)) \neq L(K(v))\, .$$
%Finding matrices such that $MN=NM$ is an important problem in mathematics.
%We promise

Here is an example of matrices acting on objects in three dimensions that also shows matrices not commuting.
\begin{example}
You learned in a \hyperlink{rotationprob}{Review Problem} that the matrix
$$M=\begin{pmatrix}\cos\theta & \sin\theta \\ -\sin \theta & \cos\theta\end{pmatrix}\, ,$$
rotates vectors in the plane by an angle~$\theta$. 
We can generalize this, using block matrices, to three dimensions.
In fact the following matrices built from a $2\times 2$ rotation matrix, a $1\times 1$ identity matrix and zeroes everywhere else
$$
M=\begin{pmatrix}\cos\theta & \sin\theta &0\\ -\sin \theta & \cos\theta&0\\0&0&1\end{pmatrix}\qquad\mbox{and}\qquad
N=\begin{pmatrix}1&0&0\\0&\cos\theta & \sin\theta \\ 0&-\sin \theta & \cos\theta\end{pmatrix}\, ,
$$
perform rotations by an angle $\theta$ in the $xy$ and $yz$ planes, respectively. Because, they rotate single vectors, you can also use them to rotate objects built from a collection of vectors like pretty colored blocks! Here is a picture of $M$ and then $N$ acting on such a block, compared with the case of $N$ followed by $M$. The special case of $\theta=90^\circ$ is shown.
\begin{center}
\includegraphics[scale=.3]{MNNM.jpg}
\end{center}
Notice how the end product of $MN$ and $NM$ are different, so $MN\neq NM$ here.
\end{example}



\section{Block Matrices}

It is often convenient to partition a matrix $M$ into smaller matrices called \emph{blocks}, like so:

\[
M=\begin{pmat}{ccc|c}
1 & 2 & 3 & 1 \\
4 & 5 & 6 & 0 \\
7 & 8 & 9 & 1 \\
\cline{1-4}
0 & 1 & 2 & 0 \\
\end{pmat}
=
\begin{pmat}{c|c}
A & B \\
\cline{1-2}
C & D \\
\end{pmat}
\]
Here $A = \begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{pmatrix}$, $B=\colvec{1\\0\\1}$, $C=\rowvec{0 & 1 & 2 }$, $D=(0)$.

\begin{itemize}
\item The blocks of a block matrix\index{Block matrix}  must fit together to form a rectangle.  So 
$\begin{pmat}{c|c}
B & A \\
\cline{1-2}
D & C \\
\end{pmat}
$ makes sense, but 
$\begin{pmat}{c|c}
C & B \\
\cline{1-2}
D & A \\
\end{pmat}
$ does not.

%\href{\webworkurl ReadingHomework9/1/}{Reading homework: problem 9.1}
\reading{9}{1}

\item There are many ways to cut up an $n\times n$ matrix into blocks.  Often context or the entries of the matrix will suggest a useful way to divide the matrix into blocks.  For example, if there are large blocks of zeros in a matrix, or blocks that look like an identity matrix, it can be useful to partition the matrix accordingly.

\item Matrix operations on block matrices can be carried out by treating the blocks as matrix entries.  In the example above,
\begin{eqnarray*}
M^2 & = & \begin{pmat}{c|c}
A & B \\
\cline{1-2}
C & D \\
\end{pmat}
\begin{pmat}{c|c}
A & B \\
\cline{1-2}
C & D \\
\end{pmat} \\
& = & \begin{pmat}{c|c}
A^2+BC & AB+BD \\
\cline{1-2}
CA+DC & CB+D^2 \\
\end{pmat} \\
\end{eqnarray*}

Computing the individual blocks, we get:
\begin{eqnarray*}
A^2+BC &=& \begin{pmatrix}
	30 & 37 & 44 \\
	66 & 81 & 96 \\
	102&127 &152 \\
	\end{pmatrix} \\
AB+BD  &=& \colvec{ 4 \\ 10 \\ 16 } \\
CA+DC  &=& \rowvec{ 18 \\ 21 \\ 24 } \\
CB+D^2 &=& (2) 
\end{eqnarray*}

Assembling these pieces into a block matrix gives:
\[
\begin{pmat}{ccc|c}
30 & 37 & 44 & 4 \\
66 & 81 & 96 & 10 \\
102 & 127 & 152 & 16 \\
\cline{1-4}
4 & 10 & 16 & 2 \\
\end{pmat}
\]

This is exactly $M^2$.
\end{itemize}

\section{The Algebra of Square Matrices }\index{Square matrices}

Not every pair of matrices can be multiplied.  When multiplying two matrices, the number of rows in the left matrix must equal the number of columns in the right.  For an $r\times k$ matrix $M$ and an $s\times l$ matrix $N$, then we must have~$k=s$.

This is not a problem for square matrices of the same size, though.  Two $n\times n$ matrices can be multiplied in either order.  For a single matrix $M \in M^n_n$, we can form $M^2=MM$, $M^3=MMM$, and so on, and define $M^0=I_n$, the identity matrix.

As a result, any polynomial equation can be evaluated on a matrix.

\begin{example}
Let $f(x) = x - 2x^2 + 3x^3$.

Let $M=\begin{pmatrix}
1 & t \\
0 & 1 \\
\end{pmatrix}$.  Then:
\[
M^2 = \begin{pmatrix}
1 & 2t \\
0 & 1 \\
\end{pmatrix},
M^3 = \begin{pmatrix}
1 & 3t \\
0 & 1 \\
\end{pmatrix}, \ldots
\]

Hence:
\begin{eqnarray*}
f(M) &=& \begin{pmatrix}
	1 & t \\
	0 & 1 \\
	\end{pmatrix} 
- 2 \begin{pmatrix}
	1 & 2t \\
	0 & 1 \\
	\end{pmatrix} 
+ 3 \begin{pmatrix}
	1 & 3t \\
	0 & 1 \\
	\end{pmatrix} \\
&=& \begin{pmatrix}
	2 & 6t \\
	0 & 2 \\
	\end{pmatrix}
\end{eqnarray*}
\end{example}

Suppose $f(x)$ is any function defined by a convergent Taylor Series:
\[
f(x) = f(0) + f'(0)x + \frac{1}{2!}f''(0)x^2 + \cdots
\]
Then we can define the matrix function by just plugging in $M$:
\[
f(M) = f(0) + f'(0)M + \frac{1}{2!}f''(0)M^2 + \cdots
\]
There are additional techniques to determine the convergence of Taylor Series of matrices, based on the fact that the convergence problem is simple for diagonal matrices.  It also turns out that $\exp (M) = 1 + M + \frac{1}{2}M^2 + \frac{1}{3!}M^3 + \cdots$ always converges.

\videoscriptlink{properties_of_matrices_example.mp4}{Matrix Exponential Example}{properties_of_matrices_example}


\section{Trace}\index{Trace}

Matrices contain a great deal of information, so finding ways to extract essential information is useful. Here we need to assume that $n < \infty$ otherwise there are subtleties with convergence that we'd have to address.

\begin{definition}
The \emph{trace} of a square matrix $M=(m_j^i)$ is the sum of its diagonal entries.
\[
\tr M = \sum_{i=1}^{n}m_i^i\, .
\]
\end{definition}

\begin{example}
\[
\tr \begin{pmatrix}
2 & 7 & 6\\
9 & 5 & 1\\
4 & 3 & 8\\
\end{pmatrix} = 2+5+8 = 15
\]
\end{example}
While matrix multiplication does not commute, the trace of a product of matrices does not depend on the order of multiplication:

\begin{eqnarray*}
\tr(MN) & = & \tr( \sum_l M_l^i N_j^l ) \\
& = & \sum_i \sum_l M_l^i N_i^l \\
& = & \sum_l \sum_i N_i^l M_l^i \\
& = & \tr( \sum_i N_i^l M_l^i ) \\
& = & \tr( NM ).
\end{eqnarray*}
\videoscriptlink{properties_of_matrices_trace_proof.mp4}{Explanation of this Proof}{scripts_properties_of_matrices_trace_proof}
Thus we have a Theorem:
\begin{theorem} $$\tr(MN)=\tr(NM)$$ for any square matrices $M$ and $N$.
\end{theorem}

\begin{example}
Continuing from the previous example, 

\[
M= \begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix}, N=
\begin{pmatrix}
1 & 0 \\
1 & 1 \\
\end{pmatrix}.
\]
so
\[
MN = \begin{pmatrix}
2 & 1 \\
1 & 1 \\
\end{pmatrix} \neq
NM = \begin{pmatrix}
1 & 1 \\
1 & 2 \\
\end{pmatrix}.
\]
However, $\tr(MN) = 2+1 = 3 = 1+2 = \tr(NM)$.
\end{example}

Another useful property of the trace is that:
\[\tr M = \tr M^T\] 
This is true because the trace only uses the diagonal entries, which are fixed by the transpose.  For example:
$\tr \begin{pmatrix}
1 & 1 \\
2 & 3 \\
\end{pmatrix} = 4 = \tr \begin{pmatrix}
1 & 2 \\
1 & 3 \\
\end{pmatrix} = \tr \begin{pmatrix}
1 & 2 \\
1 & 3 \\
\end{pmatrix}^T
$

Finally, trace is a linear transformation from matrices to the real numbers.  This is easy to check.

\videoscriptlink{properties_of_matrices_trace.mp4}{More on the trace function}{scripts_properties_of_matrices_trace}

%\begin{remark}[Linear Systems Redux]
%
%Recall that we can view a linear system as a matrix equation
%\[MX=V,\] 
%with $M$ an $r\times k$ matrix of coefficients, $X$ a $k\times 1$ matrix of unknowns, and $V$ an $r\times 1$ matrix of constants.  If $M$ is a square matrix, then the number of equations~$r$ is the same as the number of unknowns~$k$, so we have hope of finding a single solution.
%
%Above we discussed functions of matrices.  An extremely useful function would be $f(M)=\frac{1}{M}$, where $M\frac{1}{M}=I$.  If we could compute $\frac{1}{M}$, then we would multiply both sides of the equation $MX=V$ by $\frac{1}{M}$ to obtain the solution immediately: $X=\frac{1}{M}V$.
%
%Clearly, if the linear system has no solution, then there can be no hope of finding $\frac{1}{M}$, since if it existed we could find a solution.  On the other hand, if the system has more than one solution, it also seems unlikely that $\frac{1}{M}$ would exist, since $X=\frac{1}{M}V$ yields only a single solution.  
%
%Therefore $\frac{1}{M}$ only sometimes exists.  It is called the \emph{inverse}\index{Inverse matrix!concept of} of $M$, and is usually written $M^{-1}$.
%\end{remark}
%

%\section*{References}
%Beezer: Part T, Section T
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Trace_(linear_algebra)}{Trace (Linear Algebra)}
%\item \href{http://en.wikipedia.org/wiki/Block_matrix}{Block Matrix}
%\end{itemize}
%

\section{Review Problems}
\input{\propMatricesPath/problems}

",lesson
11,Inverse Matrix,"\chapter{\inverseMatTitle}
\label{inverse_matrix}


\begin{definition}
A square matrix $M$ is \emph{invertible} (or \emph{nonsingular})\index{Invertible}\index{Nonsingular} if there exists a matrix $M^{-1}$ such that
\[
M^{-1}M=I=M^{-1}M.
\]
\end{definition}

%\begin{figure}
%\begin{center}
%\includegraphics[scale=.27]{\inverseMatPath/defn_inverse_matrix.jpg}
%\end{center}
%\end{figure}


\begin{remark}[Inverse of a $2\times 2$ Matrix] Let $M$ and $N$ be the matrices:
\[
M=\begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix},\qquad N=\begin{pmatrix}
d & -b \\
-c & a \\
\end{pmatrix}
\]
Multiplying these matrices gives:
\[
MN=\begin{pmatrix}
ad-bc & 0 \\
0 & ad-bc \\
\end{pmatrix}=(ad-bc)I
\]

Then $M^{-1}=\frac{1}{ad-bc}\begin{pmatrix}
d & -b \\
-c & a \\
\end{pmatrix}$, so long as $ad-bc\neq 0$.    
\end{remark}

\begin{figure}
\begin{center}
\includegraphics[scale=.24]{\inverseMatPath/2x2_inverse.jpg}
\end{center}
\caption{The formula for the inverse of a 2$\times$2 matrix is worth memorizing!}
\end{figure}



\section{Three Properties of the Inverse}

\begin{enumerate}
\item If $A$ is a square matrix and $B$ is the inverse of $A$, then $A$ is the inverse of $B$, since $AB=I=BA$.  Then we have the identity:
\[
(A^{-1})^{-1}=A
\]

\item Notice that $B^{-1}A^{-1}AB=B^{-1}IB=I=ABB^{-1}A^{-1}$.
Then:
\[
(AB)^{-1}=B^{-1}A^{-1}
\]
Then much like the transpose, taking the inverse of a product \emph{reverses} the order of the product.


\begin{center}
\includegraphics[scale=.26]{\inverseMatPath/inverse_inverse.jpg}
\end{center}

\item Finally, recall that $(AB)^T=B^TA^T$.  Since $I^T=I$, then $(A^{-1}A)^T=A^T(A^{-1})^T=I$.  Similarly, $(AA^{-1})^T=(A^{-1})^TA^T=I$.  Then:
\[
(A^{-1})^T=(A^T)^{-1}
\]
%As such, we could even write $A^{-T}$ for the inverse of the transpose of $A$ (or equivalently the transpose of the inverse).
\begin{center}
\includegraphics[scale=.20]{\inverseMatPath/transpose_inverse.jpg}
\end{center}
\end{enumerate}

\videoscriptlink{inverse_matrix_2by2_example.mp4}{Example}{scripts_inverse_matrix_2by2_example}




\section{Finding Inverses}

Suppose $M$ is a square matrix and $MX=V$ is a linear system with unique solution $X_0$.  Since there is a unique solution, $M^{-1}V$, then the reduced row echelon form of the linear system has an identity matrix on the left:
\[
\begin{amatrix}{1}
M & V
\end{amatrix}
\sim
\begin{amatrix}{1}
I & M^{-1}V
\end{amatrix}
\]
Solving the linear system $MX=V$ then tells us what $M^{-1}V$ is.  

To solve many linear systems with the same matrix at once, 
$$MX=V_1,~MX=V_2$$

we can consider augmented matrices with 
%a matrix on the right side instead of a column vector, 
many columns on the right 
 and then apply Gaussian row reduction to the left side of the matrix.  Once the identity matrix is on the left side of the augmented matrix, then the solution of each of the individual linear systems is on the right.
 \[
\left(\begin{array}{c|cc}
M & V_1&V_2
\end{array}\right)
\sim
\left(\begin{array}{c|cc}
I & M^{-1}V_1 & M^{-1}V_2
\end{array}\right)
\]


To compute $M^{-1}$, we would like $M^{-1}$, rather than $M^{-1}V$ to appear on the right side of our augmented matrix.
This is achieved by  solving the collection of systems $MX=e_k$, where $e_k$ is the column vector of zeroes with a $1$ in the $k$th entry.  
{\it I.e.,} the $n\times n$ identity matrix can be viewed as a bunch of column vectors $I_n=(e_1 \ e_2 \ \cdots e_n)$. So, putting the $e_k$'s together into an identity matrix, we get:
\[
\begin{amatrix}{1}
M & I
\end{amatrix}
\sim
\begin{amatrix}{1}
I & M^{-1}I
\end{amatrix}
=\begin{amatrix}{1}
I & M^{-1}
\end{amatrix}
\]


\begin{example}
Find $\begin{pmatrix}
-1 & 2 & -3 \\
2 & 1 & 0 \\
4 & -2 & 5 \\
\end{pmatrix}^{-1}
$.
Start by writing the augmented matrix, then apply row reduction to the left side.

\begin{eqnarray*}
\begin{pmat}{rrr|ccc}
-1 & 2 & -3 & 1 & 0 & 0 \\[1mm]
2  & 1 &  0 & 0 & 1 & 0 \\[1mm]
 4 & -2 & 5 & 0 & 0 & 1 \\[1mm]
\end{pmat} & \sim & \begin{pmat}{crr|ccc}
1  & -2&  3  & 1 & 0 & 0 \\[1mm]
0  & 5 &  -6 & 2 & 1 & 0 \\[1mm]
 0 & 6 & -7  & 4 & 0 & 1 \\[1mm]
\end{pmat} \\[2mm]
& \sim & \begin{pmat}{ccr|rrc}
1  & 0 &  \frac{3}{5}  & -\frac{1}{4} & \frac{2}{5} & 0 \\[1mm]
0  & 1 &  -\frac{6}{5} & \frac{2}{5} & \frac{1}{5}  & 0 \\[1mm]
 0 & 0 &  \frac{1}{5}  & \frac{4}{5} & -\frac{6}{5} & 1 \\[1mm]
\end{pmat} \\[2mm]
& \sim & \begin{pmat}{ccc|rrr}
1  & 0 &  0  & -5 & 4 & -3 \\[1mm]
0  & 1 &  0  & 10 & -7 & 6 \\[1mm]
 0 & 0 &  1  & 8 & -6 & 5 \\[1mm]
\end{pmat} \\
\end{eqnarray*}
At this point, we know $M^{-1}$ assuming we didn't goof up\index{Goofing up}.  However, row reduction is a lengthy and arithmetically involved process, so we should~\emph{check our answer,} by confirming that $MM^{-1}=I$ (or if you prefer $M^{-1}M=I$):
\[MM^{-1} = 
\begin{pmatrix}
-1 & 2 & -3 \\
2 & 1 & 0 \\
4 & -2 & 5 \\
\end{pmatrix}\begin{pmatrix}
-5 & 4 & -3 \\
10 & -7 & 6 \\
 8 & -6 & 5 \\
\end{pmatrix}
=\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
\]  
The product of the two matrices is indeed the identity matrix, so we're done.
\end{example}

%\href{\webworkurl ReadingHomework10/1/}{Reading homework: problem 10.1}
\reading{10}{1}
\section{Linear Systems and Inverses}

If $M^{-1}$ exists and is known, then we can immediately solve linear systems associated to $M$.

\begin{example}
Consider the linear system:

\[
      \begin{linsys}{2}
            -x & +2y & -3z         &=& 1  \\[1mm]
            2x & +\ y\,   &             &=& 2 \\[1mm]
            4x & -2y & +5z         &=& 0  
      \end{linsys}
\]
The associated matrix equation is $MX=\colvec{1\\2\\0},$ where \(M\) is the same as in the previous section.  Then:

\[
\colvec{x\\y\\z}=\begin{pmatrix}
-1 & 2 & -3 \\
2 & 1 & 0 \\
4 & -2 & 5 \\
\end{pmatrix}^{-1}\colvec{1\\2\\0}
=\begin{pmatrix}
-5 & 4 & -3 \\
10 & -7 & 6 \\
 8 & -6 & 5 \\
\end{pmatrix}\colvec{1\\2\\0}
=\colvec{3\\-4\\-4}
\]
Then $\colvec{x\\y\\z}=\colvec{3\\-4\\-4}$.  
In summary, when $M^{-1}$ exists, then $$MX=V \Rightarrow X=M^{-1}V\, .$$
\end{example}



%\href{\webworkurl ReadingHomework10/2/}{Reading homework: problem 10.2}
\reading{10}{2}

\section{Homogeneous Systems}

\begin{theorem}
A square matrix $M$ is invertible if and only if the homogeneous system $$MX=0$$ has no non-zero solutions.
\end{theorem}

\begin{proof}
First, suppose that $M^{-1}$ exists.  Then $MX=0 \Rightarrow X=M^{-1}0=0$.  Thus, if $M$ is invertible, then $MX=0$ has no non-zero solutions.

On the other hand, $MX=0$ always has the solution $X=0$.  If no other solutions exist, then $M$ can be put into reduced row echelon form with every variable a pivot.  In this case, $M^{-1}$ can be computed using the process in the previous section.
\end{proof}

%\begin{figure}
\begin{center}
\includegraphics[scale=.26]{\inverseMatPath/inverse_theorem.jpg}
\end{center}
%\end{figure}

%A great test of your linear algebra knowledge is to make a list of conditions for a matrix to be singular.
%You will learn more of these as the course goes by, but can also skip straight to the list in Section~\ref{thelist}. 

\section{Bit Matrices}\index{Bit matrices}
In computer science, information is recorded using binary strings of data.  For example, the following string contains an English word:
\[
011011000110100101101110011001010110000101110010
\]
A \hypertarget{bits}{\emph{bit}} is the basic unit of information, keeping track of a single one or zero.  Computers can add and multiply individual bits very quickly.

Consider the set $\Z_2=\{0,1 \}$ with addition and multiplication given by the following tables:
\label{Z2}
\[
\begin{array}{c|cc}
+ & 0 & 1 \\ \hline
0 & 0 & 1 \\
1 & 1 & 0 \\
\end{array}
\qquad
\begin{array}{c|cc}
\cdot & 0 & 1 \\ \hline
0 & 0 & 0 \\
1 & 0 & 1 \\
\end{array}
\]
Notice that $-1=1$, since $1+1=0$.

It turns out that $\Z_2$ is almost as good as the real or complex numbers (they are all \hyperref[fields]{fields}), so we can apply all of the linear algebra we have learned thus far to matrices with $\Z_2$ entries.  A matrix with entries in $\Z_2$ is sometimes called a \emph{bit matrix}\footnote{Note that bits in a bit arithmetic shorthand do not ``add'' and ``multiply'' as elements in $\mathbb{Z}_2$ does since these operators corresponding to ``bitwise or'' and ``bitwise and'' respectively.}.

\begin{example}
$\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}$ is an invertible matrix over $\Z_2$:

\[\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}^{-1}=\begin{pmatrix}
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}
\]

This can be easily verified by multiplying:

\[\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}\begin{pmatrix}
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}=\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
\]
\end{example}

\begin{remark}[Application: Cryptography]  
A very simple way to hide information is to use a substitution cipher, in which the alphabet is permuted and each letter in a message is systematically exchanged for another.  For example, the ROT-13 cypher just exchanges a letter with the letter thirteen places before or after it in the alphabet.  For example, HELLO becomes URYYB.  Applying the algorithm again decodes the message, turning URYYB back into HELLO.  Substitution ciphers are easy to break, but the basic idea can be extended to create cryptographic systems that are practically uncrackable.  For example, a \emph{one-time pad} is a system that uses a different substitution for each letter in the message.  So long as a particular set of substitutions is not used on more than one message, the one-time pad is unbreakable.

English characters are often stored in computers in the ASCII format.  In ASCII, a single character is represented by a string of eight bits, which we can consider as a vector in $\Z_2^8$ (which is like vectors in $\Re^8$, where the entries are zeros and ones).  One way to create a substitution cipher, then, is to choose an $8\times 8$ invertible bit matrix $M$, and multiply each letter of the message by $M$.  Then to decode the message, each string of eight characters would be multiplied by $M^{-1}$.  

To make the message a bit tougher to decode, one could consider pairs (or longer sequences) of letters as a single vector in $\Z_2^{16}$ (or a higher-dimensional space), and then use an appropriately-sized invertible matrix.

For more on cryptography, see ``The Code Book,'' by Simon Singh (1999, Doubleday).
\end{remark}

%{\it You should now be ready to attempt the \hyperref[sample1]{first sample midterm}.}
%\begin{center}
%\shabox{
%\begin{tabular}{c}
%\it \hyperref[sample1]{You are now ready to attempt the first sample midterm}. 
%\\
%\hyperref[sample1]{\includegraphics[scale=.15]{midterm.jpg}}
%\end{tabular}
%}
%\end{center}
%
%\section*{References}
%
%Hefferon: Chapter Three, Section IV.2
%\\
%Beezer: Chapter M, Section MISLE
%\\
%Wikipedia:
%\href{http://en.wikipedia.org/wiki/Invertible_matrix}{Invertible Matrix}
%
%
\section{Review Problems}
\input{\inverseMatPath/problems}

\newpage


","\chapter{\inverseMatTitle}
\label{inverse_matrix}


\begin{definition}
A square matrix $M$ is \emph{invertible} (or \emph{nonsingular})\index{Invertible}\index{Nonsingular} if there exists a matrix $M^{-1}$ such that
\[
M^{-1}M=I=M^{-1}M.
\]
\end{definition}

%\begin{figure}
%\begin{center}
%\includegraphics[scale=.27]{\inverseMatPath/defn_inverse_matrix.jpg}
%\end{center}
%\end{figure}


\begin{remark}[Inverse of a $2\times 2$ Matrix] Let $M$ and $N$ be the matrices:
\[
M=\begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix},\qquad N=\begin{pmatrix}
d & -b \\
-c & a \\
\end{pmatrix}
\]
Multiplying these matrices gives:
\[
MN=\begin{pmatrix}
ad-bc & 0 \\
0 & ad-bc \\
\end{pmatrix}=(ad-bc)I
\]

Then $M^{-1}=\frac{1}{ad-bc}\begin{pmatrix}
d & -b \\
-c & a \\
\end{pmatrix}$, so long as $ad-bc\neq 0$.    
\end{remark}

\begin{figure}
\begin{center}
\includegraphics[scale=.24]{\inverseMatPath/2x2_inverse.jpg}
\end{center}
\caption{The formula for the inverse of a 2$\times$2 matrix is worth memorizing!}
\end{figure}



\section{Three Properties of the Inverse}

\begin{enumerate}
\item If $A$ is a square matrix and $B$ is the inverse of $A$, then $A$ is the inverse of $B$, since $AB=I=BA$.  Then we have the identity:
\[
(A^{-1})^{-1}=A
\]

\item Notice that $B^{-1}A^{-1}AB=B^{-1}IB=I=ABB^{-1}A^{-1}$.
Then:
\[
(AB)^{-1}=B^{-1}A^{-1}
\]
Then much like the transpose, taking the inverse of a product \emph{reverses} the order of the product.


\begin{center}
\includegraphics[scale=.26]{\inverseMatPath/inverse_inverse.jpg}
\end{center}

\item Finally, recall that $(AB)^T=B^TA^T$.  Since $I^T=I$, then $(A^{-1}A)^T=A^T(A^{-1})^T=I$.  Similarly, $(AA^{-1})^T=(A^{-1})^TA^T=I$.  Then:
\[
(A^{-1})^T=(A^T)^{-1}
\]
%As such, we could even write $A^{-T}$ for the inverse of the transpose of $A$ (or equivalently the transpose of the inverse).
\begin{center}
\includegraphics[scale=.20]{\inverseMatPath/transpose_inverse.jpg}
\end{center}
\end{enumerate}

\videoscriptlink{inverse_matrix_2by2_example.mp4}{Example}{scripts_inverse_matrix_2by2_example}




\section{Finding Inverses}

Suppose $M$ is a square matrix and $MX=V$ is a linear system with unique solution $X_0$.  Since there is a unique solution, $M^{-1}V$, then the reduced row echelon form of the linear system has an identity matrix on the left:
\[
\begin{amatrix}{1}
M & V
\end{amatrix}
\sim
\begin{amatrix}{1}
I & M^{-1}V
\end{amatrix}
\]
Solving the linear system $MX=V$ then tells us what $M^{-1}V$ is.  

To solve many linear systems with the same matrix at once, 
$$MX=V_1,~MX=V_2$$

we can consider augmented matrices with 
%a matrix on the right side instead of a column vector, 
many columns on the right 
 and then apply Gaussian row reduction to the left side of the matrix.  Once the identity matrix is on the left side of the augmented matrix, then the solution of each of the individual linear systems is on the right.
 \[
\left(\begin{array}{c|cc}
M & V_1&V_2
\end{array}\right)
\sim
\left(\begin{array}{c|cc}
I & M^{-1}V_1 & M^{-1}V_2
\end{array}\right)
\]


To compute $M^{-1}$, we would like $M^{-1}$, rather than $M^{-1}V$ to appear on the right side of our augmented matrix.
This is achieved by  solving the collection of systems $MX=e_k$, where $e_k$ is the column vector of zeroes with a $1$ in the $k$th entry.  
{\it I.e.,} the $n\times n$ identity matrix can be viewed as a bunch of column vectors $I_n=(e_1 \ e_2 \ \cdots e_n)$. So, putting the $e_k$'s together into an identity matrix, we get:
\[
\begin{amatrix}{1}
M & I
\end{amatrix}
\sim
\begin{amatrix}{1}
I & M^{-1}I
\end{amatrix}
=\begin{amatrix}{1}
I & M^{-1}
\end{amatrix}
\]


\begin{example}
Find $\begin{pmatrix}
-1 & 2 & -3 \\
2 & 1 & 0 \\
4 & -2 & 5 \\
\end{pmatrix}^{-1}
$.
Start by writing the augmented matrix, then apply row reduction to the left side.

\begin{eqnarray*}
\begin{pmat}{rrr|ccc}
-1 & 2 & -3 & 1 & 0 & 0 \\[1mm]
2  & 1 &  0 & 0 & 1 & 0 \\[1mm]
 4 & -2 & 5 & 0 & 0 & 1 \\[1mm]
\end{pmat} & \sim & \begin{pmat}{crr|ccc}
1  & -2&  3  & 1 & 0 & 0 \\[1mm]
0  & 5 &  -6 & 2 & 1 & 0 \\[1mm]
 0 & 6 & -7  & 4 & 0 & 1 \\[1mm]
\end{pmat} \\[2mm]
& \sim & \begin{pmat}{ccr|rrc}
1  & 0 &  \frac{3}{5}  & -\frac{1}{4} & \frac{2}{5} & 0 \\[1mm]
0  & 1 &  -\frac{6}{5} & \frac{2}{5} & \frac{1}{5}  & 0 \\[1mm]
 0 & 0 &  \frac{1}{5}  & \frac{4}{5} & -\frac{6}{5} & 1 \\[1mm]
\end{pmat} \\[2mm]
& \sim & \begin{pmat}{ccc|rrr}
1  & 0 &  0  & -5 & 4 & -3 \\[1mm]
0  & 1 &  0  & 10 & -7 & 6 \\[1mm]
 0 & 0 &  1  & 8 & -6 & 5 \\[1mm]
\end{pmat} \\
\end{eqnarray*}
At this point, we know $M^{-1}$ assuming we didn't goof up\index{Goofing up}.  However, row reduction is a lengthy and arithmetically involved process, so we should~\emph{check our answer,} by confirming that $MM^{-1}=I$ (or if you prefer $M^{-1}M=I$):
\[MM^{-1} = 
\begin{pmatrix}
-1 & 2 & -3 \\
2 & 1 & 0 \\
4 & -2 & 5 \\
\end{pmatrix}\begin{pmatrix}
-5 & 4 & -3 \\
10 & -7 & 6 \\
 8 & -6 & 5 \\
\end{pmatrix}
=\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
\]  
The product of the two matrices is indeed the identity matrix, so we're done.
\end{example}

%\href{\webworkurl ReadingHomework10/1/}{Reading homework: problem 10.1}
\reading{10}{1}
\section{Linear Systems and Inverses}

If $M^{-1}$ exists and is known, then we can immediately solve linear systems associated to $M$.

\begin{example}
Consider the linear system:

\[
      \begin{linsys}{2}
            -x & +2y & -3z         &=& 1  \\[1mm]
            2x & +\ y\,   &             &=& 2 \\[1mm]
            4x & -2y & +5z         &=& 0  
      \end{linsys}
\]
The associated matrix equation is $MX=\colvec{1\\2\\0},$ where \(M\) is the same as in the previous section.  Then:

\[
\colvec{x\\y\\z}=\begin{pmatrix}
-1 & 2 & -3 \\
2 & 1 & 0 \\
4 & -2 & 5 \\
\end{pmatrix}^{-1}\colvec{1\\2\\0}
=\begin{pmatrix}
-5 & 4 & -3 \\
10 & -7 & 6 \\
 8 & -6 & 5 \\
\end{pmatrix}\colvec{1\\2\\0}
=\colvec{3\\-4\\-4}
\]
Then $\colvec{x\\y\\z}=\colvec{3\\-4\\-4}$.  
In summary, when $M^{-1}$ exists, then $$MX=V \Rightarrow X=M^{-1}V\, .$$
\end{example}



%\href{\webworkurl ReadingHomework10/2/}{Reading homework: problem 10.2}
\reading{10}{2}

\section{Homogeneous Systems}

\begin{theorem}
A square matrix $M$ is invertible if and only if the homogeneous system $$MX=0$$ has no non-zero solutions.
\end{theorem}

\begin{proof}
First, suppose that $M^{-1}$ exists.  Then $MX=0 \Rightarrow X=M^{-1}0=0$.  Thus, if $M$ is invertible, then $MX=0$ has no non-zero solutions.

On the other hand, $MX=0$ always has the solution $X=0$.  If no other solutions exist, then $M$ can be put into reduced row echelon form with every variable a pivot.  In this case, $M^{-1}$ can be computed using the process in the previous section.
\end{proof}

%\begin{figure}
\begin{center}
\includegraphics[scale=.26]{\inverseMatPath/inverse_theorem.jpg}
\end{center}
%\end{figure}

%A great test of your linear algebra knowledge is to make a list of conditions for a matrix to be singular.
%You will learn more of these as the course goes by, but can also skip straight to the list in Section~\ref{thelist}. 

\section{Bit Matrices}\index{Bit matrices}
In computer science, information is recorded using binary strings of data.  For example, the following string contains an English word:
\[
011011000110100101101110011001010110000101110010
\]
A \hypertarget{bits}{\emph{bit}} is the basic unit of information, keeping track of a single one or zero.  Computers can add and multiply individual bits very quickly.

Consider the set $\Z_2=\{0,1 \}$ with addition and multiplication given by the following tables:
\label{Z2}
\[
\begin{array}{c|cc}
+ & 0 & 1 \\ \hline
0 & 0 & 1 \\
1 & 1 & 0 \\
\end{array}
\qquad
\begin{array}{c|cc}
\cdot & 0 & 1 \\ \hline
0 & 0 & 0 \\
1 & 0 & 1 \\
\end{array}
\]
Notice that $-1=1$, since $1+1=0$.

It turns out that $\Z_2$ is almost as good as the real or complex numbers (they are all \hyperref[fields]{fields}), so we can apply all of the linear algebra we have learned thus far to matrices with $\Z_2$ entries.  A matrix with entries in $\Z_2$ is sometimes called a \emph{bit matrix}\footnote{Note that bits in a bit arithmetic shorthand do not ``add'' and ``multiply'' as elements in $\mathbb{Z}_2$ does since these operators corresponding to ``bitwise or'' and ``bitwise and'' respectively.}.

\begin{example}
$\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}$ is an invertible matrix over $\Z_2$:

\[\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}^{-1}=\begin{pmatrix}
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}
\]

This can be easily verified by multiplying:

\[\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}\begin{pmatrix}
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 1 \\
\end{pmatrix}=\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
\]
\end{example}

\begin{remark}[Application: Cryptography]  
A very simple way to hide information is to use a substitution cipher, in which the alphabet is permuted and each letter in a message is systematically exchanged for another.  For example, the ROT-13 cypher just exchanges a letter with the letter thirteen places before or after it in the alphabet.  For example, HELLO becomes URYYB.  Applying the algorithm again decodes the message, turning URYYB back into HELLO.  Substitution ciphers are easy to break, but the basic idea can be extended to create cryptographic systems that are practically uncrackable.  For example, a \emph{one-time pad} is a system that uses a different substitution for each letter in the message.  So long as a particular set of substitutions is not used on more than one message, the one-time pad is unbreakable.

English characters are often stored in computers in the ASCII format.  In ASCII, a single character is represented by a string of eight bits, which we can consider as a vector in $\Z_2^8$ (which is like vectors in $\Re^8$, where the entries are zeros and ones).  One way to create a substitution cipher, then, is to choose an $8\times 8$ invertible bit matrix $M$, and multiply each letter of the message by $M$.  Then to decode the message, each string of eight characters would be multiplied by $M^{-1}$.  

To make the message a bit tougher to decode, one could consider pairs (or longer sequences) of letters as a single vector in $\Z_2^{16}$ (or a higher-dimensional space), and then use an appropriately-sized invertible matrix.

For more on cryptography, see ``The Code Book,'' by Simon Singh (1999, Doubleday).
\end{remark}

%{\it You should now be ready to attempt the \hyperref[sample1]{first sample midterm}.}
%\begin{center}
%\shabox{
%\begin{tabular}{c}
%\it \hyperref[sample1]{You are now ready to attempt the first sample midterm}. 
%\\
%\hyperref[sample1]{\includegraphics[scale=.15]{midterm.jpg}}
%\end{tabular}
%}
%\end{center}
%
%\section*{References}
%
%Hefferon: Chapter Three, Section IV.2
%\\
%Beezer: Chapter M, Section MISLE
%\\
%Wikipedia:
%\href{http://en.wikipedia.org/wiki/Invertible_matrix}{Invertible Matrix}
%
%
\section{Review Problems}
\input{\inverseMatPath/problems}

\newpage


",lesson
12,Lu Decomposition,"\chapter{\luDecompTitle}
\label{LUdecomp}

Certain matrices are easier to work with than others.  In this section, we will see how to write any square\footnote{The case where $M$ is not square is dealt with at the end of the lecture.} matrix $M$ as the product of two simpler matrices.  We will write $$M=LU\, ,$$ where:
\begin{itemize}
\item $L$ is \emph{lower triangular}\index{Lower triangular matrix}.  This means that all entries above the main diagonal are zero.  In notation,
$L=(l^i_j)$ with $l^i_j=0$ for all $j>i$.
\[L=\begin{pmatrix}
l^1_1 & 0 & 0 & \cdots \\
l^2_1 & l^2_2 & 0 & \cdots \\
l^3_1 & l^3_2 & l^3_3 & \cdots \\
\vdots & \vdots & \vdots & \ddots \\
\end{pmatrix}
\]

\item $U$ is \emph{upper triangular}\index{Upper triangular matrix}.  This means that all entries below the main diagonal are zero.  In notation,
$U=(u^i_j)$ with $u^i_j=0$ for all $j<i$.
\[U=\begin{pmatrix}
u^1_1 & u^1_2 & u^1_3 & \cdots \\
0 & u^2_2 & u^2_3 & \cdots \\
0 & 0 & u^3_3 & \cdots \\
\vdots & \vdots & \vdots & \ddots \\
\end{pmatrix}
\]
\end{itemize}
$M=LU$ is called an \emph{$LU$ decomposition}\index{LU@$LU$ decomposition} of $M$.

This is a useful trick for  computational reasons; it is much easier to compute the inverse of an upper or lower triangular matrix than general matrices.  Since inverses are useful for solving linear systems, this makes solving any linear system associated to the matrix much faster as well.  The determinant---a very important quantity associated with any square matrix---is very easy to compute for triangular matrices.

\begin{example}
Linear systems associated to upper triangular matrices are very easy to solve by back substitution.
\[
\begin{amatrix}{2}
a & b & 1 \\
0 & c & e \\
\end{amatrix} \ \Rightarrow \ y=\frac{e}{c}\, , \quad x=\frac{1}{a}\left(1-\frac{be}{c}\right)
\]

\[
\begin{amatrix}{3}
1 & 0 & 0 & d \\
a & 1 & 0 & e \\
b & c & 1 & f \\
\end{amatrix} \Rightarrow x=d\, , \qquad y=e-ad\, , \qquad z=f-bd-c(e-ad)
\]
For lower triangular matrices, \emph{back} substitution\index{Back substitution} gives a quick solution; for upper triangular matrices, \emph{forward} substitution\index{Forward substitution} gives the solution.
\end{example}





\section{Using $LU$ Decomposition to Solve Linear Systems}

Suppose we have $M=LU$ and want to solve the system
\[
MX=LUX=V.
\]

\begin{itemize}
\item{Step 1:} Set $W=\colvec{u\\v\\w}=UX$.  

\item{Step 2:} Solve the system $LW=V$.  This should be simple by forward substitution since $L$ is lower triangular.  Suppose the solution to $LW=V$ is $W_0$.  

\item{Step 3:} Now solve the system $UX=W_0$.  This should be easy by backward substitution, since $U$ is upper triangular.  The solution to this system is the solution to the original system.
\end{itemize}
We can think of this as using the matrix $L$ to perform row operations on the matrix $U$ in order to solve the system; this idea also appears in the  study of determinants.

%\href{\webworkurl ReadingHomework11/1/}{Reading homework: problem 11.1}
\reading{11}{1}

\begin{example}
Consider the linear system:
\[
      \begin{linsys}{4}
            6x & +&18y & +&3z         &=& 3  \\[1mm]
            2x & +&12y & +&z	    &=& 19 \\[1mm]
            4x & +&15y & +&3z         &=& 0  
      \end{linsys}
\]

An $LU$ decomposition for the associated matrix $M$ is:
\[
\begin{pmatrix}
6 & 18 & 3 \\
2 & 12 & 1 \\
4 & 15 & 3 
\end{pmatrix} =
\begin{pmatrix}
3 & 0 & 0 \\
1 & 6 & 0 \\
2 & 3 & 1 
\end{pmatrix}
\begin{pmatrix}
2 & 6 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{pmatrix}.
\]

\begin{itemize}
\item{Step 1:} \hypertarget{LUproc}{Set} $W=\colvec{u\\v\\w}=UX$.  

\item{Step 2:} Solve the system $LW=V$:

\[
\begin{pmatrix}
3 & 0 & 0 \\
1 & 6 & 0 \\
2 & 3 & 1 
\end{pmatrix}
\colvec{u\\v\\w} =
\colvec{3\\19\\0}
\]

By substitution, we get $u=1$, $v=3$, and $w=-11$.  Then 
\[W_0=\colvec{1\\3\\-11}\]

\item{Step 3:} Solve the system $UX=W_0$.  
\[
\begin{pmatrix}
2 & 6 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{pmatrix}
\colvec{x\\y\\z} =
\colvec{1\\3\\-11}
\]
Back substitution gives $z=-11, y=3$, and $x=-3$.  

Then $X=\colvec{-3\\3\\-11}$, and we're done.
\end{itemize}
\end{example}

\videoscriptlink{lu_decomposition_using_lu_decomp.mp4}{Using a $LU$ decomposition}{scripts_lu_decomposition_using_lu_example}

%\begin{figure}
\begin{center}
\includegraphics[scale=.3]{\luDecompPath/LU_solution.jpg}
\end{center}
%\end{figure}

\section{Finding an $LU$ Decomposition.}
\label{finding_LU_decomp}
 
For any given matrix, there are actually many different $LU$ decompositions.  However, there is a unique $LU$ decomposition in which the $L$ matrix has ones on the diagonal. In that case $L$ is called a \emph{lower unit triangular matrix}\index{Lower unit triangular matrix}.

To find the $LU$ decomposition, we'll create two sequences of matrices $L_0, L_1, \ldots$ and $U_0, U_1, \ldots$ such that at each step, $L_iU_i=M$.  Each of the $L_i$ will be lower triangular, but only the last $U_i$ will be upper triangular.

Start by setting $L_0=I$ and $U_0=M$, because $L_0U_0=M$. A main concept of this calculation is captured by the following example:

\begin{example}
Consider $$E=\begin{pmatrix}1&0\\\lambda&1\end{pmatrix}\, ,\qquad M=\begin{pmatrix}a&b&c&\cdots\\d&e&f&\cdots\end{pmatrix}\, .$$
Lets compute $EM$
$$
EM=\begin{pmatrix}a&b&c&\cdots\\d+\lambda a&e+\lambda b&f+\lambda c&\cdots\end{pmatrix}\, ,.
$$
Something neat happened here: multiplying $M$ by $E$ performed the row operation $R_2\to R_2+\lambda R-1$ on $M$.
Another interesting fact:
$$
E^{-1}:=\begin{pmatrix}1&0\\-\lambda&1\end{pmatrix}
$$ 
obeys (check this yourself...)
$$
E^{-1} E = 1\, .
$$
Hence $M=E^{-1} E M$ or, writing this out
$$
\begin{pmatrix}a&b&c&\cdots\\d&e&f&\cdots\end{pmatrix}=\begin{pmatrix}1&0\\-\lambda&1\end{pmatrix} \begin{pmatrix}a&b&c&\cdots\\d+\lambda a&e+\lambda b&f+\lambda c&\cdots\end{pmatrix}\, .
$$
Here the matrix on the left is lower triangular, while the matrix on the right has had a row operation performed on it.
\end{example}




\vspace{2mm}
We would like to  use the first row of $U_0$ to zero out the first entry of every row below it.  For our running example, $$U_0=M=\begin{pmatrix}
6 & 18 & 3 \\
2 & 12 & 1 \\
4 & 15 & 3 
\end{pmatrix}\, ,$$ so we would like to perform the row operations $R_2\to R_2 -\frac 13 R_1$ and $R_3\to R_3-\frac 23R_1$.
%so the second row minus $\frac{1}{3}$ of the first row will zero out the first entry in the second row.  Likewise, the third row minus $\frac{2}{3}$ of the first row will zero out the first entry in the third row.
If we perform these row operations on $U_0$ to produce 
$$U_1=\begin{pmatrix}
6 & 18 & 3 \\
0 & 6 & 0 \\
0 & 3 & 1 
\end{pmatrix}\, ,$$
we need to multiply this on the left by a lower triangular matrix $L_1$ so that the product $L_1U_1=M$ still.
The above example shows how to do this:
Set $L_1$ to be the lower triangular matrix whose first column is filled with the minus constants used to zero out the first column of $M$.  Then $$L_1 = \begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & 0 & 1 
\end{pmatrix}\, .$$  
%Set $U_1$ to be the matrix obtained by zeroing out the first column of $M$.  Then $U_1=\begin{pmatrix}
%6 & 18 & 3 \\
%0 & 6 & 0 \\
%0 & 3 & 1 
%\end{pmatrix}$.
By construction $L_1 U_1=M$, but you should compute this yourself as a double check.

Now repeat the process by zeroing the second column of $U_1$ below the diagonal using the second row of $U_1$ using the row operation
$R_3\to R_3-\frac 12 R_2$ to produce
$$U_2=\begin{pmatrix}6&18&3\\0&6&0\\0&0&1\end{pmatrix}\, .$$
The matrix that undoes this row operation is obtained in the same way we found $L_1$ above and is:
$$
\begin{pmatrix}
1&0&0\\
0&1&0\\
0&\frac 12& 0
\end{pmatrix}\, .
$$
Thus our answer for $L_2$ is the product of this matrix with $L_1$, namely
$$
L_2=
\begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & 0 & 1 
\end{pmatrix}\begin{pmatrix}
1&0&0\\
0&1&0\\
0&\frac 12& 0
\end{pmatrix}
=\begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}\, .
$$
Notice that it is lower triangular because 

\begin{center}
\textcolor{brown}{THE PRODUCT OF LOWER TRIANGULAR MATRICES IS ALWAYS LOWER TRIANGULAR!}
\end{center}

\noindent
Moreover it is obtained by recording minus the constants used for all our row operations in the appropriate columns (this always works this way).
Moreover, $U_2$ is upper triangular and $M=L_2U_2$, we are done!
Putting this all together we have
$$M=\begin{pmatrix}
6 & 18 & 3 \\
2 & 12 & 1 \\
4 & 15 & 3 
\end{pmatrix}= \begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}\begin{pmatrix}
6 & 18 & 3 \\
0 & 6 & 0 \\
0 & 0 & 1 
\end{pmatrix}\, .$$  
%Since $U_2$ is upper-triangular, we're done.  Inserting the new number into $L_1$ to get $L_2$ really is safe: the numbers in the first column don't affect the second column of $U_1$, since the first column of $U_1$ is already zeroed out.

If the matrix you're working with has more than three rows, just continue this process by zeroing out the next column below the diagonal, and repeat until there's nothing left to do.

\videoscriptlink{lu_decomposition_example.mp4}{Another $LU$ decomposition example}{scripts_lu_decomposition_example}

The fractions in the $L$ matrix are admittedly ugly.  For two matrices $LU$, we can multiply one entire column of $L$ by a constant $\lambda$ and divide the corresponding row of $U$ by the same constant without changing the product of the two matrices.  Then:

\begin{eqnarray*}
LU &=& \begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}
I
\begin{pmatrix}
6 & 18 & 3 \\
0 & 6 & 0 \\
0 & 0 & 1 
\end{pmatrix} \\
&=&
\begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}
\begin{pmatrix}
3 & 0 & 0 \\
0 & 6 & 0 \\
0 & 0 & 1 
\end{pmatrix}
\begin{pmatrix}
\frac{1}{3} & 0 & 0 \\[1mm]
0 & \frac{1}{6} & 0 \\[1mm]
0 & 0 & 1 
\end{pmatrix}
\begin{pmatrix}
6 & 18 & 3 \\
0 & 6 & 0 \\
0 & 0 & 1 
\end{pmatrix} \\
&=&
\begin{pmatrix}
3 & 0 & 0 \\
1 & 6 & 0 \\
2 & 3 & 1 
\end{pmatrix}\begin{pmatrix}
2 & 6 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{pmatrix}.
\end{eqnarray*}
The resulting matrix looks nicer, but isn't in standard (lower unit triangular matrix) form.

\reading{11}{2}
%\href{\webworkurl ReadingHomework11/2/}{Reading homework: problem 11.2}

For matrices that are not square, $LU$ decomposition still makes sense.  Given an $m\times n$ matrix $M$, for example we could write $M=LU$ with $L$ a square lower unit triangular matrix, and $U$ a rectangular matrix.  Then $L$ will be an $m\times m$ matrix, and $U$ will be an $m\times n$ matrix (of the same shape as $M$).  From here, the process is exactly the same as for a square matrix.  We create a sequence of matrices $L_i$ and $U_i$ that is eventually the $LU$ decomposition.  Again, we start with $L_0=I$ and $U_0=M$.

\begin{example}
Let's find the $LU$ decomposition of $M=U_0=\begin{pmatrix}
-2 & 1 & 3 \\
-4 & 4 & 1 
\end{pmatrix}$.  Since $M$ is a $2\times 3$ matrix, our decomposition will consist of a $2\times 2$ matrix and a $2\times 3$ matrix.  Then we start with $L_0=I_2=\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}$.

The next step is to zero-out the first column of $M$ below the diagonal.  There is only one row to cancel, then, and it can be removed by subtracting $2$ times the first row of $M$ to the second row of $M$.  Then:

\[
L_1=\begin{pmatrix}
1 & 0 \\
2 & 1
\end{pmatrix}, \qquad 
U_1 = \begin{pmatrix}
-2 & 1 & 3 \\
0 & 2 & -5 
\end{pmatrix}
\]
Since $U_1$ is upper triangular, we're done.  With a larger matrix, we would just continue the process.
\end{example}





\section{Block $LDU$ Decomposition}

Let $M$ be a square block matrix with square blocks $X,Y,Z,W$ such that $X^{-1}$ exists.  Then $M$ can be decomposed as a block $LDU$ decomposition, where $D$ is block diagonal, as follows:
\[
M=\begin{pmatrix}
X & Y \\
Z & W
\end{pmatrix}
\]

Then: \[M=\begin{pmatrix}
I &  0 \\
ZX^{-1} & I
\end{pmatrix}\begin{pmatrix}
X & 0 \\
0 & W-ZX^{-1}Y
\end{pmatrix}\begin{pmatrix}
I & X^{-1}Y \\
0 & I
\end{pmatrix}.\]
This can be checked explicitly simply by block-multiplying these three matrices.

\videoscriptlink{lu_decomposition_blocks.mp4}{Block $LDU$ Explanation}{scripts_lu_decomposition_blocks}

\begin{example}
For a $2\times 2$ matrix, we can regard each entry as a block.
\[
\begin{pmatrix}
1 & 2 \\
3 & 4
\end{pmatrix}=
\begin{pmatrix}
1 & 0 \\
3 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
0 & -2
\end{pmatrix}
\begin{pmatrix}
1 & 2 \\
0 & 1
\end{pmatrix}
\]
By multiplying the diagonal matrix by the upper triangular matrix, we get the standard $LU$ decomposition of the matrix.
\end{example}


%\section*{References}
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/LU_decomposition}{$LU$ Decomposition}
%\item \href{http://en.wikipedia.org/wiki/Block_LU_decomposition}{Block $LU$ Decomposition}
%\end{itemize}

\section{Review Problems}
\input{\luDecompPath/problems}

\newpage
","\chapter{\luDecompTitle}
\label{LUdecomp}

Certain matrices are easier to work with than others.  In this section, we will see how to write any square\footnote{The case where $M$ is not square is dealt with at the end of the lecture.} matrix $M$ as the product of two simpler matrices.  We will write $$M=LU\, ,$$ where:
\begin{itemize}
\item $L$ is \emph{lower triangular}\index{Lower triangular matrix}.  This means that all entries above the main diagonal are zero.  In notation,
$L=(l^i_j)$ with $l^i_j=0$ for all $j>i$.
\[L=\begin{pmatrix}
l^1_1 & 0 & 0 & \cdots \\
l^2_1 & l^2_2 & 0 & \cdots \\
l^3_1 & l^3_2 & l^3_3 & \cdots \\
\vdots & \vdots & \vdots & \ddots \\
\end{pmatrix}
\]

\item $U$ is \emph{upper triangular}\index{Upper triangular matrix}.  This means that all entries below the main diagonal are zero.  In notation,
$U=(u^i_j)$ with $u^i_j=0$ for all $j<i$.
\[U=\begin{pmatrix}
u^1_1 & u^1_2 & u^1_3 & \cdots \\
0 & u^2_2 & u^2_3 & \cdots \\
0 & 0 & u^3_3 & \cdots \\
\vdots & \vdots & \vdots & \ddots \\
\end{pmatrix}
\]
\end{itemize}
$M=LU$ is called an \emph{$LU$ decomposition}\index{LU@$LU$ decomposition} of $M$.

This is a useful trick for  computational reasons; it is much easier to compute the inverse of an upper or lower triangular matrix than general matrices.  Since inverses are useful for solving linear systems, this makes solving any linear system associated to the matrix much faster as well.  The determinant---a very important quantity associated with any square matrix---is very easy to compute for triangular matrices.

\begin{example}
Linear systems associated to upper triangular matrices are very easy to solve by back substitution.
\[
\begin{amatrix}{2}
a & b & 1 \\
0 & c & e \\
\end{amatrix} \ \Rightarrow \ y=\frac{e}{c}\, , \quad x=\frac{1}{a}\left(1-\frac{be}{c}\right)
\]

\[
\begin{amatrix}{3}
1 & 0 & 0 & d \\
a & 1 & 0 & e \\
b & c & 1 & f \\
\end{amatrix} \Rightarrow x=d\, , \qquad y=e-ad\, , \qquad z=f-bd-c(e-ad)
\]
For lower triangular matrices, \emph{back} substitution\index{Back substitution} gives a quick solution; for upper triangular matrices, \emph{forward} substitution\index{Forward substitution} gives the solution.
\end{example}





\section{Using $LU$ Decomposition to Solve Linear Systems}

Suppose we have $M=LU$ and want to solve the system
\[
MX=LUX=V.
\]

\begin{itemize}
\item{Step 1:} Set $W=\colvec{u\\v\\w}=UX$.  

\item{Step 2:} Solve the system $LW=V$.  This should be simple by forward substitution since $L$ is lower triangular.  Suppose the solution to $LW=V$ is $W_0$.  

\item{Step 3:} Now solve the system $UX=W_0$.  This should be easy by backward substitution, since $U$ is upper triangular.  The solution to this system is the solution to the original system.
\end{itemize}
We can think of this as using the matrix $L$ to perform row operations on the matrix $U$ in order to solve the system; this idea also appears in the  study of determinants.

%\href{\webworkurl ReadingHomework11/1/}{Reading homework: problem 11.1}
\reading{11}{1}

\begin{example}
Consider the linear system:
\[
      \begin{linsys}{4}
            6x & +&18y & +&3z         &=& 3  \\[1mm]
            2x & +&12y & +&z	    &=& 19 \\[1mm]
            4x & +&15y & +&3z         &=& 0  
      \end{linsys}
\]

An $LU$ decomposition for the associated matrix $M$ is:
\[
\begin{pmatrix}
6 & 18 & 3 \\
2 & 12 & 1 \\
4 & 15 & 3 
\end{pmatrix} =
\begin{pmatrix}
3 & 0 & 0 \\
1 & 6 & 0 \\
2 & 3 & 1 
\end{pmatrix}
\begin{pmatrix}
2 & 6 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{pmatrix}.
\]

\begin{itemize}
\item{Step 1:} \hypertarget{LUproc}{Set} $W=\colvec{u\\v\\w}=UX$.  

\item{Step 2:} Solve the system $LW=V$:

\[
\begin{pmatrix}
3 & 0 & 0 \\
1 & 6 & 0 \\
2 & 3 & 1 
\end{pmatrix}
\colvec{u\\v\\w} =
\colvec{3\\19\\0}
\]

By substitution, we get $u=1$, $v=3$, and $w=-11$.  Then 
\[W_0=\colvec{1\\3\\-11}\]

\item{Step 3:} Solve the system $UX=W_0$.  
\[
\begin{pmatrix}
2 & 6 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{pmatrix}
\colvec{x\\y\\z} =
\colvec{1\\3\\-11}
\]
Back substitution gives $z=-11, y=3$, and $x=-3$.  

Then $X=\colvec{-3\\3\\-11}$, and we're done.
\end{itemize}
\end{example}

\videoscriptlink{lu_decomposition_using_lu_decomp.mp4}{Using a $LU$ decomposition}{scripts_lu_decomposition_using_lu_example}

%\begin{figure}
\begin{center}
\includegraphics[scale=.3]{\luDecompPath/LU_solution.jpg}
\end{center}
%\end{figure}

\section{Finding an $LU$ Decomposition.}
\label{finding_LU_decomp}
 
For any given matrix, there are actually many different $LU$ decompositions.  However, there is a unique $LU$ decomposition in which the $L$ matrix has ones on the diagonal. In that case $L$ is called a \emph{lower unit triangular matrix}\index{Lower unit triangular matrix}.

To find the $LU$ decomposition, we'll create two sequences of matrices $L_0, L_1, \ldots$ and $U_0, U_1, \ldots$ such that at each step, $L_iU_i=M$.  Each of the $L_i$ will be lower triangular, but only the last $U_i$ will be upper triangular.

Start by setting $L_0=I$ and $U_0=M$, because $L_0U_0=M$. A main concept of this calculation is captured by the following example:

\begin{example}
Consider $$E=\begin{pmatrix}1&0\\\lambda&1\end{pmatrix}\, ,\qquad M=\begin{pmatrix}a&b&c&\cdots\\d&e&f&\cdots\end{pmatrix}\, .$$
Lets compute $EM$
$$
EM=\begin{pmatrix}a&b&c&\cdots\\d+\lambda a&e+\lambda b&f+\lambda c&\cdots\end{pmatrix}\, ,.
$$
Something neat happened here: multiplying $M$ by $E$ performed the row operation $R_2\to R_2+\lambda R-1$ on $M$.
Another interesting fact:
$$
E^{-1}:=\begin{pmatrix}1&0\\-\lambda&1\end{pmatrix}
$$ 
obeys (check this yourself...)
$$
E^{-1} E = 1\, .
$$
Hence $M=E^{-1} E M$ or, writing this out
$$
\begin{pmatrix}a&b&c&\cdots\\d&e&f&\cdots\end{pmatrix}=\begin{pmatrix}1&0\\-\lambda&1\end{pmatrix} \begin{pmatrix}a&b&c&\cdots\\d+\lambda a&e+\lambda b&f+\lambda c&\cdots\end{pmatrix}\, .
$$
Here the matrix on the left is lower triangular, while the matrix on the right has had a row operation performed on it.
\end{example}




\vspace{2mm}
We would like to  use the first row of $U_0$ to zero out the first entry of every row below it.  For our running example, $$U_0=M=\begin{pmatrix}
6 & 18 & 3 \\
2 & 12 & 1 \\
4 & 15 & 3 
\end{pmatrix}\, ,$$ so we would like to perform the row operations $R_2\to R_2 -\frac 13 R_1$ and $R_3\to R_3-\frac 23R_1$.
%so the second row minus $\frac{1}{3}$ of the first row will zero out the first entry in the second row.  Likewise, the third row minus $\frac{2}{3}$ of the first row will zero out the first entry in the third row.
If we perform these row operations on $U_0$ to produce 
$$U_1=\begin{pmatrix}
6 & 18 & 3 \\
0 & 6 & 0 \\
0 & 3 & 1 
\end{pmatrix}\, ,$$
we need to multiply this on the left by a lower triangular matrix $L_1$ so that the product $L_1U_1=M$ still.
The above example shows how to do this:
Set $L_1$ to be the lower triangular matrix whose first column is filled with the minus constants used to zero out the first column of $M$.  Then $$L_1 = \begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & 0 & 1 
\end{pmatrix}\, .$$  
%Set $U_1$ to be the matrix obtained by zeroing out the first column of $M$.  Then $U_1=\begin{pmatrix}
%6 & 18 & 3 \\
%0 & 6 & 0 \\
%0 & 3 & 1 
%\end{pmatrix}$.
By construction $L_1 U_1=M$, but you should compute this yourself as a double check.

Now repeat the process by zeroing the second column of $U_1$ below the diagonal using the second row of $U_1$ using the row operation
$R_3\to R_3-\frac 12 R_2$ to produce
$$U_2=\begin{pmatrix}6&18&3\\0&6&0\\0&0&1\end{pmatrix}\, .$$
The matrix that undoes this row operation is obtained in the same way we found $L_1$ above and is:
$$
\begin{pmatrix}
1&0&0\\
0&1&0\\
0&\frac 12& 0
\end{pmatrix}\, .
$$
Thus our answer for $L_2$ is the product of this matrix with $L_1$, namely
$$
L_2=
\begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & 0 & 1 
\end{pmatrix}\begin{pmatrix}
1&0&0\\
0&1&0\\
0&\frac 12& 0
\end{pmatrix}
=\begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}\, .
$$
Notice that it is lower triangular because 

\begin{center}
\textcolor{brown}{THE PRODUCT OF LOWER TRIANGULAR MATRICES IS ALWAYS LOWER TRIANGULAR!}
\end{center}

\noindent
Moreover it is obtained by recording minus the constants used for all our row operations in the appropriate columns (this always works this way).
Moreover, $U_2$ is upper triangular and $M=L_2U_2$, we are done!
Putting this all together we have
$$M=\begin{pmatrix}
6 & 18 & 3 \\
2 & 12 & 1 \\
4 & 15 & 3 
\end{pmatrix}= \begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}\begin{pmatrix}
6 & 18 & 3 \\
0 & 6 & 0 \\
0 & 0 & 1 
\end{pmatrix}\, .$$  
%Since $U_2$ is upper-triangular, we're done.  Inserting the new number into $L_1$ to get $L_2$ really is safe: the numbers in the first column don't affect the second column of $U_1$, since the first column of $U_1$ is already zeroed out.

If the matrix you're working with has more than three rows, just continue this process by zeroing out the next column below the diagonal, and repeat until there's nothing left to do.

\videoscriptlink{lu_decomposition_example.mp4}{Another $LU$ decomposition example}{scripts_lu_decomposition_example}

The fractions in the $L$ matrix are admittedly ugly.  For two matrices $LU$, we can multiply one entire column of $L$ by a constant $\lambda$ and divide the corresponding row of $U$ by the same constant without changing the product of the two matrices.  Then:

\begin{eqnarray*}
LU &=& \begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}
I
\begin{pmatrix}
6 & 18 & 3 \\
0 & 6 & 0 \\
0 & 0 & 1 
\end{pmatrix} \\
&=&
\begin{pmatrix}
1 & 0 & 0 \\[1mm]
\frac{1}{3} & 1 & 0 \\[1mm]
\frac{2}{3} & \frac{1}{2} & 1 
\end{pmatrix}
\begin{pmatrix}
3 & 0 & 0 \\
0 & 6 & 0 \\
0 & 0 & 1 
\end{pmatrix}
\begin{pmatrix}
\frac{1}{3} & 0 & 0 \\[1mm]
0 & \frac{1}{6} & 0 \\[1mm]
0 & 0 & 1 
\end{pmatrix}
\begin{pmatrix}
6 & 18 & 3 \\
0 & 6 & 0 \\
0 & 0 & 1 
\end{pmatrix} \\
&=&
\begin{pmatrix}
3 & 0 & 0 \\
1 & 6 & 0 \\
2 & 3 & 1 
\end{pmatrix}\begin{pmatrix}
2 & 6 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{pmatrix}.
\end{eqnarray*}
The resulting matrix looks nicer, but isn't in standard (lower unit triangular matrix) form.

\reading{11}{2}
%\href{\webworkurl ReadingHomework11/2/}{Reading homework: problem 11.2}

For matrices that are not square, $LU$ decomposition still makes sense.  Given an $m\times n$ matrix $M$, for example we could write $M=LU$ with $L$ a square lower unit triangular matrix, and $U$ a rectangular matrix.  Then $L$ will be an $m\times m$ matrix, and $U$ will be an $m\times n$ matrix (of the same shape as $M$).  From here, the process is exactly the same as for a square matrix.  We create a sequence of matrices $L_i$ and $U_i$ that is eventually the $LU$ decomposition.  Again, we start with $L_0=I$ and $U_0=M$.

\begin{example}
Let's find the $LU$ decomposition of $M=U_0=\begin{pmatrix}
-2 & 1 & 3 \\
-4 & 4 & 1 
\end{pmatrix}$.  Since $M$ is a $2\times 3$ matrix, our decomposition will consist of a $2\times 2$ matrix and a $2\times 3$ matrix.  Then we start with $L_0=I_2=\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}$.

The next step is to zero-out the first column of $M$ below the diagonal.  There is only one row to cancel, then, and it can be removed by subtracting $2$ times the first row of $M$ to the second row of $M$.  Then:

\[
L_1=\begin{pmatrix}
1 & 0 \\
2 & 1
\end{pmatrix}, \qquad 
U_1 = \begin{pmatrix}
-2 & 1 & 3 \\
0 & 2 & -5 
\end{pmatrix}
\]
Since $U_1$ is upper triangular, we're done.  With a larger matrix, we would just continue the process.
\end{example}





\section{Block $LDU$ Decomposition}

Let $M$ be a square block matrix with square blocks $X,Y,Z,W$ such that $X^{-1}$ exists.  Then $M$ can be decomposed as a block $LDU$ decomposition, where $D$ is block diagonal, as follows:
\[
M=\begin{pmatrix}
X & Y \\
Z & W
\end{pmatrix}
\]

Then: \[M=\begin{pmatrix}
I &  0 \\
ZX^{-1} & I
\end{pmatrix}\begin{pmatrix}
X & 0 \\
0 & W-ZX^{-1}Y
\end{pmatrix}\begin{pmatrix}
I & X^{-1}Y \\
0 & I
\end{pmatrix}.\]
This can be checked explicitly simply by block-multiplying these three matrices.

\videoscriptlink{lu_decomposition_blocks.mp4}{Block $LDU$ Explanation}{scripts_lu_decomposition_blocks}

\begin{example}
For a $2\times 2$ matrix, we can regard each entry as a block.
\[
\begin{pmatrix}
1 & 2 \\
3 & 4
\end{pmatrix}=
\begin{pmatrix}
1 & 0 \\
3 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
0 & -2
\end{pmatrix}
\begin{pmatrix}
1 & 2 \\
0 & 1
\end{pmatrix}
\]
By multiplying the diagonal matrix by the upper triangular matrix, we get the standard $LU$ decomposition of the matrix.
\end{example}


%\section*{References}
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/LU_decomposition}{$LU$ Decomposition}
%\item \href{http://en.wikipedia.org/wiki/Block_LU_decomposition}{Block $LU$ Decomposition}
%\end{itemize}

\section{Review Problems}
\input{\luDecompPath/problems}

\newpage
",lesson
13,Elementary Matrices Determinants,"\chapter{Determinants}\label{elementarydeterminants}
\chaptermark{Determinants}

Given a square matrix, is there an easy way to know when it is invertible?  Answering this fundamental question is 
the goal of this chapter.

\section{The Determinant Formula}

The determinant boils down a square matrix to a a single number. That number determines whether the square matrix is invertible or not.  
Lets see how this works for small matrices first.

\subsection{Simple Examples}

For small cases, we already know when a matrix is invertible.  If $M$ is a $1\times 1$ matrix, then $M=(m) \Rightarrow M^{-1}=(1/m)$.  Then $M$ is invertible if and only if $m\neq 0$.

\vspace{.3cm}

For $M$ a $2\times 2$ matrix, chapter~\ref{Matrices} section~\ref{inverse_matrix} shows that if $$M=\begin{pmatrix}
m^1_1 & m^1_2 \\[1mm]
m^2_1 & m^2_2 \\
\end{pmatrix}\, ,$$ then $$M^{-1}=\frac{1}{m^1_1m^2_2-m^1_2m^2_1}\begin{pmatrix}
m^2_2 & -m^1_2 \\[1mm]
-m^2_1 & m^1_1 \\
\end{pmatrix}\, .$$ Thus $M$ is invertible if and only if 
\vspace{.3cm}

$$m^1_1m^2_2-m^1_2m^2_1\neq 0\, .$$  For $2\times 2$ matrices, this quantity is called the \emph{determinant of $M$}\index{Determinant!$2\times 2$ matrix}.
\[
\det M = \det \begin{pmatrix}
m^1_1 & m^1_2 \\[1mm]
m^2_1 & m^2_2 \\
\end{pmatrix} = m^1_1m^2_2-m^1_2m^2_1\, .
\]

\begin{figure}
\begin{center}
\includegraphics[scale=.35]{\elemMatDetPath/det2x2.jpg}
\end{center}
\caption{Memorize the determinant formula for a 2$\times$2 matrix!}
\end{figure}


\begin{example}
For a $3\times 3$ matrix, $$M=\begin{pmatrix}
m^1_1 & m^1_2 & m^1_3\\[1mm]
m^2_1 & m^2_2 & m^2_3\\[1mm]
m^3_1 & m^3_2 & m^3_3\\
\end{pmatrix}\, ,$$ then---see \hyperref[det33]{review question}~\ref{det33}---$M$ is non-singular if and only if\index{Determinant!$3\times 3$ matrix}:
\vspace{.1cm}
\[
\det M= 
m^1_1m^2_2m^3_3 
- m^1_1m^2_3m^3_2 
+ m^1_2m^2_3m^3_1
- m^1_2m^2_1m^3_3  
+ m^1_3m^2_1m^3_2
- m^1_3m^2_2m^3_1
\neq 0.
\]

\noindent
Notice that in the subscripts, each ordering of the numbers $1$, $2$, and $3$ occurs exactly once.  Each of these is a \emph{permutation} of the set $\{1,2,3\}$.
\end{example}



\subsection{Permutations}
Consider $n$ objects labeled $1$ through $n$ and shuffle them.  Each possible shuffle is called a \emph{permutation}\index{Permutation}.  
For example, here is an example of a permutation of $1$--$5$:
\[
\sigma = \begin{bmatrix}
1 & 2 & 3 & 4 & 5 \\
4 & 2 & 5 & 1 & 3 
\end{bmatrix}
\]
We can consider a permutation $\sigma$ as an invertible function from the set of numbers $[n] := \{1, 2, \dotsc, n\}$ to $[n]$, so can  write $\sigma(3) = 5$ in the above example. In general we can write
\[\left[\!
\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5 \\[1mm]
\sigma(1) & \sigma(2) & \sigma(3) & \sigma(4) & \sigma(5)
\end{array}\!\right]\, ,
\]
but since the top line of any permutation is always the same, we can omit it and just write:
\[
\sigma = \begin{bmatrix}
\sigma(1) & \sigma(2) & \sigma(3) & \sigma(4) & \sigma(5)
\end{bmatrix}
\]
and so our example becomes simply  $\sigma = [4\, 2\, 5\, 1\, 3]$. 
%There is also one more notation called cycle notation, but we do not discuss it here.

The mathematics of permutations is extensive; there are a few key properties of permutations that we'll need:

\begin{itemize}
\item There are $n!$ permutations of $n$ distinct objects, since there are $n$ choices for the first object, $n-1$ choices for the second once the first has been chosen, and so on.

\item Every permutation can be built up by successively swapping pairs of objects.  For example, to build up the permutation $\begin{bmatrix} 3 & 1 & 2 \end{bmatrix}$ from the trivial permutation $\begin{bmatrix} 1 & 2 & 3 \end{bmatrix}$, you can first swap $2$ and $3$, and then swap $1$ and $3$.

\item \hypertarget{permutation_parity}{For any given} permutation $\sigma$, there is some number of swaps it takes to build up the permutation.  (It's simplest to use the minimum number of swaps, but you don't have to: it turns out that \emph{any} way of building up the permutation from swaps will have have the same parity of swaps, either even or odd.) 
If this number happens to be even, then $\sigma$ is called an \emph{even permutation};\index{Even permutation} if this number is odd, then $\sigma$ is an \emph{odd permutation}\index{Odd permutation}.  In fact, $n!$ is even for all $n\geq 2$, and exactly half of the permutations are even and the other half are odd.  It's worth noting that the trivial permutation (which sends $i\rightarrow i$ for every $i$) is an even permutation, since it uses zero swaps.
\end{itemize}

\begin{definition}
The {\bf sign function}\index{Sign function} is a function $\text{sgn}$ that sends permutations to the set $\{-1,1\}$ with rule of correspondence defined by
\[ \text{sgn}(\sigma) = 
\left\{ \begin{array}{rl}
1 & \mbox{if $\sigma$ is even}\\
-1 & \mbox{if $\sigma$ is odd}.\end{array} \right.
\]
\end{definition}

%For more on the swaps (also known as inversions) and the sign function, see \hyperref[prob_inversion_number]{Problem~\ref*{prob_inversion_number}}.

\Videoscriptlink{elementary_matrices_permutations.mp4}{Permutation Example}{scripts_elementary_matrices_permutations}

%\begin{center}\href{\webworkurl ReadingHomework12/1/}{Reading homework: problem 12.1}\end{center}
\Reading{Determinants}{1}

We can use permutations to give a definition of the determinant.

\begin{definition}  The {\bf determinant}\index{Determinant} of $n \times n$ matrix $M$ is 
\begin{center}
\scalebox{1.2}{
\shabox{
$
\det M = {\textstyle \sum\limits_{\sigma}}\  \text{sgn}(\sigma)\,  m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)}.
$
}}\end{center}
\end{definition}
The sum is over all permutations of $n$ objects; a sum over the all elements of $\{ \sigma: \{1,\dots,n\}\to  \{1,\dots,n\}\}$.  Each summand is a product of n entries from the matrix  with each factor from a different row. 
In different terms of the sum the column numbers are shuffled by different permutations~$\sigma$.

The last statement about the summands yields a nice property of the determinant:
\begin{theorem}
If $M=(m^i_j)$ has a row consisting entirely of zeros, then $m^i_{\sigma(i)}=0$ for every $\sigma$ and some $i$.  Moreover
$\det M=0$.
\end{theorem}


\begin{example}
Because there are many permutations of $n$, writing the determinant this way for a general matrix gives a very long sum.  For $n=4$, there are $24=4!$ permutations, and for $n=5$, there are already $120=5!$ permutations.\\

\noindent
For a $4\times 4$ matrix, $M=\begin{pmatrix}
m^1_1 & m^1_2 & m^1_3 & m^1_4\\[1mm]
m^2_1 & m^2_2 & m^2_3 & m^2_4\\[1mm]
m^3_1 & m^3_2 & m^3_3 & m^3_4\\[1mm]
m^4_1 & m^4_2 & m^4_3 & m^4_4\\
\end{pmatrix}$, then $\det M$ is:
\begin{eqnarray*}
\det M &=& 
 m^1_1m^2_2m^3_3m^4_4
-m^1_1m^2_3m^3_2m^4_4
-m^1_1m^2_2m^3_4m^4_3 \\[1mm]
& -&m^1_2m^2_1m^3_3m^4_4
+m^1_1m^2_3m^3_4m^4_2
+m^1_1m^2_4m^3_2m^4_3 \\[1mm]
&+ & m^1_2m^2_3m^3_1m^4_4
+m^1_2m^2_1m^3_4m^4_3
\pm \text{16 more terms}.
\end{eqnarray*}
\end{example}
This is very cumbersome.

Luckily, it is very easy to compute the determinants of certain matrices.  For example, if $M$ is diagonal, meaning that $M^i_j=0$ whenever $i\neq j$,  then all summands of the determinant involving off-diagonal entries vanish and 
\[
\det M = \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)}= m^1_{1}m^2_{2}\cdots m^n_{n}.
\]
\Shabox{1}{\begin{tabular}{c}The determinant of a diagonal matrix is\\  the product of its diagonal entries.\end{tabular}}
Since the identity matrix is diagonal with all diagonal entries equal to one, we have
\[
\det I=1.
\]

We would like to use the determinant to decide whether a matrix is invertible.  Previously, we computed the inverse of a matrix by applying row operations.  Therefore we ask what happens to the determinant when row operations are applied to a matrix.

\begin{remark}[Swapping rows]
Lets \hypertarget{rowswap}{swap}
 rows $i$ and $j$ of  a matrix $M$ and then compute its determinant.  For the permutation $\sigma$, let $\hat{\sigma}$ be the permutation obtained by swapping positions $i$ and $j$.  Clearly $$\text{sgn}(\hat{\sigma})=-\text{sgn}(\sigma)\, .$$  Let  $M'$ be the matrix $M$ with rows $i$ and $j$ swapped.  Then  (assuming $i<j$):
\begin{eqnarray}
\det M' & = & \sum_{\sigma} \text{sgn}(\sigma)\, m^1_{\sigma(1)}\cdots m^j_{\sigma(i)}\cdots m^i_{\sigma(j)} \cdots m^n_{\sigma(n)} \nonumber \\
& = & \sum_{\sigma} \text{sgn}(\sigma) \,m^1_{\sigma(1)}\cdots m^i_{\sigma(j)}\cdots m^j_{\sigma(i)} \cdots m^n_{\sigma(n)} \nonumber \\
& = & \sum_{\sigma}(-\text{sgn}(\hat{\sigma})) \,m^1_{\hat{\sigma}(1)}\cdots m^i_{\hat{\sigma}(i)}\cdots m^j_{\hat{\sigma}(j)} \cdots m^n_{\hat{\sigma}(n)} \nonumber \\
& = & - \sum_{\hat{\sigma}} \text{sgn}(\hat{\sigma}) \,m^1_{\hat{\sigma}(1)}\cdots m^i_{\hat{\sigma}(i)}\cdots m^j_{\hat{\sigma}(j)} \cdots m^n_{\hat{\sigma}(n)} \nonumber \\
& = & -\det M.\nn
\end{eqnarray}
The step replacing $\sum_\sigma$ by $\sum_{\hat \sigma}$ often causes confusion; it holds since we sum over all permutations (see review problem~\ref{hatsum}).
Thus we see that swapping rows changes the sign of the determinant. {\it I.e.}, $$\det M' = - \det M\, .$$

\begin{center}\href{\webworkurl ReadingHomework8/2/}{Reading homework: problem 8.2}\end{center}

Applying this result to $M=I$ (the identity matrix) yields
$$\det E^i_j=-1\, ,$$
where the matrix $E^i_j$ is the identity matrix with rows $i$ and $j$ swapped. It is a row swap  elementary matrix.
%and we will meet it again \hyperlink{elem_matrix_row_swap}{soon}.

This implies another nice property of the determinant.  If two rows of the matrix are identical, then swapping the rows changes the sign of the matrix, but leaves the matrix unchanged.  Then we see the following:
\begin{theorem}
If $M$ has two identical rows, then $\det M = 0$.
\end{theorem}

\end{remark}


\begin{figure}
\begin{center}
\includegraphics[scale=.3]{\elemMatDetPath/row_swap_theorem.jpg}
\end{center}
\caption{Remember what row swap does to determinants!}
\end{figure}

\section{Elementary Matrices and Determinants}\index{Elementary matrix}

In chapter~\ref{systems} we found
the matrices that perform the  row operations involved in Gaussian elimination; we called them elementary matrices.

As a reminder, for any matrix $M$, and a matrix $M'$ equal to~$M$ after a row operation, multiplying by an elementary matrix $E$ gave $M'=EM$.

\Videoscriptlink{elementary_matrices_determinant_explanation.mp4}{Elementary Matrices}{scripts_elementary_matrices_explanation}

We now examine what the elementary matrices to do determinants.

\subsection{Row Swap}
Our first elementary matrix   swaps  rows $i$ and~$j$ when it is applied to  a matrix $M$. 
Explicitly, 
let $R^1$ through $R^n$ denote the rows  of $M$, and let $M'$ be the matrix $M$ with rows $i$ and $j$ swapped.  Then $M$ and $M'$ can be regarded as a block matrices (where the blocks are rows);
\[
M=\ccolvec{\vdots \\ R^i \\ \vdots \\ R^j \\ \vdots} \text{ and }
M'=\ccolvec{\vdots \\ R^j \\ \vdots \\ R^i \\ \vdots}.
\]
Then notice that
\[
M'=\ccolvec{\\ \vdots \\ R^j \\ \vdots \\ R^i \\ \vdots\\ \\  }= 
\begin{pmatrix}
1 & & & & & & \\
& \ddots & & & & & \\
& & 0 & & 1 & & \\
& & & \ddots & & & \\
& & 1 & & 0 & & \\
& & & & & \ddots & \\
& & & & & & 1 \\
\end{pmatrix}
\ccolvec{\\ \vdots \\ R^i \\ \vdots \\ R^j \\ \vdots \\ \\  }.
\]
The matrix 
$$
\begin{pmatrix}
1 & & & & & & \\
& \ddots & & & & & \\
& & 0 & & 1 & & \\
& & & \ddots & & & \\
& & 1 & & 0 & & \\
& & & & & \ddots & \\
& & & & & & 1 \\
\end{pmatrix}=:E^i_j
$$
is just the identity matrix with rows $i$ and $j$ swapped.  \hypertarget{elem_matrix_row_swap}The matrix $E^i_j$ is an \emph{elementary matrix}\index{Elementary matrix!swapping rows} and
\[
M'=E^i_jM\, .
\]
Because $\det I=1$ and swapping a pair of rows changes the sign of the determinant, we have found that 
\[
\det E^i_j = -1\, .
\]


%\begin{center}
%\includegraphics[scale=.25]{\elemMatDetIIPath/detEij.jpg}
%\end{center}

Now we know that swapping a pair of rows flips the sign of the determinant so $\det M'=-det M$.
But $\det E_j^i=-1$ and $M'=E^i_j M$ so 
$$
\det E^i_j M = \det E^i_j \, \det M\, .
$$
This result hints at a general rule for determinants of products of matrices. 
%Stare at it again before reading the next Lecture:
%\begin{center}
%\includegraphics[scale=.2]{\elemMatDetPath/detEM=detEdetM.jpg}
%\end{center}


%\section*{References}
%Hefferon, Chapter Four, Section I.1 and I.3
%\\
%Beezer, Chapter D, Section DM, Subsection EM
%\\
%Beezer, Chapter D, Section PDM
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Determinant}{Determinant}
%\item \href{http://en.wikipedia.org/wiki/Permutation}{Permutation}
%\item \href{http://en.wikipedia.org/wiki/Elementary_matrix}{Elementary Matrix}
%\end{itemize}
%

%\section{Review Problems}
%\input{\elemMatDetPath/problems}

%\section{\elemMatDetIITitle}\label{elementarydeterminantsII}

%In section~\ref{elementarydeterminants}, we saw the definition of the determinant and derived an elementary matrix that exchanges two rows of a matrix.  

\subsection{Row  Multiplication}
The next row operation is multiplying a row by a scalar.
Consider $$M=\ccolvec{R^1 \\ \vdots \\ R^n }\, ,$$ where $R^i$ are row vectors.  Let $R^i(\lambda)$ be the identity matrix, with the $i$th diagonal entry replaced by $\lambda$, not to be confused with the row vectors. {\it I.e.},
$$
R^i(\lambda)=
\begin{pmatrix}
1 & & & & \\
  & \ddots & & & \\
  & & \lambda & & \\
  & & & \ddots & \\
  & & & & 1 \\
\end{pmatrix}
\, .$$
Then:
\[
M'=R^i(\lambda)M=\ccolvec{R^1 \\ \vdots \\ \lambda R^i \\ \vdots \\ R^n }\, ,
\]
equals $M$ with one row multiplied by~$\lambda$.

What effect does multiplication by the elementary matrix $R^i(\lambda)$ have on the determinant?

\begin{eqnarray*}
\det M' & = & \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots \lambda m^i_{\sigma(i)} \cdots m^n_{\sigma(n)} \\
& = & \lambda \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots m^i_{\sigma(i)} \cdots m^n_{\sigma(n)} \\
& = & \lambda \det M
\end{eqnarray*}
Thus, multiplying a row by $\lambda$ multiplies the determinant by $\lambda$.
{\it I.e.,} $$\det R^i(\lambda) M = \lambda \det M\, .$$


\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/row_mult_thm.jpg}
\end{center}
\caption{Rescaling a row rescales the determinant.}
\end{figure}


Since $R^i(\lambda)$ is just the identity matrix with a single row multiplied by $\lambda$, then by the above rule, the determinant of $R^i(\lambda)$ is $\lambda$.  Thus

\[
\det R^i(\lambda) = \det \begin{pmatrix}
1 & & & & \\
  & \ddots & & & \\
  & & \lambda & & \\
  & & & \ddots & \\
  & & & & 1 \\
\end{pmatrix} = \lambda\, ,
\]
and once again we have a product of determinants formula
$$
\det \left( R^i(\lambda) M \right) = \det\left( R^i(\lambda) \right)\det M.
$$

\subsection{Row Addition}
The final row operation is adding $\mu R^j$ to $R^i$.  This is done with the elementary matrix~$S^i_j(\mu)$, which is an identity matrix but with an additional  $\mu$ in the $i,j$ position;

\[
S^i_j(\mu) = \begin{pmatrix}
1 & 	& 	& 	& & & 	\\
  & \ddots & 	&	& & &	\\
  & 	& 1 	& 	& \mu & &	\\
  & 	& 	& \ddots & & &	\\
  & 	& 	& 	& 1 & & 	\\
  & 	& 	& 	& 	& \ddots & 	\\
  & 	& 	& 	& 	& 	 & 1	\\
\end{pmatrix}\, .
\]
Then multiplying $M$ by $S^i_j(\mu)$  performs a row addition;

\[
\begin{pmatrix}
1 & 	& 	& 	& & & 	\\
  & \ddots & 	&	& & &	\\
  & 	& 1 	& 	& \mu & &	\\
  & 	& 	& \ddots & & &	\\
  & 	& 	& 	& 1 & & 	\\
  & 	& 	& 	& 	& \ddots & 	\\
  & 	& 	& 	& 	& 	 & 1	\\
\end{pmatrix}\ccolvec{\\ \vdots \\ R^i \\ \vdots \\ R^j \\ \vdots\\ \\}
=
\ccolvec{\\ \vdots \\ R^i +\mu R^j \\ \vdots \\ R^j \\ \vdots\\ \\ }\, .
\]
What is the effect of multiplying by $S^i_j(\mu)$ on the determinant?  Let $M'=S^i_j(\mu)M$, and let $M''$ be the matrix $M$ but with $R^i$ replaced by $R^j$
Then

\begin{eqnarray*}
\det M' & = & \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots (m^i_{\sigma(i)}+ \mu m^j_{\sigma(i)}) \cdots m^n_{\sigma(n)} \\
& = & \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots m^i_{\sigma(i)} \cdots m^n_{\sigma(n)} \\
&   & \qquad + \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots \mu m^j_{\sigma(j)} \cdots m^j_{\sigma(j)} \cdots m^n_{\sigma(n)} \\
& = & \det M + \mu \det M''
\end{eqnarray*}
Since $M''$ has two identical rows, its determinant is $0$ so
$$
\det M' = \det M,
$$
when $M'$ is obtained from $M$ by adding $\mu$ times row~$j$ to row~$i$.

%\href{\webworkurl ReadingHomework13/1/}{Reading homework: problem 13.1}
\Reading{Determinants}{3}

\noindent
We also have learnt that
$$\det \left( S^i_j(\mu)M \right) = \det M\, .$$
Notice that if $M$ is the identity matrix, then we have $$\det S^i_j(\mu) = \det (S^i_j(\mu)I) = \det I = 1\, .$$



\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/row_addition_thm.jpg}
\end{center}
\caption{Adding one row to another leaves the determinant unchanged.}
\end{figure}

\subsection{Determinant of Products}
In summary, the elementary matrices for each of the row operations obey

\[
\begin{array}{cccc}
E^i_j &=& I \text{ with rows $i,j$ swapped;} &\det E^i_j=-1 \\[3mm]
R^i(\lambda) &=& I \text{ with $\lambda$ in position $i,i$;} 
	&\det R^i(\lambda)=\lambda \\[3mm]
S^i_j(\mu) &=& I \text{ with $\mu$ in position $i,j$;} 
	&\det S^i_j(\mu)=1 \\[3mm]
\end{array}
\]
\Videoscriptlink{elementary_matrices_and_determinants_ii_dets.mp4}{Elementary Determinants}{scripts_elementary_matrices_determinants_ii_dets}

Moreover  we found a useful formula for determinants of products:

\begin{theorem}
If $E$ is \emph{ any} of the elementary matrices $E^i_j, R^i(\lambda), S^i_j(\mu)$, then $\det(EM)=\det E \det M$.
\end{theorem}


%\begin{center}
%\hspace{3mm}\includegraphics[scale=.27]{\elemMatDetIIPath/summary.jpg}
%\end{center}


We have seen that any matrix $M$ can be put into reduced row echelon form via a sequence of row operations, and we have seen that any row operation can be achieved via left matrix multiplication by an elementary matrix.  Suppose that $\rref(M)$ is the reduced row echelon form of $M$.  Then $$\rref(M)=E_1E_2\cdots E_kM\, ,$$ where each $E_i$ is an elementary matrix.
We know how to compute determinants of elementary matrices and products thereof, so we ask:

\begin{center}
What is the determinant of a square matrix in reduced row echelon form?  
\end{center}
The answer has two cases:
\begin{enumerate}
\item If $M$ is not invertible, then some row of $\rref(M)$ contains only zeros.  Then we can multiply the zero row by any constant $\lambda$ without changing~$M$; by our previous observation, this scales the determinant of $M$ by $\lambda$.  Thus, if $M$ is not invertible, $\det \rref(M)=\lambda \det \rref(M)$, and so $\det \rref(M)=0$.  

\item Otherwise, every row of $\rref(M)$ has a pivot on the diagonal; since $M$ is square, this means that $\rref(M)$ is the identity matrix.  So if $M$ is invertible, $\det \rref(M)=1$.
\end{enumerate}
Notice that because $\det \rref(M) = \det (E_1E_2\cdots E_kM)$, by the theorem above, $$\det \rref(M)=\det (E_1) \cdots \det (E_k) \det M\, .$$  Since each $E_i$ has non-zero determinant, then $\det \rref(M)=0$ if and only if $\det M=0$.
This establishes an important theorem:


\begin{theorem}
\label{detinvertible}
For any square matrix $M$, $\det M\neq 0$ if and only if $M$ is invertible.
\end{theorem}
Since we know the determinants of the elementary matrices, we can immediately obtain the following:


\Videoscriptlink{elementary_matrices_ii_inverses_determinants.mp4}{Determinants and Inverses}{scripts_elementary_matrices_determinants_ii_inverses}

\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/theorem_invertible.jpg}
\end{center}
\caption{Determinants measure if a matrix is invertible.}
\end{figure}

\begin{corollary}
Any elementary matrix $E^i_j, R^i(\lambda), S^i_j(\mu)$ is invertible, except for $R^i(0)$.  In fact, the inverse of an elementary matrix is another elementary matrix.
\end{corollary}


To obtain one last important result, suppose that $M$ and $N$ are square $n\times n$ matrices, with reduced row echelon forms such that, for elementary matrices  $E_i$ and $F_i$, $$M=E_1E_2\cdots E_k \, \rref(M)\, ,$$ and  $$N=F_1F_2\cdots F_l \, \rref(N)\, .$$  If $\rref(M)$ is the identity matrix ({\it i.e.}, $M$ is invertible), then:

\begin{eqnarray*}
\det (MN) & = & \det (E_1E_2\cdots E_k\,  \rref(M) F_1F_2\cdots F_l \, \rref(N) )\\
& = & \det (E_1E_2\cdots E_k I F_1F_2\cdots F_l\,  \rref(N) )\\
& = & \det (E_1) \cdots \det(E_k)\det(I)\det(F_1)\cdots\det(F_l)\det\rref(N)\\
& = & \det(M)\det(N)
\end{eqnarray*}
Otherwise, $M$ is not invertible, and $\det M=0=\det 
\rref(M)$.  Then there exists a row of zeros in $
\rref(M)$, so $R^n(\lambda)
\rref(M)=
\rref(M)$ {\it for any~$\lambda$}.  Then:
\begin{eqnarray*}
\det (MN) & = & 
\det (E_1E_2\cdots E_k \, \rref(M) N )\\
& = & 
\det (E_1) \cdots \det(E_k)\det( \rref(M)N)\\
& = & \det (E_1) \cdots \det(E_k)\det( R^n(\lambda) 
\, \rref(M)N)\\
& = & \det (E_1) \cdots \det(E_k)\lambda \det( 
\rref(M)N)\\
& = & \lambda \det (MN)
\end{eqnarray*}
Which implies that $\det (MN)=0=\det M \det N$.

Thus we have shown that for {\it any} matrices $M$ and $N$, 
\label{detmultiplicative}
\[
\det (MN) = \det M \det N
\]
This result is {\it extremely important}; do not forget it!

\Videoscriptlink{elementary_matrices_determinant_ii_product.mp4}{Alternative proof}{scripts_elementary_matrices_determinants_ii_product}

\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/detMN.jpg}
\end{center}
\caption{``The determinant of a product is the product of determinants.''}
\end{figure}





%\href{\webworkurl ReadingHomework13/2/}{Reading homework: problem 13.2}
\Reading{Determinants}{4}

%\section*{References}
%Hefferon, Chapter Four, Section I.1 and I.3
%\\
%Beezer, Chapter D, Section DM, Subsection EM
%\\
%Beezer, Chapter D, Section PDM
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Determinant}{Determinant}
%\item \href{http://en.wikipedia.org/wiki/Elementary_matrix}{Elementary Matrix}
%\end{itemize}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{Determinants}{1},
 \hwrref{Determinants}{2},
  \hwrref{Determinants}{3},
   \hwrref{Determinants}{4}\\
 $2\times 2$ Determinant & \hwref{Determinants}{7}\\
 Determinants and invertibility & \hwref{Determinants}{8},
 \hwref{Determinants}{9},
 \hwref{Determinants}{10},
 \hwref{Determinants}{11}
 \\\hline
\end{tabular}


\input{\elemMatDetIIPath/problems}

\newpage

\section{\propDetTitle}

%In section~\ref{elementarydeterminantsII} we \hyperref[detinvertible]{showed} 
We now know that the determinant of a matrix is non-zero if and only if that matrix is invertible.  We also 
know 
%\hyperref[detmultiplicative]{showed} 
that the determinant is a \emph{multiplicative} function\index{Multiplicative function}, in the sense that $\det (MN)=\det M \det N$.  Now we will devise some methods for calculating the determinant.

Recall that:
\[
\det M = \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)}.
\]

A \emph{minor}\index{Minor} of an $n\times n$ matrix $M$ is the determinant of any square matrix obtained from $M$ by deleting one row and one column.  In particular, any entry $m^i_j$ of a square matrix~$M$ is associated to a minor obtained by deleting the $i$th row and $j$th column of~$M$.

It is possible to write the determinant of a matrix in terms of  its minors\index{Expansion by minors} as follows:

\begin{eqnarray*}
\det M &=& \sum_{\sigma} \text{sgn}(\sigma)\, m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)} \\
&=& m^1_1\, \sum_{\slashed{\sigma}^1} \text{sgn}(\slashed{\sigma}^1)\, m^2_{\slashed{\sigma}^1(2)}\cdots m^n_{\slashed{\sigma}^1(n)} \\
& +&  m^1_2\, \sum_{\slashed{\sigma}^2} \text{sgn}(\slashed{\sigma}^2)\, m^2_{\slashed{\sigma}^2(1)}
m^3_{\slashed{\sigma}^2(3)}\cdots m^n_{\slashed{\sigma}^2(n)} \\
& +&  m^1_3\,  \sum_{\slashed{\sigma}^3} \text{sgn}(\slashed{\sigma}^3)\, m^2_{\slashed{\sigma}^3(1)}m^3_{\slashed{\sigma}^3(2)}m^4_{\slashed{\sigma}^3(4)}\cdots m^n_{\slashed{\sigma}^3(n)}\\ &+& \cdots
\end{eqnarray*}
Here the symbols $\slashed{\sigma}^k$ 
refers to the permutation $\sigma$ with the input $k$ removed.
The summand on  the $j$'th line of the above formula looks like the determinant of the minor obtained by removing the first  and $j$'th column of $M$. However we still need to  replace sum of $\slashed{\sigma}^j$ by a sum over permutations of  column numbers of the matrix entries of this minor. This costs a minus sign whenever $j-1$ is odd.
%refer to permutations of $n-1$ objects.  What we're doing here is collecting up all of the terms of the original sum that contain the first 
%row entry $m^1_j$ for each column number $j$.  Each term in that collection is associated to a permutation sending $1\rightarrow j$.  The remainder of any such permutation maps the set $\{2, \ldots, n \}\rightarrow \{1, \ldots, j-1, j+1, \ldots, n \}$.  We call this partial permutation $\hat{\sigma}=\begin{bmatrix} \sigma(2) & \cdots & \sigma(n) \end{bmatrix}$.
%The last issue is that the permutation $\hat{\sigma}$ may not have the same sign as $\sigma$.  From previous homework, we know that a permutation has the same parity as its inversion number.  Removing $1\rightarrow j$ from a permutation  reduces the inversion number by the number of elements right of $j$ that are less than~$j$.  Since $j$ comes first in the permutation $\begin{bmatrix}j & \sigma(2) & \cdots & \sigma(n) \end{bmatrix}$, the inversion number of $\hat{\sigma}$ is reduced by $j-1$.  Then the sign of~$\sigma$ differs from the sign of~$\hat{\sigma}$ if $\sigma$ sends $1$ to an even number.
In other words, to expand by minors we pick an entry $m^1_j$ of the first row, then add $(-1)^{j-1}$ times the determinant of the matrix with row $i$ and column~$j$ deleted. An example will probably help:

\begin{example}
Let's compute the determinant of 
$$M=\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{pmatrix}$$ using expansion by minors:

\begin{eqnarray*}
\det M & = & 1\det \begin{pmatrix}
5 & 6 \\
8 & 9 \\
\end{pmatrix}
-2 \det \begin{pmatrix}
4 & 6 \\
7 & 9 \\
\end{pmatrix}
+3 \det \begin{pmatrix}
4 & 5 \\
7 & 8 \\
\end{pmatrix} \\
& = & 1(5\cdot 9- 8\cdot 6) -2 (4\cdot 9- 7\cdot 6) + 3 (4\cdot 8- 7\cdot 5) \\[1mm]
& = & 0 \\
\end{eqnarray*}
Here, $M^{-1}$ does not exist because\footnote{A fun exercise is to compute the determinant of a $4\times 4$ matrix filled in order, from left to right,  with the numbers $1,2,3,\ldots, 16$. What do you observe? Try the same for a $5\times 5$ matrix with $1,2,3,\ldots, 25$. Is there a pattern? Can you explain it?} $\det M=0.$
\end{example}


\begin{example}
Sometimes the entries of a matrix allow us to simplify the calculation of the determinant.  Take $N= \begin{pmatrix}
1 & 2 & 3 \\
4 & 0 & 0 \\
7 & 8 & 9 \\
\end{pmatrix}$.  Notice that the second row has many zeros; then we can switch the first and second rows of $N$ before expanding in minors to get:

\begin{eqnarray*}
\det \begin{pmatrix}
1 & 2 & 3 \\
4 & 0 & 0 \\
7 & 8 & 9 \\
\end{pmatrix} & = & -\det \begin{pmatrix}
4 & 0 & 0 \\
1 & 2 & 3 \\
7 & 8 & 9 \\
\end{pmatrix}\\
&=& -4 \det \begin{pmatrix}
2 & 3 \\
8 & 9 \\
\end{pmatrix} \\
&=& 24
\end{eqnarray*}
\end{example}
 
\Videoscriptlink{properties_of_determinant_practice.mp4}{Example}{video_properties_of_determinant_practice}

Since we know how the determinant of a matrix changes when you perform row operations, it is often very beneficial to perform row
operations before computing the determinant by brute force.

\begin{example}
\begin{eqnarray*}
\det\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{pmatrix}
=
\det\begin{pmatrix}
1 & 2 & 3 \\
3 & 3 & 3 \\
6 & 6 & 6 \\
\end{pmatrix}
=
\det\begin{pmatrix}
1 & 2 & 3 \\
3 & 3 & 3 \\
0 & 0 & 0 \\
\end{pmatrix}=0\, .
\end{eqnarray*}
Try to determine which row operations we made at each step of this computation.
\end{example}

You might suspect that determinants have similar properties with respect to columns as what applies to rows:

\begin{center}
\shabox{If $M$ is a square matrix then $\det M^T = \det M\, .$
}
\end{center}

\begin{proof}
  By definition, \[
\det M = \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)}.
\]

For any permutation $\sigma$, there is a unique inverse permutation $\sigma^{-1}$ that undoes $\sigma$.  If $\sigma$ sends $i\rightarrow j$, then $\sigma^{-1}$ sends $j\rightarrow i$.  In the two-line notation for a permutation, this corresponds to just flipping the permutation over.  For example, if $\sigma=\begin{bmatrix} 
1 & 2 & 3 \\
2 & 3 & 1
\end{bmatrix}$, then we can find $\sigma^{-1}$ by flipping the permutation and then putting the columns in order:

\[
\sigma^{-1}=\begin{bmatrix} 
2 & 3 & 1 \\
1 & 2 & 3
\end{bmatrix}=\begin{bmatrix} 
1 & 2 & 3 \\
3 & 1 & 2
\end{bmatrix}\, .
\]
Since any permutation can be built up by transpositions, one can also find the inverse of a permutation $\sigma$ by undoing each of the transpositions used to build up $\sigma$; this shows that one can use the same number of transpositions to build $\sigma$ and $\sigma^{-1}$.  In particular, $\sgn \sigma= \sgn \sigma^{-1}$.

%\begin{center}\href{\webworkurl ReadingHomework14/1/}{Reading homework: problem 14.1}\end{center}
\Reading{Determinants}{5}

Then we can write out the above in formulas as follows:
\begin{eqnarray*}
\det M &=& \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)} \\
&=& \sum_{\sigma} \text{sgn}(\sigma) m_1^{\sigma^{-1}(1)}m_2^{\sigma^{-1}(2)}\cdots m_n^{\sigma^{-1}(n)} \\
&=& \sum_{\sigma} \text{sgn}(\sigma^{-1}) m_1^{\sigma^{-1}(1)}m_2^{\sigma^{-1}(2)}\cdots m_n^{\sigma^{-1}(n)} \\
&=& \sum_{\sigma} \text{sgn}(\sigma) m_1^{\sigma(1)}m_2^{\sigma(2)}\cdots m_n^{\sigma(n)} \\
&=& \det M^T.
\end{eqnarray*}
The second-to-last equality is due to the existence of a unique inverse permutation: summing over permutations is the same as summing over all inverses of permutations (see review problem~\ref{invsum}).  The final equality is by the definition of the transpose.
\end{proof}

\begin{figure}
\begin{center}
\includegraphics[scale=.25]{\propDetPath/detMT.jpg}
\end{center}
\caption{Transposes leave the determinant unchanged.}
\end{figure}

\begin{example}
Because of this, we see that expansion by minors also works over columns.  Let $$M=\begin{pmatrix}
1 & 2 & 3 \\
0 & 5 & 6 \\
0 & 8 & 9 \\
\end{pmatrix}\, .$$  Then $$\det M = \det M^T = 1\det \begin{pmatrix}
5 & 8 \\
6 & 9 \\
\end{pmatrix}=-3\, .$$
\end{example}

\subsection{Determinant of the Inverse}

Let $M$ and $N$ be $n\times n$ matrices.
We previously showed that 

\[
\det (MN)=\det M \det N \text{, and } \det I=1.
\]
Then $1 = \det I = \det (MM^{-1}) = \det M \det M^{-1}$.  As such we have:
\begin{theorem}
\[
\det M^{-1} = \frac{1}{\det M}
\]
\end{theorem}

%Just so you don't forget this:
\begin{center}
\includegraphics[scale=.28]{\propDetPath/detMm1.jpg}
\end{center}


\subsection{Adjoint of a Matrix}


Recall that for a $2\times 2$ matrix 
$$
\begin{pmatrix}d & -b \\ -c & a\end{pmatrix}\begin{pmatrix}a & b \\ c & d\end{pmatrix}
=\det \begin{pmatrix}a & b \\ c & d\end{pmatrix}\, I\, .
$$
%\begin{figure}
%\begin{center}
%\includegraphics[scale=.2]{\propDetPath/adj2x2.jpg}
%\end{center}
%\end{figure}
Or in a more careful notation: if
$$M=\begin{pmatrix}
m^1_1 & m^1_2 \\[1mm]
m^2_1 & m^2_2 \\
\end{pmatrix}\, ,$$ 
then $$M^{-1}=\frac{1}{m^1_1m^2_2-m^1_2m^2_1}\begin{pmatrix}
m^2_2 & -m^1_2 \\
-m^2_1 & m^1_1 \\
\end{pmatrix}\, ,$$
so long as $\det M=m^1_1m^2_2-m^1_2m^2_1\neq 0$.
  The  matrix $\begin{pmatrix}
m^2_2 & -m^1_2 \\
-m^2_1 & m^1_1 \\
\end{pmatrix}$ that appears above is a special matrix, called the \emph{adjoint} of $M$.  Let's define the adjoint for an $n \times n$ matrix.


The {\bf cofactor}\index{Cofactor} of $M$ corresponding to the entry $m^i_j$ of $M$ 
%and then deleting the $i$th row and $j$th column of $M$, taking the determinant of the 
is the product of the minor associated to $m^i_j$
%resulting matrix, and 
%multiplying by
and $(-1)^{i+j}$.  This is written $\cofactor(m^i_j)$.

\begin{definition}
For $M=(m^i_j)$ a square matrix, the {\bf adjoint matrix} $\adj M$ is given by
\[
\adj M = (\cofactor(m^i_j))^T.
\]
\end{definition}

\begin{example}
\[
\adj \begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix}
=
\begin{pmatrix}
\mc{\det \begin{pmatrix}
2 & 0 \\
1 & 1 
\end{pmatrix}}
& \mc{-\det \begin{pmatrix}
1 & 0 \\
0 & 1 
\end{pmatrix}}
&\mc{ \det \begin{pmatrix}
1 & 2 \\
0 & 1 
\end{pmatrix}}
\\[4mm]
-\det \begin{pmatrix}
-1 & -1 \\
1 & 1 
\end{pmatrix}
& \det \begin{pmatrix}
3 & -1 \\
0 & 1 
\end{pmatrix}
& -\det \begin{pmatrix}
3 & -1 \\
0 & 1 
\end{pmatrix}
\\[4mm]
\det \begin{pmatrix}
-1 & -1 \\
2 & 0 
\end{pmatrix}
& -\det \begin{pmatrix}
3 & -1 \\
1 & 0 
\end{pmatrix}
& \det \begin{pmatrix}
3 & -1 \\
1 & 2 
\end{pmatrix}
\\
\end{pmatrix}^T
\]
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework14/2/}{Reading homework: problem 14.2}\end{center}
\Reading{Determinants}{6}

Let's compute the product $M\adj M$.  For any matrix $N$, the $i, j$ entry of $MN$ is given by taking the dot product of the $i$th row of $M$ and the $j$th column of $N$.  
Notice that the dot product of the $i$th row of $M$ and the $i$th column of $\adj M$ is just the expansion by minors of $\det M$ in the $i$th row.
Further, notice that the dot product of the $i$th row of $M$ and the $j$th column of $\adj M$ with $j\neq i$ is the same as expanding $M$ by minors, but with the $j$th row replaced by the $i$th row.  Since the determinant of any matrix with a row repeated is zero, then these dot products are zero as well.

We know that the $i,j$ entry of the product of two matrices is the dot product of the $i$th row of the first by the $j$th column of the second.  Then:
\[
M\adj M = (\det M) I
\]

Thus, when $\det M\neq 0$, the adjoint gives an explicit formula for $M^{-1}$.


\begin{theorem}
For $M$ a square matrix with $\det M\neq 0$ (equivalently, if $M$ is invertible), then
\[
M^{-1}=\frac{1}{\det M}\adj M
\]
\end{theorem}

\videoscriptlink{properties_of_determinant_adjoint.mp4}{The Adjoint Matrix}{video_properties_of_determinant_adjoint}


\begin{figure}
\begin{center}
\includegraphics[scale=.3]{\propDetPath/adjM.jpg}
\end{center}
\end{figure}

\begin{example}
Continuing with the previous example,
\[
\adj \begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix} = \begin{pmatrix}
2 & 0 & 2 \\
-1 & 3 & -1 \\
1 & -3 & 7 \\
\end{pmatrix}.
\]

Now, multiply:

\begin{eqnarray*}
\begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix}
\begin{pmatrix}
2 & 0 & 2 \\
-1 & 3 & -1 \\
1 & -3 & 7 \\
\end{pmatrix}
&=&
\begin{pmatrix}
6 & 0 & 0 \\
0 & 6 & 0 \\
0 & 0 & 6 \\
\end{pmatrix} \\[1mm]
\Rightarrow \begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix}^{-1} & = & \frac{1}{6}\begin{pmatrix}
2 & 0 & 2 \\
-1 & 3 & -1 \\
1 & -3 & 7 \\
\end{pmatrix}
\end{eqnarray*}

This process for finding the inverse matrix is sometimes called \emph{Cramer's~Rule}~\index{Cramer's rule}.
\end{example}

\subsection{Application: Volume of a Parallelepiped}

Given three vectors $u,v,w$ in $\Re^3$, the parallelepiped\index{Parallelepiped} determined by the three vectors is the ``squished'' box whose edges are parallel to $u, v$, and $w$ as depicted in Figure~\ref{parallelepiped}.

You probably learnt in a  calculus course  that the volume of this object is $|u\dotprod (v\times w)|$.  This is the same as expansion by minors of the matrix whose columns are $u,v,w$.  Then:
\[
\text{Volume}=\big|\det \begin{pmatrix}u & v & w \end{pmatrix} \big|
\] 



\begin{figure}
\begin{center}
\includegraphics[scale=.4]{parallelepiped.jpg}
\caption{A parallelepiped.\label{parallelepiped}}
\end{center}
\end{figure}



%\section*{References}
%Hefferon, Chapter Four, Section I.1 and I.3
%\\
%Beezer, Chapter D, Section DM, Subsection DD
%\\
%Beezer, Chapter D, Section DM, Subsection CD
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Determinant}{Determinant}
%\item \href{http://en.wikipedia.org/wiki/Elementary_matrix}{Elementary Matrix}
%\item \href{http://en.wikipedia.org/wiki/Cramers_rule}{Cramer's Rule}
%\end{itemize}




\section{Review Problems}
{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{Determinants}{5},\hwrref{Determinants}{6}\\
 Row of zeros & \hwref{Determinants}{12}\\
 $3\times 3$ determinant & \hwref{Determinants}{13}\\
 Triangular determinants & \hwref{Determinants}{14},\hwref{Determinants}{15},\hwref{Determinants}{16},\hwref{Determinants}{17}\\
Expanding in a column & \hwref{Determinants}{18}\\ 
 Minors and cofactors & \hwref{Determinants}{19}\\
 \hline
\end{tabular}


\input{\propDetPath/problems}

\newpage














","\chapter{Determinants}\label{elementarydeterminants}
\chaptermark{Determinants}

Given a square matrix, is there an easy way to know when it is invertible?  Answering this fundamental question is 
the goal of this chapter.

\section{The Determinant Formula}

The determinant boils down a square matrix to a a single number. That number determines whether the square matrix is invertible or not.  
Lets see how this works for small matrices first.

\subsection{Simple Examples}

For small cases, we already know when a matrix is invertible.  If $M$ is a $1\times 1$ matrix, then $M=(m) \Rightarrow M^{-1}=(1/m)$.  Then $M$ is invertible if and only if $m\neq 0$.

\vspace{.3cm}

For $M$ a $2\times 2$ matrix, chapter~\ref{Matrices} section~\ref{inverse_matrix} shows that if $$M=\begin{pmatrix}
m^1_1 & m^1_2 \\[1mm]
m^2_1 & m^2_2 \\
\end{pmatrix}\, ,$$ then $$M^{-1}=\frac{1}{m^1_1m^2_2-m^1_2m^2_1}\begin{pmatrix}
m^2_2 & -m^1_2 \\[1mm]
-m^2_1 & m^1_1 \\
\end{pmatrix}\, .$$ Thus $M$ is invertible if and only if 
\vspace{.3cm}

$$m^1_1m^2_2-m^1_2m^2_1\neq 0\, .$$  For $2\times 2$ matrices, this quantity is called the \emph{determinant of $M$}\index{Determinant!$2\times 2$ matrix}.
\[
\det M = \det \begin{pmatrix}
m^1_1 & m^1_2 \\[1mm]
m^2_1 & m^2_2 \\
\end{pmatrix} = m^1_1m^2_2-m^1_2m^2_1\, .
\]

\begin{figure}
\begin{center}
\includegraphics[scale=.35]{\elemMatDetPath/det2x2.jpg}
\end{center}
\caption{Memorize the determinant formula for a 2$\times$2 matrix!}
\end{figure}


\begin{example}
For a $3\times 3$ matrix, $$M=\begin{pmatrix}
m^1_1 & m^1_2 & m^1_3\\[1mm]
m^2_1 & m^2_2 & m^2_3\\[1mm]
m^3_1 & m^3_2 & m^3_3\\
\end{pmatrix}\, ,$$ then---see \hyperref[det33]{review question}~\ref{det33}---$M$ is non-singular if and only if\index{Determinant!$3\times 3$ matrix}:
\vspace{.1cm}
\[
\det M= 
m^1_1m^2_2m^3_3 
- m^1_1m^2_3m^3_2 
+ m^1_2m^2_3m^3_1
- m^1_2m^2_1m^3_3  
+ m^1_3m^2_1m^3_2
- m^1_3m^2_2m^3_1
\neq 0.
\]

\noindent
Notice that in the subscripts, each ordering of the numbers $1$, $2$, and $3$ occurs exactly once.  Each of these is a \emph{permutation} of the set $\{1,2,3\}$.
\end{example}



\subsection{Permutations}
Consider $n$ objects labeled $1$ through $n$ and shuffle them.  Each possible shuffle is called a \emph{permutation}\index{Permutation}.  
For example, here is an example of a permutation of $1$--$5$:
\[
\sigma = \begin{bmatrix}
1 & 2 & 3 & 4 & 5 \\
4 & 2 & 5 & 1 & 3 
\end{bmatrix}
\]
We can consider a permutation $\sigma$ as an invertible function from the set of numbers $[n] := \{1, 2, \dotsc, n\}$ to $[n]$, so can  write $\sigma(3) = 5$ in the above example. In general we can write
\[\left[\!
\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5 \\[1mm]
\sigma(1) & \sigma(2) & \sigma(3) & \sigma(4) & \sigma(5)
\end{array}\!\right]\, ,
\]
but since the top line of any permutation is always the same, we can omit it and just write:
\[
\sigma = \begin{bmatrix}
\sigma(1) & \sigma(2) & \sigma(3) & \sigma(4) & \sigma(5)
\end{bmatrix}
\]
and so our example becomes simply  $\sigma = [4\, 2\, 5\, 1\, 3]$. 
%There is also one more notation called cycle notation, but we do not discuss it here.

The mathematics of permutations is extensive; there are a few key properties of permutations that we'll need:

\begin{itemize}
\item There are $n!$ permutations of $n$ distinct objects, since there are $n$ choices for the first object, $n-1$ choices for the second once the first has been chosen, and so on.

\item Every permutation can be built up by successively swapping pairs of objects.  For example, to build up the permutation $\begin{bmatrix} 3 & 1 & 2 \end{bmatrix}$ from the trivial permutation $\begin{bmatrix} 1 & 2 & 3 \end{bmatrix}$, you can first swap $2$ and $3$, and then swap $1$ and $3$.

\item \hypertarget{permutation_parity}{For any given} permutation $\sigma$, there is some number of swaps it takes to build up the permutation.  (It's simplest to use the minimum number of swaps, but you don't have to: it turns out that \emph{any} way of building up the permutation from swaps will have have the same parity of swaps, either even or odd.) 
If this number happens to be even, then $\sigma$ is called an \emph{even permutation};\index{Even permutation} if this number is odd, then $\sigma$ is an \emph{odd permutation}\index{Odd permutation}.  In fact, $n!$ is even for all $n\geq 2$, and exactly half of the permutations are even and the other half are odd.  It's worth noting that the trivial permutation (which sends $i\rightarrow i$ for every $i$) is an even permutation, since it uses zero swaps.
\end{itemize}

\begin{definition}
The {\bf sign function}\index{Sign function} is a function $\text{sgn}$ that sends permutations to the set $\{-1,1\}$ with rule of correspondence defined by
\[ \text{sgn}(\sigma) = 
\left\{ \begin{array}{rl}
1 & \mbox{if $\sigma$ is even}\\
-1 & \mbox{if $\sigma$ is odd}.\end{array} \right.
\]
\end{definition}

%For more on the swaps (also known as inversions) and the sign function, see \hyperref[prob_inversion_number]{Problem~\ref*{prob_inversion_number}}.

\Videoscriptlink{elementary_matrices_permutations.mp4}{Permutation Example}{scripts_elementary_matrices_permutations}

%\begin{center}\href{\webworkurl ReadingHomework12/1/}{Reading homework: problem 12.1}\end{center}
\Reading{Determinants}{1}

We can use permutations to give a definition of the determinant.

\begin{definition}  The {\bf determinant}\index{Determinant} of $n \times n$ matrix $M$ is 
\begin{center}
\scalebox{1.2}{
\shabox{
$
\det M = {\textstyle \sum\limits_{\sigma}}\  \text{sgn}(\sigma)\,  m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)}.
$
}}\end{center}
\end{definition}
The sum is over all permutations of $n$ objects; a sum over the all elements of $\{ \sigma: \{1,\dots,n\}\to  \{1,\dots,n\}\}$.  Each summand is a product of n entries from the matrix  with each factor from a different row. 
In different terms of the sum the column numbers are shuffled by different permutations~$\sigma$.

The last statement about the summands yields a nice property of the determinant:
\begin{theorem}
If $M=(m^i_j)$ has a row consisting entirely of zeros, then $m^i_{\sigma(i)}=0$ for every $\sigma$ and some $i$.  Moreover
$\det M=0$.
\end{theorem}


\begin{example}
Because there are many permutations of $n$, writing the determinant this way for a general matrix gives a very long sum.  For $n=4$, there are $24=4!$ permutations, and for $n=5$, there are already $120=5!$ permutations.\\

\noindent
For a $4\times 4$ matrix, $M=\begin{pmatrix}
m^1_1 & m^1_2 & m^1_3 & m^1_4\\[1mm]
m^2_1 & m^2_2 & m^2_3 & m^2_4\\[1mm]
m^3_1 & m^3_2 & m^3_3 & m^3_4\\[1mm]
m^4_1 & m^4_2 & m^4_3 & m^4_4\\
\end{pmatrix}$, then $\det M$ is:
\begin{eqnarray*}
\det M &=& 
 m^1_1m^2_2m^3_3m^4_4
-m^1_1m^2_3m^3_2m^4_4
-m^1_1m^2_2m^3_4m^4_3 \\[1mm]
& -&m^1_2m^2_1m^3_3m^4_4
+m^1_1m^2_3m^3_4m^4_2
+m^1_1m^2_4m^3_2m^4_3 \\[1mm]
&+ & m^1_2m^2_3m^3_1m^4_4
+m^1_2m^2_1m^3_4m^4_3
\pm \text{16 more terms}.
\end{eqnarray*}
\end{example}
This is very cumbersome.

Luckily, it is very easy to compute the determinants of certain matrices.  For example, if $M$ is diagonal, meaning that $M^i_j=0$ whenever $i\neq j$,  then all summands of the determinant involving off-diagonal entries vanish and 
\[
\det M = \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)}= m^1_{1}m^2_{2}\cdots m^n_{n}.
\]
\Shabox{1}{\begin{tabular}{c}The determinant of a diagonal matrix is\\  the product of its diagonal entries.\end{tabular}}
Since the identity matrix is diagonal with all diagonal entries equal to one, we have
\[
\det I=1.
\]

We would like to use the determinant to decide whether a matrix is invertible.  Previously, we computed the inverse of a matrix by applying row operations.  Therefore we ask what happens to the determinant when row operations are applied to a matrix.

\begin{remark}[Swapping rows]
Lets \hypertarget{rowswap}{swap}
 rows $i$ and $j$ of  a matrix $M$ and then compute its determinant.  For the permutation $\sigma$, let $\hat{\sigma}$ be the permutation obtained by swapping positions $i$ and $j$.  Clearly $$\text{sgn}(\hat{\sigma})=-\text{sgn}(\sigma)\, .$$  Let  $M'$ be the matrix $M$ with rows $i$ and $j$ swapped.  Then  (assuming $i<j$):
\begin{eqnarray}
\det M' & = & \sum_{\sigma} \text{sgn}(\sigma)\, m^1_{\sigma(1)}\cdots m^j_{\sigma(i)}\cdots m^i_{\sigma(j)} \cdots m^n_{\sigma(n)} \nonumber \\
& = & \sum_{\sigma} \text{sgn}(\sigma) \,m^1_{\sigma(1)}\cdots m^i_{\sigma(j)}\cdots m^j_{\sigma(i)} \cdots m^n_{\sigma(n)} \nonumber \\
& = & \sum_{\sigma}(-\text{sgn}(\hat{\sigma})) \,m^1_{\hat{\sigma}(1)}\cdots m^i_{\hat{\sigma}(i)}\cdots m^j_{\hat{\sigma}(j)} \cdots m^n_{\hat{\sigma}(n)} \nonumber \\
& = & - \sum_{\hat{\sigma}} \text{sgn}(\hat{\sigma}) \,m^1_{\hat{\sigma}(1)}\cdots m^i_{\hat{\sigma}(i)}\cdots m^j_{\hat{\sigma}(j)} \cdots m^n_{\hat{\sigma}(n)} \nonumber \\
& = & -\det M.\nn
\end{eqnarray}
The step replacing $\sum_\sigma$ by $\sum_{\hat \sigma}$ often causes confusion; it holds since we sum over all permutations (see review problem~\ref{hatsum}).
Thus we see that swapping rows changes the sign of the determinant. {\it I.e.}, $$\det M' = - \det M\, .$$

\begin{center}\href{\webworkurl ReadingHomework8/2/}{Reading homework: problem 8.2}\end{center}

Applying this result to $M=I$ (the identity matrix) yields
$$\det E^i_j=-1\, ,$$
where the matrix $E^i_j$ is the identity matrix with rows $i$ and $j$ swapped. It is a row swap  elementary matrix.
%and we will meet it again \hyperlink{elem_matrix_row_swap}{soon}.

This implies another nice property of the determinant.  If two rows of the matrix are identical, then swapping the rows changes the sign of the matrix, but leaves the matrix unchanged.  Then we see the following:
\begin{theorem}
If $M$ has two identical rows, then $\det M = 0$.
\end{theorem}

\end{remark}


\begin{figure}
\begin{center}
\includegraphics[scale=.3]{\elemMatDetPath/row_swap_theorem.jpg}
\end{center}
\caption{Remember what row swap does to determinants!}
\end{figure}

\section{Elementary Matrices and Determinants}\index{Elementary matrix}

In chapter~\ref{systems} we found
the matrices that perform the  row operations involved in Gaussian elimination; we called them elementary matrices.

As a reminder, for any matrix $M$, and a matrix $M'$ equal to~$M$ after a row operation, multiplying by an elementary matrix $E$ gave $M'=EM$.

\Videoscriptlink{elementary_matrices_determinant_explanation.mp4}{Elementary Matrices}{scripts_elementary_matrices_explanation}

We now examine what the elementary matrices to do determinants.

\subsection{Row Swap}
Our first elementary matrix   swaps  rows $i$ and~$j$ when it is applied to  a matrix $M$. 
Explicitly, 
let $R^1$ through $R^n$ denote the rows  of $M$, and let $M'$ be the matrix $M$ with rows $i$ and $j$ swapped.  Then $M$ and $M'$ can be regarded as a block matrices (where the blocks are rows);
\[
M=\ccolvec{\vdots \\ R^i \\ \vdots \\ R^j \\ \vdots} \text{ and }
M'=\ccolvec{\vdots \\ R^j \\ \vdots \\ R^i \\ \vdots}.
\]
Then notice that
\[
M'=\ccolvec{\\ \vdots \\ R^j \\ \vdots \\ R^i \\ \vdots\\ \\  }= 
\begin{pmatrix}
1 & & & & & & \\
& \ddots & & & & & \\
& & 0 & & 1 & & \\
& & & \ddots & & & \\
& & 1 & & 0 & & \\
& & & & & \ddots & \\
& & & & & & 1 \\
\end{pmatrix}
\ccolvec{\\ \vdots \\ R^i \\ \vdots \\ R^j \\ \vdots \\ \\  }.
\]
The matrix 
$$
\begin{pmatrix}
1 & & & & & & \\
& \ddots & & & & & \\
& & 0 & & 1 & & \\
& & & \ddots & & & \\
& & 1 & & 0 & & \\
& & & & & \ddots & \\
& & & & & & 1 \\
\end{pmatrix}=:E^i_j
$$
is just the identity matrix with rows $i$ and $j$ swapped.  \hypertarget{elem_matrix_row_swap}The matrix $E^i_j$ is an \emph{elementary matrix}\index{Elementary matrix!swapping rows} and
\[
M'=E^i_jM\, .
\]
Because $\det I=1$ and swapping a pair of rows changes the sign of the determinant, we have found that 
\[
\det E^i_j = -1\, .
\]


%\begin{center}
%\includegraphics[scale=.25]{\elemMatDetIIPath/detEij.jpg}
%\end{center}

Now we know that swapping a pair of rows flips the sign of the determinant so $\det M'=-det M$.
But $\det E_j^i=-1$ and $M'=E^i_j M$ so 
$$
\det E^i_j M = \det E^i_j \, \det M\, .
$$
This result hints at a general rule for determinants of products of matrices. 
%Stare at it again before reading the next Lecture:
%\begin{center}
%\includegraphics[scale=.2]{\elemMatDetPath/detEM=detEdetM.jpg}
%\end{center}


%\section*{References}
%Hefferon, Chapter Four, Section I.1 and I.3
%\\
%Beezer, Chapter D, Section DM, Subsection EM
%\\
%Beezer, Chapter D, Section PDM
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Determinant}{Determinant}
%\item \href{http://en.wikipedia.org/wiki/Permutation}{Permutation}
%\item \href{http://en.wikipedia.org/wiki/Elementary_matrix}{Elementary Matrix}
%\end{itemize}
%

%\section{Review Problems}
%\input{\elemMatDetPath/problems}

%\section{\elemMatDetIITitle}\label{elementarydeterminantsII}

%In section~\ref{elementarydeterminants}, we saw the definition of the determinant and derived an elementary matrix that exchanges two rows of a matrix.  

\subsection{Row  Multiplication}
The next row operation is multiplying a row by a scalar.
Consider $$M=\ccolvec{R^1 \\ \vdots \\ R^n }\, ,$$ where $R^i$ are row vectors.  Let $R^i(\lambda)$ be the identity matrix, with the $i$th diagonal entry replaced by $\lambda$, not to be confused with the row vectors. {\it I.e.},
$$
R^i(\lambda)=
\begin{pmatrix}
1 & & & & \\
  & \ddots & & & \\
  & & \lambda & & \\
  & & & \ddots & \\
  & & & & 1 \\
\end{pmatrix}
\, .$$
Then:
\[
M'=R^i(\lambda)M=\ccolvec{R^1 \\ \vdots \\ \lambda R^i \\ \vdots \\ R^n }\, ,
\]
equals $M$ with one row multiplied by~$\lambda$.

What effect does multiplication by the elementary matrix $R^i(\lambda)$ have on the determinant?

\begin{eqnarray*}
\det M' & = & \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots \lambda m^i_{\sigma(i)} \cdots m^n_{\sigma(n)} \\
& = & \lambda \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots m^i_{\sigma(i)} \cdots m^n_{\sigma(n)} \\
& = & \lambda \det M
\end{eqnarray*}
Thus, multiplying a row by $\lambda$ multiplies the determinant by $\lambda$.
{\it I.e.,} $$\det R^i(\lambda) M = \lambda \det M\, .$$


\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/row_mult_thm.jpg}
\end{center}
\caption{Rescaling a row rescales the determinant.}
\end{figure}


Since $R^i(\lambda)$ is just the identity matrix with a single row multiplied by $\lambda$, then by the above rule, the determinant of $R^i(\lambda)$ is $\lambda$.  Thus

\[
\det R^i(\lambda) = \det \begin{pmatrix}
1 & & & & \\
  & \ddots & & & \\
  & & \lambda & & \\
  & & & \ddots & \\
  & & & & 1 \\
\end{pmatrix} = \lambda\, ,
\]
and once again we have a product of determinants formula
$$
\det \left( R^i(\lambda) M \right) = \det\left( R^i(\lambda) \right)\det M.
$$

\subsection{Row Addition}
The final row operation is adding $\mu R^j$ to $R^i$.  This is done with the elementary matrix~$S^i_j(\mu)$, which is an identity matrix but with an additional  $\mu$ in the $i,j$ position;

\[
S^i_j(\mu) = \begin{pmatrix}
1 & 	& 	& 	& & & 	\\
  & \ddots & 	&	& & &	\\
  & 	& 1 	& 	& \mu & &	\\
  & 	& 	& \ddots & & &	\\
  & 	& 	& 	& 1 & & 	\\
  & 	& 	& 	& 	& \ddots & 	\\
  & 	& 	& 	& 	& 	 & 1	\\
\end{pmatrix}\, .
\]
Then multiplying $M$ by $S^i_j(\mu)$  performs a row addition;

\[
\begin{pmatrix}
1 & 	& 	& 	& & & 	\\
  & \ddots & 	&	& & &	\\
  & 	& 1 	& 	& \mu & &	\\
  & 	& 	& \ddots & & &	\\
  & 	& 	& 	& 1 & & 	\\
  & 	& 	& 	& 	& \ddots & 	\\
  & 	& 	& 	& 	& 	 & 1	\\
\end{pmatrix}\ccolvec{\\ \vdots \\ R^i \\ \vdots \\ R^j \\ \vdots\\ \\}
=
\ccolvec{\\ \vdots \\ R^i +\mu R^j \\ \vdots \\ R^j \\ \vdots\\ \\ }\, .
\]
What is the effect of multiplying by $S^i_j(\mu)$ on the determinant?  Let $M'=S^i_j(\mu)M$, and let $M''$ be the matrix $M$ but with $R^i$ replaced by $R^j$
Then

\begin{eqnarray*}
\det M' & = & \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots (m^i_{\sigma(i)}+ \mu m^j_{\sigma(i)}) \cdots m^n_{\sigma(n)} \\
& = & \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots m^i_{\sigma(i)} \cdots m^n_{\sigma(n)} \\
&   & \qquad + \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots \mu m^j_{\sigma(j)} \cdots m^j_{\sigma(j)} \cdots m^n_{\sigma(n)} \\
& = & \det M + \mu \det M''
\end{eqnarray*}
Since $M''$ has two identical rows, its determinant is $0$ so
$$
\det M' = \det M,
$$
when $M'$ is obtained from $M$ by adding $\mu$ times row~$j$ to row~$i$.

%\href{\webworkurl ReadingHomework13/1/}{Reading homework: problem 13.1}
\Reading{Determinants}{3}

\noindent
We also have learnt that
$$\det \left( S^i_j(\mu)M \right) = \det M\, .$$
Notice that if $M$ is the identity matrix, then we have $$\det S^i_j(\mu) = \det (S^i_j(\mu)I) = \det I = 1\, .$$



\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/row_addition_thm.jpg}
\end{center}
\caption{Adding one row to another leaves the determinant unchanged.}
\end{figure}

\subsection{Determinant of Products}
In summary, the elementary matrices for each of the row operations obey

\[
\begin{array}{cccc}
E^i_j &=& I \text{ with rows $i,j$ swapped;} &\det E^i_j=-1 \\[3mm]
R^i(\lambda) &=& I \text{ with $\lambda$ in position $i,i$;} 
	&\det R^i(\lambda)=\lambda \\[3mm]
S^i_j(\mu) &=& I \text{ with $\mu$ in position $i,j$;} 
	&\det S^i_j(\mu)=1 \\[3mm]
\end{array}
\]
\Videoscriptlink{elementary_matrices_and_determinants_ii_dets.mp4}{Elementary Determinants}{scripts_elementary_matrices_determinants_ii_dets}

Moreover  we found a useful formula for determinants of products:

\begin{theorem}
If $E$ is \emph{ any} of the elementary matrices $E^i_j, R^i(\lambda), S^i_j(\mu)$, then $\det(EM)=\det E \det M$.
\end{theorem}


%\begin{center}
%\hspace{3mm}\includegraphics[scale=.27]{\elemMatDetIIPath/summary.jpg}
%\end{center}


We have seen that any matrix $M$ can be put into reduced row echelon form via a sequence of row operations, and we have seen that any row operation can be achieved via left matrix multiplication by an elementary matrix.  Suppose that $\rref(M)$ is the reduced row echelon form of $M$.  Then $$\rref(M)=E_1E_2\cdots E_kM\, ,$$ where each $E_i$ is an elementary matrix.
We know how to compute determinants of elementary matrices and products thereof, so we ask:

\begin{center}
What is the determinant of a square matrix in reduced row echelon form?  
\end{center}
The answer has two cases:
\begin{enumerate}
\item If $M$ is not invertible, then some row of $\rref(M)$ contains only zeros.  Then we can multiply the zero row by any constant $\lambda$ without changing~$M$; by our previous observation, this scales the determinant of $M$ by $\lambda$.  Thus, if $M$ is not invertible, $\det \rref(M)=\lambda \det \rref(M)$, and so $\det \rref(M)=0$.  

\item Otherwise, every row of $\rref(M)$ has a pivot on the diagonal; since $M$ is square, this means that $\rref(M)$ is the identity matrix.  So if $M$ is invertible, $\det \rref(M)=1$.
\end{enumerate}
Notice that because $\det \rref(M) = \det (E_1E_2\cdots E_kM)$, by the theorem above, $$\det \rref(M)=\det (E_1) \cdots \det (E_k) \det M\, .$$  Since each $E_i$ has non-zero determinant, then $\det \rref(M)=0$ if and only if $\det M=0$.
This establishes an important theorem:


\begin{theorem}
\label{detinvertible}
For any square matrix $M$, $\det M\neq 0$ if and only if $M$ is invertible.
\end{theorem}
Since we know the determinants of the elementary matrices, we can immediately obtain the following:


\Videoscriptlink{elementary_matrices_ii_inverses_determinants.mp4}{Determinants and Inverses}{scripts_elementary_matrices_determinants_ii_inverses}

\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/theorem_invertible.jpg}
\end{center}
\caption{Determinants measure if a matrix is invertible.}
\end{figure}

\begin{corollary}
Any elementary matrix $E^i_j, R^i(\lambda), S^i_j(\mu)$ is invertible, except for $R^i(0)$.  In fact, the inverse of an elementary matrix is another elementary matrix.
\end{corollary}


To obtain one last important result, suppose that $M$ and $N$ are square $n\times n$ matrices, with reduced row echelon forms such that, for elementary matrices  $E_i$ and $F_i$, $$M=E_1E_2\cdots E_k \, \rref(M)\, ,$$ and  $$N=F_1F_2\cdots F_l \, \rref(N)\, .$$  If $\rref(M)$ is the identity matrix ({\it i.e.}, $M$ is invertible), then:

\begin{eqnarray*}
\det (MN) & = & \det (E_1E_2\cdots E_k\,  \rref(M) F_1F_2\cdots F_l \, \rref(N) )\\
& = & \det (E_1E_2\cdots E_k I F_1F_2\cdots F_l\,  \rref(N) )\\
& = & \det (E_1) \cdots \det(E_k)\det(I)\det(F_1)\cdots\det(F_l)\det\rref(N)\\
& = & \det(M)\det(N)
\end{eqnarray*}
Otherwise, $M$ is not invertible, and $\det M=0=\det 
\rref(M)$.  Then there exists a row of zeros in $
\rref(M)$, so $R^n(\lambda)
\rref(M)=
\rref(M)$ {\it for any~$\lambda$}.  Then:
\begin{eqnarray*}
\det (MN) & = & 
\det (E_1E_2\cdots E_k \, \rref(M) N )\\
& = & 
\det (E_1) \cdots \det(E_k)\det( \rref(M)N)\\
& = & \det (E_1) \cdots \det(E_k)\det( R^n(\lambda) 
\, \rref(M)N)\\
& = & \det (E_1) \cdots \det(E_k)\lambda \det( 
\rref(M)N)\\
& = & \lambda \det (MN)
\end{eqnarray*}
Which implies that $\det (MN)=0=\det M \det N$.

Thus we have shown that for {\it any} matrices $M$ and $N$, 
\label{detmultiplicative}
\[
\det (MN) = \det M \det N
\]
This result is {\it extremely important}; do not forget it!

\Videoscriptlink{elementary_matrices_determinant_ii_product.mp4}{Alternative proof}{scripts_elementary_matrices_determinants_ii_product}

\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/detMN.jpg}
\end{center}
\caption{``The determinant of a product is the product of determinants.''}
\end{figure}





%\href{\webworkurl ReadingHomework13/2/}{Reading homework: problem 13.2}
\Reading{Determinants}{4}

%\section*{References}
%Hefferon, Chapter Four, Section I.1 and I.3
%\\
%Beezer, Chapter D, Section DM, Subsection EM
%\\
%Beezer, Chapter D, Section PDM
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Determinant}{Determinant}
%\item \href{http://en.wikipedia.org/wiki/Elementary_matrix}{Elementary Matrix}
%\end{itemize}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{Determinants}{1},
 \hwrref{Determinants}{2},
  \hwrref{Determinants}{3},
   \hwrref{Determinants}{4}\\
 $2\times 2$ Determinant & \hwref{Determinants}{7}\\
 Determinants and invertibility & \hwref{Determinants}{8},
 \hwref{Determinants}{9},
 \hwref{Determinants}{10},
 \hwref{Determinants}{11}
 \\\hline
\end{tabular}


\input{\elemMatDetIIPath/problems}

\newpage

\section{\propDetTitle}

%In section~\ref{elementarydeterminantsII} we \hyperref[detinvertible]{showed} 
We now know that the determinant of a matrix is non-zero if and only if that matrix is invertible.  We also 
know 
%\hyperref[detmultiplicative]{showed} 
that the determinant is a \emph{multiplicative} function\index{Multiplicative function}, in the sense that $\det (MN)=\det M \det N$.  Now we will devise some methods for calculating the determinant.

Recall that:
\[
\det M = \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)}.
\]

A \emph{minor}\index{Minor} of an $n\times n$ matrix $M$ is the determinant of any square matrix obtained from $M$ by deleting one row and one column.  In particular, any entry $m^i_j$ of a square matrix~$M$ is associated to a minor obtained by deleting the $i$th row and $j$th column of~$M$.

It is possible to write the determinant of a matrix in terms of  its minors\index{Expansion by minors} as follows:

\begin{eqnarray*}
\det M &=& \sum_{\sigma} \text{sgn}(\sigma)\, m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)} \\
&=& m^1_1\, \sum_{\slashed{\sigma}^1} \text{sgn}(\slashed{\sigma}^1)\, m^2_{\slashed{\sigma}^1(2)}\cdots m^n_{\slashed{\sigma}^1(n)} \\
& +&  m^1_2\, \sum_{\slashed{\sigma}^2} \text{sgn}(\slashed{\sigma}^2)\, m^2_{\slashed{\sigma}^2(1)}
m^3_{\slashed{\sigma}^2(3)}\cdots m^n_{\slashed{\sigma}^2(n)} \\
& +&  m^1_3\,  \sum_{\slashed{\sigma}^3} \text{sgn}(\slashed{\sigma}^3)\, m^2_{\slashed{\sigma}^3(1)}m^3_{\slashed{\sigma}^3(2)}m^4_{\slashed{\sigma}^3(4)}\cdots m^n_{\slashed{\sigma}^3(n)}\\ &+& \cdots
\end{eqnarray*}
Here the symbols $\slashed{\sigma}^k$ 
refers to the permutation $\sigma$ with the input $k$ removed.
The summand on  the $j$'th line of the above formula looks like the determinant of the minor obtained by removing the first  and $j$'th column of $M$. However we still need to  replace sum of $\slashed{\sigma}^j$ by a sum over permutations of  column numbers of the matrix entries of this minor. This costs a minus sign whenever $j-1$ is odd.
%refer to permutations of $n-1$ objects.  What we're doing here is collecting up all of the terms of the original sum that contain the first 
%row entry $m^1_j$ for each column number $j$.  Each term in that collection is associated to a permutation sending $1\rightarrow j$.  The remainder of any such permutation maps the set $\{2, \ldots, n \}\rightarrow \{1, \ldots, j-1, j+1, \ldots, n \}$.  We call this partial permutation $\hat{\sigma}=\begin{bmatrix} \sigma(2) & \cdots & \sigma(n) \end{bmatrix}$.
%The last issue is that the permutation $\hat{\sigma}$ may not have the same sign as $\sigma$.  From previous homework, we know that a permutation has the same parity as its inversion number.  Removing $1\rightarrow j$ from a permutation  reduces the inversion number by the number of elements right of $j$ that are less than~$j$.  Since $j$ comes first in the permutation $\begin{bmatrix}j & \sigma(2) & \cdots & \sigma(n) \end{bmatrix}$, the inversion number of $\hat{\sigma}$ is reduced by $j-1$.  Then the sign of~$\sigma$ differs from the sign of~$\hat{\sigma}$ if $\sigma$ sends $1$ to an even number.
In other words, to expand by minors we pick an entry $m^1_j$ of the first row, then add $(-1)^{j-1}$ times the determinant of the matrix with row $i$ and column~$j$ deleted. An example will probably help:

\begin{example}
Let's compute the determinant of 
$$M=\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{pmatrix}$$ using expansion by minors:

\begin{eqnarray*}
\det M & = & 1\det \begin{pmatrix}
5 & 6 \\
8 & 9 \\
\end{pmatrix}
-2 \det \begin{pmatrix}
4 & 6 \\
7 & 9 \\
\end{pmatrix}
+3 \det \begin{pmatrix}
4 & 5 \\
7 & 8 \\
\end{pmatrix} \\
& = & 1(5\cdot 9- 8\cdot 6) -2 (4\cdot 9- 7\cdot 6) + 3 (4\cdot 8- 7\cdot 5) \\[1mm]
& = & 0 \\
\end{eqnarray*}
Here, $M^{-1}$ does not exist because\footnote{A fun exercise is to compute the determinant of a $4\times 4$ matrix filled in order, from left to right,  with the numbers $1,2,3,\ldots, 16$. What do you observe? Try the same for a $5\times 5$ matrix with $1,2,3,\ldots, 25$. Is there a pattern? Can you explain it?} $\det M=0.$
\end{example}


\begin{example}
Sometimes the entries of a matrix allow us to simplify the calculation of the determinant.  Take $N= \begin{pmatrix}
1 & 2 & 3 \\
4 & 0 & 0 \\
7 & 8 & 9 \\
\end{pmatrix}$.  Notice that the second row has many zeros; then we can switch the first and second rows of $N$ before expanding in minors to get:

\begin{eqnarray*}
\det \begin{pmatrix}
1 & 2 & 3 \\
4 & 0 & 0 \\
7 & 8 & 9 \\
\end{pmatrix} & = & -\det \begin{pmatrix}
4 & 0 & 0 \\
1 & 2 & 3 \\
7 & 8 & 9 \\
\end{pmatrix}\\
&=& -4 \det \begin{pmatrix}
2 & 3 \\
8 & 9 \\
\end{pmatrix} \\
&=& 24
\end{eqnarray*}
\end{example}
 
\Videoscriptlink{properties_of_determinant_practice.mp4}{Example}{video_properties_of_determinant_practice}

Since we know how the determinant of a matrix changes when you perform row operations, it is often very beneficial to perform row
operations before computing the determinant by brute force.

\begin{example}
\begin{eqnarray*}
\det\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{pmatrix}
=
\det\begin{pmatrix}
1 & 2 & 3 \\
3 & 3 & 3 \\
6 & 6 & 6 \\
\end{pmatrix}
=
\det\begin{pmatrix}
1 & 2 & 3 \\
3 & 3 & 3 \\
0 & 0 & 0 \\
\end{pmatrix}=0\, .
\end{eqnarray*}
Try to determine which row operations we made at each step of this computation.
\end{example}

You might suspect that determinants have similar properties with respect to columns as what applies to rows:

\begin{center}
\shabox{If $M$ is a square matrix then $\det M^T = \det M\, .$
}
\end{center}

\begin{proof}
  By definition, \[
\det M = \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)}.
\]

For any permutation $\sigma$, there is a unique inverse permutation $\sigma^{-1}$ that undoes $\sigma$.  If $\sigma$ sends $i\rightarrow j$, then $\sigma^{-1}$ sends $j\rightarrow i$.  In the two-line notation for a permutation, this corresponds to just flipping the permutation over.  For example, if $\sigma=\begin{bmatrix} 
1 & 2 & 3 \\
2 & 3 & 1
\end{bmatrix}$, then we can find $\sigma^{-1}$ by flipping the permutation and then putting the columns in order:

\[
\sigma^{-1}=\begin{bmatrix} 
2 & 3 & 1 \\
1 & 2 & 3
\end{bmatrix}=\begin{bmatrix} 
1 & 2 & 3 \\
3 & 1 & 2
\end{bmatrix}\, .
\]
Since any permutation can be built up by transpositions, one can also find the inverse of a permutation $\sigma$ by undoing each of the transpositions used to build up $\sigma$; this shows that one can use the same number of transpositions to build $\sigma$ and $\sigma^{-1}$.  In particular, $\sgn \sigma= \sgn \sigma^{-1}$.

%\begin{center}\href{\webworkurl ReadingHomework14/1/}{Reading homework: problem 14.1}\end{center}
\Reading{Determinants}{5}

Then we can write out the above in formulas as follows:
\begin{eqnarray*}
\det M &=& \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)} \\
&=& \sum_{\sigma} \text{sgn}(\sigma) m_1^{\sigma^{-1}(1)}m_2^{\sigma^{-1}(2)}\cdots m_n^{\sigma^{-1}(n)} \\
&=& \sum_{\sigma} \text{sgn}(\sigma^{-1}) m_1^{\sigma^{-1}(1)}m_2^{\sigma^{-1}(2)}\cdots m_n^{\sigma^{-1}(n)} \\
&=& \sum_{\sigma} \text{sgn}(\sigma) m_1^{\sigma(1)}m_2^{\sigma(2)}\cdots m_n^{\sigma(n)} \\
&=& \det M^T.
\end{eqnarray*}
The second-to-last equality is due to the existence of a unique inverse permutation: summing over permutations is the same as summing over all inverses of permutations (see review problem~\ref{invsum}).  The final equality is by the definition of the transpose.
\end{proof}

\begin{figure}
\begin{center}
\includegraphics[scale=.25]{\propDetPath/detMT.jpg}
\end{center}
\caption{Transposes leave the determinant unchanged.}
\end{figure}

\begin{example}
Because of this, we see that expansion by minors also works over columns.  Let $$M=\begin{pmatrix}
1 & 2 & 3 \\
0 & 5 & 6 \\
0 & 8 & 9 \\
\end{pmatrix}\, .$$  Then $$\det M = \det M^T = 1\det \begin{pmatrix}
5 & 8 \\
6 & 9 \\
\end{pmatrix}=-3\, .$$
\end{example}

\subsection{Determinant of the Inverse}

Let $M$ and $N$ be $n\times n$ matrices.
We previously showed that 

\[
\det (MN)=\det M \det N \text{, and } \det I=1.
\]
Then $1 = \det I = \det (MM^{-1}) = \det M \det M^{-1}$.  As such we have:
\begin{theorem}
\[
\det M^{-1} = \frac{1}{\det M}
\]
\end{theorem}

%Just so you don't forget this:
\begin{center}
\includegraphics[scale=.28]{\propDetPath/detMm1.jpg}
\end{center}


\subsection{Adjoint of a Matrix}


Recall that for a $2\times 2$ matrix 
$$
\begin{pmatrix}d & -b \\ -c & a\end{pmatrix}\begin{pmatrix}a & b \\ c & d\end{pmatrix}
=\det \begin{pmatrix}a & b \\ c & d\end{pmatrix}\, I\, .
$$
%\begin{figure}
%\begin{center}
%\includegraphics[scale=.2]{\propDetPath/adj2x2.jpg}
%\end{center}
%\end{figure}
Or in a more careful notation: if
$$M=\begin{pmatrix}
m^1_1 & m^1_2 \\[1mm]
m^2_1 & m^2_2 \\
\end{pmatrix}\, ,$$ 
then $$M^{-1}=\frac{1}{m^1_1m^2_2-m^1_2m^2_1}\begin{pmatrix}
m^2_2 & -m^1_2 \\
-m^2_1 & m^1_1 \\
\end{pmatrix}\, ,$$
so long as $\det M=m^1_1m^2_2-m^1_2m^2_1\neq 0$.
  The  matrix $\begin{pmatrix}
m^2_2 & -m^1_2 \\
-m^2_1 & m^1_1 \\
\end{pmatrix}$ that appears above is a special matrix, called the \emph{adjoint} of $M$.  Let's define the adjoint for an $n \times n$ matrix.


The {\bf cofactor}\index{Cofactor} of $M$ corresponding to the entry $m^i_j$ of $M$ 
%and then deleting the $i$th row and $j$th column of $M$, taking the determinant of the 
is the product of the minor associated to $m^i_j$
%resulting matrix, and 
%multiplying by
and $(-1)^{i+j}$.  This is written $\cofactor(m^i_j)$.

\begin{definition}
For $M=(m^i_j)$ a square matrix, the {\bf adjoint matrix} $\adj M$ is given by
\[
\adj M = (\cofactor(m^i_j))^T.
\]
\end{definition}

\begin{example}
\[
\adj \begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix}
=
\begin{pmatrix}
\mc{\det \begin{pmatrix}
2 & 0 \\
1 & 1 
\end{pmatrix}}
& \mc{-\det \begin{pmatrix}
1 & 0 \\
0 & 1 
\end{pmatrix}}
&\mc{ \det \begin{pmatrix}
1 & 2 \\
0 & 1 
\end{pmatrix}}
\\[4mm]
-\det \begin{pmatrix}
-1 & -1 \\
1 & 1 
\end{pmatrix}
& \det \begin{pmatrix}
3 & -1 \\
0 & 1 
\end{pmatrix}
& -\det \begin{pmatrix}
3 & -1 \\
0 & 1 
\end{pmatrix}
\\[4mm]
\det \begin{pmatrix}
-1 & -1 \\
2 & 0 
\end{pmatrix}
& -\det \begin{pmatrix}
3 & -1 \\
1 & 0 
\end{pmatrix}
& \det \begin{pmatrix}
3 & -1 \\
1 & 2 
\end{pmatrix}
\\
\end{pmatrix}^T
\]
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework14/2/}{Reading homework: problem 14.2}\end{center}
\Reading{Determinants}{6}

Let's compute the product $M\adj M$.  For any matrix $N$, the $i, j$ entry of $MN$ is given by taking the dot product of the $i$th row of $M$ and the $j$th column of $N$.  
Notice that the dot product of the $i$th row of $M$ and the $i$th column of $\adj M$ is just the expansion by minors of $\det M$ in the $i$th row.
Further, notice that the dot product of the $i$th row of $M$ and the $j$th column of $\adj M$ with $j\neq i$ is the same as expanding $M$ by minors, but with the $j$th row replaced by the $i$th row.  Since the determinant of any matrix with a row repeated is zero, then these dot products are zero as well.

We know that the $i,j$ entry of the product of two matrices is the dot product of the $i$th row of the first by the $j$th column of the second.  Then:
\[
M\adj M = (\det M) I
\]

Thus, when $\det M\neq 0$, the adjoint gives an explicit formula for $M^{-1}$.


\begin{theorem}
For $M$ a square matrix with $\det M\neq 0$ (equivalently, if $M$ is invertible), then
\[
M^{-1}=\frac{1}{\det M}\adj M
\]
\end{theorem}

\videoscriptlink{properties_of_determinant_adjoint.mp4}{The Adjoint Matrix}{video_properties_of_determinant_adjoint}


\begin{figure}
\begin{center}
\includegraphics[scale=.3]{\propDetPath/adjM.jpg}
\end{center}
\end{figure}

\begin{example}
Continuing with the previous example,
\[
\adj \begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix} = \begin{pmatrix}
2 & 0 & 2 \\
-1 & 3 & -1 \\
1 & -3 & 7 \\
\end{pmatrix}.
\]

Now, multiply:

\begin{eqnarray*}
\begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix}
\begin{pmatrix}
2 & 0 & 2 \\
-1 & 3 & -1 \\
1 & -3 & 7 \\
\end{pmatrix}
&=&
\begin{pmatrix}
6 & 0 & 0 \\
0 & 6 & 0 \\
0 & 0 & 6 \\
\end{pmatrix} \\[1mm]
\Rightarrow \begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix}^{-1} & = & \frac{1}{6}\begin{pmatrix}
2 & 0 & 2 \\
-1 & 3 & -1 \\
1 & -3 & 7 \\
\end{pmatrix}
\end{eqnarray*}

This process for finding the inverse matrix is sometimes called \emph{Cramer's~Rule}~\index{Cramer's rule}.
\end{example}

\subsection{Application: Volume of a Parallelepiped}

Given three vectors $u,v,w$ in $\Re^3$, the parallelepiped\index{Parallelepiped} determined by the three vectors is the ``squished'' box whose edges are parallel to $u, v$, and $w$ as depicted in Figure~\ref{parallelepiped}.

You probably learnt in a  calculus course  that the volume of this object is $|u\dotprod (v\times w)|$.  This is the same as expansion by minors of the matrix whose columns are $u,v,w$.  Then:
\[
\text{Volume}=\big|\det \begin{pmatrix}u & v & w \end{pmatrix} \big|
\] 



\begin{figure}
\begin{center}
\includegraphics[scale=.4]{parallelepiped.jpg}
\caption{A parallelepiped.\label{parallelepiped}}
\end{center}
\end{figure}



%\section*{References}
%Hefferon, Chapter Four, Section I.1 and I.3
%\\
%Beezer, Chapter D, Section DM, Subsection DD
%\\
%Beezer, Chapter D, Section DM, Subsection CD
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Determinant}{Determinant}
%\item \href{http://en.wikipedia.org/wiki/Elementary_matrix}{Elementary Matrix}
%\item \href{http://en.wikipedia.org/wiki/Cramers_rule}{Cramer's Rule}
%\end{itemize}




\section{Review Problems}
{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{Determinants}{5},\hwrref{Determinants}{6}\\
 Row of zeros & \hwref{Determinants}{12}\\
 $3\times 3$ determinant & \hwref{Determinants}{13}\\
 Triangular determinants & \hwref{Determinants}{14},\hwref{Determinants}{15},\hwref{Determinants}{16},\hwref{Determinants}{17}\\
Expanding in a column & \hwref{Determinants}{18}\\ 
 Minors and cofactors & \hwref{Determinants}{19}\\
 \hline
\end{tabular}


\input{\propDetPath/problems}

\newpage














",lesson
14,Properties Of Determinant,"
\chapter{\propDetTitle}

In Lecture~\ref{elementarydeterminantsII} we \hyperref[detinvertible]{showed} that the determinant of a matrix is non-zero if and only if that matrix is invertible.  We also \hyperref[detmultiplicative]{showed} that the determinant is a \emph{multiplicative} function\index{Multiplicative function}, in the sense that $\det (MN)=\det M \det N$.  Now we will devise some methods for calculating the determinant.

Recall that:
\[
\det M = \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)}.
\]

A \emph{minor}\index{Minor} of an $n\times n$ matrix $M$ is the determinant of any square matrix obtained from $M$ by deleting one row and one column.  In particular, any entry $m^i_j$ of a square matrix~$M$ is associated to a minor obtained by deleting the $i$th row and $j$th column of~$M$.

It is possible to write the determinant of a matrix in terms of  its minors\index{Expansion by minors} as follows:

\begin{eqnarray*}
\det M &=& \sum_{\sigma} \text{sgn}(\sigma)\, m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)} \\
&=& m^1_1\, \sum_{\hat{\sigma}} \text{sgn}(\hat{\sigma})\, m^2_{\hat{\sigma}(2)}\cdots m^n_{\hat{\sigma}(n)} \\
& -&  m^1_2\, \sum_{\hat{\sigma}} \text{sgn}(\hat{\sigma})\, m^2_{\hat{\sigma}(1)}m^3_{\hat{\sigma}(3)}\cdots m^n_{\hat{\sigma}(n)} \\
& +&  m^1_3\,  \sum_{\hat{\sigma}} \text{sgn}(\hat{\sigma})\, m^2_{\hat{\sigma}(1)}m^3_{\hat{\sigma}(2)}m^4_{\hat{\sigma}(4)}\cdots m^n_{\hat{\sigma}(n)} \pm \cdots
\end{eqnarray*}
Here the symbols $\hat{\sigma}$ refer to permutations of $n-1$ objects.  What we're doing here is collecting up all of the terms of the original sum that contain the first row entry $m^1_j$ for each column number $j$.  Each term in that collection is associated to a permutation sending $1\rightarrow j$.  The remainder of any such permutation maps the set $\{2, \ldots, n \}\rightarrow \{1, \ldots, j-1, j+1, \ldots, n \}$.  We call this partial permutation $\hat{\sigma}=\begin{bmatrix} \sigma(2) & \cdots & \sigma(n) \end{bmatrix}$.

The last issue is that the permutation $\hat{\sigma}$ may not have the same sign as $\sigma$.  From previous homework, we know that a permutation has the same parity as its inversion number.  Removing $1\rightarrow j$ from a permutation  reduces the inversion number by the number of elements right of $j$ that are less than~$j$.  Since $j$ comes first in the permutation $\begin{bmatrix}j & \sigma(2) & \cdots & \sigma(n) \end{bmatrix}$, the inversion number of $\hat{\sigma}$ is reduced by $j-1$.  Then the sign of~$\sigma$ differs from the sign of~$\hat{\sigma}$ if $\sigma$ sends $1$ to an even number.

In other words, to expand by minors we pick an entry $m^1_j$ of the first row, then add $(-1)^{j-1}$ times the determinant of the matrix with row $i$ and column~$j$ deleted.

\begin{example}
Let's compute the determinant of 
$M=\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{pmatrix}$ using expansion by minors.

\begin{eqnarray*}
\det M & = & 1\det \begin{pmatrix}
5 & 6 \\
8 & 9 \\
\end{pmatrix}
-2 \det \begin{pmatrix}
4 & 6 \\
7 & 9 \\
\end{pmatrix}
+3 \det \begin{pmatrix}
4 & 5 \\
7 & 8 \\
\end{pmatrix} \\
& = & 1(5\cdot 9- 8\cdot 6) -2 (4\cdot 9- 7\cdot 6) + 3 (4\cdot 8- 7\cdot 5) \\
& = & 0 \\
\end{eqnarray*}
Here, $M^{-1}$ does not exist because\footnote{A fun exercise is to compute the determinant of a $4\times 4$ matrix filled in order, from left to right,  with the numbers $1,2,3,\ldots 16$. What do you observe? Try the same for a $5\times 5$ matrix with $1,2,3\ldots 25$. Is there a pattern? Can you explain it?} $\det M=0.$
\end{example}


\begin{example}
Sometimes the entries of a matrix allow us to simplify the calculation of the determinant.  Take $N= \begin{pmatrix}
1 & 2 & 3 \\
4 & 0 & 0 \\
7 & 8 & 9 \\
\end{pmatrix}$.  Notice that the second row has many zeros; then we can switch the first and second rows of $N$ to get:

\begin{eqnarray*}
\det \begin{pmatrix}
1 & 2 & 3 \\
4 & 0 & 0 \\
7 & 8 & 9 \\
\end{pmatrix} & = & -\det \begin{pmatrix}
4 & 0 & 0 \\
1 & 2 & 3 \\
7 & 8 & 9 \\
\end{pmatrix}\\
&=& -4 \det \begin{pmatrix}
2 & 3 \\
8 & 9 \\
\end{pmatrix} \\
&=& 24
\end{eqnarray*}
\end{example}
 
\videoscriptlink{properties_of_determinant_practice.mp4}{Example}{video_properties_of_determinant_practice}

\begin{theorem}
For any square matrix $M$, we have:
\[
\det M^T = \det M
\]
\end{theorem}
\begin{proof}
  By definition, \[
\det M = \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)}.
\]

For any permutation $\sigma$, there is a unique inverse permutation $\sigma^{-1}$ that undoes $\sigma$.  If $\sigma$ sends $i\rightarrow j$, then $\sigma^{-1}$ sends $j\rightarrow i$.  In the two-line notation for a permutation, this corresponds to just flipping the permutation over.  For example, if $\sigma=\begin{bmatrix} 
1 & 2 & 3 \\
2 & 3 & 1
\end{bmatrix}$, then we can find $\sigma^{-1}$ by flipping the permutation and then putting the columns in order:

\[
\sigma^{-1}=\begin{bmatrix} 
2 & 3 & 1 \\
1 & 2 & 3
\end{bmatrix}=\begin{bmatrix} 
1 & 2 & 3 \\
3 & 1 & 2
\end{bmatrix}
\]
Since any permutation can be built up by transpositions, one can also find the inverse of a permutation $\sigma$ by undoing each of the transpositions used to build up $\sigma$; this shows that one can use the same number of transpositions to build $\sigma$ and $\sigma^{-1}$.  In particular, $\sgn \sigma= \sgn \sigma^{-1}$.

%\begin{center}\href{\webworkurl ReadingHomework14/1/}{Reading homework: problem 14.1}\end{center}
\reading{14}{1}

Then we can write out the above in formulas as follows:
\begin{eqnarray*}
\det M &=& \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)} \\
&=& \sum_{\sigma} \text{sgn}(\sigma) m_1^{\sigma^{-1}(1)}m_2^{\sigma^{-1}(2)}\cdots m_n^{\sigma^{-1}(n)} \\
&=& \sum_{\sigma} \text{sgn}(\sigma^{-1}) m_1^{\sigma^{-1}(1)}m_2^{\sigma^{-1}(2)}\cdots m_n^{\sigma^{-1}(n)} \\
&=& \sum_{\sigma} \text{sgn}(\sigma) m_1^{\sigma(1)}m_2^{\sigma(2)}\cdots m_n^{\sigma(n)} \\
&=& \det M^T.
\end{eqnarray*}
The second-to-last equality is due to the existence of a unique inverse permutation: summing over permutations is the same as summing over all inverses of permutations.  The final equality is by the definition of the transpose.
\end{proof}

\begin{center}
\includegraphics[scale=.25]{\propDetPath/detMT.jpg}
\end{center}

\begin{example}
Because of this theorem, we see that expansion by minors also works over columns.  Let $M=\begin{pmatrix}
1 & 2 & 3 \\
0 & 5 & 6 \\
0 & 8 & 9 \\
\end{pmatrix}$.  Then $$\det M = \det M^T = 1\det \begin{pmatrix}
5 & 8 \\
6 & 9 \\
\end{pmatrix}=-3\, .$$
\end{example}

\section{Determinant of the Inverse}

Let $M$ and $N$ be $n\times n$ matrices.
We previously showed that 

\[
\det (MN)=\det M \det N \text{, and } \det I=1.
\]
Then $1 = \det I = \det (MM^{-1}) = \det M \det M^{-1}$.  As such we have:
\begin{theorem}
\[
\det M^{-1} = \frac{1}{\det M}
\]
\end{theorem}

Just so you don't forget this:
\begin{center}
\includegraphics[scale=.3]{\propDetPath/detMm1.jpg}
\end{center}


\section{Adjoint of a Matrix}


Recall that for the $2\times 2$ matrix 
%\begin{figure}
\begin{center}
\includegraphics[scale=.2]{\propDetPath/adj2x2.jpg}
\end{center}
%\end{figure}
Or in a more careful notation: if
$M=\begin{pmatrix}
m^1_1 & m^1_2 \\
m^2_1 & m^2_2 \\
\end{pmatrix}$, then $$M^{-1}=\frac{1}{m^1_1m^2_2-m^1_2m^2_1}\begin{pmatrix}
m^2_2 & -m^1_2 \\
-m^2_1 & m^1_1 \\
\end{pmatrix}\, ,$$
so long as $\det M=m^1_1m^2_2-m^1_2m^2_1\neq 0$.
  The  matrix $\begin{pmatrix}
m^2_2 & -m^1_2 \\
-m^2_1 & m^1_1 \\
\end{pmatrix}$ that appears above is a special matrix, called the \emph{adjoint} of $M$.  Let's define the adjoint for an $n \times n$ matrix.


The \emph{cofactor}\index{Cofactor} of $M$ corresponding to the entry $m^i_j$ of $M$ 
%and then deleting the $i$th row and $j$th column of $M$, taking the determinant of the 
is the prooduct of the minor associated to $m^i_j$
%resulting matrix, and 
%multiplying by
times $(-1)^{i+j}$.  This is written $\cofactor(m^i_j)$.

\begin{definition}
For $M=(m^i_j)$ a square matrix, The \emph{adjoint matrix} $\adj M$ is given by:
\[
\adj M = (\cofactor(m^i_j))^T
\]
\end{definition}

\begin{example}
\[
\adj \begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix}
=
\begin{pmatrix}
\det \begin{pmatrix}
2 & 0 \\
1 & 1 
\end{pmatrix}
& -\det \begin{pmatrix}
1 & 0 \\
0 & 1 
\end{pmatrix}
& \det \begin{pmatrix}
1 & 2 \\
0 & 1 
\end{pmatrix}
\\
-\det \begin{pmatrix}
-1 & -1 \\
1 & 1 
\end{pmatrix}
& \det \begin{pmatrix}
3 & -1 \\
0 & 1 
\end{pmatrix}
& -\det \begin{pmatrix}
3 & -1 \\
0 & 1 
\end{pmatrix}
\\
\det \begin{pmatrix}
-1 & -1 \\
2 & 0 
\end{pmatrix}
& -\det \begin{pmatrix}
3 & -1 \\
1 & 0 
\end{pmatrix}
& \det \begin{pmatrix}
3 & -1 \\
1 & 2 
\end{pmatrix}
\\
\end{pmatrix}^T
\]
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework14/2/}{Reading homework: problem 14.2}\end{center}
\reading{14}{2}

Let's multiply $M\adj M$.  For any matrix $N$, the $i, j$ entry of $MN$ is given by taking the dot product of the $i$th row of $M$ and the $j$th column of $N$.  
Notice that the dot product of the $i$th row of $M$ and the $i$th column of $\adj M$ is just the expansion by minors of $\det M$ in the $i$th row.
Further, notice that the dot product of the $i$th row of $M$ and the $j$th column of $\adj M$ with $j\neq i$ is the same as expanding $M$ by minors, but with the $j$th row replaced by the $i$th row.  Since the determinant of any matrix with a row repeated is zero, then these dot products are zero as well.

We know that the $i,j$ entry of the product of two matrices is the dot product of the $i$th row of the first by the $j$th column of the second.  Then:
\[
M\adj M = (\det M) I
\]

Thus, when $\det M\neq 0$, the adjoint gives an explicit formula for $M^{-1}$.


\begin{theorem}
For $M$ a square matrix with $\det M\neq 0$ (equivalently, if $M$ is invertible), then
\[
M^{-1}=\frac{1}{\det M}\adj M
\]
\end{theorem}

\videoscriptlink{properties_of_determinant_adjoint.mp4}{The Adjoint Matrix}{video_properties_of_determinant_adjoint}


\begin{figure}
\begin{center}
\includegraphics[scale=.3]{\propDetPath/adjM.jpg}
\end{center}
\end{figure}

\begin{example}
Continuing with the previous example,
\[
\adj \begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix} = \begin{pmatrix}
2 & 0 & 2 \\
-1 & 3 & -1 \\
1 & -3 & 7 \\
\end{pmatrix}.
\]

Now, multiply:

\begin{eqnarray*}
\begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix}
\begin{pmatrix}
2 & 0 & 2 \\
-1 & 3 & -1 \\
1 & -3 & 7 \\
\end{pmatrix}
&=&
\begin{pmatrix}
6 & 0 & 0 \\
0 & 6 & 0 \\
0 & 0 & 6 \\
\end{pmatrix} \\[1mm]
\Rightarrow \begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix}^{-1} & = & \frac{1}{6}\begin{pmatrix}
2 & 0 & 2 \\
-1 & 3 & -1 \\
1 & -3 & 7 \\
\end{pmatrix}
\end{eqnarray*}

This process for finding the inverse matrix is sometimes called \emph{Cramer's~Rule}~\index{Cramer's rule}.
\end{example}

\section{Application: Volume of a Parallelepiped}

Given three vectors $u,v,w$ in $\Re^3$, the parallelepiped\index{Parallelepiped} determined by the three vectors is the ``squished'' box whose edges are parallel to $u, v$, and $w$ as depicted in Figure~\ref{parallelepiped}.

From calculus, we know that the volume of this object is $|u\dotprod (v\times w)|$.  This is the same as expansion by minors of the matrix whose columns are $u,v,w$.  Then:
\[
\text{Volume}=\big|\det \begin{pmatrix}u & v & w \end{pmatrix} \big|
\] 



\begin{figure}
\begin{center}
\includegraphics[scale=.4]{parallelepiped.jpg}
\caption{A parallelepiped.\label{parallelepiped}}
\end{center}
\end{figure}



%\section*{References}
%Hefferon, Chapter Four, Section I.1 and I.3
%\\
%Beezer, Chapter D, Section DM, Subsection DD
%\\
%Beezer, Chapter D, Section DM, Subsection CD
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Determinant}{Determinant}
%\item \href{http://en.wikipedia.org/wiki/Elementary_matrix}{Elementary Matrix}
%\item \href{http://en.wikipedia.org/wiki/Cramers_rule}{Cramer's Rule}
%\end{itemize}




\section{Review Problems}
\input{\propDetPath/problems}

\newpage




","
\chapter{\propDetTitle}

In Lecture~\ref{elementarydeterminantsII} we \hyperref[detinvertible]{showed} that the determinant of a matrix is non-zero if and only if that matrix is invertible.  We also \hyperref[detmultiplicative]{showed} that the determinant is a \emph{multiplicative} function\index{Multiplicative function}, in the sense that $\det (MN)=\det M \det N$.  Now we will devise some methods for calculating the determinant.

Recall that:
\[
\det M = \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)}.
\]

A \emph{minor}\index{Minor} of an $n\times n$ matrix $M$ is the determinant of any square matrix obtained from $M$ by deleting one row and one column.  In particular, any entry $m^i_j$ of a square matrix~$M$ is associated to a minor obtained by deleting the $i$th row and $j$th column of~$M$.

It is possible to write the determinant of a matrix in terms of  its minors\index{Expansion by minors} as follows:

\begin{eqnarray*}
\det M &=& \sum_{\sigma} \text{sgn}(\sigma)\, m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)} \\
&=& m^1_1\, \sum_{\hat{\sigma}} \text{sgn}(\hat{\sigma})\, m^2_{\hat{\sigma}(2)}\cdots m^n_{\hat{\sigma}(n)} \\
& -&  m^1_2\, \sum_{\hat{\sigma}} \text{sgn}(\hat{\sigma})\, m^2_{\hat{\sigma}(1)}m^3_{\hat{\sigma}(3)}\cdots m^n_{\hat{\sigma}(n)} \\
& +&  m^1_3\,  \sum_{\hat{\sigma}} \text{sgn}(\hat{\sigma})\, m^2_{\hat{\sigma}(1)}m^3_{\hat{\sigma}(2)}m^4_{\hat{\sigma}(4)}\cdots m^n_{\hat{\sigma}(n)} \pm \cdots
\end{eqnarray*}
Here the symbols $\hat{\sigma}$ refer to permutations of $n-1$ objects.  What we're doing here is collecting up all of the terms of the original sum that contain the first row entry $m^1_j$ for each column number $j$.  Each term in that collection is associated to a permutation sending $1\rightarrow j$.  The remainder of any such permutation maps the set $\{2, \ldots, n \}\rightarrow \{1, \ldots, j-1, j+1, \ldots, n \}$.  We call this partial permutation $\hat{\sigma}=\begin{bmatrix} \sigma(2) & \cdots & \sigma(n) \end{bmatrix}$.

The last issue is that the permutation $\hat{\sigma}$ may not have the same sign as $\sigma$.  From previous homework, we know that a permutation has the same parity as its inversion number.  Removing $1\rightarrow j$ from a permutation  reduces the inversion number by the number of elements right of $j$ that are less than~$j$.  Since $j$ comes first in the permutation $\begin{bmatrix}j & \sigma(2) & \cdots & \sigma(n) \end{bmatrix}$, the inversion number of $\hat{\sigma}$ is reduced by $j-1$.  Then the sign of~$\sigma$ differs from the sign of~$\hat{\sigma}$ if $\sigma$ sends $1$ to an even number.

In other words, to expand by minors we pick an entry $m^1_j$ of the first row, then add $(-1)^{j-1}$ times the determinant of the matrix with row $i$ and column~$j$ deleted.

\begin{example}
Let's compute the determinant of 
$M=\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{pmatrix}$ using expansion by minors.

\begin{eqnarray*}
\det M & = & 1\det \begin{pmatrix}
5 & 6 \\
8 & 9 \\
\end{pmatrix}
-2 \det \begin{pmatrix}
4 & 6 \\
7 & 9 \\
\end{pmatrix}
+3 \det \begin{pmatrix}
4 & 5 \\
7 & 8 \\
\end{pmatrix} \\
& = & 1(5\cdot 9- 8\cdot 6) -2 (4\cdot 9- 7\cdot 6) + 3 (4\cdot 8- 7\cdot 5) \\
& = & 0 \\
\end{eqnarray*}
Here, $M^{-1}$ does not exist because\footnote{A fun exercise is to compute the determinant of a $4\times 4$ matrix filled in order, from left to right,  with the numbers $1,2,3,\ldots 16$. What do you observe? Try the same for a $5\times 5$ matrix with $1,2,3\ldots 25$. Is there a pattern? Can you explain it?} $\det M=0.$
\end{example}


\begin{example}
Sometimes the entries of a matrix allow us to simplify the calculation of the determinant.  Take $N= \begin{pmatrix}
1 & 2 & 3 \\
4 & 0 & 0 \\
7 & 8 & 9 \\
\end{pmatrix}$.  Notice that the second row has many zeros; then we can switch the first and second rows of $N$ to get:

\begin{eqnarray*}
\det \begin{pmatrix}
1 & 2 & 3 \\
4 & 0 & 0 \\
7 & 8 & 9 \\
\end{pmatrix} & = & -\det \begin{pmatrix}
4 & 0 & 0 \\
1 & 2 & 3 \\
7 & 8 & 9 \\
\end{pmatrix}\\
&=& -4 \det \begin{pmatrix}
2 & 3 \\
8 & 9 \\
\end{pmatrix} \\
&=& 24
\end{eqnarray*}
\end{example}
 
\videoscriptlink{properties_of_determinant_practice.mp4}{Example}{video_properties_of_determinant_practice}

\begin{theorem}
For any square matrix $M$, we have:
\[
\det M^T = \det M
\]
\end{theorem}
\begin{proof}
  By definition, \[
\det M = \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)}.
\]

For any permutation $\sigma$, there is a unique inverse permutation $\sigma^{-1}$ that undoes $\sigma$.  If $\sigma$ sends $i\rightarrow j$, then $\sigma^{-1}$ sends $j\rightarrow i$.  In the two-line notation for a permutation, this corresponds to just flipping the permutation over.  For example, if $\sigma=\begin{bmatrix} 
1 & 2 & 3 \\
2 & 3 & 1
\end{bmatrix}$, then we can find $\sigma^{-1}$ by flipping the permutation and then putting the columns in order:

\[
\sigma^{-1}=\begin{bmatrix} 
2 & 3 & 1 \\
1 & 2 & 3
\end{bmatrix}=\begin{bmatrix} 
1 & 2 & 3 \\
3 & 1 & 2
\end{bmatrix}
\]
Since any permutation can be built up by transpositions, one can also find the inverse of a permutation $\sigma$ by undoing each of the transpositions used to build up $\sigma$; this shows that one can use the same number of transpositions to build $\sigma$ and $\sigma^{-1}$.  In particular, $\sgn \sigma= \sgn \sigma^{-1}$.

%\begin{center}\href{\webworkurl ReadingHomework14/1/}{Reading homework: problem 14.1}\end{center}
\reading{14}{1}

Then we can write out the above in formulas as follows:
\begin{eqnarray*}
\det M &=& \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}m^2_{\sigma(2)}\cdots m^n_{\sigma(n)} \\
&=& \sum_{\sigma} \text{sgn}(\sigma) m_1^{\sigma^{-1}(1)}m_2^{\sigma^{-1}(2)}\cdots m_n^{\sigma^{-1}(n)} \\
&=& \sum_{\sigma} \text{sgn}(\sigma^{-1}) m_1^{\sigma^{-1}(1)}m_2^{\sigma^{-1}(2)}\cdots m_n^{\sigma^{-1}(n)} \\
&=& \sum_{\sigma} \text{sgn}(\sigma) m_1^{\sigma(1)}m_2^{\sigma(2)}\cdots m_n^{\sigma(n)} \\
&=& \det M^T.
\end{eqnarray*}
The second-to-last equality is due to the existence of a unique inverse permutation: summing over permutations is the same as summing over all inverses of permutations.  The final equality is by the definition of the transpose.
\end{proof}

\begin{center}
\includegraphics[scale=.25]{\propDetPath/detMT.jpg}
\end{center}

\begin{example}
Because of this theorem, we see that expansion by minors also works over columns.  Let $M=\begin{pmatrix}
1 & 2 & 3 \\
0 & 5 & 6 \\
0 & 8 & 9 \\
\end{pmatrix}$.  Then $$\det M = \det M^T = 1\det \begin{pmatrix}
5 & 8 \\
6 & 9 \\
\end{pmatrix}=-3\, .$$
\end{example}

\section{Determinant of the Inverse}

Let $M$ and $N$ be $n\times n$ matrices.
We previously showed that 

\[
\det (MN)=\det M \det N \text{, and } \det I=1.
\]
Then $1 = \det I = \det (MM^{-1}) = \det M \det M^{-1}$.  As such we have:
\begin{theorem}
\[
\det M^{-1} = \frac{1}{\det M}
\]
\end{theorem}

Just so you don't forget this:
\begin{center}
\includegraphics[scale=.3]{\propDetPath/detMm1.jpg}
\end{center}


\section{Adjoint of a Matrix}


Recall that for the $2\times 2$ matrix 
%\begin{figure}
\begin{center}
\includegraphics[scale=.2]{\propDetPath/adj2x2.jpg}
\end{center}
%\end{figure}
Or in a more careful notation: if
$M=\begin{pmatrix}
m^1_1 & m^1_2 \\
m^2_1 & m^2_2 \\
\end{pmatrix}$, then $$M^{-1}=\frac{1}{m^1_1m^2_2-m^1_2m^2_1}\begin{pmatrix}
m^2_2 & -m^1_2 \\
-m^2_1 & m^1_1 \\
\end{pmatrix}\, ,$$
so long as $\det M=m^1_1m^2_2-m^1_2m^2_1\neq 0$.
  The  matrix $\begin{pmatrix}
m^2_2 & -m^1_2 \\
-m^2_1 & m^1_1 \\
\end{pmatrix}$ that appears above is a special matrix, called the \emph{adjoint} of $M$.  Let's define the adjoint for an $n \times n$ matrix.


The \emph{cofactor}\index{Cofactor} of $M$ corresponding to the entry $m^i_j$ of $M$ 
%and then deleting the $i$th row and $j$th column of $M$, taking the determinant of the 
is the prooduct of the minor associated to $m^i_j$
%resulting matrix, and 
%multiplying by
times $(-1)^{i+j}$.  This is written $\cofactor(m^i_j)$.

\begin{definition}
For $M=(m^i_j)$ a square matrix, The \emph{adjoint matrix} $\adj M$ is given by:
\[
\adj M = (\cofactor(m^i_j))^T
\]
\end{definition}

\begin{example}
\[
\adj \begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix}
=
\begin{pmatrix}
\det \begin{pmatrix}
2 & 0 \\
1 & 1 
\end{pmatrix}
& -\det \begin{pmatrix}
1 & 0 \\
0 & 1 
\end{pmatrix}
& \det \begin{pmatrix}
1 & 2 \\
0 & 1 
\end{pmatrix}
\\
-\det \begin{pmatrix}
-1 & -1 \\
1 & 1 
\end{pmatrix}
& \det \begin{pmatrix}
3 & -1 \\
0 & 1 
\end{pmatrix}
& -\det \begin{pmatrix}
3 & -1 \\
0 & 1 
\end{pmatrix}
\\
\det \begin{pmatrix}
-1 & -1 \\
2 & 0 
\end{pmatrix}
& -\det \begin{pmatrix}
3 & -1 \\
1 & 0 
\end{pmatrix}
& \det \begin{pmatrix}
3 & -1 \\
1 & 2 
\end{pmatrix}
\\
\end{pmatrix}^T
\]
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework14/2/}{Reading homework: problem 14.2}\end{center}
\reading{14}{2}

Let's multiply $M\adj M$.  For any matrix $N$, the $i, j$ entry of $MN$ is given by taking the dot product of the $i$th row of $M$ and the $j$th column of $N$.  
Notice that the dot product of the $i$th row of $M$ and the $i$th column of $\adj M$ is just the expansion by minors of $\det M$ in the $i$th row.
Further, notice that the dot product of the $i$th row of $M$ and the $j$th column of $\adj M$ with $j\neq i$ is the same as expanding $M$ by minors, but with the $j$th row replaced by the $i$th row.  Since the determinant of any matrix with a row repeated is zero, then these dot products are zero as well.

We know that the $i,j$ entry of the product of two matrices is the dot product of the $i$th row of the first by the $j$th column of the second.  Then:
\[
M\adj M = (\det M) I
\]

Thus, when $\det M\neq 0$, the adjoint gives an explicit formula for $M^{-1}$.


\begin{theorem}
For $M$ a square matrix with $\det M\neq 0$ (equivalently, if $M$ is invertible), then
\[
M^{-1}=\frac{1}{\det M}\adj M
\]
\end{theorem}

\videoscriptlink{properties_of_determinant_adjoint.mp4}{The Adjoint Matrix}{video_properties_of_determinant_adjoint}


\begin{figure}
\begin{center}
\includegraphics[scale=.3]{\propDetPath/adjM.jpg}
\end{center}
\end{figure}

\begin{example}
Continuing with the previous example,
\[
\adj \begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix} = \begin{pmatrix}
2 & 0 & 2 \\
-1 & 3 & -1 \\
1 & -3 & 7 \\
\end{pmatrix}.
\]

Now, multiply:

\begin{eqnarray*}
\begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix}
\begin{pmatrix}
2 & 0 & 2 \\
-1 & 3 & -1 \\
1 & -3 & 7 \\
\end{pmatrix}
&=&
\begin{pmatrix}
6 & 0 & 0 \\
0 & 6 & 0 \\
0 & 0 & 6 \\
\end{pmatrix} \\[1mm]
\Rightarrow \begin{pmatrix}
3 & -1 & -1 \\
1 & 2 & 0 \\
0 & 1 & 1 \\
\end{pmatrix}^{-1} & = & \frac{1}{6}\begin{pmatrix}
2 & 0 & 2 \\
-1 & 3 & -1 \\
1 & -3 & 7 \\
\end{pmatrix}
\end{eqnarray*}

This process for finding the inverse matrix is sometimes called \emph{Cramer's~Rule}~\index{Cramer's rule}.
\end{example}

\section{Application: Volume of a Parallelepiped}

Given three vectors $u,v,w$ in $\Re^3$, the parallelepiped\index{Parallelepiped} determined by the three vectors is the ``squished'' box whose edges are parallel to $u, v$, and $w$ as depicted in Figure~\ref{parallelepiped}.

From calculus, we know that the volume of this object is $|u\dotprod (v\times w)|$.  This is the same as expansion by minors of the matrix whose columns are $u,v,w$.  Then:
\[
\text{Volume}=\big|\det \begin{pmatrix}u & v & w \end{pmatrix} \big|
\] 



\begin{figure}
\begin{center}
\includegraphics[scale=.4]{parallelepiped.jpg}
\caption{A parallelepiped.\label{parallelepiped}}
\end{center}
\end{figure}



%\section*{References}
%Hefferon, Chapter Four, Section I.1 and I.3
%\\
%Beezer, Chapter D, Section DM, Subsection DD
%\\
%Beezer, Chapter D, Section DM, Subsection CD
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Determinant}{Determinant}
%\item \href{http://en.wikipedia.org/wiki/Elementary_matrix}{Elementary Matrix}
%\item \href{http://en.wikipedia.org/wiki/Cramers_rule}{Cramer's Rule}
%\end{itemize}




\section{Review Problems}
\input{\propDetPath/problems}

\newpage




",lesson
15,Elementary Matrices Determinants Ii,"
\chapter{\elemMatDetIITitle}\label{elementarydeterminantsII}

In section~\ref{elementarydeterminants}, we saw the definition of the determinant and derived an elementary matrix that exchanges two rows of a matrix.  Next, we need to find elementary matrices corresponding to the other two row operations; multiplying a row by a scalar, and adding a multiple of one row to another.  As a consequence, we will derive some important properties of the determinant.

Consider $M=\colvec{R^1 \\ \vdots \\ R^n }$, where $R^i$ are row vectors.  Let $R^i(\lambda)$ be the identity matrix, with the $i$th diagonal entry replaced by $\lambda$, not to be confused with the row vectors. {\it I.e.}
$$
R^i(\lambda)=
\begin{pmatrix}
1 & & & & \\
  & \ddots & & & \\
  & & \lambda & & \\
  & & & \ddots & \\
  & & & & 1 \\
\end{pmatrix}
\, .$$
Then:

\[
M'=R^i(\lambda)M=\colvec{R^1 \\ \vdots \\ \lambda R^i \\ \vdots \\ R^n }
\]
What effect does multiplication by $R^i(\lambda)$ have on the determinant?

\begin{eqnarray*}
\det M' & = & \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots \lambda m^i_{\sigma(i)} \cdots m^n_{\sigma(n)} \\
& = & \lambda \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots m^i_{\sigma(i)} \cdots m^n_{\sigma(n)} \\
& = & \lambda \det M
\end{eqnarray*}
Thus, multiplying a row by $\lambda$ multiplies the determinant by $\lambda$.
{\it I.e.,} $$\det R^i(\lambda) M = \lambda \det M\, .$$


\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/row_mult_thm.jpg}
\end{center}
\end{figure}


Since $R^i(\lambda)$ is just the identity matrix with a single row multiplied by $\lambda$, then by the above rule, the determinant of $R^i(\lambda)$ is $\lambda$.  Thus:

\[
\det R^i(\lambda) = \det \begin{pmatrix}
1 & & & & \\
  & \ddots & & & \\
  & & \lambda & & \\
  & & & \ddots & \\
  & & & & 1 \\
\end{pmatrix} = \lambda
\]

The final row operation is adding $\lambda R^j$ to $R^i$.  This is done with the matrix~$S^i_j(\lambda)$, which is an identity matrix but with a $\lambda$ in the $i,j$ position.

\[
S^i_j(\lambda) = \begin{pmatrix}
1 & 	& 	& 	& & & 	\\
  & \ddots & 	&	& & &	\\
  & 	& 1 	& 	& \lambda & &	\\
  & 	& 	& \ddots & & &	\\
  & 	& 	& 	& 1 & & 	\\
  & 	& 	& 	& 	& \ddots & 	\\
  & 	& 	& 	& 	& 	 & 1	\\
\end{pmatrix}
\]
Then multiplying $S^i_j(\lambda)$ by $M$ gives the following:

\[
\begin{pmatrix}
1 & 	& 	& 	& & & 	\\
  & \ddots & 	&	& & &	\\
  & 	& 1 	& 	& \lambda & &	\\
  & 	& 	& \ddots & & &	\\
  & 	& 	& 	& 1 & & 	\\
  & 	& 	& 	& 	& \ddots & 	\\
  & 	& 	& 	& 	& 	 & 1	\\
\end{pmatrix}\colvec{\\ \vdots \\ R^i \\ \vdots \\ R^j \\ \vdots\\ \\}
=
\colvec{\\ \vdots \\ R^i +\lambda R^j \\ \vdots \\ R^j \\ \vdots\\ \\ }
\]
What is the effect of multiplying by $S^i_j(\lambda)$ on the determinant?  Let $M'=S^i_j(\lambda)M$, and let $M''$ be the matrix $M$ but with $R^i$ replaced by $R^j$.

\begin{eqnarray*}
\det M' & = & \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots (m^i_{\sigma(i)}+ \lambda m^j_{\sigma(j)}) \cdots m^n_{\sigma(n)} \\
& = & \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots m^i_{\sigma(i)} \cdots m^n_{\sigma(n)} \\
&   & \qquad + \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots \lambda m^j_{\sigma(j)} \cdots m^j_{\sigma(j)} \cdots m^n_{\sigma(n)} \\
& = & \det M + \lambda \det M''
\end{eqnarray*}
Since $M''$ has two identical rows, its determinant is $0$.  Then $$\det S^i_j(\lambda)M = \det M\, .$$
Notice that if $M$ is the identity matrix, then we have $$\det S^i_j(\lambda) = \det (S^i_j(\lambda)I) = \det I = 1\, .$$

\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/row_addition_thm.jpg}
\end{center}
\end{figure}

We now have elementary matrices associated to each of the row operations.

\[
\begin{array}{cccc}
E^i_j &=& I \text{ with rows $i,j$ swapped;} &\det E^i_j=-1 \\[3mm]
R^i(\lambda) &=& I \text{ with $\lambda$ in position $i,i$;} 
	&\det R^i(\lambda)=\lambda \\[3mm]
S^i_j(\lambda) &=& I \text{ with $\lambda$ in position $i,j$;} 
	&\det S^i_j(\lambda)=1 \\[3mm]
\end{array}
\]
\videoscriptlink{elementary_matrices_and_determinants_ii_dets.mp4}{Elementary Determinants}{scripts_elementary_matrices_determinants_ii_dets}
We have also proved the following theorem along the way:

\begin{theorem}
If $E$ is \emph{any} of the elementary matrices $E^i_j, R^i(\lambda), S^i_j(\lambda)$, then $\det(EM)=\det E \det M$.
\end{theorem}

%\href{\webworkurl ReadingHomework13/1/}{Reading homework: problem 13.1}
\reading{13}{1}


\begin{center}
\hspace{3mm}\includegraphics[scale=.27]{\elemMatDetIIPath/summary.jpg}
\end{center}


We have seen that any matrix $M$ can be put into reduced row echelon form via a sequence of row operations, and we have seen that any row operation can be emulated with left matrix multiplication by an elementary matrix.  Suppose that $\rref(M)$ is the reduced row echelon form of $M$.  Then $\rref(M)=E_1E_2\cdots E_kM$ where each $E_i$ is an elementary matrix.

What is the determinant of a square matrix in reduced row echelon form?  
\begin{itemize}
\item If $M$ is not invertible, then some row of $\rref(M)$ contains only zeros.  Then we can multiply the zero row by any constant $\lambda$ without changing~$M$; by our previous observation, this scales the determinant of $M$ by $\lambda$.  Thus, if $M$ is not invertible, $\det \rref(M)=\lambda \det \rref(M)$, and so $\det \rref(M)=0$.  

\item Otherwise, every row of $\rref(M)$ has a pivot on the diagonal; since $M$ is square, this means that $\rref(M)$ is the identity matrix.  Then if $M$ is invertible, $\det \rref(M)=1$.

\item Additionally, notice that $\det \rref(M) = \det (E_1E_2\cdots E_kM)$.  Then by the theorem above, $\det \rref(M)=\det (E_1) \cdots \det (E_k) \det M$.  Since each $E_i$ has non-zero determinant, then $\det \rref(M)=0$ if and only if $\det M=0$.
\end{itemize}
Then we have shown:

\begin{theorem}
\label{detinvertible}
For any square matrix $M$, $\det M\neq 0$ if and only if $M$ is invertible.
\end{theorem}
Since we know the determinants of the elementary matrices, we can immediately obtain the following:


\videoscriptlink{elementary_matrices_ii_inverses_determinants.mp4}{Determinants and Inverses}{scripts_elementary_matrices_determinants_ii_inverses}

\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/theorem_invertible.jpg}
\end{center}
\end{figure}

\begin{corollary}
Any elementary matrix $E^i_j, R^i(\lambda), S^i_j(\lambda)$ is invertible, except for $R^i(0)$.  In fact, the inverse of an elementary matrix is another elementary matrix.
\end{corollary}


To obtain one last important result, suppose that $M$ and $N$ are square $n\times n$ matrices, with reduced row echelon forms such that, for elementary matrices  $E_i$ and $F_i$, $$M=E_1E_2\cdots E_k \, \rref(M)\, ,$$ and  $$N=F_1F_2\cdots F_l \, \rref(N)\, .$$  If $\rref(M)$ is the identity matrix ({\it i.e.}, $M$ is invertible), then:

\begin{eqnarray*}
\det (MN) & = & \det (E_1E_2\cdots E_k\,  \rref(M) F_1F_2\cdots F_l \, \rref(N) )\\
& = & \det (E_1E_2\cdots E_k I F_1F_2\cdots F_l\,  \rref(N) )\\
& = & \det (E_1) \cdots \det(E_k)\det(I)\det(F_1)\cdots\det(F_l)\det(\rref(N)\\
& = & \det(M)\det(N)
\end{eqnarray*}
Otherwise, $M$ is not invertible, and $\det M=0=\det 
\rref(M)$.  Then there exists a row of zeros in $
\rref(M)$, so $R^n(\lambda)
\rref(M)=
\rref(M)$.  Then:
\begin{eqnarray*}
\det (MN) & = & \det (E_1E_2\cdots E_k 
\, \rref(M) N )\\
& = & \det (E_1E_2\cdots E_k 
\, \rref(M) N )\\
& = & \det (E_1) \cdots \det(E_k)\det( 
\rref(M)N)\\
& = & \det (E_1) \cdots \det(E_k)\det( R^n(\lambda) 
\, \rref(M)N)\\
& = & \det (E_1) \cdots \det(E_k)\lambda \det( 
\rref(M)N)\\
& = & \lambda \det (MN)
\end{eqnarray*}
Which implies that $\det (MN)=0=\det M \det N$.

Thus we have shown that for {\it any} matrices $M$ and $N$, 
\label{detmultiplicative}
\[
\det (MN) = \det M \det N
\]
This result is {\it extremely important}; do not forget it!

\videoscriptlink{elementary_matrices_determinant_ii_product.mp4}{Alternative proof}{scripts_elementary_matrices_determinants_ii_product}

\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/detMN.jpg}
\end{center}
\end{figure}

%\href{\webworkurl ReadingHomework13/2/}{Reading homework: problem 13.2}
\reading{13}{2}

%\section*{References}
%Hefferon, Chapter Four, Section I.1 and I.3
%\\
%Beezer, Chapter D, Section DM, Subsection EM
%\\
%Beezer, Chapter D, Section PDM
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Determinant}{Determinant}
%\item \href{http://en.wikipedia.org/wiki/Elementary_matrix}{Elementary Matrix}
%\end{itemize}

\section{Review Problems}

\input{\elemMatDetIIPath/problems}

\newpage



","
\chapter{\elemMatDetIITitle}\label{elementarydeterminantsII}

In section~\ref{elementarydeterminants}, we saw the definition of the determinant and derived an elementary matrix that exchanges two rows of a matrix.  Next, we need to find elementary matrices corresponding to the other two row operations; multiplying a row by a scalar, and adding a multiple of one row to another.  As a consequence, we will derive some important properties of the determinant.

Consider $M=\colvec{R^1 \\ \vdots \\ R^n }$, where $R^i$ are row vectors.  Let $R^i(\lambda)$ be the identity matrix, with the $i$th diagonal entry replaced by $\lambda$, not to be confused with the row vectors. {\it I.e.}
$$
R^i(\lambda)=
\begin{pmatrix}
1 & & & & \\
  & \ddots & & & \\
  & & \lambda & & \\
  & & & \ddots & \\
  & & & & 1 \\
\end{pmatrix}
\, .$$
Then:

\[
M'=R^i(\lambda)M=\colvec{R^1 \\ \vdots \\ \lambda R^i \\ \vdots \\ R^n }
\]
What effect does multiplication by $R^i(\lambda)$ have on the determinant?

\begin{eqnarray*}
\det M' & = & \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots \lambda m^i_{\sigma(i)} \cdots m^n_{\sigma(n)} \\
& = & \lambda \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots m^i_{\sigma(i)} \cdots m^n_{\sigma(n)} \\
& = & \lambda \det M
\end{eqnarray*}
Thus, multiplying a row by $\lambda$ multiplies the determinant by $\lambda$.
{\it I.e.,} $$\det R^i(\lambda) M = \lambda \det M\, .$$


\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/row_mult_thm.jpg}
\end{center}
\end{figure}


Since $R^i(\lambda)$ is just the identity matrix with a single row multiplied by $\lambda$, then by the above rule, the determinant of $R^i(\lambda)$ is $\lambda$.  Thus:

\[
\det R^i(\lambda) = \det \begin{pmatrix}
1 & & & & \\
  & \ddots & & & \\
  & & \lambda & & \\
  & & & \ddots & \\
  & & & & 1 \\
\end{pmatrix} = \lambda
\]

The final row operation is adding $\lambda R^j$ to $R^i$.  This is done with the matrix~$S^i_j(\lambda)$, which is an identity matrix but with a $\lambda$ in the $i,j$ position.

\[
S^i_j(\lambda) = \begin{pmatrix}
1 & 	& 	& 	& & & 	\\
  & \ddots & 	&	& & &	\\
  & 	& 1 	& 	& \lambda & &	\\
  & 	& 	& \ddots & & &	\\
  & 	& 	& 	& 1 & & 	\\
  & 	& 	& 	& 	& \ddots & 	\\
  & 	& 	& 	& 	& 	 & 1	\\
\end{pmatrix}
\]
Then multiplying $S^i_j(\lambda)$ by $M$ gives the following:

\[
\begin{pmatrix}
1 & 	& 	& 	& & & 	\\
  & \ddots & 	&	& & &	\\
  & 	& 1 	& 	& \lambda & &	\\
  & 	& 	& \ddots & & &	\\
  & 	& 	& 	& 1 & & 	\\
  & 	& 	& 	& 	& \ddots & 	\\
  & 	& 	& 	& 	& 	 & 1	\\
\end{pmatrix}\colvec{\\ \vdots \\ R^i \\ \vdots \\ R^j \\ \vdots\\ \\}
=
\colvec{\\ \vdots \\ R^i +\lambda R^j \\ \vdots \\ R^j \\ \vdots\\ \\ }
\]
What is the effect of multiplying by $S^i_j(\lambda)$ on the determinant?  Let $M'=S^i_j(\lambda)M$, and let $M''$ be the matrix $M$ but with $R^i$ replaced by $R^j$.

\begin{eqnarray*}
\det M' & = & \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots (m^i_{\sigma(i)}+ \lambda m^j_{\sigma(j)}) \cdots m^n_{\sigma(n)} \\
& = & \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots m^i_{\sigma(i)} \cdots m^n_{\sigma(n)} \\
&   & \qquad + \sum_{\sigma} \text{sgn}(\sigma) m^1_{\sigma(1)}\cdots \lambda m^j_{\sigma(j)} \cdots m^j_{\sigma(j)} \cdots m^n_{\sigma(n)} \\
& = & \det M + \lambda \det M''
\end{eqnarray*}
Since $M''$ has two identical rows, its determinant is $0$.  Then $$\det S^i_j(\lambda)M = \det M\, .$$
Notice that if $M$ is the identity matrix, then we have $$\det S^i_j(\lambda) = \det (S^i_j(\lambda)I) = \det I = 1\, .$$

\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/row_addition_thm.jpg}
\end{center}
\end{figure}

We now have elementary matrices associated to each of the row operations.

\[
\begin{array}{cccc}
E^i_j &=& I \text{ with rows $i,j$ swapped;} &\det E^i_j=-1 \\[3mm]
R^i(\lambda) &=& I \text{ with $\lambda$ in position $i,i$;} 
	&\det R^i(\lambda)=\lambda \\[3mm]
S^i_j(\lambda) &=& I \text{ with $\lambda$ in position $i,j$;} 
	&\det S^i_j(\lambda)=1 \\[3mm]
\end{array}
\]
\videoscriptlink{elementary_matrices_and_determinants_ii_dets.mp4}{Elementary Determinants}{scripts_elementary_matrices_determinants_ii_dets}
We have also proved the following theorem along the way:

\begin{theorem}
If $E$ is \emph{any} of the elementary matrices $E^i_j, R^i(\lambda), S^i_j(\lambda)$, then $\det(EM)=\det E \det M$.
\end{theorem}

%\href{\webworkurl ReadingHomework13/1/}{Reading homework: problem 13.1}
\reading{13}{1}


\begin{center}
\hspace{3mm}\includegraphics[scale=.27]{\elemMatDetIIPath/summary.jpg}
\end{center}


We have seen that any matrix $M$ can be put into reduced row echelon form via a sequence of row operations, and we have seen that any row operation can be emulated with left matrix multiplication by an elementary matrix.  Suppose that $\rref(M)$ is the reduced row echelon form of $M$.  Then $\rref(M)=E_1E_2\cdots E_kM$ where each $E_i$ is an elementary matrix.

What is the determinant of a square matrix in reduced row echelon form?  
\begin{itemize}
\item If $M$ is not invertible, then some row of $\rref(M)$ contains only zeros.  Then we can multiply the zero row by any constant $\lambda$ without changing~$M$; by our previous observation, this scales the determinant of $M$ by $\lambda$.  Thus, if $M$ is not invertible, $\det \rref(M)=\lambda \det \rref(M)$, and so $\det \rref(M)=0$.  

\item Otherwise, every row of $\rref(M)$ has a pivot on the diagonal; since $M$ is square, this means that $\rref(M)$ is the identity matrix.  Then if $M$ is invertible, $\det \rref(M)=1$.

\item Additionally, notice that $\det \rref(M) = \det (E_1E_2\cdots E_kM)$.  Then by the theorem above, $\det \rref(M)=\det (E_1) \cdots \det (E_k) \det M$.  Since each $E_i$ has non-zero determinant, then $\det \rref(M)=0$ if and only if $\det M=0$.
\end{itemize}
Then we have shown:

\begin{theorem}
\label{detinvertible}
For any square matrix $M$, $\det M\neq 0$ if and only if $M$ is invertible.
\end{theorem}
Since we know the determinants of the elementary matrices, we can immediately obtain the following:


\videoscriptlink{elementary_matrices_ii_inverses_determinants.mp4}{Determinants and Inverses}{scripts_elementary_matrices_determinants_ii_inverses}

\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/theorem_invertible.jpg}
\end{center}
\end{figure}

\begin{corollary}
Any elementary matrix $E^i_j, R^i(\lambda), S^i_j(\lambda)$ is invertible, except for $R^i(0)$.  In fact, the inverse of an elementary matrix is another elementary matrix.
\end{corollary}


To obtain one last important result, suppose that $M$ and $N$ are square $n\times n$ matrices, with reduced row echelon forms such that, for elementary matrices  $E_i$ and $F_i$, $$M=E_1E_2\cdots E_k \, \rref(M)\, ,$$ and  $$N=F_1F_2\cdots F_l \, \rref(N)\, .$$  If $\rref(M)$ is the identity matrix ({\it i.e.}, $M$ is invertible), then:

\begin{eqnarray*}
\det (MN) & = & \det (E_1E_2\cdots E_k\,  \rref(M) F_1F_2\cdots F_l \, \rref(N) )\\
& = & \det (E_1E_2\cdots E_k I F_1F_2\cdots F_l\,  \rref(N) )\\
& = & \det (E_1) \cdots \det(E_k)\det(I)\det(F_1)\cdots\det(F_l)\det(\rref(N)\\
& = & \det(M)\det(N)
\end{eqnarray*}
Otherwise, $M$ is not invertible, and $\det M=0=\det 
\rref(M)$.  Then there exists a row of zeros in $
\rref(M)$, so $R^n(\lambda)
\rref(M)=
\rref(M)$.  Then:
\begin{eqnarray*}
\det (MN) & = & \det (E_1E_2\cdots E_k 
\, \rref(M) N )\\
& = & \det (E_1E_2\cdots E_k 
\, \rref(M) N )\\
& = & \det (E_1) \cdots \det(E_k)\det( 
\rref(M)N)\\
& = & \det (E_1) \cdots \det(E_k)\det( R^n(\lambda) 
\, \rref(M)N)\\
& = & \det (E_1) \cdots \det(E_k)\lambda \det( 
\rref(M)N)\\
& = & \lambda \det (MN)
\end{eqnarray*}
Which implies that $\det (MN)=0=\det M \det N$.

Thus we have shown that for {\it any} matrices $M$ and $N$, 
\label{detmultiplicative}
\[
\det (MN) = \det M \det N
\]
This result is {\it extremely important}; do not forget it!

\videoscriptlink{elementary_matrices_determinant_ii_product.mp4}{Alternative proof}{scripts_elementary_matrices_determinants_ii_product}

\begin{figure}
\begin{center}
\includegraphics[scale=.27]{\elemMatDetIIPath/detMN.jpg}
\end{center}
\end{figure}

%\href{\webworkurl ReadingHomework13/2/}{Reading homework: problem 13.2}
\reading{13}{2}

%\section*{References}
%Hefferon, Chapter Four, Section I.1 and I.3
%\\
%Beezer, Chapter D, Section DM, Subsection EM
%\\
%Beezer, Chapter D, Section PDM
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Determinant}{Determinant}
%\item \href{http://en.wikipedia.org/wiki/Elementary_matrix}{Elementary Matrix}
%\end{itemize}

\section{Review Problems}

\input{\elemMatDetIIPath/problems}

\newpage



",lesson
16,Subspaces And Spanning Sets,"\chapter{\subspacesTitle}\label{subspacesspanning}

It is time to study vector spaces more carefully and return to  some fundamental questions:

\begin{enumerate}
\item \emph{Subspaces}: When is a subset of a vector space itself a vector space?  (This is the notion of a \emph{subspace}.)

\item \emph{Linear Independence}: Given a collection of vectors, is there a way to tell whether they are independent, or if one is a ``linear combination'' of the others? 

\item \emph{Dimension}: Is there a consistent definition of how ``big'' a vector space is?

\item \emph{Basis}:  How do we label vectors?  Can we write any vector as a sum of some basic set of vectors?  How do we change our point of view from vectors labeled one way to vectors labeled in another way?
\end{enumerate}
\index{Subspace!notion of}\index{Linear independence!concept of}\index{Dimension!notion of}\index{Basis!concept of}
Let's start at the top!

\section{Subspaces}

\begin{definition}
We say that a subset $U$ of a vector space $V$ is a {\bf subspace}\index{Subspace} of $V$ if $U$ is a vector space under the inherited addition and scalar multiplication operations of $V$. 
\end{definition}

\begin{example}
Consider a plane $P$ in $\Re^3$ through the origin:
\[
ax+by+cz=0.
\]

\begin{center}
\includegraphics[scale=.3]{\subspacesPath/subspace_plane.jpg}
\end{center}
This equation can be expressed as the homogeneous system $\rowvec{a & b & c}\colvec{x\\y\\z}=0$, or $MX=0$ with $M$ the matrix $\rowvec{a & b & c}$.  If $X_1$ and $X_2$ are both solutions to $MX=0$, then, by linearity of matrix multiplication, so is $\mu X_1 + \nu X_2$:
\[
M(\mu X_1 + \nu X_2) = \mu MX_1 + \nu MX_2 = 0.
\]
So $P$ is closed under addition and scalar multiplication.  Additionally, $P$ contains the origin (which can be derived from the above by setting $\mu=\nu=0$).  All other vector space requirements hold for $P$ because they hold for all vectors in $\Re^3$.
\end{example}


%\begin{figure}
%\begin{center}
%\includegraphics[scale=.33]{\subspacesPath/subspace_thm.jpg}
%\end{center}
%\end{figure}

\begin{theorem}[\hypertarget{sst}{Subspace Theorem}]\index{Subspace theorem}\label{subspacetheorem}
Let $U$ be a non-empty subset of a vector space $V$.  Then $U$ is a subspace if and only if $\mu u_1 + \nu u_2 \in U$ for arbitrary $u_1, u_2$ in $U$, and arbitrary constants $\mu, \nu$.  
\end{theorem}

\begin{proof}
One direction of this proof is easy: if \(U\) is a subspace, then it is a vector space, and so by the additive closure and multiplicative closure properties of vector spaces, it has to be true that \(\mu u_1 + \nu u_2 \in U\) for all \(u_1,u_2\) in \(U\) and all constants constants \(\mu, \nu\).

The other direction is almost as easy: we need to show that if \(\mu u_1 + \nu u_2 \in U\) for all \(u_1, u_2\) in \(U\) and all constants \(\mu, \nu\), then \(U\) is a vector space. That is, we need to show that the \hyperref[vectorspace]{ten properties of vector spaces} are satisfied. We already know that the additive closure and multiplicative closure properties are satisfied. Further, \(U\) has all of the other eight properties  because \(V\) has them. \end{proof}

\noindent
Note that the requirements of the subspace theorem are often referred to as ``closure''\index{Closure}.

%From now on, w
We can use this theorem to check if a set is a vector space. That is, if we have some set \(U\) of vectors that come from some bigger vector space \(V\), to check if \(U\) itself forms a smaller vector space we need check only two things: 
\begin{enumerate}
\item If we add any two vectors in \(U\), do we end up with a vector in \(U\)?
\item If we multiply any vector in \(U\) by any constant, do we end up with a vector in \(U\)? 
\end{enumerate}
If the answer to both of these questions is yes, then \(U\) is a vector space. If not, \(U\) is not a vector space.

%\begin{center}\href{\webworkurl ReadingHomework15/1/}{Reading homework: problem \ref{subspacesspanning}.1}\end{center}
\Reading{SubspacesAndSpans}{1}


\section{Building Subspaces}

Consider the set 
\[
U= \left\{ \colvec{1\\0\\0}, \colvec{0\\1\\0} \right\} \subset \Re^3.
\]
Because $U$ consists of only two vectors, it clear that $U$ is \emph{not} a vector space, since any constant multiple of these vectors should also be in $U$.  For example, the $0$-vector is not in $U$, nor is $U$ closed under vector addition.

But we know that any two vectors define a plane:
\begin{center}
\includegraphics[scale=.3]{\subspacesPath/span_plane.jpg}
\end{center}
 In this case, the vectors in $U$ define the $xy$-plane in $\Re^3$.  We can view the $xy$-plane as the set of all vectors that arise as a linear combination of the two vectors in $U$.  We call this set of all linear combinations the \emph{span}\index{Span} of $U$:
\[
\spa(U)=\left\{ x \colvec{1\\0\\0}+y \colvec{0\\1\\0} \middle| x,y\in \Re \right\}.
\]
Notice that any vector in the $xy$-plane is of the form
\[
\colvec{x\\y\\0} = x \colvec{1\\0\\0}+y \colvec{0\\1\\0} \in \spa(U).
\]

\begin{definition}
Let $V$ be a vector space and $S=\{ s_1, s_2, \ldots \} \subset V$ a subset of~$V$.  Then the {\bf span of $S$}, denoted $\spa(S)$, is the set
\[
\spa(S):=\{ r^1s_1+r^2s_2+\cdots + r^Ns_N ~|~ r^i\in \Re, N\in \N \}.
\]
\end{definition}

That is, the span of \(S\) is the set of all finite linear combinations\footnote{Usually our vector spaces are defined over \(\mathbb{R}\), but in general we can have vector spaces defined over different base fields such as \(\mathbb{C}\) or \(\mathbb{Z}_2\). The coefficients \(r^i\) should come from whatever our base field is (usually \(\mathbb{R}\)).} of elements of \(S\). Any {\it finite} sum of the form ``a constant times \(s_1\) plus a constant times \(s_2\) plus a constant times \(s_3\) and so on'' is in the span of \(S\).\footnote{It is important that we only allow finitely many terms in our linear combinations; in the definition above, \(N\) must be a finite number. It can be any finite number, but it must be finite. We can relax the requirement that $S=\{s_1,s_2,\ldots\}$ and just let $S$ be any set of vectors. Then we shall write $\spa(S):=\{ r^1s_1+r^2s_2+\cdots + r^Ns_N ~|~ r^i\in \Re, s_i\in S, N\in \N, \}$
 }.

\begin{example}
Let $V=\Re^3$ and $X\subset V$ be the $x$-axis.  Let $P=\colvec{0\\1\\0}$, and set $$S=X \cup \{P\}\, .$$
The vector \(\colvec{2 \\ 3 \\ 0}\) is in \(\spa(S),\) because \(\colvec{2\\3\\0}=\colvec{2\\0\\0}+3\colvec{0\\1\\0}.\) Similarly, the vector \(\colvec{-12 \\ 17.5 \\ 0}\) is in \(\spa(S),\) because \(\colvec{-12\\17.5\\0}=\colvec{-12\\0\\0}+17.5\colvec{0\\1\\0}.\)
Similarly, any vector of the form
\[
\colvec{x\\0\\0}+y \colvec{0\\1\\0} = \colvec{x\\y\\0}
\]
is in \(\spa(S)\). On the other hand, any vector in \(\spa(S)\) must have a zero in the \(z\)-coordinate. (Why?) 
So $\spa(S)$ is the $xy$-plane, which is a vector space.  (Try drawing a picture to verify this!)
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework15/2/}{Reading homework: problem \ref{subspacesspanning}.2}\end{center}
\Reading{SubspacesAndSpans}{2}

\begin{lemma}
For any subset $S\subset V$, $\spa(S)$ is a subspace of $V$.
\end{lemma}

\begin{proof}
We need to show that $\spa(S)$ is a vector space.

It suffices to show that $\spa(S)$ is closed under linear combinations.  Let $u,v\in \spa(S)$ and $\lambda, \mu$ be constants.  By the definition of $\spa(S)$, there are constants $c^i$ and $d^i$ (some of which could be zero) such that:
\begin{eqnarray*}
u & = & c^1s_1+c^2s_2+\cdots \\
v & = & d^1s_1+d^2s_2+\cdots \\
\Rightarrow \lambda u + \mu v & = & \lambda (c^1s_1+c^2s_2+\cdots ) + \mu (d^1s_1+d^2s_2+\cdots ) \\
& = & (\lambda c^1+\mu d^1)s_1 + (\lambda c^2+\mu d^2)s_2 + \cdots
\end{eqnarray*}
This last sum is a linear combination of elements of $S$, and is thus in $\spa(S)$.  Then $\spa(S)$ is closed under linear combinations, and is thus a subspace of~$V$.
\end{proof}

Note that this proof, like many proofs, consisted of little more than just writing out the definitions.



\begin{example}
For which values of $a$ does

\[
\spa \left\{ \colvec{1\\0\\a} , \colvec{1\\2\\-3} , \colvec{a\\1\\0}   \right\} = \Re^3?
\]
Given an arbitrary vector $\colvec{x\\y\\z}$ in $\Re^3$, we need to find constants $r^1, r^2, r^3$ such that 

\[
r^1 \colvec{1\\0\\a} + r^2\colvec{1\\2\\-3} +r^3 \colvec{a\\1\\0} = \colvec{x\\y\\z}.
\]
We can write this as a linear system in the unknowns $r^1, r^2, r^3$ as follows:

\[
\begin{pmatrix}
1 & 1 & a \\ 
0 & 2 & 1 \\
a & -3 & 0
\end{pmatrix}
\colvec{r^1\\r^2\\r^3}
= \colvec{x\\y\\z}.
\]
If the matrix $M=\begin{pmatrix}
1 & 1 & a \\ 
0 & 2 & 1 \\
a & -3 & 0
\end{pmatrix}$ is invertible, then we can find a solution 
\[
M^{-1}\colvec{x\\y\\z}=\colvec{r^1\\r^2\\r^3}
\]
for \emph{any} vector $\colvec{x\\y\\z} \in \Re^3$.

Therefore we should choose $a$ so that $M$ is invertible:  

\[
i.e.,\;  0 \neq \det M = -2a^2 + 3 + a = -(2a-3)(a+1). 
\]
Then the span is $\Re^3$ if and only if $a \neq -1, \frac{3}{2}$.
\end{example}

\Videoscriptlink{subspaces_and_spanning_sets_example.mp4}{Linear systems as spanning sets}{scripts_subspaces_and_spanning_sets_example}

Some other very important ways of building subspaces are given in the following examples.

\begin{example}
(The kernel of a linear map).\\[-2mm]

\noindent
Suppose $L:U\to V$ is a linear map between vector spaces. Then if
$$
L(u)=0=L(u')\, ,
$$
linearity tells us that
$$
L(\alpha u + \beta u') = \alpha L(u) + \beta L(u') =\alpha 0 + \beta 0 = 0\, .
$$
Hence, thanks to the subspace theorem,  the set of all vectors in $U$ that are mapped to the zero vector is a subspace of $V$.
It is called the kernel of $L$:
$$
{\rm ker} L:=\{u\in U| L(u) = 0\}\subset U.
$$
Note that finding a kernel means finding a solution to a homogeneous linear equation. 
\end{example}

\begin{example}
(The image of a linear map).\\[-2mm]

\noindent
Suppose $L:U\to V$ is a linear map between vector spaces. Then if
$$
v=L(u) \mbox{ and } v'=L(u')\, ,
$$
linearity tells us that
$$
\alpha v + \beta v' = \alpha L(u) + \beta L(u') =L(\alpha u +\beta u')\, .
$$
Hence, calling once again on the subspace theorem,  the set of all vectors in $V$ that are obtained as outputs of the
map $L$ is a subspace.
It is called the image of $L$:
$$
{\rm im} L:=\{L(u) \ |\  u\in U \}\subset V.
$$
\end{example}

\begin{example}
(An eigenspace of a linear map).\\[-2mm]

\noindent
Suppose $L:V\to V$ is a linear map and $V$ is a vector space. Then if
$$
L(u)=\lambda u \mbox{ and } L(v)=\lambda v\, ,
$$
linearity tells us that
$$
L(\alpha u + \beta v) = \alpha L(u) + \beta L(v) =\alpha L(u) + \beta L(v) =\alpha \lambda u  + \beta \lambda v = \lambda (\alpha u + \beta v)\, .
$$
Hence, again by subspace theorem, the set of all vectors in $V$ that
obey the {\it eigenvector equation} $L(v)=\lambda v$ is a subspace of $V$. 
It is called an eigenspace
$$
V_\lambda:=\{v\in V| L(v) = \lambda v\}.
$$
For most scalars $\lambda$, the only solution to $L(v) = \lambda v$ will be $v=0$, which yields the trivial subspace $\{0\}$.
When there are nontrivial solutions to $L(v)=\lambda v$, the number~$\lambda$ is called an eigenvalue, and carries
essential information about the map~$L$. 
\end{example}

Kernels, images and eigenspaces are discussed in great depth in chapters~\ref{kernelrank} and~\ref{eigenvalseigenvects}.


%\section*{References}
%Hefferon, Chapter Two, Section I.2: Subspaces and Spanning Sets
%\\
%Beezer, Chapter VS, Section S
%\\
%Beezer, Chapter V, Section LC
%\\
%Beezer, Chapter V, Section SS
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Linear_subspace}{Linear Subspace}
%\item \href{http://en.wikipedia.org/wiki/Linear_span}{Linear Span}
%\end{itemize}
%

\section{Review Problems}
{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{SubspacesAndSpans}{1}, \hwrref{SubspacesAndSpans}{2}\\
 Subspaces &\hwref{SubspacesAndSpans}{3},
 \hwref{SubspacesAndSpans}{4},
 \hwref{SubspacesAndSpans}{5},
 \hwref{SubspacesAndSpans}{6}\\
 Spans &\hwref{SubspacesAndSpans}{7},
 \hwref{SubspacesAndSpans}{8}\\
  \hline
\end{tabular}



\input{\subspacesPath/problems}



\newpage
","\chapter{\subspacesTitle}\label{subspacesspanning}

It is time to study vector spaces more carefully and return to  some fundamental questions:

\begin{enumerate}
\item \emph{Subspaces}: When is a subset of a vector space itself a vector space?  (This is the notion of a \emph{subspace}.)

\item \emph{Linear Independence}: Given a collection of vectors, is there a way to tell whether they are independent, or if one is a ``linear combination'' of the others? 

\item \emph{Dimension}: Is there a consistent definition of how ``big'' a vector space is?

\item \emph{Basis}:  How do we label vectors?  Can we write any vector as a sum of some basic set of vectors?  How do we change our point of view from vectors labeled one way to vectors labeled in another way?
\end{enumerate}
\index{Subspace!notion of}\index{Linear independence!concept of}\index{Dimension!notion of}\index{Basis!concept of}
Let's start at the top!

\section{Subspaces}

\begin{definition}
We say that a subset $U$ of a vector space $V$ is a {\bf subspace}\index{Subspace} of $V$ if $U$ is a vector space under the inherited addition and scalar multiplication operations of $V$. 
\end{definition}

\begin{example}
Consider a plane $P$ in $\Re^3$ through the origin:
\[
ax+by+cz=0.
\]

\begin{center}
\includegraphics[scale=.3]{\subspacesPath/subspace_plane.jpg}
\end{center}
This equation can be expressed as the homogeneous system $\rowvec{a & b & c}\colvec{x\\y\\z}=0$, or $MX=0$ with $M$ the matrix $\rowvec{a & b & c}$.  If $X_1$ and $X_2$ are both solutions to $MX=0$, then, by linearity of matrix multiplication, so is $\mu X_1 + \nu X_2$:
\[
M(\mu X_1 + \nu X_2) = \mu MX_1 + \nu MX_2 = 0.
\]
So $P$ is closed under addition and scalar multiplication.  Additionally, $P$ contains the origin (which can be derived from the above by setting $\mu=\nu=0$).  All other vector space requirements hold for $P$ because they hold for all vectors in $\Re^3$.
\end{example}


%\begin{figure}
%\begin{center}
%\includegraphics[scale=.33]{\subspacesPath/subspace_thm.jpg}
%\end{center}
%\end{figure}

\begin{theorem}[\hypertarget{sst}{Subspace Theorem}]\index{Subspace theorem}\label{subspacetheorem}
Let $U$ be a non-empty subset of a vector space $V$.  Then $U$ is a subspace if and only if $\mu u_1 + \nu u_2 \in U$ for arbitrary $u_1, u_2$ in $U$, and arbitrary constants $\mu, \nu$.  
\end{theorem}

\begin{proof}
One direction of this proof is easy: if \(U\) is a subspace, then it is a vector space, and so by the additive closure and multiplicative closure properties of vector spaces, it has to be true that \(\mu u_1 + \nu u_2 \in U\) for all \(u_1,u_2\) in \(U\) and all constants constants \(\mu, \nu\).

The other direction is almost as easy: we need to show that if \(\mu u_1 + \nu u_2 \in U\) for all \(u_1, u_2\) in \(U\) and all constants \(\mu, \nu\), then \(U\) is a vector space. That is, we need to show that the \hyperref[vectorspace]{ten properties of vector spaces} are satisfied. We already know that the additive closure and multiplicative closure properties are satisfied. Further, \(U\) has all of the other eight properties  because \(V\) has them. \end{proof}

\noindent
Note that the requirements of the subspace theorem are often referred to as ``closure''\index{Closure}.

%From now on, w
We can use this theorem to check if a set is a vector space. That is, if we have some set \(U\) of vectors that come from some bigger vector space \(V\), to check if \(U\) itself forms a smaller vector space we need check only two things: 
\begin{enumerate}
\item If we add any two vectors in \(U\), do we end up with a vector in \(U\)?
\item If we multiply any vector in \(U\) by any constant, do we end up with a vector in \(U\)? 
\end{enumerate}
If the answer to both of these questions is yes, then \(U\) is a vector space. If not, \(U\) is not a vector space.

%\begin{center}\href{\webworkurl ReadingHomework15/1/}{Reading homework: problem \ref{subspacesspanning}.1}\end{center}
\Reading{SubspacesAndSpans}{1}


\section{Building Subspaces}

Consider the set 
\[
U= \left\{ \colvec{1\\0\\0}, \colvec{0\\1\\0} \right\} \subset \Re^3.
\]
Because $U$ consists of only two vectors, it clear that $U$ is \emph{not} a vector space, since any constant multiple of these vectors should also be in $U$.  For example, the $0$-vector is not in $U$, nor is $U$ closed under vector addition.

But we know that any two vectors define a plane:
\begin{center}
\includegraphics[scale=.3]{\subspacesPath/span_plane.jpg}
\end{center}
 In this case, the vectors in $U$ define the $xy$-plane in $\Re^3$.  We can view the $xy$-plane as the set of all vectors that arise as a linear combination of the two vectors in $U$.  We call this set of all linear combinations the \emph{span}\index{Span} of $U$:
\[
\spa(U)=\left\{ x \colvec{1\\0\\0}+y \colvec{0\\1\\0} \middle| x,y\in \Re \right\}.
\]
Notice that any vector in the $xy$-plane is of the form
\[
\colvec{x\\y\\0} = x \colvec{1\\0\\0}+y \colvec{0\\1\\0} \in \spa(U).
\]

\begin{definition}
Let $V$ be a vector space and $S=\{ s_1, s_2, \ldots \} \subset V$ a subset of~$V$.  Then the {\bf span of $S$}, denoted $\spa(S)$, is the set
\[
\spa(S):=\{ r^1s_1+r^2s_2+\cdots + r^Ns_N ~|~ r^i\in \Re, N\in \N \}.
\]
\end{definition}

That is, the span of \(S\) is the set of all finite linear combinations\footnote{Usually our vector spaces are defined over \(\mathbb{R}\), but in general we can have vector spaces defined over different base fields such as \(\mathbb{C}\) or \(\mathbb{Z}_2\). The coefficients \(r^i\) should come from whatever our base field is (usually \(\mathbb{R}\)).} of elements of \(S\). Any {\it finite} sum of the form ``a constant times \(s_1\) plus a constant times \(s_2\) plus a constant times \(s_3\) and so on'' is in the span of \(S\).\footnote{It is important that we only allow finitely many terms in our linear combinations; in the definition above, \(N\) must be a finite number. It can be any finite number, but it must be finite. We can relax the requirement that $S=\{s_1,s_2,\ldots\}$ and just let $S$ be any set of vectors. Then we shall write $\spa(S):=\{ r^1s_1+r^2s_2+\cdots + r^Ns_N ~|~ r^i\in \Re, s_i\in S, N\in \N, \}$
 }.

\begin{example}
Let $V=\Re^3$ and $X\subset V$ be the $x$-axis.  Let $P=\colvec{0\\1\\0}$, and set $$S=X \cup \{P\}\, .$$
The vector \(\colvec{2 \\ 3 \\ 0}\) is in \(\spa(S),\) because \(\colvec{2\\3\\0}=\colvec{2\\0\\0}+3\colvec{0\\1\\0}.\) Similarly, the vector \(\colvec{-12 \\ 17.5 \\ 0}\) is in \(\spa(S),\) because \(\colvec{-12\\17.5\\0}=\colvec{-12\\0\\0}+17.5\colvec{0\\1\\0}.\)
Similarly, any vector of the form
\[
\colvec{x\\0\\0}+y \colvec{0\\1\\0} = \colvec{x\\y\\0}
\]
is in \(\spa(S)\). On the other hand, any vector in \(\spa(S)\) must have a zero in the \(z\)-coordinate. (Why?) 
So $\spa(S)$ is the $xy$-plane, which is a vector space.  (Try drawing a picture to verify this!)
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework15/2/}{Reading homework: problem \ref{subspacesspanning}.2}\end{center}
\Reading{SubspacesAndSpans}{2}

\begin{lemma}
For any subset $S\subset V$, $\spa(S)$ is a subspace of $V$.
\end{lemma}

\begin{proof}
We need to show that $\spa(S)$ is a vector space.

It suffices to show that $\spa(S)$ is closed under linear combinations.  Let $u,v\in \spa(S)$ and $\lambda, \mu$ be constants.  By the definition of $\spa(S)$, there are constants $c^i$ and $d^i$ (some of which could be zero) such that:
\begin{eqnarray*}
u & = & c^1s_1+c^2s_2+\cdots \\
v & = & d^1s_1+d^2s_2+\cdots \\
\Rightarrow \lambda u + \mu v & = & \lambda (c^1s_1+c^2s_2+\cdots ) + \mu (d^1s_1+d^2s_2+\cdots ) \\
& = & (\lambda c^1+\mu d^1)s_1 + (\lambda c^2+\mu d^2)s_2 + \cdots
\end{eqnarray*}
This last sum is a linear combination of elements of $S$, and is thus in $\spa(S)$.  Then $\spa(S)$ is closed under linear combinations, and is thus a subspace of~$V$.
\end{proof}

Note that this proof, like many proofs, consisted of little more than just writing out the definitions.



\begin{example}
For which values of $a$ does

\[
\spa \left\{ \colvec{1\\0\\a} , \colvec{1\\2\\-3} , \colvec{a\\1\\0}   \right\} = \Re^3?
\]
Given an arbitrary vector $\colvec{x\\y\\z}$ in $\Re^3$, we need to find constants $r^1, r^2, r^3$ such that 

\[
r^1 \colvec{1\\0\\a} + r^2\colvec{1\\2\\-3} +r^3 \colvec{a\\1\\0} = \colvec{x\\y\\z}.
\]
We can write this as a linear system in the unknowns $r^1, r^2, r^3$ as follows:

\[
\begin{pmatrix}
1 & 1 & a \\ 
0 & 2 & 1 \\
a & -3 & 0
\end{pmatrix}
\colvec{r^1\\r^2\\r^3}
= \colvec{x\\y\\z}.
\]
If the matrix $M=\begin{pmatrix}
1 & 1 & a \\ 
0 & 2 & 1 \\
a & -3 & 0
\end{pmatrix}$ is invertible, then we can find a solution 
\[
M^{-1}\colvec{x\\y\\z}=\colvec{r^1\\r^2\\r^3}
\]
for \emph{any} vector $\colvec{x\\y\\z} \in \Re^3$.

Therefore we should choose $a$ so that $M$ is invertible:  

\[
i.e.,\;  0 \neq \det M = -2a^2 + 3 + a = -(2a-3)(a+1). 
\]
Then the span is $\Re^3$ if and only if $a \neq -1, \frac{3}{2}$.
\end{example}

\Videoscriptlink{subspaces_and_spanning_sets_example.mp4}{Linear systems as spanning sets}{scripts_subspaces_and_spanning_sets_example}

Some other very important ways of building subspaces are given in the following examples.

\begin{example}
(The kernel of a linear map).\\[-2mm]

\noindent
Suppose $L:U\to V$ is a linear map between vector spaces. Then if
$$
L(u)=0=L(u')\, ,
$$
linearity tells us that
$$
L(\alpha u + \beta u') = \alpha L(u) + \beta L(u') =\alpha 0 + \beta 0 = 0\, .
$$
Hence, thanks to the subspace theorem,  the set of all vectors in $U$ that are mapped to the zero vector is a subspace of $V$.
It is called the kernel of $L$:
$$
{\rm ker} L:=\{u\in U| L(u) = 0\}\subset U.
$$
Note that finding a kernel means finding a solution to a homogeneous linear equation. 
\end{example}

\begin{example}
(The image of a linear map).\\[-2mm]

\noindent
Suppose $L:U\to V$ is a linear map between vector spaces. Then if
$$
v=L(u) \mbox{ and } v'=L(u')\, ,
$$
linearity tells us that
$$
\alpha v + \beta v' = \alpha L(u) + \beta L(u') =L(\alpha u +\beta u')\, .
$$
Hence, calling once again on the subspace theorem,  the set of all vectors in $V$ that are obtained as outputs of the
map $L$ is a subspace.
It is called the image of $L$:
$$
{\rm im} L:=\{L(u) \ |\  u\in U \}\subset V.
$$
\end{example}

\begin{example}
(An eigenspace of a linear map).\\[-2mm]

\noindent
Suppose $L:V\to V$ is a linear map and $V$ is a vector space. Then if
$$
L(u)=\lambda u \mbox{ and } L(v)=\lambda v\, ,
$$
linearity tells us that
$$
L(\alpha u + \beta v) = \alpha L(u) + \beta L(v) =\alpha L(u) + \beta L(v) =\alpha \lambda u  + \beta \lambda v = \lambda (\alpha u + \beta v)\, .
$$
Hence, again by subspace theorem, the set of all vectors in $V$ that
obey the {\it eigenvector equation} $L(v)=\lambda v$ is a subspace of $V$. 
It is called an eigenspace
$$
V_\lambda:=\{v\in V| L(v) = \lambda v\}.
$$
For most scalars $\lambda$, the only solution to $L(v) = \lambda v$ will be $v=0$, which yields the trivial subspace $\{0\}$.
When there are nontrivial solutions to $L(v)=\lambda v$, the number~$\lambda$ is called an eigenvalue, and carries
essential information about the map~$L$. 
\end{example}

Kernels, images and eigenspaces are discussed in great depth in chapters~\ref{kernelrank} and~\ref{eigenvalseigenvects}.


%\section*{References}
%Hefferon, Chapter Two, Section I.2: Subspaces and Spanning Sets
%\\
%Beezer, Chapter VS, Section S
%\\
%Beezer, Chapter V, Section LC
%\\
%Beezer, Chapter V, Section SS
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Linear_subspace}{Linear Subspace}
%\item \href{http://en.wikipedia.org/wiki/Linear_span}{Linear Span}
%\end{itemize}
%

\section{Review Problems}
{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{SubspacesAndSpans}{1}, \hwrref{SubspacesAndSpans}{2}\\
 Subspaces &\hwref{SubspacesAndSpans}{3},
 \hwref{SubspacesAndSpans}{4},
 \hwref{SubspacesAndSpans}{5},
 \hwref{SubspacesAndSpans}{6}\\
 Spans &\hwref{SubspacesAndSpans}{7},
 \hwref{SubspacesAndSpans}{8}\\
  \hline
\end{tabular}



\input{\subspacesPath/problems}



\newpage
",lesson
17,Linear Independence,"\chapter{\linIndepTitle}\label{linearind}

Consider a plane $P$ that includes the origin in $\Re^3$ and non-zero vectors $\{u,v,w\}$ in $P$.
\begin{center}
\includegraphics[scale=.3]{\linIndepPath/span_plane.jpg}
\end{center}
If no two of $u, v$ and $w$ are parallel, then $P=\spa \{u,v,w\}$.  But any two vectors determines a plane, so we should be able to span the plane using only two of the vectors $u,v,w$.  Then we could choose two of the vectors in $\{u,v,w\}$ whose span is $P$, and express the other as a linear combination of those two.  Suppose $u$ and $v$ span $P$.  Then there exist constants $d^1, d^2$ (not both zero) such that
$w=d^1u+d^2v$.  Since $w$ can be expressed in terms of $u$ and $v$ we say that it is not independent.
More generally, the relationship
\[
c^1u+c^2v+c^3w=0 \qquad c^i \in \Re, \text{ some $c^i\neq 0$}
\]
expresses the fact that $u,v,w$ are not all independent.

\begin{definition}
\label{independent}
We say that the vectors $v_1, v_2, \ldots, v_n$ are {\bf linearly dependent}\index{Linearly dependent} if there exist constants\footnote{Usually our vector spaces are defined over \(\mathbb{R}\), but in general we can have vector spaces defined over different base fields such as \(\mathbb{C}\) or \(\mathbb{Z}_2\). The coefficients \(c^i\) should come from whatever our base field is (usually \(\mathbb{R}\)).} $c^1, c^2, \ldots, c^n$ not all zero such that
\[
c^1v_1 + c^2v_2+ \cdots +c^nv_n=0.
\]
Otherwise, the vectors $v_1, v_2, \ldots, v_n$ are {\bf linearly independent.}\index{Linearly independent} 
\end{definition}

\begin{remark}
The zero vector $0_V$ can {\it never} be on a list of independent vectors because $\alpha 0_V=0_V$ for any scalar $\alpha$.
\end{remark}

\begin{example}
Consider the following vectors in \(\Re^3\):
\[
v_1=\colvec{4\\-1\\3}, \qquad
v_2=\colvec{-3\\7\\4}, \qquad
v_3=\colvec{5\\12\\17}, \qquad
v_4=\colvec{-1\\1\\0}.
\]
Are these vectors linearly independent?

No, since \(3v_1+2v_2-v_3+v_4=0\), the vectors are linearly {\it dependent}.
\end{example}

\Videoscriptlink{linear_independence_example.mp4}{Worked Example}{linear_independence_example}

\section{Showing Linear Dependence}
In the above example we were given the linear combination \(3v_1+2v_2-v_3+v_4\) seemingly by magic. The next example shows how to find such a linear combination, if it exists.

\begin{example}
Consider the following vectors in $\Re^3$:
\[
v_1=\colvec{0\\0\\1}, 
\qquad v_2=\colvec{1\\2\\1},
\qquad v_3=\colvec{1\\2\\3}.
\]
Are they linearly independent?

We need to see whether the system 
\[
c^1v_1 + c^2v_2+ c^3v_3=0
\]
has any solutions for $c^1, c^2, c^3$.  We can rewrite this as a homogeneous system by building a matrix whose columns are the vectors $v_1$, $v_2$ and $v_3$:
\[
\rowvec{v_1&v_2&v_3}\colvec{c^1\\c^2\\c^3}=0.
\]
This system has solutions if and only if the matrix $M=\rowvec{v_1&v_2&v_3}$ is singular, so we should find the determinant of $M$:
\[
\det M = \det \begin{pmatrix}
0 & 1 & 1 \\
0 & 2 & 2 \\
1 & 1 & 3 \\
\end{pmatrix}
= \det \begin{pmatrix}
1 & 1 \\
2 & 2 \\
\end{pmatrix}
=0.
\]

Therefore nontrivial solutions exist.  At this point we know that the vectors are linearly dependent.  If we need to, we can find coefficients that demonstrate linear dependence by solving
\[
\begin{amatrix}{3}
0 & 1 & 1 & 0\\
0 & 2 & 2 & 0\\
1 & 1 & 3 & 0\\
\end{amatrix} \sim
\begin{amatrix}{3}
1 & 1 & 3 & 0\\
0 & 1 & 1 & 0\\
0 & 0 & 0 & 0\\
\end{amatrix} \sim
\begin{amatrix}{3}
1 & 0 & 2 & 0\\
0 & 1 & 1 & 0\\
0 & 0 & 0 & 0\\
\end{amatrix}.
\]
The solution set  $\{ \mu ( -2,-1,1) ~| ~\mu \in \mathbb{R} \}$ encodes the linear combinations equal to zero;  any choice of $\mu$ will produce coefficients $c^1,c^2,c^3$ that satisfy the linear homogeneous equation.  
In particular, $\mu=1$ corresponds to the equation
\[
c^1v_1 + c^2v_2+ c^3v_3=0 
\Rightarrow -2v_1 - v_2 + v_3=0.
\]
\end{example}

\Reading{LinearIndependence}{1}
%\begin{center}\href{\webworkurl ReadingHomework16/1/}{Reading homework: problem \ref{linearind}.1}\end{center}

\begin{definition}
Any sum of vectors $v_1,\ldots, v_k$ multiplied by scalars $c^1,\ldots,c^k$, namely
$$
c^1 v_1+\cdots + c^k v_k\, ,
$$
is called a {\it linear combination}\index{Linear combination} of $v_1,\ldots , v_k$.
\end{definition}

\begin{theorem}[Linear Dependence]\index{Linear dependence theorem}
\label{linear_dependence}
An ordered set of non-zero vectors $( v_1, \ldots, v_n )$ is linearly dependent if and only if one of the vectors $v_k$ is expressible as a linear combination of the preceding vectors.
\end{theorem}

\begin{proof}
The theorem is an if and only if statement, so there are two things to show.

\begin{itemize}
\item[$i.$]  First, we show that if $v_k=c^1v_1+\cdots c^{k-1}v_{k-1}$ then the set is linearly dependent.

This is easy.  We just rewrite the assumption:
\[
c^1v_1+\cdots+c^{k-1}v_{k-1}-v_k + 0v_{k+1}+\cdots +0v_n=0.
\]
This is a vanishing linear combination of the vectors $\{ v_1, \ldots, v_n \}$ with not all coefficients equal to zero, so $\{ v_1, \ldots, v_n \}$ is a linearly dependent set.
 
\item[$ii.$]  Now we show that linear dependence implies that there exists $k$ for which $v_k$ is a linear combination of the vectors $\{ v_1, \ldots, v_{k-1} \}$.

The assumption says that
\[
c^1v_1 + c^2v_2+ \cdots +c^nv_n=0.
\]
Take $k$ to be the largest number for which $c_k$ is not equal to zero.  So:
\[
c^1v_1 + c^2v_2+ \cdots +c^{k-1}v_{k-1}+c^kv_k=0.
\]

(Note that $k>1$, since otherwise we would have $c^1v_1=0\Rightarrow v_1=0$, contradicting the assumption that none of the $v_i$ are the zero vector.)

So we can rearrange the equation:
\begin{eqnarray*}
c^1v_1 + c^2v_2+ \cdots +c^{k-1}v_{k-1}&=&-c^kv_k\\ \Rightarrow\ 
-\frac{c^1}{c^k}v_1 - \frac{c^2}{c^k}v_2 - \cdots -\frac{c^{k-1}}{c^k}v_{k-1}&=&v_k.
\end{eqnarray*}

Therefore we have expressed $v_k$ as a linear combination of the previous vectors, and we are done.
\end{itemize}
\end{proof}

\Videoscriptlink{linear_independence_thm.mp4}{Worked proof}{scripts_linear_independence_thm}

\begin{example}
Consider the vector space $P_2(t)$ of polynomials of degree less than or equal to $2$.  Set:
\begin{eqnarray*}
v_1 &=& 1+t \\
v_2 &=& 1+t^2 \\
v_3 &=& t+t^2 \\
v_4 &=& 2+t+t^2 \\
v_5 &=& 1+t+t^2. \\
\end{eqnarray*}
The set $\{ v_1, \ldots, v_5 \}$ is linearly dependent, because $v_4 = v_1+v_2$.  
\end{example}

\section{Showing Linear Independence} 
We have seen two different ways to show a set of vectors is linearly dependent: we can either find a linear combination of the vectors which is equal to zero, or we can express one of the vectors as a linear combination of the other vectors. On the other hand, to check that a set of vectors is linearly {\it independent}, we must check that every  linear combination of our vectors with non-vanishing coefficients gives something other than the zero vector. Equivalently, to show that the set \(v_1, v_2, \ldots, v_n\) is linearly independent, we must show that the equation \(c_1 v_1+c_2v_2 + \cdots + c_n v_n=0\) has no solutions other than \(c_1=c_2=\cdots=c_n=0.\)

\begin{example}
Consider the following vectors in $\Re^3$:
\[
v_1=\colvec{0\\0\\2},
\qquad v_2=\colvec{2\\2\\1},
\qquad v_3=\colvec{1\\4\\3}.
\]
Are they linearly independent?

We need to see whether the system
\[
c^1v_1 + c^2v_2+ c^3v_3=0
\]
has any solutions for $c^1, c^2, c^3$.  We can rewrite this as a homogeneous system:
\[
\rowvec{v_1&v_2&v_3}\colvec{c^1\\c^2\\c^3}=0.
\]
This system has solutions if and only if the matrix $M=\rowvec{v_1&v_2&v_3}$ is singular, so we should find the determinant of $M$:
\[
\det M = \det \begin{pmatrix}
0 & 2 & 1 \\
0 & 2 & 4 \\
2 & 1 & 3 \\
\end{pmatrix}
= 2 \det \begin{pmatrix}
2 & 1 \\
2 & 4 \\
\end{pmatrix}
=12.
\]
Since the matrix \(M\) has non-zero determinant, the only solution to the system of equations
\[
\rowvec{v_1&v_2&v_3}\colvec{c^1\\c^2\\c^3}=0
\]
is \(c_1=c_2=c_3=0\). So the vectors \(v_1, v_2, v_3\) are linearly independent.
\end{example}

Here is another example with bits:

\begin{example}
Let $\mathbb{Z}_2^3$ be the space of $3\times 1$ bit-valued matrices (i.e., column vectors).  Is the following subset linearly independent?
\[
\left\{ \colvec{1\\1\\0}, \colvec{1\\0\\1}, 
\colvec{0\\1\\1} \right\}
\]

If the set is linearly dependent, then we can find non-zero solutions to the system:
\[
c^1\colvec{1\\1\\0}+ c^2 \colvec{1\\0\\1} 
+c^3 \colvec{0\\1\\1}=0,
\]
which becomes the linear system
\[
\begin{pmatrix}
1 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 1 \\
\end{pmatrix}\colvec{c^1\\c^2\\c^3}=0.
\]
Solutions exist if and only if the determinant of the matrix is non-zero.  But:
\[
\det \begin{pmatrix}
1 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 1 \\
\end{pmatrix} = 1 \det \begin{pmatrix}
0 & 1 \\
1 & 1 \\
\end{pmatrix} -1 \det \begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix} = -1-1=1+1=0
\]
Therefore non-trivial solutions exist, and the set is not linearly independent.

\end{example}


\Reading{LinearIndependence}{2}
%\begin{center}\href{\webworkurl ReadingHomework16/2/}{Reading homework: problem \ref{linearind}.2}\end{center}

\section{From Dependent Independent } 
Now suppose vectors 
$v_1,\ldots, v_n$ are linearly dependent, 
\[
c^1v_1 + c^2v_2+ \cdots +c^nv_n=0
\]
with $c^1\neq 0$.  Then:
\[
\spa \{v_1,\ldots, v_n\} = \spa \{ v_2,\ldots, v_n\}
\]
because any $x\in \spa \{v_1,\ldots, v_n\}$ is given by
\begin{eqnarray*}
x &=& a^1v_1 + \cdots+ a^nv_n \\
&=& a^1\left( -\frac{c^2}{c_1}v_2- \cdots -\frac{c^n}{c_1}v_n \right) + a^2v_2 + \cdots + a^nv_n \\
&=& \left(a^2-a^1\frac{c^2}{c_1}\right)v_2 + \cdots + \left(a^n-a^1\frac{c^n}{c_1}\right)v_n.
\end{eqnarray*}
Then $x$ is in $\spa \{v_2,\ldots, v_n\}$.

When we write a vector space as the span of a list of vectors, we would like that list to be as short as possible (this idea is explored further in \hyperref[dimension]{chapter~\ref*{sec:dimension}}).
This can be achieved by iterating the above procedure.

\begin{example}
In the above example, we found that $v_4=v_1+v_2$.  In this case, any expression for a vector as a linear combination involving $v_4$ can be turned into a combination without $v_4$ by making the substitution $v_4=v_1+v_2$.

Then:
\begin{eqnarray*}
S &=& \spa \{ 1+t , 1+t^2, t+t^2, 2+t+t^2, 1+t+t^2 \} \\
&=& \spa \{ 1+t , 1+t^2, t+t^2, 1+t+t^2 \}.
\end{eqnarray*}
Now we notice that $1+t+t^2=\frac{1}{2}(1+t) +\frac{1}{2}(1+t^2) + \frac{1}{2}(t+t^2)$.  So the vector $1+t+t^2=v_5$ is also extraneous, since it can be expressed as a linear combination of the remaining three vectors, $v_1, v_2,v_3$.  Therefore 
\[
S = \spa \{ 1+t , 1+t^2, t+t^2 \}.
\]

In fact, you can check that there are no (non-zero) solutions to the linear system
\[
c^1(1+t) + c^2(1+t^2) + c^3(t+t^2)=0.
\]
Therefore the remaining vectors $\{ 1+t , 1+t^2, t+t^2 \}$ are linearly independent, and span the vector space $S$.  Then these vectors are a minimal spanning set\index{Minimal spanning set}, in the sense that no more vectors can be removed since the vectors are linearly independent.
Such a set is called a \emph{basis}\index{Basis!example of} for $S$.
\end{example}





%To summarize, the key definition in this lecture was:
%\begin{center}
%\includegraphics[scale=.3]{\linIndepPath/linear_dependent.jpg}
%\end{center}
%Perhaps the most useful Theorem was:
%\begin{center}
%\includegraphics[scale=.3]{\linIndepPath/linear_dependence_thm.jpg}
%\end{center}

%\section*{References}
%Hefferon, Chapter Two, Section II: Linear Independence
%\\
%Hefferon, Chapter Two, Section III.1: Basis
%\\
%Beezer, Chapter V, Section LI
%\\
%Beezer, Chapter V, Section LDS
%\\
%Beezer, Chapter VS, Section LISS, Subsection LI
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Linear_independence}{Linear Independence}
%\item \href{http://en.wikipedia.org/wiki/Basis_(linear_algebra)}{Basis}
%\end{itemize}

\section{Review Problems}
{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{LinearIndependence}{1},\hwrref{LinearIndependence}{2}\\
 Testing for linear independence &\hwref{LinearIndependence}{3},
 \hwref{LinearIndependence}{4}\\
 Gaussian elimination &\hwref{LinearIndependence}{5}\\
 Spanning and linear independence &\hwref{LinearIndependence}{6}\\
  \hline
\end{tabular}

\input{\linIndepPath/problems}


","\chapter{\linIndepTitle}\label{linearind}

Consider a plane $P$ that includes the origin in $\Re^3$ and non-zero vectors $\{u,v,w\}$ in $P$.
\begin{center}
\includegraphics[scale=.3]{\linIndepPath/span_plane.jpg}
\end{center}
If no two of $u, v$ and $w$ are parallel, then $P=\spa \{u,v,w\}$.  But any two vectors determines a plane, so we should be able to span the plane using only two of the vectors $u,v,w$.  Then we could choose two of the vectors in $\{u,v,w\}$ whose span is $P$, and express the other as a linear combination of those two.  Suppose $u$ and $v$ span $P$.  Then there exist constants $d^1, d^2$ (not both zero) such that
$w=d^1u+d^2v$.  Since $w$ can be expressed in terms of $u$ and $v$ we say that it is not independent.
More generally, the relationship
\[
c^1u+c^2v+c^3w=0 \qquad c^i \in \Re, \text{ some $c^i\neq 0$}
\]
expresses the fact that $u,v,w$ are not all independent.

\begin{definition}
\label{independent}
We say that the vectors $v_1, v_2, \ldots, v_n$ are {\bf linearly dependent}\index{Linearly dependent} if there exist constants\footnote{Usually our vector spaces are defined over \(\mathbb{R}\), but in general we can have vector spaces defined over different base fields such as \(\mathbb{C}\) or \(\mathbb{Z}_2\). The coefficients \(c^i\) should come from whatever our base field is (usually \(\mathbb{R}\)).} $c^1, c^2, \ldots, c^n$ not all zero such that
\[
c^1v_1 + c^2v_2+ \cdots +c^nv_n=0.
\]
Otherwise, the vectors $v_1, v_2, \ldots, v_n$ are {\bf linearly independent.}\index{Linearly independent} 
\end{definition}

\begin{remark}
The zero vector $0_V$ can {\it never} be on a list of independent vectors because $\alpha 0_V=0_V$ for any scalar $\alpha$.
\end{remark}

\begin{example}
Consider the following vectors in \(\Re^3\):
\[
v_1=\colvec{4\\-1\\3}, \qquad
v_2=\colvec{-3\\7\\4}, \qquad
v_3=\colvec{5\\12\\17}, \qquad
v_4=\colvec{-1\\1\\0}.
\]
Are these vectors linearly independent?

No, since \(3v_1+2v_2-v_3+v_4=0\), the vectors are linearly {\it dependent}.
\end{example}

\Videoscriptlink{linear_independence_example.mp4}{Worked Example}{linear_independence_example}

\section{Showing Linear Dependence}
In the above example we were given the linear combination \(3v_1+2v_2-v_3+v_4\) seemingly by magic. The next example shows how to find such a linear combination, if it exists.

\begin{example}
Consider the following vectors in $\Re^3$:
\[
v_1=\colvec{0\\0\\1}, 
\qquad v_2=\colvec{1\\2\\1},
\qquad v_3=\colvec{1\\2\\3}.
\]
Are they linearly independent?

We need to see whether the system 
\[
c^1v_1 + c^2v_2+ c^3v_3=0
\]
has any solutions for $c^1, c^2, c^3$.  We can rewrite this as a homogeneous system by building a matrix whose columns are the vectors $v_1$, $v_2$ and $v_3$:
\[
\rowvec{v_1&v_2&v_3}\colvec{c^1\\c^2\\c^3}=0.
\]
This system has solutions if and only if the matrix $M=\rowvec{v_1&v_2&v_3}$ is singular, so we should find the determinant of $M$:
\[
\det M = \det \begin{pmatrix}
0 & 1 & 1 \\
0 & 2 & 2 \\
1 & 1 & 3 \\
\end{pmatrix}
= \det \begin{pmatrix}
1 & 1 \\
2 & 2 \\
\end{pmatrix}
=0.
\]

Therefore nontrivial solutions exist.  At this point we know that the vectors are linearly dependent.  If we need to, we can find coefficients that demonstrate linear dependence by solving
\[
\begin{amatrix}{3}
0 & 1 & 1 & 0\\
0 & 2 & 2 & 0\\
1 & 1 & 3 & 0\\
\end{amatrix} \sim
\begin{amatrix}{3}
1 & 1 & 3 & 0\\
0 & 1 & 1 & 0\\
0 & 0 & 0 & 0\\
\end{amatrix} \sim
\begin{amatrix}{3}
1 & 0 & 2 & 0\\
0 & 1 & 1 & 0\\
0 & 0 & 0 & 0\\
\end{amatrix}.
\]
The solution set  $\{ \mu ( -2,-1,1) ~| ~\mu \in \mathbb{R} \}$ encodes the linear combinations equal to zero;  any choice of $\mu$ will produce coefficients $c^1,c^2,c^3$ that satisfy the linear homogeneous equation.  
In particular, $\mu=1$ corresponds to the equation
\[
c^1v_1 + c^2v_2+ c^3v_3=0 
\Rightarrow -2v_1 - v_2 + v_3=0.
\]
\end{example}

\Reading{LinearIndependence}{1}
%\begin{center}\href{\webworkurl ReadingHomework16/1/}{Reading homework: problem \ref{linearind}.1}\end{center}

\begin{definition}
Any sum of vectors $v_1,\ldots, v_k$ multiplied by scalars $c^1,\ldots,c^k$, namely
$$
c^1 v_1+\cdots + c^k v_k\, ,
$$
is called a {\it linear combination}\index{Linear combination} of $v_1,\ldots , v_k$.
\end{definition}

\begin{theorem}[Linear Dependence]\index{Linear dependence theorem}
\label{linear_dependence}
An ordered set of non-zero vectors $( v_1, \ldots, v_n )$ is linearly dependent if and only if one of the vectors $v_k$ is expressible as a linear combination of the preceding vectors.
\end{theorem}

\begin{proof}
The theorem is an if and only if statement, so there are two things to show.

\begin{itemize}
\item[$i.$]  First, we show that if $v_k=c^1v_1+\cdots c^{k-1}v_{k-1}$ then the set is linearly dependent.

This is easy.  We just rewrite the assumption:
\[
c^1v_1+\cdots+c^{k-1}v_{k-1}-v_k + 0v_{k+1}+\cdots +0v_n=0.
\]
This is a vanishing linear combination of the vectors $\{ v_1, \ldots, v_n \}$ with not all coefficients equal to zero, so $\{ v_1, \ldots, v_n \}$ is a linearly dependent set.
 
\item[$ii.$]  Now we show that linear dependence implies that there exists $k$ for which $v_k$ is a linear combination of the vectors $\{ v_1, \ldots, v_{k-1} \}$.

The assumption says that
\[
c^1v_1 + c^2v_2+ \cdots +c^nv_n=0.
\]
Take $k$ to be the largest number for which $c_k$ is not equal to zero.  So:
\[
c^1v_1 + c^2v_2+ \cdots +c^{k-1}v_{k-1}+c^kv_k=0.
\]

(Note that $k>1$, since otherwise we would have $c^1v_1=0\Rightarrow v_1=0$, contradicting the assumption that none of the $v_i$ are the zero vector.)

So we can rearrange the equation:
\begin{eqnarray*}
c^1v_1 + c^2v_2+ \cdots +c^{k-1}v_{k-1}&=&-c^kv_k\\ \Rightarrow\ 
-\frac{c^1}{c^k}v_1 - \frac{c^2}{c^k}v_2 - \cdots -\frac{c^{k-1}}{c^k}v_{k-1}&=&v_k.
\end{eqnarray*}

Therefore we have expressed $v_k$ as a linear combination of the previous vectors, and we are done.
\end{itemize}
\end{proof}

\Videoscriptlink{linear_independence_thm.mp4}{Worked proof}{scripts_linear_independence_thm}

\begin{example}
Consider the vector space $P_2(t)$ of polynomials of degree less than or equal to $2$.  Set:
\begin{eqnarray*}
v_1 &=& 1+t \\
v_2 &=& 1+t^2 \\
v_3 &=& t+t^2 \\
v_4 &=& 2+t+t^2 \\
v_5 &=& 1+t+t^2. \\
\end{eqnarray*}
The set $\{ v_1, \ldots, v_5 \}$ is linearly dependent, because $v_4 = v_1+v_2$.  
\end{example}

\section{Showing Linear Independence} 
We have seen two different ways to show a set of vectors is linearly dependent: we can either find a linear combination of the vectors which is equal to zero, or we can express one of the vectors as a linear combination of the other vectors. On the other hand, to check that a set of vectors is linearly {\it independent}, we must check that every  linear combination of our vectors with non-vanishing coefficients gives something other than the zero vector. Equivalently, to show that the set \(v_1, v_2, \ldots, v_n\) is linearly independent, we must show that the equation \(c_1 v_1+c_2v_2 + \cdots + c_n v_n=0\) has no solutions other than \(c_1=c_2=\cdots=c_n=0.\)

\begin{example}
Consider the following vectors in $\Re^3$:
\[
v_1=\colvec{0\\0\\2},
\qquad v_2=\colvec{2\\2\\1},
\qquad v_3=\colvec{1\\4\\3}.
\]
Are they linearly independent?

We need to see whether the system
\[
c^1v_1 + c^2v_2+ c^3v_3=0
\]
has any solutions for $c^1, c^2, c^3$.  We can rewrite this as a homogeneous system:
\[
\rowvec{v_1&v_2&v_3}\colvec{c^1\\c^2\\c^3}=0.
\]
This system has solutions if and only if the matrix $M=\rowvec{v_1&v_2&v_3}$ is singular, so we should find the determinant of $M$:
\[
\det M = \det \begin{pmatrix}
0 & 2 & 1 \\
0 & 2 & 4 \\
2 & 1 & 3 \\
\end{pmatrix}
= 2 \det \begin{pmatrix}
2 & 1 \\
2 & 4 \\
\end{pmatrix}
=12.
\]
Since the matrix \(M\) has non-zero determinant, the only solution to the system of equations
\[
\rowvec{v_1&v_2&v_3}\colvec{c^1\\c^2\\c^3}=0
\]
is \(c_1=c_2=c_3=0\). So the vectors \(v_1, v_2, v_3\) are linearly independent.
\end{example}

Here is another example with bits:

\begin{example}
Let $\mathbb{Z}_2^3$ be the space of $3\times 1$ bit-valued matrices (i.e., column vectors).  Is the following subset linearly independent?
\[
\left\{ \colvec{1\\1\\0}, \colvec{1\\0\\1}, 
\colvec{0\\1\\1} \right\}
\]

If the set is linearly dependent, then we can find non-zero solutions to the system:
\[
c^1\colvec{1\\1\\0}+ c^2 \colvec{1\\0\\1} 
+c^3 \colvec{0\\1\\1}=0,
\]
which becomes the linear system
\[
\begin{pmatrix}
1 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 1 \\
\end{pmatrix}\colvec{c^1\\c^2\\c^3}=0.
\]
Solutions exist if and only if the determinant of the matrix is non-zero.  But:
\[
\det \begin{pmatrix}
1 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 1 \\
\end{pmatrix} = 1 \det \begin{pmatrix}
0 & 1 \\
1 & 1 \\
\end{pmatrix} -1 \det \begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix} = -1-1=1+1=0
\]
Therefore non-trivial solutions exist, and the set is not linearly independent.

\end{example}


\Reading{LinearIndependence}{2}
%\begin{center}\href{\webworkurl ReadingHomework16/2/}{Reading homework: problem \ref{linearind}.2}\end{center}

\section{From Dependent Independent } 
Now suppose vectors 
$v_1,\ldots, v_n$ are linearly dependent, 
\[
c^1v_1 + c^2v_2+ \cdots +c^nv_n=0
\]
with $c^1\neq 0$.  Then:
\[
\spa \{v_1,\ldots, v_n\} = \spa \{ v_2,\ldots, v_n\}
\]
because any $x\in \spa \{v_1,\ldots, v_n\}$ is given by
\begin{eqnarray*}
x &=& a^1v_1 + \cdots+ a^nv_n \\
&=& a^1\left( -\frac{c^2}{c_1}v_2- \cdots -\frac{c^n}{c_1}v_n \right) + a^2v_2 + \cdots + a^nv_n \\
&=& \left(a^2-a^1\frac{c^2}{c_1}\right)v_2 + \cdots + \left(a^n-a^1\frac{c^n}{c_1}\right)v_n.
\end{eqnarray*}
Then $x$ is in $\spa \{v_2,\ldots, v_n\}$.

When we write a vector space as the span of a list of vectors, we would like that list to be as short as possible (this idea is explored further in \hyperref[dimension]{chapter~\ref*{sec:dimension}}).
This can be achieved by iterating the above procedure.

\begin{example}
In the above example, we found that $v_4=v_1+v_2$.  In this case, any expression for a vector as a linear combination involving $v_4$ can be turned into a combination without $v_4$ by making the substitution $v_4=v_1+v_2$.

Then:
\begin{eqnarray*}
S &=& \spa \{ 1+t , 1+t^2, t+t^2, 2+t+t^2, 1+t+t^2 \} \\
&=& \spa \{ 1+t , 1+t^2, t+t^2, 1+t+t^2 \}.
\end{eqnarray*}
Now we notice that $1+t+t^2=\frac{1}{2}(1+t) +\frac{1}{2}(1+t^2) + \frac{1}{2}(t+t^2)$.  So the vector $1+t+t^2=v_5$ is also extraneous, since it can be expressed as a linear combination of the remaining three vectors, $v_1, v_2,v_3$.  Therefore 
\[
S = \spa \{ 1+t , 1+t^2, t+t^2 \}.
\]

In fact, you can check that there are no (non-zero) solutions to the linear system
\[
c^1(1+t) + c^2(1+t^2) + c^3(t+t^2)=0.
\]
Therefore the remaining vectors $\{ 1+t , 1+t^2, t+t^2 \}$ are linearly independent, and span the vector space $S$.  Then these vectors are a minimal spanning set\index{Minimal spanning set}, in the sense that no more vectors can be removed since the vectors are linearly independent.
Such a set is called a \emph{basis}\index{Basis!example of} for $S$.
\end{example}





%To summarize, the key definition in this lecture was:
%\begin{center}
%\includegraphics[scale=.3]{\linIndepPath/linear_dependent.jpg}
%\end{center}
%Perhaps the most useful Theorem was:
%\begin{center}
%\includegraphics[scale=.3]{\linIndepPath/linear_dependence_thm.jpg}
%\end{center}

%\section*{References}
%Hefferon, Chapter Two, Section II: Linear Independence
%\\
%Hefferon, Chapter Two, Section III.1: Basis
%\\
%Beezer, Chapter V, Section LI
%\\
%Beezer, Chapter V, Section LDS
%\\
%Beezer, Chapter VS, Section LISS, Subsection LI
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Linear_independence}{Linear Independence}
%\item \href{http://en.wikipedia.org/wiki/Basis_(linear_algebra)}{Basis}
%\end{itemize}

\section{Review Problems}
{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{LinearIndependence}{1},\hwrref{LinearIndependence}{2}\\
 Testing for linear independence &\hwref{LinearIndependence}{3},
 \hwref{LinearIndependence}{4}\\
 Gaussian elimination &\hwref{LinearIndependence}{5}\\
 Spanning and linear independence &\hwref{LinearIndependence}{6}\\
  \hline
\end{tabular}

\input{\linIndepPath/problems}


",lesson
18,Basis And Dimension,"\chapter{\basisDimTitle}\label{basisdimension}

\label{sec:dimension}
\label{dimension}
In chapter~\ref{linearind}, the notions of   a linearly independent set of vectors in a vector space $V$, and of a set of vectors that span $V$ were established; any set of vectors that span $V$ can be reduced to some minimal collection of linearly independent vectors; such a minimal set is called a \emph{basis} of the subspace $V$.  

\begin{definition}
Let $V$ be a vector space.  Then a set $S$ is a {\bf basis}\index{Basis} for $V$ if $S$ is linearly independent and $V=\spa S$.


If $S$ is a basis of $V$ and $S$ has only finitely many elements, then we say that $V$ is {\bf finite-dimensional}.  The number of vectors in $S$ is the {\bf dimension}\index{Dimension} of~$V$.
\end{definition}

Suppose $V$ is a \emph{finite-dimensional}\index{Vector space!finite dimensional} vector space, and $S$ and $T$ are two different bases for $V$.  One might worry that $S$ and $T$ have a different number of vectors; then we would have to talk about the dimension of $V$ in terms of the basis $S$ or in terms of the basis $T$.  Luckily this isn't what happens.
Later in this chapter, we will show that $S$ and $T$ must have the same number of vectors.  This means that the dimension of a vector space is basis-independent.  In fact, dimension is a very important  characteristic of a vector space.% $V$.

\begin{example}
$P_n(t)$ (polynomials in $t$ of degree $n$ or less) has a basis $\{1,t,\ldots , t^n \}$, since every vector in this space is a sum
\[
a^0\,1+a^1\,t+\cdots +a^n\,t^n, \qquad a^i\in \Re\, ,
\]
so $P_n(t)=\spa \{1,t,\ldots , t^n \}$.  This set of vectors is linearly independent;  If the polynomial $p(t)=c^01+c^1t+\cdots +c^nt^n=0$, then $c^0=c^1=\cdots =c^n=0$, so $p(t)$ is the zero polynomial.  
Thus $P_n(t)$ is finite dimensional, and $\dim P_n(t)=n+1$.
\end{example}



\begin{theorem}\label{uniqvec}
Let $S=\{v_1, \ldots, v_n \}$  be a basis for a vector space $V$.  Then every vector $w \in V$ can be written \emph{uniquely} as a linear combination of vectors in the basis $S$:
\[
w=c^1v_1+\cdots + c^nv_n.
\]
\end{theorem}

\begin{proof}
Since $S$ is a basis for $V$, then $\spa S=V$, and so there exist constants~$c^i$ such that $w=c^1v_1+\cdots + c^nv_n$.

Suppose there exists a second set of constants $d^i$ such that 
$$w=d^1v_1+\cdots + d^nv_n\, .$$  Then
\begin{eqnarray*}
0_V&=&w-w\\
&=&c^1v_1+\cdots + c^nv_n-d^1v_1-\cdots - d^nv_n \\[1mm]
&=&(c^1-d^1)v_1+\cdots + (c^n-d^n)v_n. \\
\end{eqnarray*}
If it occurs exactly once that $c^i\neq d^i$, then the equation reduces to $0=(c^i-d^i)v_i$, which is a contradiction since the vectors $v_i$ are assumed to be non-zero.

If we have more than one $i$ for which $c^i\neq d^i$, we can use this last equation to write one of the vectors in $S$ as a linear combination of other vectors in $S$, which contradicts the assumption that $S$ is linearly independent.  Then for every $i$, $c^i=d^i$.
\end{proof}

\Videoscriptlink{basis_and_dimension_thm.mp4}{Proof Explanation}{basis_and_dimension_thm}

\begin{remark}
This theorem is the one that makes bases so useful--they allow us to convert abstract vectors into column vectors.
By ordering the set $S$ we obtain $B=(v_1,\ldots,v_n)$ and can write
$$
w=(v_1,\ldots,v_n) \ccolvec{c^1\\ \vdots\\ c^n }=\ccolvec{c^1\\ \vdots\\ c^n }_B\, .
$$
Remember that in general it makes no sense to drop the subscript $B$ on the column vector on the right--most vector spaces  are not made from  columns of numbers!
\end{remark}

\Videoscriptlink{eigenvectors_and_eigenvalues_matrix.mp4}{Worked Example}{scripts_eigenvalseigenvects_matrix}

Next, we would like to establish a method for determining whether a collection of vectors forms a basis for $\Re^n$.  But first, we need to show that any two bases for a finite-dimensional vector space has the same number of vectors.

\begin{lemma}\label{mlessn}
If $S=\{v_1, \ldots, v_n \}$ is a basis for a vector space $V$ and $T=\{w_1, \ldots, w_m \}$ is a linearly independent set of vectors in $V$, then $m\leq n$.
\end{lemma}

%\begin{figure}
%\begin{center}
%\includegraphics[scale=.28]{\basisDimPath/indep_span.jpg}
%\end{center}
%\end{figure}

The idea of the proof is to start with the set $S$ and replace vectors in $S$ one at a time with vectors from $T$, such that after each replacement we still have a basis for $V$.

%\begin{center}\href{\webworkurl ReadingHomework17/1/}{Reading homework: problem \ref{basisdimension}.1}\end{center}
\Reading{BasisAndDimension}{1}

\begin{proof}
Since $S$ spans $V$, then the set $\{w_1, v_1, \ldots, v_n \}$ is linearly dependent.  Then we can write $w_1$ as a linear combination of the $v_i$; using that equation, we can express one of the $v_i$ in terms of $w_1$ and the remaining $v_j$ with~$j\neq i$.  Then we can discard one of the $v_i$ from this set to obtain a linearly independent set that still spans $V$.  Now we need to prove that $S_1$ is a basis; we must show that $S_1$ is linearly independent and that $S_1$ spans $V$.

The set $S_1=\{w_1, v_1, \ldots, v_{i-1}, v_{i+1},\ldots, v_n \}$ is linearly independent:  By the previous theorem, there was a unique way to express $w_1$ in terms of the set~$S$.  Now, to obtain a contradiction, suppose there is some $k$ and constants~$c^i$ such that
\[
v_k = c^0w_1+c^1v_1+\cdots + c^{i-1}v_{i-1} + c^{i+1}v_{i+1} + \cdots + c^nv_n.
\]
Then replacing $w_1$ with its expression in terms of the collection $S$ gives a way to express the vector $v_k$ as a linear combination of the vectors in $S$, which contradicts the linear independence of $S$.  On the other hand, we cannot express $w_1$ as a linear combination of the vectors in $\{v_j | j\neq i\}$, since the expression of $w_1$ in terms of $S$ was unique, and had a non-zero coefficient for the vector $v_i$.  Then no vector in $S_1$ can be expressed as a combination of other vectors in $S_1$, which demonstrates that $S_1$ is linearly independent.

The set $S_1$ spans $V$:  For any $u\in V$, we can express $u$ as a linear combination of vectors in $S$.  But we can express $v_i$ as a linear combination of vectors in the collection $S_1$; rewriting $v_i$ as such allows us to express $u$ as a linear combination of the vectors in $S_1$. Thus $S_1$ is a basis of $V$ with $n$ vectors.

We can now iterate this process, replacing one of the $v_i$ in $S_1$ with $w_2$, and so on.  If $m\leq n$, this process ends with the set $S_m=\{w_1,\ldots, w_m$, $v_{i_1},\ldots,v_{i_{n-m}}  \}$, which is fine.

Otherwise, we have $m>n$, and the set $S_n=\{w_1,\ldots, w_n \}$ is a basis for~$V$.  But we still have some vector 
$w_{n+1}$  in $T$ that is not in $S_n$.  Since $S_n$ is a basis, we can write $w_{n+1}$ as a combination of the vectors in $S_n$, which contradicts the linear independence of the set $T$.  Then it must be the case that $m\leq n$, as desired.
\end{proof}

\Videoscriptlink{basis_and_dimension_example.mp4}{Worked Example}{basis_and_dimension_example}

\begin{corollary}\label{corsame}
For a finite-dimensional vector space $V$, any two bases for~$V$ have the same number of vectors.
\end{corollary}

\begin{proof}
Let $S$ and $T$ be two bases for $V$.  Then both are linearly independent sets that span $V$.  Suppose $S$ has $n$ vectors and $T$ has $m$ vectors.  Then by the previous lemma, we have that $m\leq n$.  But (exchanging the roles of $S$ and $T$ in application of the lemma) we also see that $n\leq m$.  Then $m=n$, as desired.
\end{proof}

%\begin{center}\href{\webworkurl ReadingHomework17/2/}{Reading homework: problem \ref{basisdimension}.2}\end{center}
\Reading{BasisAndDimension}{2}

\section{Bases in $\Re^n$.}

In review question~\ref{stdbasis}, chapter~\ref{linearind} you checked that
\[
\Re^n = \spa \left\{ \colvec{1\\0\\ \vdots \\ 0}, 
\colvec{0\\1\\ \vdots \\ 0}, \ldots, \colvec{0\\0\\ \vdots \\ 1}\right\},
\]
and that this set of vectors is linearly independent. (If you didn't do that problem, check this before reading any further!)  So this set of vectors is a basis for~$\Re^n$, and $\dim \Re^n=n$.  This basis is often called the \emph{standard} or \emph{canonical basis}\index{Standard basis}\index{Canonical basis|seealso{Standard basis}} for $\Re^n$.  The vector with a one in the $i$th position and zeros everywhere else is written 
$e_i$. (You could also view it as the function $\{1,2,\ldots,n\}\to {\mathbb R}$ where $e_i(j)=1$ if $i=j$ and $0$ if $i\neq j$.)  It points in the direction of the $i$th coordinate axis, and has unit length.  In multivariable calculus classes, this basis is often written $\{ \hat i,\hat j,\hat k \}$ for $\Re^3$. 

%\begin{figure}
%\begin{center}
%\includegraphics[scale=.32]{\basisDimPath/canonical.jpg}
%\end{center}
%\end{figure}

Note that it is often convenient to order  basis elements, so rather than writing a set
of vectors, we would write a list. This is called an ordered basis. For example, the canonical ordered basis for ${\mathbb R^n}$ is $(e_1,e_2,\ldots,e_n)$. The possibility to reorder basis vectors is not the only way in which bases are non-unique.

\begin{remark}[Bases are not unique.]
While there exists a unique way to express a vector in terms of any particular basis, bases themselves are far from unique.
For example, both of the sets 
\[
\left\{ \colvec{1\\0}, \colvec{0\\1} \right\} \text{ and }
\left\{ \colvec{1\\1}, \colvec{1\\-1} \right\}
\]
are bases for $\Re^2$.  Rescaling any vector in one of these sets is already enough to show that~$\Re^2$ has infinitely many bases.  But even if we require that all of the basis vectors have unit length, it turns out that there are still infinitely many bases for $\Re^2$ (see review question~\ref{lotsofbases}).
\end{remark}


To see whether a set of vectors $S=\{v_1, \ldots, v_m \}$ is a basis for~$\Re^n$, we have to check that the elements  are linearly independent and that they span~$\Re^n$.  From the previous discussion, we also know that $m$ must equal $n$, so lets assume~$S$ has $n$ vectors.
If $S$ is linearly independent, then there is no non-trivial solution of the equation
\[
0 = x^1v_1+\cdots + x^nv_n.
\]
Let $M$ be a matrix whose columns are the vectors $v_i$ and $X$ the column vector with entries $x^i$.  Then the above equation is equivalent to requiring that there is a unique solution to \[MX=0\, .\]

To see if $S$ spans $\Re^n$, we take an arbitrary vector $w$ and solve the linear system
\[
w=x^1v_1+\cdots + x^nv_n
\]
in the unknowns $x^i$.  For this, we need to find a unique solution for the linear system $MX=w$.  

Thus, we need to show that $M^{-1}$ exists, so that 
\[
X=M^{-1}w
\]
is the unique solution we desire.  Then we see that $S$ is a basis for $\mathbb{R}^n$ if and only if $\det M\neq 0$.




\begin{theorem}
Let $S=\{v_1, \ldots, v_m \}$ be a collection of vectors in $\Re^n$.  Let~$M$ be the matrix whose columns are the vectors in $S$.  Then $S$ is a basis for $V$ if and only if $m$ is the dimension of $V$ and 
\[
\det M \neq 0.
\]
\end{theorem}

\begin{remark}
Also observe that  $S$ is a basis if and only if ${\rm RREF}(M)=I$.
\end{remark}

\begin{example}
Let 
\[
S=\left\{ \colvec{1\\0}, \colvec{0\\1} \right\} \text{ and }
T=\left\{ \colvec{1\\1}, \colvec{1\\-1} \right\}.
\]
Then set $M_S=\begin{pmatrix}
1 & 0\\
0 & 1\\
\end{pmatrix}$.  Since $\det M_S=1\neq 0$, then $S$ is a basis for $\Re^2$.\\

\noindent
Likewise, set $M_T=\begin{pmatrix}
1 & 1\\
1 & -1\\
\end{pmatrix}$.  Since $\det M_T=-2\neq 0$, then $T$ is a basis for $\Re^2$.
\end{example}


\section{Matrix of a Linear Transformation (Redux)}\index{Matrix of a linear transformation}

Not only do bases allow us to describe arbitrary vectors as column vectors, they also permit linear transformations
 to be expressed as matrices. This is a very powerful tool for computations, which is covered in chapter~\ref{Matrices} and
 reviewed again here.
 
Suppose we have a linear transformation $L \colon V\rightarrow W$ and 
ordered input and output bases $E=(e_1, \ldots, e_n)$ and $F=(f_1, \ldots, f_m)$ for $V$ and $W$ respectively (of course, these need not be the standard basis--in all likelihood $V$ is {\it not} ${\mathbb R}^n$). 
Since for each $e_j$, $L(e_j)$ is a vector in $W$, there exist unique  numbers~$m^i_j$ such that
\[
L(e_j)=f_1m^1_j + \cdots + f_mm^m_j =(f_1,\ldots, f_m) \ccolvec{m^1_j\\\vdots\\m^m_j}\, .
%= \sum_{i=1}^m f_iM^i_j
%=\rowvec{f_1 & f_2 & \cdots & f_m}\colvec{M^1_j \\[1mm] M^2_j \\[1mm] \vdots \\[1mm] M^m_j}\, .
\]
%We've written the $M^i_j$ on the right side of the $f$'s to agree with our previous notation for matrix multiplication.  
%We have an ``up-hill rule'' where the matching indices for the multiplied objects run up and to the right, like so:~$f_iM^i_j$.
The number $m^i_j$ is the $i$th component of $L(e_j)$ in the basis $F$, while the $f_i$ are vectors (note that if $\alpha$ is a scalar, and $v$ a vector, $\alpha v=v\alpha$, we have used the latter---rather uncommon---notation in the above formula).
The numbers $m^i_j$ naturally form a matrix whose $j$th column is the column vector displayed above. 
%we can see that the $j$th column of $M$ is the coefficients of $L(e_j)$ in the basis $F$.
%The most efficient way to see this is through Einstein notation:
%by linearity, for any vector $v$ in $V$ 
%\begin{eqnarray*}
%Lv=L(e_iv^i)=L(e_i)v^i=f_i M^i_j v^j
%\\
%\end{eqnarray*}
%\begin{eqnarray*}
%L(v) & = & L( v^1e_1 + v^2e_2 + \cdots + v^ne_n) \\[2mm]
%     & = & v^1L(e_1) + v^2L(e_2) + \cdots + v^nL(e_n)\\[2mm]
%     &=&\rowvec{L(e_1) & L(e_2) & \cdots & L(e_n)}\colvec{v^1 \\ v^2 \\ \vdots \\ v^n}\, .
%\end{eqnarray*}
%This is a vector in $W$.  Let's compute its components in $W$.
%
%%%%%%%%%%%%%%%%%%%
Indeed, if $$v=e_1v^1+\cdots+e_n v^n\, ,$$
Then
\begin{eqnarray*}
L(v) & = & L( v^1e_1 + v^2e_2 + \cdots + v^ne_n) \\[1mm]
     & = & v^1L(e_1) + v^2L(e_2) + \cdots + v^nL(e_n) 
     \: = \: \sum_{j=1}^m L(e_j) v^j \\
     & = & \sum_{j=1}^m ( f_1 m^1_j+ \cdots + f_mm^m_j) v^j 
     \: = \: \sum_{i=1}^n f_i \left[ \sum_{j=1}^m M^i_jv^j \right]\\
     &=&\rowvec{f_1 & f_2 & \cdots & f_m}
             \left(\!\begin{array}{cccc}m^1_1&m^1_2&\cdots &m^1_n\\ m^2_1 & m^2_2 && \\
                                              \vdots &&\ddots&\vdots\\ m^m_1 &&\cdots & m^m_n\end{array}\!\right)\colvec{v^1 \\ v^2 \\ \vdots \\ v^n}
\end{eqnarray*}
%The last equality above comes from the definition of matrix multiplication. 
In the column vector-basis notation this equality looks familiar: 
\[
L\ccolvec{v^1\\ \vdots \\ v^n}_E 
%\stackrel{L}{\mapsto}
=
\left(
\left(\!\begin{array}{ccc}
m^1_1 & \ldots & m^1_n \\
\vdots & & \vdots \\
m^m_1 & \ldots & m^m_n \\
\end{array}\!\right)
\ccolvec{v^1\\ \vdots \\ v^n}
\right)_F.
\]
The array of numbers $M=(m^i_j)$ is called the matrix of 
$L$ in the input and output bases $E$ and $F$ for $V$ and $W$, respectively. 
This matrix will change if we change either of the bases. 
Also observe that the columns of $M$ are computed by examining $L$ acting on each basis vector in $V$ expanded in the 
basis vectors of $W$.
%
%An efficient procedure for finding the matrix for a linear operator in particular bases for its domain and target is to calculate the way the linear operator acts on the basis vectors from the domain one at a time. This allows you to read off the columns of the matrix as
%$$
%L(e_j)=
%\rowvec{f_1 & f_2 & \cdots & f_m}\ccolvec{M^1_j \\[1mm] M^2_j \\[1mm] \vdots \\[1mm] M^m_j} .
%$$
\begin{example}

\noindent
Let $L \colon P_1(t) \mapsto P_1(t)$, such that $L(a+bt)=(a+b)t$.  Since $V=P_1(t)=W$, let's choose the same ordered basis $B=(1-t, 1+t )$ for $V$ and $W$.
\begin{eqnarray*}
L(1-t)&=&(1-1)t=\ 0\ =(1-t)\cdot 0 + (1+t)\cdot 0=
\rowvec{1-t, 1+t}\colvec{0\\0} \\[2mm]
L(1+t)&=&(1+1)t=2t\ =(1-t)\cdot -1 + (1+t)\cdot 1=
\rowvec{1-t, 1+t}\colvec{-1\\1}\\[2mm]
&\Rightarrow& 
L\colvec {a\\b }_B= 
\left( 
\begin{pmatrix}
0 & -1 \\
0 & 1 \\
\end{pmatrix}
\colvec{a\\b}
\right)_B.
\end{eqnarray*}
\end{example}


%
%\begin{example}
%Consider a linear transformation $$L \colon \Re^2\rightarrow \Re^2\, .$$  Suppose we know that $L\colvec{1\\0}=\colvec{a\\c}$ and $L\colvec{0\\1}=\colvec{b\\d}$.  Then, because of linearity, we can determine what $L$ does to any vector $\colvec{x\\y}$:
%
%\[
%L\colvec{x\\y}=L\left(x\colvec{1\\0}+y\colvec{0\\1}\right)=xL\colvec{1\\0}+yL\colvec{0\\1}=x\colvec{a\\c}+y\colvec{b\\d}=\colvec{ax+by\\cx+dy}.
%\]
%Now notice that for any vector $\colvec{x\\y}$, we have 
%
%\[
%\begin{pmatrix}
%a & b \\
%c & d \\
%\end{pmatrix}
%\colvec{x\\y}=\colvec{ax+by\\cx+dy}=L\colvec{x\\y}.
%\]
%Then the matrix $\begin{pmatrix}
%a & b \\
%c & d \\
%\end{pmatrix}$ acts by matrix multiplication in the same way that~$L$ does.  This is the  \emph{matrix of $L$} in the standard (ordered) \emph{basis} $\left(\colvec{1\\0}, \colvec{0\\1} \right)$.
%\end{example}

When the vector space is~${\mathbb R^n}$ and the standard basis is used,  the problem of finding the matrix of a 
linear transformation will seem almost trivial. It is worthwhile working through it once in the above language though.


\begin{example}
Any vector in $\Re^n$ can be written as a linear combination of the \emph{standard (ordered) basis}\index{Standard basis} $(e_1,\dots e_n)$.  
The vector $e_i$ has a one in the $i$th position, and zeros everywhere else.  {\it I.e.}
$$
e_1=\colvec{1\\ 0\\ \vdots \\0}\, ,\quad e_2=\colvec{0\\ 1\\ \vdots \\0}\, ,\ldots,\quad e_n=\colvec{0\\ 0\\ \vdots \\1 }\, .
$$
Then to find the matrix of any linear transformation $L \colon \Re^n \rightarrow \Re^n$, it suffices to know what $L(e_i)$ is for every $i$.  

For any matrix $M$, observe that $Me_i$ is equal to the $i$th column of $M$.  Then if the $i$th column of $M$ equals $L(e_i)$ for every $i$, then $Mv=L(v)$ for every $v\in \Re^n$.  Then the matrix representing $L$ in the standard basis is just the matrix whose $i$th column is~$L(e_i)$. 

For example, if 
$$
L\colvec{1\\0\\0}=\colvec{1\\4\\7}\, ,\quad
L\colvec{0\\1\\0}=\colvec{2\\5\\8}\, ,\quad
L\colvec{0\\0\\1}=\colvec{3\\6\\9}\, ,
$$
then the matrix of $L$ in the standard basis is simply
$$
\begin{pmatrix}1&2&3\\4&5&6\\7&8&9\end{pmatrix}\, .
$$
Alternatively, this information would often be presented as
$$
L\colvec{x\\y\\z}=\ccolvec{x+2y+3z\\4x+5y+6z\\7x+8y+9z}\, .
$$
You could either rewrite this  as 
$$
L\colvec{x\\y\\z}=\begin{pmatrix}1&2&3\\4&5&6\\7&8&9\end{pmatrix}\colvec{x\\y\\z}\, ,
$$
to immediately learn the matrix of $L$, or taking a more circuitous route:
\begin{eqnarray*}
L\colvec{x\\y\\z}&=&L\left[x\colvec{1\\0\\0}+y\colvec{0\\0\\1}+z
\colvec{0\\0\\1}\right]\\[2mm]&=&
x\colvec{1\\4\\7}+y\colvec{2\\5\\8}+z
\colvec{3\\6\\9}\:=\:\begin{pmatrix}1&2&3\\4&5&6\\7&8&9\end{pmatrix}\colvec{x\\y\\z}\, .
\end{eqnarray*}
\end{example}

%\section*{References}
%Hefferon, Chapter Two, Section II: Linear Independence
%\\
%Hefferon, Chapter Two, Section III.1: Basis
%\\
%Beezer, Chapter VS, Section B, Subsections B-BNM
%\\
%Beezer, Chapter VS, Section D, Subsections D-DVS
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Linear_independence}{Linear Independence}
%\item \href{http://en.wikipedia.org/wiki/Basis_(linear_algebra)}{Basis}
%\end{itemize}
%

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{BasisAndDimension}{1},\hwrref{BasisAndDimension}{2}\\
Basis checks&  \hwref{BasisAndDimension}{3},\hwref{BasisAndDimension}{4}\\
 Computing column vectors &  \hwref{BasisAndDimension}{5},\hwref{BasisAndDimension}{6}\\
  \hline
\end{tabular}
\input{\basisDimPath/problems}

\newpage

","\chapter{\basisDimTitle}\label{basisdimension}

\label{sec:dimension}
\label{dimension}
In chapter~\ref{linearind}, the notions of   a linearly independent set of vectors in a vector space $V$, and of a set of vectors that span $V$ were established; any set of vectors that span $V$ can be reduced to some minimal collection of linearly independent vectors; such a minimal set is called a \emph{basis} of the subspace $V$.  

\begin{definition}
Let $V$ be a vector space.  Then a set $S$ is a {\bf basis}\index{Basis} for $V$ if $S$ is linearly independent and $V=\spa S$.


If $S$ is a basis of $V$ and $S$ has only finitely many elements, then we say that $V$ is {\bf finite-dimensional}.  The number of vectors in $S$ is the {\bf dimension}\index{Dimension} of~$V$.
\end{definition}

Suppose $V$ is a \emph{finite-dimensional}\index{Vector space!finite dimensional} vector space, and $S$ and $T$ are two different bases for $V$.  One might worry that $S$ and $T$ have a different number of vectors; then we would have to talk about the dimension of $V$ in terms of the basis $S$ or in terms of the basis $T$.  Luckily this isn't what happens.
Later in this chapter, we will show that $S$ and $T$ must have the same number of vectors.  This means that the dimension of a vector space is basis-independent.  In fact, dimension is a very important  characteristic of a vector space.% $V$.

\begin{example}
$P_n(t)$ (polynomials in $t$ of degree $n$ or less) has a basis $\{1,t,\ldots , t^n \}$, since every vector in this space is a sum
\[
a^0\,1+a^1\,t+\cdots +a^n\,t^n, \qquad a^i\in \Re\, ,
\]
so $P_n(t)=\spa \{1,t,\ldots , t^n \}$.  This set of vectors is linearly independent;  If the polynomial $p(t)=c^01+c^1t+\cdots +c^nt^n=0$, then $c^0=c^1=\cdots =c^n=0$, so $p(t)$ is the zero polynomial.  
Thus $P_n(t)$ is finite dimensional, and $\dim P_n(t)=n+1$.
\end{example}



\begin{theorem}\label{uniqvec}
Let $S=\{v_1, \ldots, v_n \}$  be a basis for a vector space $V$.  Then every vector $w \in V$ can be written \emph{uniquely} as a linear combination of vectors in the basis $S$:
\[
w=c^1v_1+\cdots + c^nv_n.
\]
\end{theorem}

\begin{proof}
Since $S$ is a basis for $V$, then $\spa S=V$, and so there exist constants~$c^i$ such that $w=c^1v_1+\cdots + c^nv_n$.

Suppose there exists a second set of constants $d^i$ such that 
$$w=d^1v_1+\cdots + d^nv_n\, .$$  Then
\begin{eqnarray*}
0_V&=&w-w\\
&=&c^1v_1+\cdots + c^nv_n-d^1v_1-\cdots - d^nv_n \\[1mm]
&=&(c^1-d^1)v_1+\cdots + (c^n-d^n)v_n. \\
\end{eqnarray*}
If it occurs exactly once that $c^i\neq d^i$, then the equation reduces to $0=(c^i-d^i)v_i$, which is a contradiction since the vectors $v_i$ are assumed to be non-zero.

If we have more than one $i$ for which $c^i\neq d^i$, we can use this last equation to write one of the vectors in $S$ as a linear combination of other vectors in $S$, which contradicts the assumption that $S$ is linearly independent.  Then for every $i$, $c^i=d^i$.
\end{proof}

\Videoscriptlink{basis_and_dimension_thm.mp4}{Proof Explanation}{basis_and_dimension_thm}

\begin{remark}
This theorem is the one that makes bases so useful--they allow us to convert abstract vectors into column vectors.
By ordering the set $S$ we obtain $B=(v_1,\ldots,v_n)$ and can write
$$
w=(v_1,\ldots,v_n) \ccolvec{c^1\\ \vdots\\ c^n }=\ccolvec{c^1\\ \vdots\\ c^n }_B\, .
$$
Remember that in general it makes no sense to drop the subscript $B$ on the column vector on the right--most vector spaces  are not made from  columns of numbers!
\end{remark}

\Videoscriptlink{eigenvectors_and_eigenvalues_matrix.mp4}{Worked Example}{scripts_eigenvalseigenvects_matrix}

Next, we would like to establish a method for determining whether a collection of vectors forms a basis for $\Re^n$.  But first, we need to show that any two bases for a finite-dimensional vector space has the same number of vectors.

\begin{lemma}\label{mlessn}
If $S=\{v_1, \ldots, v_n \}$ is a basis for a vector space $V$ and $T=\{w_1, \ldots, w_m \}$ is a linearly independent set of vectors in $V$, then $m\leq n$.
\end{lemma}

%\begin{figure}
%\begin{center}
%\includegraphics[scale=.28]{\basisDimPath/indep_span.jpg}
%\end{center}
%\end{figure}

The idea of the proof is to start with the set $S$ and replace vectors in $S$ one at a time with vectors from $T$, such that after each replacement we still have a basis for $V$.

%\begin{center}\href{\webworkurl ReadingHomework17/1/}{Reading homework: problem \ref{basisdimension}.1}\end{center}
\Reading{BasisAndDimension}{1}

\begin{proof}
Since $S$ spans $V$, then the set $\{w_1, v_1, \ldots, v_n \}$ is linearly dependent.  Then we can write $w_1$ as a linear combination of the $v_i$; using that equation, we can express one of the $v_i$ in terms of $w_1$ and the remaining $v_j$ with~$j\neq i$.  Then we can discard one of the $v_i$ from this set to obtain a linearly independent set that still spans $V$.  Now we need to prove that $S_1$ is a basis; we must show that $S_1$ is linearly independent and that $S_1$ spans $V$.

The set $S_1=\{w_1, v_1, \ldots, v_{i-1}, v_{i+1},\ldots, v_n \}$ is linearly independent:  By the previous theorem, there was a unique way to express $w_1$ in terms of the set~$S$.  Now, to obtain a contradiction, suppose there is some $k$ and constants~$c^i$ such that
\[
v_k = c^0w_1+c^1v_1+\cdots + c^{i-1}v_{i-1} + c^{i+1}v_{i+1} + \cdots + c^nv_n.
\]
Then replacing $w_1$ with its expression in terms of the collection $S$ gives a way to express the vector $v_k$ as a linear combination of the vectors in $S$, which contradicts the linear independence of $S$.  On the other hand, we cannot express $w_1$ as a linear combination of the vectors in $\{v_j | j\neq i\}$, since the expression of $w_1$ in terms of $S$ was unique, and had a non-zero coefficient for the vector $v_i$.  Then no vector in $S_1$ can be expressed as a combination of other vectors in $S_1$, which demonstrates that $S_1$ is linearly independent.

The set $S_1$ spans $V$:  For any $u\in V$, we can express $u$ as a linear combination of vectors in $S$.  But we can express $v_i$ as a linear combination of vectors in the collection $S_1$; rewriting $v_i$ as such allows us to express $u$ as a linear combination of the vectors in $S_1$. Thus $S_1$ is a basis of $V$ with $n$ vectors.

We can now iterate this process, replacing one of the $v_i$ in $S_1$ with $w_2$, and so on.  If $m\leq n$, this process ends with the set $S_m=\{w_1,\ldots, w_m$, $v_{i_1},\ldots,v_{i_{n-m}}  \}$, which is fine.

Otherwise, we have $m>n$, and the set $S_n=\{w_1,\ldots, w_n \}$ is a basis for~$V$.  But we still have some vector 
$w_{n+1}$  in $T$ that is not in $S_n$.  Since $S_n$ is a basis, we can write $w_{n+1}$ as a combination of the vectors in $S_n$, which contradicts the linear independence of the set $T$.  Then it must be the case that $m\leq n$, as desired.
\end{proof}

\Videoscriptlink{basis_and_dimension_example.mp4}{Worked Example}{basis_and_dimension_example}

\begin{corollary}\label{corsame}
For a finite-dimensional vector space $V$, any two bases for~$V$ have the same number of vectors.
\end{corollary}

\begin{proof}
Let $S$ and $T$ be two bases for $V$.  Then both are linearly independent sets that span $V$.  Suppose $S$ has $n$ vectors and $T$ has $m$ vectors.  Then by the previous lemma, we have that $m\leq n$.  But (exchanging the roles of $S$ and $T$ in application of the lemma) we also see that $n\leq m$.  Then $m=n$, as desired.
\end{proof}

%\begin{center}\href{\webworkurl ReadingHomework17/2/}{Reading homework: problem \ref{basisdimension}.2}\end{center}
\Reading{BasisAndDimension}{2}

\section{Bases in $\Re^n$.}

In review question~\ref{stdbasis}, chapter~\ref{linearind} you checked that
\[
\Re^n = \spa \left\{ \colvec{1\\0\\ \vdots \\ 0}, 
\colvec{0\\1\\ \vdots \\ 0}, \ldots, \colvec{0\\0\\ \vdots \\ 1}\right\},
\]
and that this set of vectors is linearly independent. (If you didn't do that problem, check this before reading any further!)  So this set of vectors is a basis for~$\Re^n$, and $\dim \Re^n=n$.  This basis is often called the \emph{standard} or \emph{canonical basis}\index{Standard basis}\index{Canonical basis|seealso{Standard basis}} for $\Re^n$.  The vector with a one in the $i$th position and zeros everywhere else is written 
$e_i$. (You could also view it as the function $\{1,2,\ldots,n\}\to {\mathbb R}$ where $e_i(j)=1$ if $i=j$ and $0$ if $i\neq j$.)  It points in the direction of the $i$th coordinate axis, and has unit length.  In multivariable calculus classes, this basis is often written $\{ \hat i,\hat j,\hat k \}$ for $\Re^3$. 

%\begin{figure}
%\begin{center}
%\includegraphics[scale=.32]{\basisDimPath/canonical.jpg}
%\end{center}
%\end{figure}

Note that it is often convenient to order  basis elements, so rather than writing a set
of vectors, we would write a list. This is called an ordered basis. For example, the canonical ordered basis for ${\mathbb R^n}$ is $(e_1,e_2,\ldots,e_n)$. The possibility to reorder basis vectors is not the only way in which bases are non-unique.

\begin{remark}[Bases are not unique.]
While there exists a unique way to express a vector in terms of any particular basis, bases themselves are far from unique.
For example, both of the sets 
\[
\left\{ \colvec{1\\0}, \colvec{0\\1} \right\} \text{ and }
\left\{ \colvec{1\\1}, \colvec{1\\-1} \right\}
\]
are bases for $\Re^2$.  Rescaling any vector in one of these sets is already enough to show that~$\Re^2$ has infinitely many bases.  But even if we require that all of the basis vectors have unit length, it turns out that there are still infinitely many bases for $\Re^2$ (see review question~\ref{lotsofbases}).
\end{remark}


To see whether a set of vectors $S=\{v_1, \ldots, v_m \}$ is a basis for~$\Re^n$, we have to check that the elements  are linearly independent and that they span~$\Re^n$.  From the previous discussion, we also know that $m$ must equal $n$, so lets assume~$S$ has $n$ vectors.
If $S$ is linearly independent, then there is no non-trivial solution of the equation
\[
0 = x^1v_1+\cdots + x^nv_n.
\]
Let $M$ be a matrix whose columns are the vectors $v_i$ and $X$ the column vector with entries $x^i$.  Then the above equation is equivalent to requiring that there is a unique solution to \[MX=0\, .\]

To see if $S$ spans $\Re^n$, we take an arbitrary vector $w$ and solve the linear system
\[
w=x^1v_1+\cdots + x^nv_n
\]
in the unknowns $x^i$.  For this, we need to find a unique solution for the linear system $MX=w$.  

Thus, we need to show that $M^{-1}$ exists, so that 
\[
X=M^{-1}w
\]
is the unique solution we desire.  Then we see that $S$ is a basis for $\mathbb{R}^n$ if and only if $\det M\neq 0$.




\begin{theorem}
Let $S=\{v_1, \ldots, v_m \}$ be a collection of vectors in $\Re^n$.  Let~$M$ be the matrix whose columns are the vectors in $S$.  Then $S$ is a basis for $V$ if and only if $m$ is the dimension of $V$ and 
\[
\det M \neq 0.
\]
\end{theorem}

\begin{remark}
Also observe that  $S$ is a basis if and only if ${\rm RREF}(M)=I$.
\end{remark}

\begin{example}
Let 
\[
S=\left\{ \colvec{1\\0}, \colvec{0\\1} \right\} \text{ and }
T=\left\{ \colvec{1\\1}, \colvec{1\\-1} \right\}.
\]
Then set $M_S=\begin{pmatrix}
1 & 0\\
0 & 1\\
\end{pmatrix}$.  Since $\det M_S=1\neq 0$, then $S$ is a basis for $\Re^2$.\\

\noindent
Likewise, set $M_T=\begin{pmatrix}
1 & 1\\
1 & -1\\
\end{pmatrix}$.  Since $\det M_T=-2\neq 0$, then $T$ is a basis for $\Re^2$.
\end{example}


\section{Matrix of a Linear Transformation (Redux)}\index{Matrix of a linear transformation}

Not only do bases allow us to describe arbitrary vectors as column vectors, they also permit linear transformations
 to be expressed as matrices. This is a very powerful tool for computations, which is covered in chapter~\ref{Matrices} and
 reviewed again here.
 
Suppose we have a linear transformation $L \colon V\rightarrow W$ and 
ordered input and output bases $E=(e_1, \ldots, e_n)$ and $F=(f_1, \ldots, f_m)$ for $V$ and $W$ respectively (of course, these need not be the standard basis--in all likelihood $V$ is {\it not} ${\mathbb R}^n$). 
Since for each $e_j$, $L(e_j)$ is a vector in $W$, there exist unique  numbers~$m^i_j$ such that
\[
L(e_j)=f_1m^1_j + \cdots + f_mm^m_j =(f_1,\ldots, f_m) \ccolvec{m^1_j\\\vdots\\m^m_j}\, .
%= \sum_{i=1}^m f_iM^i_j
%=\rowvec{f_1 & f_2 & \cdots & f_m}\colvec{M^1_j \\[1mm] M^2_j \\[1mm] \vdots \\[1mm] M^m_j}\, .
\]
%We've written the $M^i_j$ on the right side of the $f$'s to agree with our previous notation for matrix multiplication.  
%We have an ``up-hill rule'' where the matching indices for the multiplied objects run up and to the right, like so:~$f_iM^i_j$.
The number $m^i_j$ is the $i$th component of $L(e_j)$ in the basis $F$, while the $f_i$ are vectors (note that if $\alpha$ is a scalar, and $v$ a vector, $\alpha v=v\alpha$, we have used the latter---rather uncommon---notation in the above formula).
The numbers $m^i_j$ naturally form a matrix whose $j$th column is the column vector displayed above. 
%we can see that the $j$th column of $M$ is the coefficients of $L(e_j)$ in the basis $F$.
%The most efficient way to see this is through Einstein notation:
%by linearity, for any vector $v$ in $V$ 
%\begin{eqnarray*}
%Lv=L(e_iv^i)=L(e_i)v^i=f_i M^i_j v^j
%\\
%\end{eqnarray*}
%\begin{eqnarray*}
%L(v) & = & L( v^1e_1 + v^2e_2 + \cdots + v^ne_n) \\[2mm]
%     & = & v^1L(e_1) + v^2L(e_2) + \cdots + v^nL(e_n)\\[2mm]
%     &=&\rowvec{L(e_1) & L(e_2) & \cdots & L(e_n)}\colvec{v^1 \\ v^2 \\ \vdots \\ v^n}\, .
%\end{eqnarray*}
%This is a vector in $W$.  Let's compute its components in $W$.
%
%%%%%%%%%%%%%%%%%%%
Indeed, if $$v=e_1v^1+\cdots+e_n v^n\, ,$$
Then
\begin{eqnarray*}
L(v) & = & L( v^1e_1 + v^2e_2 + \cdots + v^ne_n) \\[1mm]
     & = & v^1L(e_1) + v^2L(e_2) + \cdots + v^nL(e_n) 
     \: = \: \sum_{j=1}^m L(e_j) v^j \\
     & = & \sum_{j=1}^m ( f_1 m^1_j+ \cdots + f_mm^m_j) v^j 
     \: = \: \sum_{i=1}^n f_i \left[ \sum_{j=1}^m M^i_jv^j \right]\\
     &=&\rowvec{f_1 & f_2 & \cdots & f_m}
             \left(\!\begin{array}{cccc}m^1_1&m^1_2&\cdots &m^1_n\\ m^2_1 & m^2_2 && \\
                                              \vdots &&\ddots&\vdots\\ m^m_1 &&\cdots & m^m_n\end{array}\!\right)\colvec{v^1 \\ v^2 \\ \vdots \\ v^n}
\end{eqnarray*}
%The last equality above comes from the definition of matrix multiplication. 
In the column vector-basis notation this equality looks familiar: 
\[
L\ccolvec{v^1\\ \vdots \\ v^n}_E 
%\stackrel{L}{\mapsto}
=
\left(
\left(\!\begin{array}{ccc}
m^1_1 & \ldots & m^1_n \\
\vdots & & \vdots \\
m^m_1 & \ldots & m^m_n \\
\end{array}\!\right)
\ccolvec{v^1\\ \vdots \\ v^n}
\right)_F.
\]
The array of numbers $M=(m^i_j)$ is called the matrix of 
$L$ in the input and output bases $E$ and $F$ for $V$ and $W$, respectively. 
This matrix will change if we change either of the bases. 
Also observe that the columns of $M$ are computed by examining $L$ acting on each basis vector in $V$ expanded in the 
basis vectors of $W$.
%
%An efficient procedure for finding the matrix for a linear operator in particular bases for its domain and target is to calculate the way the linear operator acts on the basis vectors from the domain one at a time. This allows you to read off the columns of the matrix as
%$$
%L(e_j)=
%\rowvec{f_1 & f_2 & \cdots & f_m}\ccolvec{M^1_j \\[1mm] M^2_j \\[1mm] \vdots \\[1mm] M^m_j} .
%$$
\begin{example}

\noindent
Let $L \colon P_1(t) \mapsto P_1(t)$, such that $L(a+bt)=(a+b)t$.  Since $V=P_1(t)=W$, let's choose the same ordered basis $B=(1-t, 1+t )$ for $V$ and $W$.
\begin{eqnarray*}
L(1-t)&=&(1-1)t=\ 0\ =(1-t)\cdot 0 + (1+t)\cdot 0=
\rowvec{1-t, 1+t}\colvec{0\\0} \\[2mm]
L(1+t)&=&(1+1)t=2t\ =(1-t)\cdot -1 + (1+t)\cdot 1=
\rowvec{1-t, 1+t}\colvec{-1\\1}\\[2mm]
&\Rightarrow& 
L\colvec {a\\b }_B= 
\left( 
\begin{pmatrix}
0 & -1 \\
0 & 1 \\
\end{pmatrix}
\colvec{a\\b}
\right)_B.
\end{eqnarray*}
\end{example}


%
%\begin{example}
%Consider a linear transformation $$L \colon \Re^2\rightarrow \Re^2\, .$$  Suppose we know that $L\colvec{1\\0}=\colvec{a\\c}$ and $L\colvec{0\\1}=\colvec{b\\d}$.  Then, because of linearity, we can determine what $L$ does to any vector $\colvec{x\\y}$:
%
%\[
%L\colvec{x\\y}=L\left(x\colvec{1\\0}+y\colvec{0\\1}\right)=xL\colvec{1\\0}+yL\colvec{0\\1}=x\colvec{a\\c}+y\colvec{b\\d}=\colvec{ax+by\\cx+dy}.
%\]
%Now notice that for any vector $\colvec{x\\y}$, we have 
%
%\[
%\begin{pmatrix}
%a & b \\
%c & d \\
%\end{pmatrix}
%\colvec{x\\y}=\colvec{ax+by\\cx+dy}=L\colvec{x\\y}.
%\]
%Then the matrix $\begin{pmatrix}
%a & b \\
%c & d \\
%\end{pmatrix}$ acts by matrix multiplication in the same way that~$L$ does.  This is the  \emph{matrix of $L$} in the standard (ordered) \emph{basis} $\left(\colvec{1\\0}, \colvec{0\\1} \right)$.
%\end{example}

When the vector space is~${\mathbb R^n}$ and the standard basis is used,  the problem of finding the matrix of a 
linear transformation will seem almost trivial. It is worthwhile working through it once in the above language though.


\begin{example}
Any vector in $\Re^n$ can be written as a linear combination of the \emph{standard (ordered) basis}\index{Standard basis} $(e_1,\dots e_n)$.  
The vector $e_i$ has a one in the $i$th position, and zeros everywhere else.  {\it I.e.}
$$
e_1=\colvec{1\\ 0\\ \vdots \\0}\, ,\quad e_2=\colvec{0\\ 1\\ \vdots \\0}\, ,\ldots,\quad e_n=\colvec{0\\ 0\\ \vdots \\1 }\, .
$$
Then to find the matrix of any linear transformation $L \colon \Re^n \rightarrow \Re^n$, it suffices to know what $L(e_i)$ is for every $i$.  

For any matrix $M$, observe that $Me_i$ is equal to the $i$th column of $M$.  Then if the $i$th column of $M$ equals $L(e_i)$ for every $i$, then $Mv=L(v)$ for every $v\in \Re^n$.  Then the matrix representing $L$ in the standard basis is just the matrix whose $i$th column is~$L(e_i)$. 

For example, if 
$$
L\colvec{1\\0\\0}=\colvec{1\\4\\7}\, ,\quad
L\colvec{0\\1\\0}=\colvec{2\\5\\8}\, ,\quad
L\colvec{0\\0\\1}=\colvec{3\\6\\9}\, ,
$$
then the matrix of $L$ in the standard basis is simply
$$
\begin{pmatrix}1&2&3\\4&5&6\\7&8&9\end{pmatrix}\, .
$$
Alternatively, this information would often be presented as
$$
L\colvec{x\\y\\z}=\ccolvec{x+2y+3z\\4x+5y+6z\\7x+8y+9z}\, .
$$
You could either rewrite this  as 
$$
L\colvec{x\\y\\z}=\begin{pmatrix}1&2&3\\4&5&6\\7&8&9\end{pmatrix}\colvec{x\\y\\z}\, ,
$$
to immediately learn the matrix of $L$, or taking a more circuitous route:
\begin{eqnarray*}
L\colvec{x\\y\\z}&=&L\left[x\colvec{1\\0\\0}+y\colvec{0\\0\\1}+z
\colvec{0\\0\\1}\right]\\[2mm]&=&
x\colvec{1\\4\\7}+y\colvec{2\\5\\8}+z
\colvec{3\\6\\9}\:=\:\begin{pmatrix}1&2&3\\4&5&6\\7&8&9\end{pmatrix}\colvec{x\\y\\z}\, .
\end{eqnarray*}
\end{example}

%\section*{References}
%Hefferon, Chapter Two, Section II: Linear Independence
%\\
%Hefferon, Chapter Two, Section III.1: Basis
%\\
%Beezer, Chapter VS, Section B, Subsections B-BNM
%\\
%Beezer, Chapter VS, Section D, Subsections D-DVS
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Linear_independence}{Linear Independence}
%\item \href{http://en.wikipedia.org/wiki/Basis_(linear_algebra)}{Basis}
%\end{itemize}
%

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{BasisAndDimension}{1},\hwrref{BasisAndDimension}{2}\\
Basis checks&  \hwref{BasisAndDimension}{3},\hwref{BasisAndDimension}{4}\\
 Computing column vectors &  \hwref{BasisAndDimension}{5},\hwref{BasisAndDimension}{6}\\
  \hline
\end{tabular}
\input{\basisDimPath/problems}

\newpage

",lesson
19,Eigenvalues And Eigenvectors,"

\chapter{\eigenTitle}
\label{eigenvalseigenvects}

In a vector space with no structure other than the vector space rules, no vector other than the zero vector  is any  more important than any other.
Once one also has a linear transformation the situation changes dramatically. We begin with a fun example, of a type bound to reappear in your future scientific studies:

\begin{example}[String Theory]

Consider a vibrating string, 
whose displacement at point $x$ at time $t$ is given by a function $y(x,t)$:
\begin{center}
\includegraphics[scale=.3]{string.jpg}
\end{center}
The set of all displacement functions for the string 
can be modeled by 
a vector space 
$$V=\left\{ y:\mathbb{R}^2 \to \mathbb{R} \middle| \mbox{\rm all partial derivatives } \frac{\partial^{k+m}y(x,t)}{\partial x^k\partial t^m} \mbox{ exist}\right\}.$$ 
%The reason for the condition that the second derivatives exist is 
 The concavity and 
 the acceleration of the string at the point $(x,t)$ are 
 $\frac{\partial^2y}{\partial x^2}(x,t)$ and $\frac{\partial^2y}{\partial t^2}(x,t)$ respectively. 
 Since quantities must exist at each point on the string for the wave equation to make sense, 
 we required that all partial derivatives of $y(x,t)$ exist.
 Note also that the function $y(x,t)=0$~---drawn in grey---is the only special vector in the vector space~$V$. 

%
We now add some extra information.
The string's behavior in time and space can be modeled by a wave equation\index{Wave equation}
$$\frac{\partial^2 y}{\partial t^2}=\frac{\partial^2 y}{\partial x^2}\, ,
$$
which says that the acceleration of a point on the string is equal its concavity at that point. For example, if the string were made of stretched rubber, it would 
prefer to be in a straight line, so this equation makes good intuitive sense. 
Not all of the functions in $V$ are solutions to the wave equation; not all of the functions in the vector space $V$ describe the way a string would really vibrate.  The ways a string  would really  vibrate are (at least approximately) solutions to the wave equation above, which can rewritten as a linear function
$$Wy=0$$
where
$$
W=\left(-\frac{\partial^2 }{\partial t^2}+\frac{\partial^2 }{\partial x^2}\right):V\rightarrow V\, .
$$
Some examples of solutions are 
$$
y_1(x,t)=\sin (t) \sin (x)\, \quad
y_2(x,t)=3\sin (2t) \sin (2x)\, $$
and
$$
y_3(x,t)=\sin (t) \sin (x)+3\sin (2t) \sin (2x)\, .
$$
Since $Wy=0$ is a homogeneous linear equation, linear combinations of solutions are solutions; in other words the kernel $\ker(w)$ is a vector space.
Given the linear function $W$, some vectors are now more special than others.

We can use musical intuition to do more! If the ends of the string were held fixed, we suspect that it would prefer to vibrate at certain frequencies corresponding to musical notes.
This is modeled by looking at solutions of the form
$$
y(x,t)=\sin(\omega t) v(x)\, .
$$
Here the periodic sine function accounts for the string's vibratory motion, while the function $v(x)$ gives the shape of the string at any fixed instant of time. Observe that
$$
W\big(\sin(\omega t) v(x)\big)=\sin(\omega t)\big(\frac{d^2f}{dx^2}+\omega^2 f\big)\, .
$$
This suggests we introduce a new vector space $$U=\left\{v:{\mathbb R}\to {\mathbb R}\, \middle| \, \mbox{\rm all derivatives } \frac{d^kf}{d x^k }\text{~exist~} \right\}\, ,
$$
as well as a new linear function
$$
L:=\frac{d^2}{dx^2}: U\longrightarrow U\, .
$$
The number $\omega$ is called an angular frequency in many contexts, lets call its square $\lambda:=-\omega^2$ to match notations we will use later (notice that for this particular problem $\lambda$ must then  be negative).
Then, because we want $W(y)=0$, which implies $d^2 f/dx^2=\omega^2 f$, it follows that the vector $v(x)\in U$ determining the vibrating string's shape obeys
$$
L(v)=\lambda v\, .
$$
This is perhaps one of the most important equations in all of linear algebra! It is the eigenvalue-eigenvector equation. In this problem we have to solve it both for $\lambda$, to determine which frequencies (or musical notes) our string likes to sing, and the vector $v$ determining the string's shape. The vector $v$ is called an eigenvector and $\lambda$ its corresponding eigenvalue.
%We can incorporate a realistic complication into our model of the string: 
%an additional force may be pulling the string toward the rest position with a Hooke's law type force. In this case there are two causes for acceleration of a point on the string: the concavity and  Hooke's force.  
%In this case the acceleration of the string at each point will be the sum of the acceleration due to its concavity and the acceleration due to gravity $g$;
%$$\frac{\partial^2 y}{\partial t^2}=\frac{\partial^2 y}{\partial x^2}+ky\, ,
%$$
%or, written as a linear equation 
%$$Ly=ky.$$
%The solution set this of  equation will be different for every different value of the spring constant $k$. 
The solution sets for each $\lambda$ are called $V_\lambda$. For any $\lambda$ the set $V_k$ is a vector space since elements of this set are solutions to the homogeneous equation $(L-\lambda)v=0.$ 



\end{example}

We began this chapter by stating ``In a vector space, with no other structure, no vector is more important than any other."" Our aim is  to show you that when a linear operator $L$ acts on a vector space, vectors that   solve the equation    $L(v)=\lambda v$ play a central role.

% with the largest value of k are more important than those for smaller values of $k$. 
%You might ask ``more important in what sense?"" 
%In the sense that the function $L$ can be built out of the various values of $k$ and the vectors in $V_k$ and the biggest part of that construction is the part with the biggest values of $k$. In particular for some vectors $v_k\in V_k$
%$$L=\sum_{k} k \, v_k \,v_k^t.$$
%We are aware that we have made the startling suggestion that a derivative operator is a linear combination of products of vectors, and we hope that you are intrigued. Lets now return to the simpler case of linear functions which are matrices to develop this idea.




%Before discussing eigenvalues and eigenvectors, we need to understand very well  the relationship between linear transformations and matrices. Let us review the key ideas.
%Consider, as an example the plane ${\mathbb R}^2$
%\begin{center}
%\includegraphics[scale=.3]{\eigenPath/plane.jpg}
%\end{center}
%The  vector $v$ can be described in many ways. 
%Two possibilities are the ordered basis $E=(e_1,e_2)$ 
%and the ordered basis
%$F=(f_1,f_2)$; 
%the vector $v$ is described by the ordered pair $(x,y)^T=(2,2)^T$ in the ordered basis $E$ and by the ordered pair $(s,t)^T=(2,1)^T$ in the ordered basis $F$.  
%This can be confusing! The idea to keep firm in your mind is that the vector space and its elements---vectors---are what really ``exist''. 
%The pairs of numbers are really just different shorthand notations for the same vector. 
%\hyperlink{Basis notation}{Previously} we denoted the correspondence between pairs of numbers and vectors by labeling the pairs of numbers with the basis in which to interpret the numbers;
%$$
%v= \colvec{ 2\\2 }_E :=2e_1+2e_2, ~v= \colvec{2\\1}_F := 2f_1+1f_2\, .
%$$
%
%There is an intuitive reason for the fact that there are many ways to describe the same vector;
%typically, you will use vectors to describe configurations of the real world systems, but the coordinate system you use is your choice. 
%Things like coordinate axes and ``components of a vector'' $(x,y)$ are just mathematical tools used to label vectors.
%
%To generalize this discussion, let $V$ and $W$ be vector spaces, with ordered bases $E=(e_1, \ldots, e_n)$ and 
%$F=(f_1, \ldots, f_m)$ respectively.  
%Since these are bases, there exist constants $v^i$ and $w^j$ such that any vectors $v\in V$ and $w\in W$ can be written as:
%\begin{eqnarray*}
%v & = & v^1e_1 \ + v^2e_2 \ + \cdots + v^ne_n \\
%w & = & w^1f_1 + w^2f_2 + \cdots + w^mf_m \\
%\end{eqnarray*}
%We call the coefficients $v^1, \ldots, v^n$ the \emph{components}\index{Components of a vector} of $v$ in the ordered basis\footnote{To avoid confusion, it helps to note that components of a vector are labeled by a superscript, while basis vectors are labeled by subscripts. This is part of Einstein's notation convention.} $(e_1, \ldots, e_n)$. It is often convenient to arrange the components $v^i$ in a column vector and the basis vectors in a row vector by writing
%\[
%v=\rowvec{e_1 & e_2 & \cdots & e_n}\colvec{v^1 \\ v^2 \\ \vdots \\  v^n}\, .
%\]
%\videoscriptlink{eigenvectors_and_eigenvalues_matrix.mp4}{Worked Example}{scripts_eigenvalseigenvects_matrix}
%
%For what follows it is often convenient follow this order and put the scalar coefficients to the right of basis vectors as in 
%$$v  =  e_1v^1 \ + e_2v^2 \ + \cdots + e_nv^n .$$ 
%
%
%
%\begin{example}
%Consider the ordered basis $B=(1-t, 1+t )$ for the vector space of polynomials of order 1 or lower $P_1(t)$.  The vector $v=2t$ has components $v^1=-1, v^2=1$ in the ordered basis $B$ because 
%\[v=(1-t)(-1) + (1+t)1=\rowvec{1-t & 1+t}\colvec{-1 \\[2mm] 1}\, .\]
%\end{example}
%
%When describing general cases the adroit reader will also enjoy the compact Einstein summation notation $v=e_iv^i$.
%
%%We may consider these components as vectors in $\Re^n$ and $\Re^m$:
%%\[
%%\colvec{v^1\\ \vdots \\ v^n}\in \Re^n, \qquad 
%%\colvec{w^1\\ \vdots \\ w^m}\in \Re^m.
%%\]


\section{Invariant Directions}

Have a look at the linear transformation $L$ depicted below:
\begin{center}
\includegraphics[scale=.3]{\eigenPath/invariant.jpg}
\end{center}
It was picked at random by choosing a pair of vectors $L(e_1)$ and $L(e_2)$ as the outputs of $L$ acting on the canonical basis vectors.
Notice how the unit square with a corner at the origin is mapped to a parallelogram.  The second line of the picture 
shows these superimposed on one another. Now look at the second picture on that line. There, two vectors $f_1$ and $f_2$ have been carefully
chosen such that if the inputs into $L$ are in the parallelogram spanned by $f_1$ and $f_2$, the outputs also form a parallelogram with
edges lying along the same two directions. Clearly this is a very special situation that should correspond to  interesting properties of $L$.

Now lets try an explicit example to see if we can achieve the last picture:

\begin{example}
Consider the linear transformation $L$ such that $$L\colvec{1\\0}=\colvec{-4\\-10}\,  \mbox{ and }\, L\colvec{0\\1}=\colvec{3\\7}\, ,$$ so that the matrix of $L$ in the standard basis is $$\begin{pmatrix}
-4 & 3 \\
-10 & 7 \\
\end{pmatrix}\, .$$  Recall that a vector is a direction and a magnitude; $L$ applied to $\colvec{1\\0}$ or $\colvec{0\\1}$ changes both the direction and the magnitude of the vectors given to it.

Notice that $$L\colvec{3\\5}=\colvec{-4\cdot 3+3\cdot 5 \\ -10\cdot 3+7\cdot 5}=\colvec{3\\5}\, .$$  Then $L$ fixes the  direction (and actually also the magnitude) of the vector $v_1=\colvec{3\\5}$.  

%In fact also the vector $v_2=\colvec{1\\2}$ has its direction fixed by $M$ because $Lv_2=2v_2$.



%\begin{center}\href{\webworkurl ReadingHomework18/1/}{Reading homework: problem \ref{eigenvalseigenvects}.1}\end{center}
\Reading{EigenvaluesAndEigenvectors}{1}

Now, notice that any vector with the same direction as $v_1$ can be written as $cv_1$ for some constant $c$.  Then $L(cv_1)=cL(v_1)=cv_1$, so $L$ fixes every vector pointing in the same direction as $v_1$.

Also notice that $$L\colvec{1\\2}=\colvec{-4\cdot 1+3\cdot 2 \\ -10\cdot 1+7\cdot 2}=\colvec{2\\4}=2\colvec{1\\2}\, ,$$ so $L$ fixes the direction of the vector $v_2=\colvec{1\\2}$ but stretches $v_2$ by a factor of $2$.  Now notice that for any constant $c$, $L(cv_2)=cL(v_2)=2cv_2$.  Then $L$ stretches every vector pointing in the same direction as $v_2$ by a factor of $2$.

\end{example}

In short, given a linear transformation $L$ it is sometimes possible to find a vector $v\neq 0$ and constant $\lambda\neq 0$ such that 
$
Lv=\lambda v.
$
\begin{figure}
\begin{center}
\includegraphics[scale=.4]{\eigenPath/eigeneqn.jpg}
\end{center}
\caption{The eigenvalue--eigenvector equation is probably the most important one in linear algebra.}
\end{figure}
We call the direction of the vector $v$ an \emph{invariant direction}\index{Invariant direction}.  In fact, any vector pointing in the same direction also satisfies this  equation because   $L(cv)=cL(v)=\lambda cv$.  
More generally, any {\it non-zero} vector $v$ that solves
$$
L(v)=\lambda v
$$
is called an \emph{eigenvector}\index{Eigenvector} of $L$, and $\lambda$ (which now need not be zero) is an \emph{eigenvalue}\index{Eigenvalue}.  Since the direction is all we really care about here, then any other vector $cv$ (so long as $c\neq 0$) is an equally good choice of eigenvector. Notice that the relation ``\(u\) and \(v\) point in the same direction'' is an equivalence relation.

In our example of the linear transformation $L$ with matrix $$\begin{pmatrix}
-4 & 3 \\
-10 & 7 \\
\end{pmatrix}\, ,$$ we have seen that $L$ enjoys the property of having two invariant directions, represented by eigenvectors $v_1$ and $v_2$ with eigenvalues $1$ and $2$, respectively.  

It would be very convenient if we could write any vector $w$ as a linear combination of $v_1$ and $v_2$.  Suppose $w=rv_1+sv_2$ for some constants $r$ and~$s$.  Then
\[
L(w)=L(rv_1+sv_2)=rL(v_1)+sL(v_2)=rv_1+2sv_2.
\]
Now $L$ just multiplies the number $r$ by $1$ and the number $s$ by $2$.  If we could write this as a matrix, it would look like:
\[
\begin{pmatrix}
1 & 0 \\
0 & 2
\end{pmatrix}\colvec{s\\t}
\]
which is much slicker than the usual scenario  $$L\!\begin{pmatrix}
x\\
y
\end{pmatrix}\!=\!\begin{pmatrix}
\!a&b\! \\
\!c&d\!
\end{pmatrix} \! \!\begin{pmatrix}
x \\
y
\end{pmatrix}\!=\!
\begin{pmatrix}
\!ax+by\! \\
\!cx+dy\!
\end{pmatrix}\, .$$
Here, $s$ and $t$ give the coordinates of $w$ in terms of the vectors $v_1$ and $v_2$.  In the previous example, we multiplied the vector by the matrix $L$ and came up with a complicated expression.  In these coordinates, we see that~$L$ has a very simple \emph{diagonal matrix}, whose diagonal entries are exactly the \emph{eigenvalues} of~$L$.

This process is called \emph{diagonalization}\index{Diagonalization!concept of}. It makes complicated linear systems much easier to analyze.

%\begin{center}\href{\webworkurl ReadingHomework18/2/}{Reading homework: problem \ref{eigenvalseigenvects}.2}\end{center}
\Reading{EigenvaluesAndEigenvectors}{2}

Now that we've seen what eigenvalues and eigenvectors are, there are a number of questions that need to be answered.

\begin{itemize}
\item How do we find eigenvectors and their eigenvalues?
\item How many eigenvalues and (independent) eigenvectors does a given linear transformation have?
\item When can a linear transformation be diagonalized?
\end{itemize}
We will start by trying to find the eigenvectors for a linear transformation.

\Videoscriptlink{eigenvectors_and_eigenvalues_example.mp4}{$2\times 2$ Example}{scripts_eigenvalseigenvects_example}

\begin{example}
Let $L \colon \Re^2\rightarrow \Re^2$ such that $L(x,y)=(2x+2y, 16x+6y)$.  First, we  find the matrix of $L$, this is quickest in the standard basis:

\[
\colvec{x\\y}\stackrel{L}{\longmapsto} \begin{pmatrix}
2 & 2 \\
16 & 6
\end{pmatrix}\colvec{x\\y}.
\]
We want to find an invariant direction $v=\colvec{x\\y}$ such that
\[
Lv=\lambda v
\]
or, in matrix notation,
\begin{equation*}
\begin{array}{lrcl}
&\begin{pmatrix}
2 & 2 \\
16 & 6
\end{pmatrix}\colvec{x\\y}&=&\lambda \colvec{x\\y} \\[6mm]
\Leftrightarrow &\begin{pmatrix}
2 & 2 \\
16 & 6
\end{pmatrix}\colvec{x\\y}&=&\begin{pmatrix}
\lambda & 0 \\
0 & \lambda
\end{pmatrix} \colvec{x\\y} \\[6mm] 
\Leftrightarrow& 
\begin{pmatrix}
\mc{2-\lambda} & \mc{2} \\
\mc{16} & \mc{6-\lambda}
\end{pmatrix}\colvec{x\\y}&=& \colvec{0\\0}\, .
\end{array}
\end{equation*}
This is a homogeneous system, so it only has solutions when the matrix $\begin{pmatrix}
2-\lambda & \mc2 \\
\mc{16} & 6-\lambda
\end{pmatrix}$ is singular.  In other words, 
\begin{equation*}
\begin{array}{lrcl}
&\det \begin{pmatrix}
2-\lambda & \mc2 \\
\mc{16} & 6-\lambda
\end{pmatrix}&=&0 \\[5mm]
\Leftrightarrow& (2-\lambda)(6-\lambda)-32&=&0 \\[2mm]
\Leftrightarrow &\lambda^2-8\lambda-20&=&0\\[2mm]
\Leftrightarrow &(\lambda-10)(\lambda+2)&=&0
\end{array}
\end{equation*}

\hypertarget{characteristic_polynomial}{For any} square $n\times n$ matrix $M$, the polynomial in $\lambda$ given by $$P_M(\lambda)=\det (\lambda I-M)=(-1)^n \det (M-\lambda I)$$ is called the \emph{characteristic polynomial}\index{Characteristic polynomial} of $M$, and its roots are the eigenvalues of $M$.

In this case, we see that $L$ has two eigenvalues, $\lambda_1=10$ and $\lambda_2=-2$.  To find the eigenvectors, we need to deal with these two cases separately.
To do so, we solve the linear system $\begin{pmatrix}
2-\lambda & \mc2 \\
\mc{16} & 6-\lambda
\end{pmatrix}\colvec{x\\y}= \colvec{0\\0}$ with the particular eigenvalue $\lambda$ plugged in to the matrix.

\begin{itemize}
\item[$\underline{\lambda=10}$:]  We solve the linear system
\[
\begin{pmatrix}
-8 & 2 \\
16 & -4
\end{pmatrix}\colvec{x\\y}= \colvec{0\\0}.
\] 
Both equations say that $y=4x$, so any vector $\colvec{x\\4x}$ will do.  Since we only need the direction of the eigenvector, we can pick a value for $x$.  Setting $x=1$ is convenient, and gives the eigenvector $v_1=\colvec{1\\4}$.

\item[$\underline{\lambda=-2}$:]  We solve the linear system
\[
\begin{pmatrix}
4 & 2 \\
16 & 8
\end{pmatrix}\colvec{x\\y}= \colvec{0\\0}.
\] 
Here again both equations agree, because we chose $\lambda$ to make the system singular.  We see that $y=-2x$ works, so we can choose $v_2=\colvec{1\\-2}$.
\end{itemize}

Our process was the following:
\begin{enumerate}
\item Find the characteristic polynomial of the matrix $M$ for $L$, given by\footnote{To save writing many minus signs compute $\det(M-\lambda I)$; which is equivalent if you only need the roots.} $\det (\lambda I-M)$.
\item Find the roots of the characteristic polynomial; these are the eigenvalues of $L$.
\item For each eigenvalue $\lambda_i$, solve the linear system $(M-\lambda_i I)v=0$ to obtain an eigenvector $v$ associated to $\lambda_i$.
\end{enumerate}
\end{example}

\Videoscriptlink{eigenvalues_and_eigenvectors_jordan.mp4}{Jordan block example}{scripts_eigenvalseigenvects_jordan}



%\section*{References}
%Hefferon, Chapter Three, Section III.1: Representing Linear Maps with Matrices
%\\
%Hefferon, Chapter Five, Section II.3: Eigenvalues and Eigenvectors
%\\
%Beezer, Chapter E, Section EE
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Eigenvalue,_eigenvector_and_eigenspace}{Eigen*}
%\item \href{http://en.wikipedia.org/wiki/Characteristic_polynomial}{Characteristic Polynomial}
%\item \href{http://en.wikipedia.org/wiki/Linear_map}{Linear Transformations (and matrices thereof)}
%\end{itemize}





%\section{Review Problems}
%\input{\eigenPath/problems}




\section{The Eigenvalue--Eigenvector Equation}\label{evalsevectsII}

In section~\ref{eigenvalseigenvects}, we developed the idea of eigenvalues and eigenvectors in the case of linear transformations $\Re^2\rightarrow \Re^2$.  In this section, we will develop the idea more generally.

\Videoscriptlink{eigenvalues_and_eigenvectors_ii_lecture.mp4}{Eigenvalues}{scripts_eigenvalues_and_eigenvectors_ii_lecture}

\begin{definition}
If $L \colon V\rightarrow V$ is linear and for some scalar $\lambda$ and $v\neq 0_V$
$$
Lv=\lambda v.
$$
then $\lambda$ is an {\bf eigenvalue}\index{Eigenvalue} of $L$ with {\bf eigenvector}\index{Eigenvector} $v$.

\end{definition}
This equation says that the direction of $v$ is invariant (unchanged) under~$L$.  

Let's try to understand this equation better in terms of matrices.
Let $V$ be a finite-dimensional vector space and let $L \colon V\rightarrow V$.  
If we have a basis for $V$ we can represent $L$ by a square matrix $M$ and find eigenvalues $\lambda$ and associated eigenvectors $v$ by solving the homogeneous system

\[
(M-\lambda I)v=0.
\]
This system has non-zero solutions if and only if the matrix

\[
M-\lambda I
\]
is singular, and so we require that

\[
\det (\lambda I-M) = 0.
\]

The left hand side of this equation is a polynomial in the variable $\lambda$ called the {\bf characteristic polynomial}\index{Characteristic polynomial} $P_M(\lambda)$ of $M$.  For an $n\times n$ matrix, the characteristic polynomial has degree $n$.  Then 
\[
P_M(\lambda) = \lambda^n+c_1\lambda^{n-1}+\cdots+c_n.
\]
Notice that $P_M(0)=\det (-M)=(-1)^n\det M$.\\

Now recall the following.
\begin{theorem} (The {Fundamental Theorem of Algebra}\index{Fundamental theorem of algebra}) 
Any polynomial can be factored into a product of first order polynomials over~$\C$.  
\end{theorem}
This theorem implies that there exists a collection of $n$ complex numbers~$\lambda_i$ (possibly with repetition) such that
\[
P_M(\lambda)=(\lambda-\lambda_1)(\lambda-\lambda_2)\cdots(\lambda-\lambda_n)\: \Longrightarrow\: P_M(\lambda_i)=0.
\]
The eigenvalues $\lambda_i$ of $M$ are exactly the roots of $P_M(\lambda)$.  These eigenvalues could be real or complex or zero, and they need not all be different.  The number of times that any given root $\lambda_i$ appears in the collection of eigenvalues is called its \emph{multiplicity}\index{Eigenvalue!multiplicity of}.

\begin{figure}
\begin{center}
\includegraphics[scale=.3]{\eigenIIPath/charpoly.jpg}
\end{center}
\caption{Don't forget the characteristic polynomial; you will need it to compute eigenvalues.}
\end{figure}


\begin{example}
Let $L$ be the linear transformation $L \colon \Re^3\rightarrow \Re^3$ given by 
$$L\colvec{x\\y\\z}=\ccolvec{2x+y-z\\ x+2y-z\\ -x-y+2z}\, .$$ 
In the standard basis the matrix $M$ representing $L$ has columns $Le_i$ for each $i$, so:

\[
\colvec{x\\y\\z} \stackrel{L}{\longmapsto} 
\begin{pmatrix}
2 & 1 & -1\\
1 & 2 & -1\\
-1 & -1 & 2\\
\end{pmatrix}\colvec{x\\y\\z}.
\]
Then the characteristic polynomial of $L$ is\footnote{It is often easier (and equivalent) to solve $\det (M-\lambda I)=0$.}
\begin{eqnarray*}
P_M(\lambda)&=&\det \begin{pmatrix}
\mc{\lambda - 2} &\mc{ -1} & \mc{1}\\
\mc{-1} &\mc{ \lambda - 2} & \mc{1}\\
\mc{1} & \mc{1} & \mc{\lambda - 2}\\
\end{pmatrix}\\[2mm]
&=& (\lambda - 2)[ (\lambda - 2)^2-1 ] + 
[-(\lambda - 2)-1 ] +
[-(\lambda - 2)-1] \\[2mm]
&=& (\lambda - 1)^2(\lambda - 4)\, .
\end{eqnarray*}
So $L$ has eigenvalues $\lambda_1=1$ (with multiplicity $2$), and $\lambda_2=4$ (with multiplicity $1$).

To find the eigenvectors associated to each eigenvalue, we solve the homogeneous system $(M-\lambda_iI)X=0$ for each $i$.

\begin{itemize}
\item[\underline{$\lambda=4$:}] We set up the augmented matrix for the linear system:
\begin{eqnarray*}
\begin{amatrix}{3}
-2 & 1 &-1 & 0 \\
1 & -2 &-1 & 0 \\
-1 & -1 &-2 & 0 \\
\end{amatrix} 
 & \sim & \begin{amatrix}{3}
1 & -2 &-1 & 0 \\
0 & -3 &-3 & 0 \\
0 & -3 &-3 & 0 \\
\end{amatrix} \\
 & \sim & \begin{amatrix}{3}
1 & 0 & 1 & 0 \\
0 & 1 & 1 & 0 \\
0 & 0 & 0 & 0 \\
\end{amatrix}. \\
\end{eqnarray*}
%So we see that $z=:t$, $y=-t$, and $x=-t$ gives a formula for eigenvectors in terms of the free parameter $t$.  
Any vector of the form $t\colvec{-1\\-1\\1}$ is then an eigenvector with eigenvalue $4$; thus~$L$ leaves a line through the origin invariant.

\item[\underline{$\lambda=1$:}]  Again we set up an augmented matrix and find the solution set:
\begin{eqnarray*}
\begin{amatrix}{3}
1 & 1 &-1 & 0 \\
1 & 1 &-1 & 0 \\
-1 & -1 & 1 & 0 \\
\end{amatrix} 
 & \sim & \begin{amatrix}{3}
1 & 1 &\!\!-1 & 0 \\
0 & 0 &0 & 0 \\
0 & 0 &0 & 0 \\
\end{amatrix}.
\end{eqnarray*}
Then the solution set has two free parameters, $s$ and $t$, such that $z=z=:t$, $y=y=:s$, and $x=-s+t$.  Thus $L$ leaves invariant the set:
\[
\left\{ s\colvec{-1\\1\\0}+t\colvec{1\\0\\1} \middle| s,t\in \Re   \right\}.
\]
This set is a plane through the origin.  So the multiplicity two eigenvalue has two independent eigenvectors, $\colvec{-1\\1\\0}$ and $\colvec{1\\0\\1}$ that determine an invariant plane.
\end{itemize}
\end{example}

\begin{example}
Let $V$ be the vector space of smooth (\textit{i.e.} infinitely differentiable) functions $f \colon \Re\rightarrow \Re$.  Then the derivative is a linear operator $\frac{d}{dx} \colon V\rightarrow V$.  What are the eigenvectors of the derivative?  In this case, we don't have a matrix to work with, so we have to make do.

A function $f$ is an eigenvector of $\frac{d}{dx}$ if there exists some number $\lambda$ such that $$\frac{d}{dx}f=\lambda f\, .$$  An obvious candidate is the exponential function, $e^{\lambda x}$; indeed, $\frac{d}{dx} e^{\lambda x} = \lambda e^{\lambda x}$.
The operator $\frac{d}{dx}$ has an eigenvector $e^{\lambda x}$ for every $\lambda \in \Re$.
%This is actually the all of the eigenvectors for $\frac{d}{dx}$. 
%(This can be proved using the fact that every infinitely differentiable function has a Taylor series with infinite radius of convergence, and then using the Taylor series to show that if two functions are eigenvectors of $\frac{d}{dx}$ with eigenvalues $\lambda$, then they are scalar multiples of each other.)
\end{example}


\section{Eigenspaces}
In the previous example, we found two eigenvectors  $$\colvec{-1\\1\\0} \mbox{ and }\colvec{1\\0\\1}$$ for $L$, both with eigenvalue $1$.  Notice that  $$\colvec{-1\\1\\0} + \colvec{1\\0\\1}=
\colvec{0\\1\\1}$$ is also an eigenvector of $L$ with eigenvalue $1$.  In fact, any linear combination $$r\colvec{-1\\1\\0} + s\colvec{1\\0\\1}$$ of these two eigenvectors will be another eigenvector with the same eigenvalue.  

More generally, let $\{ v_1, v_2, \ldots \}$ be eigenvectors of some linear transformation $L$ with the same eigenvalue $\lambda$.  A \emph{linear combination}\index{Linear combination} of the $v_i$ is given by $c^1v_1+c^2v_2+\cdots$ for some constants $c^1, c^2,\ldots$.  Then
\begin{eqnarray*}
L(c^1v_1+c^2v_2+\cdots) &=& c^1Lv_1+c^2Lv_2+\cdots \text{ by linearity of $L$}\\
&=& c^1\lambda v_1\,+c^2\lambda v_2+\cdots \hspace{.5mm} \text{ since $Lv_i=\lambda v_i$ }\\
&=& \lambda (c^1v_1+c^2v_2+\cdots).
\end{eqnarray*}
So every linear combination of the $v_i$ is an eigenvector of $L$ with the same eigenvalue $\lambda$.
In simple terms, any sum of eigenvectors is again an eigenvector {\it if they share the same eigenvalue}.

The space of all vectors with eigenvalue $\lambda$ is called an {\bf eigenspace}\index{Eigenspace}.  It is, in fact, a vector space contained within the larger vector space $V$.  It contains $0_V$, since $L0_V=0_V=\lambda 0_V$, and is closed under addition and scalar multiplication by the above calculation.  All other vector space properties are inherited from the fact that $V$ itself is a vector space. In other words, the \hyperlink{sst}{subspace theorem} (\ref{subspacetheorem}, chapter~\ref{subspacesspanning}) ensures that $V_\lambda:=\{v\in V|Lv=0\}$ is a subspace of $V$.

%An eigenspace is an example of a \emph{subspace}\index{Subspace!notion of} of $V$, a notion explored in Chapter~\ref{subspacesspanning}.

\Videoscriptlink{eigenvalues_and_eigenvectors_ii_eigenspaces.mp4}{Eigenspaces}{scripts_eigenvalues_eigenvectors_ii_eigenspaces}

%\begin{center}\href{\webworkurl ReadingHomework19/1/}{Reading homework: problem \ref{evalsevectsII}.1}\end{center}
\Reading{EigenvaluesAndEigenvectors}{3}

\begin{center}
\shabox{
{\bf \hyperref[sample2]{You can now attempt the second sample midterm. 
\includegraphics[scale=.15]{midterm.jpg}}}}
\end{center}
%
%\section*{References}
%Hefferon, Chapter Three, Section III.1: Representing Linear Maps with Matrices
%\\
%Hefferon, Chapter Five, Section II.3: Eigenvalues and Eigenvectors
%\\
%Beezer, Chapter E, Section EE
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Eigenvalue,_eigenvector_and_eigenspace}{Eigen*}
%\item \href{http://en.wikipedia.org/wiki/Characteristic_polynomial}{Characteristic Polynomial}
%\item \href{http://en.wikipedia.org/wiki/Linear_map}{Linear Transformations (and matrices thereof)}
%\end{itemize}


\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{EigenvaluesAndEigenvectors}{1}, \hwrref{EigenvaluesAndEigenvectors}{2}, \hwrref{EigenvaluesAndEigenvectors}{3}\\
Characteristic polynomial&  \hwref{EigenvaluesAndEigenvectors}{4}, \hwref{EigenvaluesAndEigenvectors}{5}, \hwref{EigenvaluesAndEigenvectors}{6}\\
Eigenvalues &  \hwref{EigenvaluesAndEigenvectors}{7}, \hwref{EigenvaluesAndEigenvectors}{8}\\
Eigenspaces &  \hwref{EigenvaluesAndEigenvectors}{9}, \hwref{EigenvaluesAndEigenvectors}{10}\\
Eigenvectors &  \hwref{EigenvaluesAndEigenvectors}{11}, \hwref{EigenvaluesAndEigenvectors}{12},
\hwref{EigenvaluesAndEigenvectors}{13}, \hwref{EigenvaluesAndEigenvectors}{14}\\
Complex eigenvalues&\hwref{EigenvaluesAndEigenvectors}{15}\\
  \hline
\end{tabular}


\input{\eigenIIPath/problems}


\newpage

","

\chapter{\eigenTitle}
\label{eigenvalseigenvects}

In a vector space with no structure other than the vector space rules, no vector other than the zero vector  is any  more important than any other.
Once one also has a linear transformation the situation changes dramatically. We begin with a fun example, of a type bound to reappear in your future scientific studies:

\begin{example}[String Theory]

Consider a vibrating string, 
whose displacement at point $x$ at time $t$ is given by a function $y(x,t)$:
\begin{center}
\includegraphics[scale=.3]{string.jpg}
\end{center}
The set of all displacement functions for the string 
can be modeled by 
a vector space 
$$V=\left\{ y:\mathbb{R}^2 \to \mathbb{R} \middle| \mbox{\rm all partial derivatives } \frac{\partial^{k+m}y(x,t)}{\partial x^k\partial t^m} \mbox{ exist}\right\}.$$ 
%The reason for the condition that the second derivatives exist is 
 The concavity and 
 the acceleration of the string at the point $(x,t)$ are 
 $\frac{\partial^2y}{\partial x^2}(x,t)$ and $\frac{\partial^2y}{\partial t^2}(x,t)$ respectively. 
 Since quantities must exist at each point on the string for the wave equation to make sense, 
 we required that all partial derivatives of $y(x,t)$ exist.
 Note also that the function $y(x,t)=0$~---drawn in grey---is the only special vector in the vector space~$V$. 

%
We now add some extra information.
The string's behavior in time and space can be modeled by a wave equation\index{Wave equation}
$$\frac{\partial^2 y}{\partial t^2}=\frac{\partial^2 y}{\partial x^2}\, ,
$$
which says that the acceleration of a point on the string is equal its concavity at that point. For example, if the string were made of stretched rubber, it would 
prefer to be in a straight line, so this equation makes good intuitive sense. 
Not all of the functions in $V$ are solutions to the wave equation; not all of the functions in the vector space $V$ describe the way a string would really vibrate.  The ways a string  would really  vibrate are (at least approximately) solutions to the wave equation above, which can rewritten as a linear function
$$Wy=0$$
where
$$
W=\left(-\frac{\partial^2 }{\partial t^2}+\frac{\partial^2 }{\partial x^2}\right):V\rightarrow V\, .
$$
Some examples of solutions are 
$$
y_1(x,t)=\sin (t) \sin (x)\, \quad
y_2(x,t)=3\sin (2t) \sin (2x)\, $$
and
$$
y_3(x,t)=\sin (t) \sin (x)+3\sin (2t) \sin (2x)\, .
$$
Since $Wy=0$ is a homogeneous linear equation, linear combinations of solutions are solutions; in other words the kernel $\ker(w)$ is a vector space.
Given the linear function $W$, some vectors are now more special than others.

We can use musical intuition to do more! If the ends of the string were held fixed, we suspect that it would prefer to vibrate at certain frequencies corresponding to musical notes.
This is modeled by looking at solutions of the form
$$
y(x,t)=\sin(\omega t) v(x)\, .
$$
Here the periodic sine function accounts for the string's vibratory motion, while the function $v(x)$ gives the shape of the string at any fixed instant of time. Observe that
$$
W\big(\sin(\omega t) v(x)\big)=\sin(\omega t)\big(\frac{d^2f}{dx^2}+\omega^2 f\big)\, .
$$
This suggests we introduce a new vector space $$U=\left\{v:{\mathbb R}\to {\mathbb R}\, \middle| \, \mbox{\rm all derivatives } \frac{d^kf}{d x^k }\text{~exist~} \right\}\, ,
$$
as well as a new linear function
$$
L:=\frac{d^2}{dx^2}: U\longrightarrow U\, .
$$
The number $\omega$ is called an angular frequency in many contexts, lets call its square $\lambda:=-\omega^2$ to match notations we will use later (notice that for this particular problem $\lambda$ must then  be negative).
Then, because we want $W(y)=0$, which implies $d^2 f/dx^2=\omega^2 f$, it follows that the vector $v(x)\in U$ determining the vibrating string's shape obeys
$$
L(v)=\lambda v\, .
$$
This is perhaps one of the most important equations in all of linear algebra! It is the eigenvalue-eigenvector equation. In this problem we have to solve it both for $\lambda$, to determine which frequencies (or musical notes) our string likes to sing, and the vector $v$ determining the string's shape. The vector $v$ is called an eigenvector and $\lambda$ its corresponding eigenvalue.
%We can incorporate a realistic complication into our model of the string: 
%an additional force may be pulling the string toward the rest position with a Hooke's law type force. In this case there are two causes for acceleration of a point on the string: the concavity and  Hooke's force.  
%In this case the acceleration of the string at each point will be the sum of the acceleration due to its concavity and the acceleration due to gravity $g$;
%$$\frac{\partial^2 y}{\partial t^2}=\frac{\partial^2 y}{\partial x^2}+ky\, ,
%$$
%or, written as a linear equation 
%$$Ly=ky.$$
%The solution set this of  equation will be different for every different value of the spring constant $k$. 
The solution sets for each $\lambda$ are called $V_\lambda$. For any $\lambda$ the set $V_k$ is a vector space since elements of this set are solutions to the homogeneous equation $(L-\lambda)v=0.$ 



\end{example}

We began this chapter by stating ``In a vector space, with no other structure, no vector is more important than any other."" Our aim is  to show you that when a linear operator $L$ acts on a vector space, vectors that   solve the equation    $L(v)=\lambda v$ play a central role.

% with the largest value of k are more important than those for smaller values of $k$. 
%You might ask ``more important in what sense?"" 
%In the sense that the function $L$ can be built out of the various values of $k$ and the vectors in $V_k$ and the biggest part of that construction is the part with the biggest values of $k$. In particular for some vectors $v_k\in V_k$
%$$L=\sum_{k} k \, v_k \,v_k^t.$$
%We are aware that we have made the startling suggestion that a derivative operator is a linear combination of products of vectors, and we hope that you are intrigued. Lets now return to the simpler case of linear functions which are matrices to develop this idea.




%Before discussing eigenvalues and eigenvectors, we need to understand very well  the relationship between linear transformations and matrices. Let us review the key ideas.
%Consider, as an example the plane ${\mathbb R}^2$
%\begin{center}
%\includegraphics[scale=.3]{\eigenPath/plane.jpg}
%\end{center}
%The  vector $v$ can be described in many ways. 
%Two possibilities are the ordered basis $E=(e_1,e_2)$ 
%and the ordered basis
%$F=(f_1,f_2)$; 
%the vector $v$ is described by the ordered pair $(x,y)^T=(2,2)^T$ in the ordered basis $E$ and by the ordered pair $(s,t)^T=(2,1)^T$ in the ordered basis $F$.  
%This can be confusing! The idea to keep firm in your mind is that the vector space and its elements---vectors---are what really ``exist''. 
%The pairs of numbers are really just different shorthand notations for the same vector. 
%\hyperlink{Basis notation}{Previously} we denoted the correspondence between pairs of numbers and vectors by labeling the pairs of numbers with the basis in which to interpret the numbers;
%$$
%v= \colvec{ 2\\2 }_E :=2e_1+2e_2, ~v= \colvec{2\\1}_F := 2f_1+1f_2\, .
%$$
%
%There is an intuitive reason for the fact that there are many ways to describe the same vector;
%typically, you will use vectors to describe configurations of the real world systems, but the coordinate system you use is your choice. 
%Things like coordinate axes and ``components of a vector'' $(x,y)$ are just mathematical tools used to label vectors.
%
%To generalize this discussion, let $V$ and $W$ be vector spaces, with ordered bases $E=(e_1, \ldots, e_n)$ and 
%$F=(f_1, \ldots, f_m)$ respectively.  
%Since these are bases, there exist constants $v^i$ and $w^j$ such that any vectors $v\in V$ and $w\in W$ can be written as:
%\begin{eqnarray*}
%v & = & v^1e_1 \ + v^2e_2 \ + \cdots + v^ne_n \\
%w & = & w^1f_1 + w^2f_2 + \cdots + w^mf_m \\
%\end{eqnarray*}
%We call the coefficients $v^1, \ldots, v^n$ the \emph{components}\index{Components of a vector} of $v$ in the ordered basis\footnote{To avoid confusion, it helps to note that components of a vector are labeled by a superscript, while basis vectors are labeled by subscripts. This is part of Einstein's notation convention.} $(e_1, \ldots, e_n)$. It is often convenient to arrange the components $v^i$ in a column vector and the basis vectors in a row vector by writing
%\[
%v=\rowvec{e_1 & e_2 & \cdots & e_n}\colvec{v^1 \\ v^2 \\ \vdots \\  v^n}\, .
%\]
%\videoscriptlink{eigenvectors_and_eigenvalues_matrix.mp4}{Worked Example}{scripts_eigenvalseigenvects_matrix}
%
%For what follows it is often convenient follow this order and put the scalar coefficients to the right of basis vectors as in 
%$$v  =  e_1v^1 \ + e_2v^2 \ + \cdots + e_nv^n .$$ 
%
%
%
%\begin{example}
%Consider the ordered basis $B=(1-t, 1+t )$ for the vector space of polynomials of order 1 or lower $P_1(t)$.  The vector $v=2t$ has components $v^1=-1, v^2=1$ in the ordered basis $B$ because 
%\[v=(1-t)(-1) + (1+t)1=\rowvec{1-t & 1+t}\colvec{-1 \\[2mm] 1}\, .\]
%\end{example}
%
%When describing general cases the adroit reader will also enjoy the compact Einstein summation notation $v=e_iv^i$.
%
%%We may consider these components as vectors in $\Re^n$ and $\Re^m$:
%%\[
%%\colvec{v^1\\ \vdots \\ v^n}\in \Re^n, \qquad 
%%\colvec{w^1\\ \vdots \\ w^m}\in \Re^m.
%%\]


\section{Invariant Directions}

Have a look at the linear transformation $L$ depicted below:
\begin{center}
\includegraphics[scale=.3]{\eigenPath/invariant.jpg}
\end{center}
It was picked at random by choosing a pair of vectors $L(e_1)$ and $L(e_2)$ as the outputs of $L$ acting on the canonical basis vectors.
Notice how the unit square with a corner at the origin is mapped to a parallelogram.  The second line of the picture 
shows these superimposed on one another. Now look at the second picture on that line. There, two vectors $f_1$ and $f_2$ have been carefully
chosen such that if the inputs into $L$ are in the parallelogram spanned by $f_1$ and $f_2$, the outputs also form a parallelogram with
edges lying along the same two directions. Clearly this is a very special situation that should correspond to  interesting properties of $L$.

Now lets try an explicit example to see if we can achieve the last picture:

\begin{example}
Consider the linear transformation $L$ such that $$L\colvec{1\\0}=\colvec{-4\\-10}\,  \mbox{ and }\, L\colvec{0\\1}=\colvec{3\\7}\, ,$$ so that the matrix of $L$ in the standard basis is $$\begin{pmatrix}
-4 & 3 \\
-10 & 7 \\
\end{pmatrix}\, .$$  Recall that a vector is a direction and a magnitude; $L$ applied to $\colvec{1\\0}$ or $\colvec{0\\1}$ changes both the direction and the magnitude of the vectors given to it.

Notice that $$L\colvec{3\\5}=\colvec{-4\cdot 3+3\cdot 5 \\ -10\cdot 3+7\cdot 5}=\colvec{3\\5}\, .$$  Then $L$ fixes the  direction (and actually also the magnitude) of the vector $v_1=\colvec{3\\5}$.  

%In fact also the vector $v_2=\colvec{1\\2}$ has its direction fixed by $M$ because $Lv_2=2v_2$.



%\begin{center}\href{\webworkurl ReadingHomework18/1/}{Reading homework: problem \ref{eigenvalseigenvects}.1}\end{center}
\Reading{EigenvaluesAndEigenvectors}{1}

Now, notice that any vector with the same direction as $v_1$ can be written as $cv_1$ for some constant $c$.  Then $L(cv_1)=cL(v_1)=cv_1$, so $L$ fixes every vector pointing in the same direction as $v_1$.

Also notice that $$L\colvec{1\\2}=\colvec{-4\cdot 1+3\cdot 2 \\ -10\cdot 1+7\cdot 2}=\colvec{2\\4}=2\colvec{1\\2}\, ,$$ so $L$ fixes the direction of the vector $v_2=\colvec{1\\2}$ but stretches $v_2$ by a factor of $2$.  Now notice that for any constant $c$, $L(cv_2)=cL(v_2)=2cv_2$.  Then $L$ stretches every vector pointing in the same direction as $v_2$ by a factor of $2$.

\end{example}

In short, given a linear transformation $L$ it is sometimes possible to find a vector $v\neq 0$ and constant $\lambda\neq 0$ such that 
$
Lv=\lambda v.
$
\begin{figure}
\begin{center}
\includegraphics[scale=.4]{\eigenPath/eigeneqn.jpg}
\end{center}
\caption{The eigenvalue--eigenvector equation is probably the most important one in linear algebra.}
\end{figure}
We call the direction of the vector $v$ an \emph{invariant direction}\index{Invariant direction}.  In fact, any vector pointing in the same direction also satisfies this  equation because   $L(cv)=cL(v)=\lambda cv$.  
More generally, any {\it non-zero} vector $v$ that solves
$$
L(v)=\lambda v
$$
is called an \emph{eigenvector}\index{Eigenvector} of $L$, and $\lambda$ (which now need not be zero) is an \emph{eigenvalue}\index{Eigenvalue}.  Since the direction is all we really care about here, then any other vector $cv$ (so long as $c\neq 0$) is an equally good choice of eigenvector. Notice that the relation ``\(u\) and \(v\) point in the same direction'' is an equivalence relation.

In our example of the linear transformation $L$ with matrix $$\begin{pmatrix}
-4 & 3 \\
-10 & 7 \\
\end{pmatrix}\, ,$$ we have seen that $L$ enjoys the property of having two invariant directions, represented by eigenvectors $v_1$ and $v_2$ with eigenvalues $1$ and $2$, respectively.  

It would be very convenient if we could write any vector $w$ as a linear combination of $v_1$ and $v_2$.  Suppose $w=rv_1+sv_2$ for some constants $r$ and~$s$.  Then
\[
L(w)=L(rv_1+sv_2)=rL(v_1)+sL(v_2)=rv_1+2sv_2.
\]
Now $L$ just multiplies the number $r$ by $1$ and the number $s$ by $2$.  If we could write this as a matrix, it would look like:
\[
\begin{pmatrix}
1 & 0 \\
0 & 2
\end{pmatrix}\colvec{s\\t}
\]
which is much slicker than the usual scenario  $$L\!\begin{pmatrix}
x\\
y
\end{pmatrix}\!=\!\begin{pmatrix}
\!a&b\! \\
\!c&d\!
\end{pmatrix} \! \!\begin{pmatrix}
x \\
y
\end{pmatrix}\!=\!
\begin{pmatrix}
\!ax+by\! \\
\!cx+dy\!
\end{pmatrix}\, .$$
Here, $s$ and $t$ give the coordinates of $w$ in terms of the vectors $v_1$ and $v_2$.  In the previous example, we multiplied the vector by the matrix $L$ and came up with a complicated expression.  In these coordinates, we see that~$L$ has a very simple \emph{diagonal matrix}, whose diagonal entries are exactly the \emph{eigenvalues} of~$L$.

This process is called \emph{diagonalization}\index{Diagonalization!concept of}. It makes complicated linear systems much easier to analyze.

%\begin{center}\href{\webworkurl ReadingHomework18/2/}{Reading homework: problem \ref{eigenvalseigenvects}.2}\end{center}
\Reading{EigenvaluesAndEigenvectors}{2}

Now that we've seen what eigenvalues and eigenvectors are, there are a number of questions that need to be answered.

\begin{itemize}
\item How do we find eigenvectors and their eigenvalues?
\item How many eigenvalues and (independent) eigenvectors does a given linear transformation have?
\item When can a linear transformation be diagonalized?
\end{itemize}
We will start by trying to find the eigenvectors for a linear transformation.

\Videoscriptlink{eigenvectors_and_eigenvalues_example.mp4}{$2\times 2$ Example}{scripts_eigenvalseigenvects_example}

\begin{example}
Let $L \colon \Re^2\rightarrow \Re^2$ such that $L(x,y)=(2x+2y, 16x+6y)$.  First, we  find the matrix of $L$, this is quickest in the standard basis:

\[
\colvec{x\\y}\stackrel{L}{\longmapsto} \begin{pmatrix}
2 & 2 \\
16 & 6
\end{pmatrix}\colvec{x\\y}.
\]
We want to find an invariant direction $v=\colvec{x\\y}$ such that
\[
Lv=\lambda v
\]
or, in matrix notation,
\begin{equation*}
\begin{array}{lrcl}
&\begin{pmatrix}
2 & 2 \\
16 & 6
\end{pmatrix}\colvec{x\\y}&=&\lambda \colvec{x\\y} \\[6mm]
\Leftrightarrow &\begin{pmatrix}
2 & 2 \\
16 & 6
\end{pmatrix}\colvec{x\\y}&=&\begin{pmatrix}
\lambda & 0 \\
0 & \lambda
\end{pmatrix} \colvec{x\\y} \\[6mm] 
\Leftrightarrow& 
\begin{pmatrix}
\mc{2-\lambda} & \mc{2} \\
\mc{16} & \mc{6-\lambda}
\end{pmatrix}\colvec{x\\y}&=& \colvec{0\\0}\, .
\end{array}
\end{equation*}
This is a homogeneous system, so it only has solutions when the matrix $\begin{pmatrix}
2-\lambda & \mc2 \\
\mc{16} & 6-\lambda
\end{pmatrix}$ is singular.  In other words, 
\begin{equation*}
\begin{array}{lrcl}
&\det \begin{pmatrix}
2-\lambda & \mc2 \\
\mc{16} & 6-\lambda
\end{pmatrix}&=&0 \\[5mm]
\Leftrightarrow& (2-\lambda)(6-\lambda)-32&=&0 \\[2mm]
\Leftrightarrow &\lambda^2-8\lambda-20&=&0\\[2mm]
\Leftrightarrow &(\lambda-10)(\lambda+2)&=&0
\end{array}
\end{equation*}

\hypertarget{characteristic_polynomial}{For any} square $n\times n$ matrix $M$, the polynomial in $\lambda$ given by $$P_M(\lambda)=\det (\lambda I-M)=(-1)^n \det (M-\lambda I)$$ is called the \emph{characteristic polynomial}\index{Characteristic polynomial} of $M$, and its roots are the eigenvalues of $M$.

In this case, we see that $L$ has two eigenvalues, $\lambda_1=10$ and $\lambda_2=-2$.  To find the eigenvectors, we need to deal with these two cases separately.
To do so, we solve the linear system $\begin{pmatrix}
2-\lambda & \mc2 \\
\mc{16} & 6-\lambda
\end{pmatrix}\colvec{x\\y}= \colvec{0\\0}$ with the particular eigenvalue $\lambda$ plugged in to the matrix.

\begin{itemize}
\item[$\underline{\lambda=10}$:]  We solve the linear system
\[
\begin{pmatrix}
-8 & 2 \\
16 & -4
\end{pmatrix}\colvec{x\\y}= \colvec{0\\0}.
\] 
Both equations say that $y=4x$, so any vector $\colvec{x\\4x}$ will do.  Since we only need the direction of the eigenvector, we can pick a value for $x$.  Setting $x=1$ is convenient, and gives the eigenvector $v_1=\colvec{1\\4}$.

\item[$\underline{\lambda=-2}$:]  We solve the linear system
\[
\begin{pmatrix}
4 & 2 \\
16 & 8
\end{pmatrix}\colvec{x\\y}= \colvec{0\\0}.
\] 
Here again both equations agree, because we chose $\lambda$ to make the system singular.  We see that $y=-2x$ works, so we can choose $v_2=\colvec{1\\-2}$.
\end{itemize}

Our process was the following:
\begin{enumerate}
\item Find the characteristic polynomial of the matrix $M$ for $L$, given by\footnote{To save writing many minus signs compute $\det(M-\lambda I)$; which is equivalent if you only need the roots.} $\det (\lambda I-M)$.
\item Find the roots of the characteristic polynomial; these are the eigenvalues of $L$.
\item For each eigenvalue $\lambda_i$, solve the linear system $(M-\lambda_i I)v=0$ to obtain an eigenvector $v$ associated to $\lambda_i$.
\end{enumerate}
\end{example}

\Videoscriptlink{eigenvalues_and_eigenvectors_jordan.mp4}{Jordan block example}{scripts_eigenvalseigenvects_jordan}



%\section*{References}
%Hefferon, Chapter Three, Section III.1: Representing Linear Maps with Matrices
%\\
%Hefferon, Chapter Five, Section II.3: Eigenvalues and Eigenvectors
%\\
%Beezer, Chapter E, Section EE
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Eigenvalue,_eigenvector_and_eigenspace}{Eigen*}
%\item \href{http://en.wikipedia.org/wiki/Characteristic_polynomial}{Characteristic Polynomial}
%\item \href{http://en.wikipedia.org/wiki/Linear_map}{Linear Transformations (and matrices thereof)}
%\end{itemize}





%\section{Review Problems}
%\input{\eigenPath/problems}




\section{The Eigenvalue--Eigenvector Equation}\label{evalsevectsII}

In section~\ref{eigenvalseigenvects}, we developed the idea of eigenvalues and eigenvectors in the case of linear transformations $\Re^2\rightarrow \Re^2$.  In this section, we will develop the idea more generally.

\Videoscriptlink{eigenvalues_and_eigenvectors_ii_lecture.mp4}{Eigenvalues}{scripts_eigenvalues_and_eigenvectors_ii_lecture}

\begin{definition}
If $L \colon V\rightarrow V$ is linear and for some scalar $\lambda$ and $v\neq 0_V$
$$
Lv=\lambda v.
$$
then $\lambda$ is an {\bf eigenvalue}\index{Eigenvalue} of $L$ with {\bf eigenvector}\index{Eigenvector} $v$.

\end{definition}
This equation says that the direction of $v$ is invariant (unchanged) under~$L$.  

Let's try to understand this equation better in terms of matrices.
Let $V$ be a finite-dimensional vector space and let $L \colon V\rightarrow V$.  
If we have a basis for $V$ we can represent $L$ by a square matrix $M$ and find eigenvalues $\lambda$ and associated eigenvectors $v$ by solving the homogeneous system

\[
(M-\lambda I)v=0.
\]
This system has non-zero solutions if and only if the matrix

\[
M-\lambda I
\]
is singular, and so we require that

\[
\det (\lambda I-M) = 0.
\]

The left hand side of this equation is a polynomial in the variable $\lambda$ called the {\bf characteristic polynomial}\index{Characteristic polynomial} $P_M(\lambda)$ of $M$.  For an $n\times n$ matrix, the characteristic polynomial has degree $n$.  Then 
\[
P_M(\lambda) = \lambda^n+c_1\lambda^{n-1}+\cdots+c_n.
\]
Notice that $P_M(0)=\det (-M)=(-1)^n\det M$.\\

Now recall the following.
\begin{theorem} (The {Fundamental Theorem of Algebra}\index{Fundamental theorem of algebra}) 
Any polynomial can be factored into a product of first order polynomials over~$\C$.  
\end{theorem}
This theorem implies that there exists a collection of $n$ complex numbers~$\lambda_i$ (possibly with repetition) such that
\[
P_M(\lambda)=(\lambda-\lambda_1)(\lambda-\lambda_2)\cdots(\lambda-\lambda_n)\: \Longrightarrow\: P_M(\lambda_i)=0.
\]
The eigenvalues $\lambda_i$ of $M$ are exactly the roots of $P_M(\lambda)$.  These eigenvalues could be real or complex or zero, and they need not all be different.  The number of times that any given root $\lambda_i$ appears in the collection of eigenvalues is called its \emph{multiplicity}\index{Eigenvalue!multiplicity of}.

\begin{figure}
\begin{center}
\includegraphics[scale=.3]{\eigenIIPath/charpoly.jpg}
\end{center}
\caption{Don't forget the characteristic polynomial; you will need it to compute eigenvalues.}
\end{figure}


\begin{example}
Let $L$ be the linear transformation $L \colon \Re^3\rightarrow \Re^3$ given by 
$$L\colvec{x\\y\\z}=\ccolvec{2x+y-z\\ x+2y-z\\ -x-y+2z}\, .$$ 
In the standard basis the matrix $M$ representing $L$ has columns $Le_i$ for each $i$, so:

\[
\colvec{x\\y\\z} \stackrel{L}{\longmapsto} 
\begin{pmatrix}
2 & 1 & -1\\
1 & 2 & -1\\
-1 & -1 & 2\\
\end{pmatrix}\colvec{x\\y\\z}.
\]
Then the characteristic polynomial of $L$ is\footnote{It is often easier (and equivalent) to solve $\det (M-\lambda I)=0$.}
\begin{eqnarray*}
P_M(\lambda)&=&\det \begin{pmatrix}
\mc{\lambda - 2} &\mc{ -1} & \mc{1}\\
\mc{-1} &\mc{ \lambda - 2} & \mc{1}\\
\mc{1} & \mc{1} & \mc{\lambda - 2}\\
\end{pmatrix}\\[2mm]
&=& (\lambda - 2)[ (\lambda - 2)^2-1 ] + 
[-(\lambda - 2)-1 ] +
[-(\lambda - 2)-1] \\[2mm]
&=& (\lambda - 1)^2(\lambda - 4)\, .
\end{eqnarray*}
So $L$ has eigenvalues $\lambda_1=1$ (with multiplicity $2$), and $\lambda_2=4$ (with multiplicity $1$).

To find the eigenvectors associated to each eigenvalue, we solve the homogeneous system $(M-\lambda_iI)X=0$ for each $i$.

\begin{itemize}
\item[\underline{$\lambda=4$:}] We set up the augmented matrix for the linear system:
\begin{eqnarray*}
\begin{amatrix}{3}
-2 & 1 &-1 & 0 \\
1 & -2 &-1 & 0 \\
-1 & -1 &-2 & 0 \\
\end{amatrix} 
 & \sim & \begin{amatrix}{3}
1 & -2 &-1 & 0 \\
0 & -3 &-3 & 0 \\
0 & -3 &-3 & 0 \\
\end{amatrix} \\
 & \sim & \begin{amatrix}{3}
1 & 0 & 1 & 0 \\
0 & 1 & 1 & 0 \\
0 & 0 & 0 & 0 \\
\end{amatrix}. \\
\end{eqnarray*}
%So we see that $z=:t$, $y=-t$, and $x=-t$ gives a formula for eigenvectors in terms of the free parameter $t$.  
Any vector of the form $t\colvec{-1\\-1\\1}$ is then an eigenvector with eigenvalue $4$; thus~$L$ leaves a line through the origin invariant.

\item[\underline{$\lambda=1$:}]  Again we set up an augmented matrix and find the solution set:
\begin{eqnarray*}
\begin{amatrix}{3}
1 & 1 &-1 & 0 \\
1 & 1 &-1 & 0 \\
-1 & -1 & 1 & 0 \\
\end{amatrix} 
 & \sim & \begin{amatrix}{3}
1 & 1 &\!\!-1 & 0 \\
0 & 0 &0 & 0 \\
0 & 0 &0 & 0 \\
\end{amatrix}.
\end{eqnarray*}
Then the solution set has two free parameters, $s$ and $t$, such that $z=z=:t$, $y=y=:s$, and $x=-s+t$.  Thus $L$ leaves invariant the set:
\[
\left\{ s\colvec{-1\\1\\0}+t\colvec{1\\0\\1} \middle| s,t\in \Re   \right\}.
\]
This set is a plane through the origin.  So the multiplicity two eigenvalue has two independent eigenvectors, $\colvec{-1\\1\\0}$ and $\colvec{1\\0\\1}$ that determine an invariant plane.
\end{itemize}
\end{example}

\begin{example}
Let $V$ be the vector space of smooth (\textit{i.e.} infinitely differentiable) functions $f \colon \Re\rightarrow \Re$.  Then the derivative is a linear operator $\frac{d}{dx} \colon V\rightarrow V$.  What are the eigenvectors of the derivative?  In this case, we don't have a matrix to work with, so we have to make do.

A function $f$ is an eigenvector of $\frac{d}{dx}$ if there exists some number $\lambda$ such that $$\frac{d}{dx}f=\lambda f\, .$$  An obvious candidate is the exponential function, $e^{\lambda x}$; indeed, $\frac{d}{dx} e^{\lambda x} = \lambda e^{\lambda x}$.
The operator $\frac{d}{dx}$ has an eigenvector $e^{\lambda x}$ for every $\lambda \in \Re$.
%This is actually the all of the eigenvectors for $\frac{d}{dx}$. 
%(This can be proved using the fact that every infinitely differentiable function has a Taylor series with infinite radius of convergence, and then using the Taylor series to show that if two functions are eigenvectors of $\frac{d}{dx}$ with eigenvalues $\lambda$, then they are scalar multiples of each other.)
\end{example}


\section{Eigenspaces}
In the previous example, we found two eigenvectors  $$\colvec{-1\\1\\0} \mbox{ and }\colvec{1\\0\\1}$$ for $L$, both with eigenvalue $1$.  Notice that  $$\colvec{-1\\1\\0} + \colvec{1\\0\\1}=
\colvec{0\\1\\1}$$ is also an eigenvector of $L$ with eigenvalue $1$.  In fact, any linear combination $$r\colvec{-1\\1\\0} + s\colvec{1\\0\\1}$$ of these two eigenvectors will be another eigenvector with the same eigenvalue.  

More generally, let $\{ v_1, v_2, \ldots \}$ be eigenvectors of some linear transformation $L$ with the same eigenvalue $\lambda$.  A \emph{linear combination}\index{Linear combination} of the $v_i$ is given by $c^1v_1+c^2v_2+\cdots$ for some constants $c^1, c^2,\ldots$.  Then
\begin{eqnarray*}
L(c^1v_1+c^2v_2+\cdots) &=& c^1Lv_1+c^2Lv_2+\cdots \text{ by linearity of $L$}\\
&=& c^1\lambda v_1\,+c^2\lambda v_2+\cdots \hspace{.5mm} \text{ since $Lv_i=\lambda v_i$ }\\
&=& \lambda (c^1v_1+c^2v_2+\cdots).
\end{eqnarray*}
So every linear combination of the $v_i$ is an eigenvector of $L$ with the same eigenvalue $\lambda$.
In simple terms, any sum of eigenvectors is again an eigenvector {\it if they share the same eigenvalue}.

The space of all vectors with eigenvalue $\lambda$ is called an {\bf eigenspace}\index{Eigenspace}.  It is, in fact, a vector space contained within the larger vector space $V$.  It contains $0_V$, since $L0_V=0_V=\lambda 0_V$, and is closed under addition and scalar multiplication by the above calculation.  All other vector space properties are inherited from the fact that $V$ itself is a vector space. In other words, the \hyperlink{sst}{subspace theorem} (\ref{subspacetheorem}, chapter~\ref{subspacesspanning}) ensures that $V_\lambda:=\{v\in V|Lv=0\}$ is a subspace of $V$.

%An eigenspace is an example of a \emph{subspace}\index{Subspace!notion of} of $V$, a notion explored in Chapter~\ref{subspacesspanning}.

\Videoscriptlink{eigenvalues_and_eigenvectors_ii_eigenspaces.mp4}{Eigenspaces}{scripts_eigenvalues_eigenvectors_ii_eigenspaces}

%\begin{center}\href{\webworkurl ReadingHomework19/1/}{Reading homework: problem \ref{evalsevectsII}.1}\end{center}
\Reading{EigenvaluesAndEigenvectors}{3}

\begin{center}
\shabox{
{\bf \hyperref[sample2]{You can now attempt the second sample midterm. 
\includegraphics[scale=.15]{midterm.jpg}}}}
\end{center}
%
%\section*{References}
%Hefferon, Chapter Three, Section III.1: Representing Linear Maps with Matrices
%\\
%Hefferon, Chapter Five, Section II.3: Eigenvalues and Eigenvectors
%\\
%Beezer, Chapter E, Section EE
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Eigenvalue,_eigenvector_and_eigenspace}{Eigen*}
%\item \href{http://en.wikipedia.org/wiki/Characteristic_polynomial}{Characteristic Polynomial}
%\item \href{http://en.wikipedia.org/wiki/Linear_map}{Linear Transformations (and matrices thereof)}
%\end{itemize}


\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{EigenvaluesAndEigenvectors}{1}, \hwrref{EigenvaluesAndEigenvectors}{2}, \hwrref{EigenvaluesAndEigenvectors}{3}\\
Characteristic polynomial&  \hwref{EigenvaluesAndEigenvectors}{4}, \hwref{EigenvaluesAndEigenvectors}{5}, \hwref{EigenvaluesAndEigenvectors}{6}\\
Eigenvalues &  \hwref{EigenvaluesAndEigenvectors}{7}, \hwref{EigenvaluesAndEigenvectors}{8}\\
Eigenspaces &  \hwref{EigenvaluesAndEigenvectors}{9}, \hwref{EigenvaluesAndEigenvectors}{10}\\
Eigenvectors &  \hwref{EigenvaluesAndEigenvectors}{11}, \hwref{EigenvaluesAndEigenvectors}{12},
\hwref{EigenvaluesAndEigenvectors}{13}, \hwref{EigenvaluesAndEigenvectors}{14}\\
Complex eigenvalues&\hwref{EigenvaluesAndEigenvectors}{15}\\
  \hline
\end{tabular}


\input{\eigenIIPath/problems}


\newpage

",lesson
20,Eigenvalues And Eigenvectors Ii,,,lesson
21,Diagonalization,"\chapter{\diagTitle}\label{sec:diagonalization}



Given a linear transformation, it is highly desirable to write its matrix  with respect to a basis of eigenvectors.

\section{Diagonalizability}\index{Diagonalization}

Suppose we are lucky, and we have $L \colon V\to V$, and the ordered basis 
$B=(v_1, \ldots, v_n )$ is a set of %linearly independent 
eigenvectors for $L$, with eigenvalues $\lambda_1, \ldots, \lambda_n$.  Then:

\begin{eqnarray*}
L(v_1)&=&\lambda_1 v_1 \\
L(v_2)&=&\lambda_2 v_2 \\
&\vdots & \\
L(v_n)&=&\lambda_n v_n \\
\end{eqnarray*}
As a result, the matrix of $L$ in the basis of eigenvectors $B$ is diagonal:
\[
L\begin{pmatrix}
x^1\\
x^2\\
\vdots\\
x^n
\end{pmatrix}_B
=
\left(
\begin{pmatrix}
\lambda_1    \\
& \lambda_2 &  & \\
&  & \ddots &  \\
& & & \lambda_n
\end{pmatrix}
\begin{pmatrix}
x^1\\
x^2\\
\vdots\\
x^n
\end{pmatrix}
\right)_B
,
\]
where all entries off the diagonal are zero.

Suppose that \(V\) is any \(n\)-dimensional vector space. We call a linear transformation $L \colon V\mapsto V$ \emph{diagonalizable}\index{Diagonalizable} if there exists a collection of $n$ linearly independent eigenvectors for $L$.  In other words, $L$ is diagonalizable if there exists a basis for $V$ of eigenvectors for $L$.  

In a basis of eigenvectors, the matrix of a linear transformation is diagonal.  On the other hand, if an $n \times n$ matrix is diagonal, then the standard basis vectors $e_i$ must already be a set of $n$ linearly independent eigenvectors.  We have shown:

\begin{theorem}
Given an ordered basis $B$ for a vector space $V$ and a linear transformation $L \colon V\rightarrow V$, then the matrix for $L$ in the basis $B$ is diagonal if and only if $B$ consists of eigenvectors for $L$.
\end{theorem}

\Videoscriptlink{diagonalization_derivative.mp4}{Non-diagonalizable example}{scripts_diagonalization_derivative}

%\begin{center}\href{\webworkurl ReadingHomework20/2/}{Reading homework: problem 20.2}\end{center}
\Reading{Diagonalization}{1}

Typically, however, we do not begin a problem with a basis of eigenvectors, but rather have to compute these. Hence we need to know how to change from one basis to another:

\section{Change of Basis}\index{Change of basis}

Suppose we have two  ordered bases $S=(v_1, \ldots, v_n )$ and 
$S'=(v'_1, \ldots, v'_n )$ for a vector space $V$. (Here $v_i$ and $v'_i$ are {\it vectors}, not components of vectors in a basis!) 
Then we may write each $v'_k$ uniquely as 
\[
v'_k = \sum_i v_ip^i_k\, ,
\]
this is $v'_k$ as a linear combination of the~$v_i$. 
In  matrix notation
\[ 
\rowvec{v'_1 , v'_2 , \cdots , v'_n} = \rowvec{v_1 , v_2 , \cdots , v_n}
\begin{pmatrix}p^1_1&p^1_2&\cdots &p^1_n\\ p^2_1 & p^2_2 && \\[2mm]
                                              \mc\vdots &&&\mc\vdots\\ p^n_1 &&\cdots & p^n_n\end{pmatrix}\, .
\]
Here, the $p^i_k$ are constants, which we can regard as entries of  a square matrix~$P=(p^i_k)$.  The matrix~$P$ must have an inverse since we can also write each~$v_j$ uniquely as a linear combination of the~$v'_k$;
\[
v_j = \sum_k v_k' q^k_j.
\]

Then we can write
\[
v_j = \sum_k \sum_i v_ip^i_kq^k_j.
\]
But $\sum_k p^i_kq^k_j$ is the $k,j$ entry of the product matrix  $PQ$.  Since the  expression for $v_j$ in the basis $S$ is $v_j$ itself, then $PQ$ maps each  $v_j$ to itself.  As a result, each $v_j$ is an eigenvector for $PQ$ with eigenvalue $1$, so $PQ$ is the identity, {\it i.e.}
$$
PQ=I \Leftrightarrow Q=P^{-1}\, .
$$

\vspace{1mm}
The matrix $P$ is  called a \emph{change of basis} matrix\index{Change of basis matrix}. There is a quick and dirty trick to obtain it; look at the formula above relating the new basis vectors
$v'_1,v'_2,\ldots v'_n$ to the old ones $v_1,v_2,\ldots,v_n$. In particular focus on $v'_1$ for which
$$
v'_1= \begin{pmatrix}v_1 , v_2 , \cdots , v_n\end{pmatrix}
\begin{pmatrix}p^1_1\\p^2_1\\\mc\vdots \\ p^n_1
\end{pmatrix}\, .
$$
This says that the first column of the change of basis matrix $P$ is really just the components of the vector $v'_1$ in the basis $v_1,v_2,\ldots,v_n$. 

\Shabox{1}{\begin{tabular}{c}The columns of the change of basis matrix are the components\\ of the new basis vectors  in terms of the old basis vectors.\end{tabular} }

\begin{example}
Suppose  $S'=(v'_1,v'_2)$ is an ordered  basis for a vector space $V$ and that with respect to some other ordered basis $S=(v_1, v_2)$ for $V$ 
$$
v'_1=
\begin{pmatrix}
\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}
\end{pmatrix} _S
\quad \mbox{and} \quad 
v'_2=
\begin{pmatrix}
\frac{1}{\sqrt{3}}\\-\frac{1}{\sqrt{3}}
\end{pmatrix}_S  \, .
$$
%What is the change of basis matrix $P$ from the old basis $u_1, u_2$ to the new basis $v_1,v_2$?
%
%Before answering note that the above 
This means 
$$
v'_1=\begin{pmatrix}v_1 , v_2 \end{pmatrix}\begin{pmatrix}
\frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}
\end{pmatrix}
=\frac{v_1+v_2}{\sqrt{2}}\quad\mbox{and}\quad
v'_2=\begin{pmatrix}v_1 , v_2 \end{pmatrix}\begin{pmatrix}
\frac{1}{\sqrt{3}}\\-\frac{1}{\sqrt{3}}
\end{pmatrix}
=\frac{v_1-v_2}{\sqrt{3}}\, .
$$
The change of basis matrix has as its columns just the components of $v'_1$ and $v'_2$;
$$
P= \begin{pmatrix}
\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{3}}\\
\frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{3}}
\end{pmatrix}\, .
$$
\end{example}


\vspace{1mm}


Changing basis changes the matrix of a linear transformation. However, as a map between vector spaces, {\bf the linear transformation is the same no matter which basis we use}. Linear transformations are the actual objects of study of this book, not matrices; matrices are merely a convenient way of doing computations.

\Videoscriptlink{diagonalization_basis.mp4}{Change of Basis Example}{scripts_diagonalization_basis}

Lets now calculate how the matrix of a linear transformation changes when changing basis.
To wit, let $L \colon V \longrightarrow W$ with matrix $M=(m^i_j)$ in the ordered input and output bases $S=(v_1, \ldots, v_n )$ and $T=(w_1,\ldots,w_m)$ so
\[
L(v_i) = \sum_k w_km^k_i.
\]
Now, suppose $S'=(v'_1, \ldots, v'_n )$ and $T'=(w'_1,\ldots,w'_m)$ are new  ordered input and out bases with matrix $M'=({m'}_i^k)$. Then
\[
L(v'_i)= \sum_k w_km'^k_i\, .
\]
%where 
%$D$ 
%$$D = (d^i_j)=\begin{pmatrix}
%\lambda_1    \\
%& \lambda_2 &  & \\
%&  & \ddots &  \\
%& & & \lambda_n
%\end{pmatrix}\, ,$$
%is the diagonal matrix whose diagonal entries $d^k_k$ are the eigenvalues~$\lambda_k$. 
Let $P=(p^i_j)$ be the change of basis matrix from input basis $S$ to the basis $S'$ and $Q=(q^j_k)$ be the change of basis matrix from output basis $T$ to the basis $T'$.  Then:
\[
L(v'_j)=L\left(\sum_i v_i p^i_j\right) = \sum_i L(v_i)p^i_j
= \sum_i \sum_k w_k m^k_i p^i_j.
\]
Meanwhile, we have:
\[
L(v'_i) = \sum_kv_km'^k_i = \sum_k \sum_j v_j q^j_km^k_i.
\]
Since the expression for a vector in a basis is unique, then we see that the entries of $MP$ are the same as the entries of $QM'$.  In other words, we see that
\Shabox{1.1}{
$
MP = QM' \qquad \text{or}\qquad M'=Q^{-1}MP.
$}

\begin{example}
Let $V$ be the space of polynomials in $t$ and degree 2 or less and $\sloppy{L:V\to {\mathbb R}^2}$ where
$$
L(1)=\colvec{1\\2}\, \quad L(t)=\colvec{2\\1}\, ,\quad L(t^2)=\colvec{3\\3}\, .
$$
From this information we can immediately read off the matrix~$M$ of $L$ in the bases $S=(1,t,t^2)$ and $T=(e_1,e_2)$, the standard basis for ${\mathbb R}^2$,
because
\begin{eqnarray*}\big(L(1),L(t),L(t^2)\big)&=&(e_1+2 e_2,2e_1+e_2, 3 e_1+3e_2)\\[2mm]&=&(e_1,e_2)\begin{pmatrix}1&2&3\\2&1&3\end{pmatrix}\, \Rightarrow \, M\ =\ 
\begin{pmatrix}1&2&3\\2&1&3\end{pmatrix}\, .\end{eqnarray*}
Now suppose we are more interested in the bases $$S'=(1+t,t+t^2,1+t^2)\, , \quad T'=\left(\colvec{1\\2},\colvec{2\\1}\right)=:(w_1',w_2')\, .$$
To compute the new matrix $M'$ of $L$ we could simply calculate what $L$ does the the new input basis vectors in terms of the new output basis vectors:
\begin{eqnarray*}
\big(L(1+t),L(t+t^2),L(1+t^2))&=&\left(\colvec{1\\2}+\colvec{2\\1},
\colvec{2\\1}+\colvec{3\\3},\colvec{1\\2}+\colvec{3\\3}
\right)\\[2mm]
&=&(w'_1+w'_2,w'_1+2w'_2,2w'_1+w'_2)\\[2mm]&=&(w'_1,w'_2)
\begin{pmatrix}1&1&2\\1&2&1\end{pmatrix}\, \Rightarrow \, 
M'=\begin{pmatrix}1&1&2\\1&2&1\end{pmatrix}\, .
\end{eqnarray*}
Alternatively we could calculate the change of basis matrices $P$ and $Q$ by noting that
$$
(1+t,t+t^2,1+t^2)=(1,t,t^2)\begin{pmatrix}1&0&1\\1&1&0\\0&1&1\end{pmatrix}\, \Rightarrow\, P=\begin{pmatrix}1&0&1\\1&1&0\\0&1&1\end{pmatrix}
$$
and
$$
(w'_1,w'_2)=(e_1+2e_2,2e_1+e_2)=(e_1,e_1)\begin{pmatrix}1&2\\2&1\end{pmatrix}\, \Rightarrow\, Q=\begin{pmatrix}1&2\\2&1\end{pmatrix}\, .
$$
Hence
$$
M'=Q^{-1}MP = -\frac{1}{3}\begin{pmatrix}1&-2\\-2&1\end{pmatrix}\begin{pmatrix}1&2&3\\2&1&3\end{pmatrix}
\begin{pmatrix}1&0&1\\1&1&0\\0&1&1\end{pmatrix}=\begin{pmatrix}1&1&2\\1&2&1\end{pmatrix}\, .
$$
Notice that the change of basis matrices $P$ and $Q$ are both square and invertible. Also, since we really wanted $Q^{-1}$, 
it is more efficient to try and write $(e_1,e_2)$ in terms of $(w'_1,w'_2)$ which would yield directly $Q^{-1}$. Alternatively, one can check that
$MP=QM'$.
\end{example}

\section{Changing to a Basis of Eigenvectors}

If we are changing to a basis of eigenvectors, then there are various simplifications:
\begin{itemize}
\item Since $L:V\to V$, most likely you already know the matrix~$M$ of $L$ using the same input basis as output basis $S=(u_1,\ldots ,u_n)$ (say).
\item In the new basis of eigenvectors $S'(v_1,\ldots,v_n)$, the matrix~$D$ of $L$ is diagonal because $Lv_i=\lambda_i v_i$ and so
$$
\big(L(v_1),L(v_2),\ldots,L(v_n)\big)=(v_1,v_2,\ldots, v_n)
\begin{pmatrix}
\lambda_1&\mc0&\cdots&\mc0\\
\mc0&\lambda_2&&\mc0\\
\mc\vdots&&\ddots&\mc\vdots \\
\mc0&\mc0&\cdots&\lambda_n\end{pmatrix}\, .
$$
\item If $P$ is the change of basis matrix from $S$ to $S'$, the diagonal matrix of eigenvalues~$D$ and the original matrix are related by
\Shabox{1.1}{$D=P^{-1}MP$}
\end{itemize}

This motivates the following definition:
\begin{definition}
A matrix $M$ is {\bf diagonalizable} if there exists an invertible matrix $P$ and a diagonal matrix $D$ such that 
\[
D=P^{-1}MP.
\]
\end{definition}

We can summarize as follows.
\begin{itemize}
\item Change of basis rearranges the components of a vector by the change of basis matrix $P$, to give components in the new basis.
\item To get the matrix of a linear transformation in the new basis, we \emph{conjugate}\index{Conjugation} the matrix of $L$ by the change of basis matrix: $M\mapsto P^{-1}MP$.
\end{itemize}

If for two matrices $N$ and $M$ there exists a matrix $P$ such that $M=P^{-1}NP$, then we say that $M$ and $N$ are {\bf similar}\index{Similar matrices}.  Then the above discussion shows that diagonalizable matrices are similar to diagonal matrices.

\begin{corollary}
A square matrix $M$ is diagonalizable if and only if there exists a basis of eigenvectors for $M$. Moreover, these eigenvectors are the columns of a change of basis matrix \(P\) which diagonalizes \(M\).
\end{corollary}

%\href{\webworkurl ReadingHomework20/3/}{Reading homework: problem 20.3}
\Reading{Diagonalization}{2}

\begin{example}
Let's try to diagonalize the matrix
\[M=\begin{pmatrix}
-14 & -28 & -44 \\
-7 & -14 & -23 \\
9 & 18 & 29 \\
\end{pmatrix}.\]
The eigenvalues of \(M\) are determined by \[\det(M-\lambda I)=-\lambda^3+\lambda^2+2\lambda=0.\]
So the eigenvalues of \(M\) are \(-1,0,\) and \(2\), and associated eigenvectors turn out to be 
$$
v_1=\colvec{-8 \\ -1 \\ 3},~~ v_2=\colvec{-2 \\ 1 \\ 0}, {\rm ~and~~} v_3=\colvec{-1 \\ -1 \\ 1}.
$$ 
In order for \(M\) to be diagonalizable, we need the vectors \(v_1, v_2, v_3\) to be linearly independent. Notice that the matrix
\[P=\rowvec{v_1 & v_2 & v_3}=\begin{pmatrix}
-8 & -2 & -1 \\
-1 & 1 & -1 \\
3 & 0 & 1 \\
\end{pmatrix}\]
is invertible because its determinant is \(-1\). Therefore, the eigenvectors of \(M\) form a basis of \(\Re\), and so \(M\) is diagonalizable. 
Moreover, because the columns of $P$ are the components of eigenvectors, 
$$
MP=\rowvec{Mv_1 &Mv_2& Mv_3}=\rowvec{-1.v_1&0.v_2&2.v_3}=\rowvec{v_1& v_2 & v_3}\begin{pmatrix}
-1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 2 \\
\end{pmatrix}\, .
$$
Hence, the matrix \(P\) of eigenvectors is a change of basis matrix that diagonalizes~\(M\);
\[P^{-1}MP=\begin{pmatrix}
-1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 2 \\
\end{pmatrix}.\]
\end{example}

\Videoscriptlink{diagonalization_example.mp4}{$2\times2$ Example}{scripts_diagonalization_example}

\begin{figure}
\begin{center}
\includegraphics[scale=.33]{\diagPath/eigenbasis.jpg}
\end{center}
\caption{This theorem answers the question: ``What is diagonalization?''}
\end{figure}

%\section*{References}
%Hefferon, Chapter Three, Section V: Change of Basis
%\\
%Beezer, Chapter E, Section SD
%\\
%Beezer, Chapter R, Sections MR-CB
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Change_of_basis}{Change of Basis}
%\item \href{http://en.wikipedia.org/wiki/Diagonalizable_matrix}{Diagonalizable Matrix}
%\item \href{http://en.wikipedia.org/wiki/Similar_matrix}{Similar Matrix}
%\end{itemize}
%

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{Diagonalization}{1}, \hwrref{Diagonalization}{2}\\
No real eigenvalues &  \hwref{Diagonalization}{3}\\
Diagonalization &  \hwref{Diagonalization}{4}, \hwref{Diagonalization}{5},  \hwref{Diagonalization}{6},
 \hwref{Diagonalization}{7}\\
  \hline
\end{tabular}


\input{\diagPath/problems}

\newpage

","\chapter{\diagTitle}\label{sec:diagonalization}



Given a linear transformation, it is highly desirable to write its matrix  with respect to a basis of eigenvectors.

\section{Diagonalizability}\index{Diagonalization}

Suppose we are lucky, and we have $L \colon V\to V$, and the ordered basis 
$B=(v_1, \ldots, v_n )$ is a set of %linearly independent 
eigenvectors for $L$, with eigenvalues $\lambda_1, \ldots, \lambda_n$.  Then:

\begin{eqnarray*}
L(v_1)&=&\lambda_1 v_1 \\
L(v_2)&=&\lambda_2 v_2 \\
&\vdots & \\
L(v_n)&=&\lambda_n v_n \\
\end{eqnarray*}
As a result, the matrix of $L$ in the basis of eigenvectors $B$ is diagonal:
\[
L\begin{pmatrix}
x^1\\
x^2\\
\vdots\\
x^n
\end{pmatrix}_B
=
\left(
\begin{pmatrix}
\lambda_1    \\
& \lambda_2 &  & \\
&  & \ddots &  \\
& & & \lambda_n
\end{pmatrix}
\begin{pmatrix}
x^1\\
x^2\\
\vdots\\
x^n
\end{pmatrix}
\right)_B
,
\]
where all entries off the diagonal are zero.

Suppose that \(V\) is any \(n\)-dimensional vector space. We call a linear transformation $L \colon V\mapsto V$ \emph{diagonalizable}\index{Diagonalizable} if there exists a collection of $n$ linearly independent eigenvectors for $L$.  In other words, $L$ is diagonalizable if there exists a basis for $V$ of eigenvectors for $L$.  

In a basis of eigenvectors, the matrix of a linear transformation is diagonal.  On the other hand, if an $n \times n$ matrix is diagonal, then the standard basis vectors $e_i$ must already be a set of $n$ linearly independent eigenvectors.  We have shown:

\begin{theorem}
Given an ordered basis $B$ for a vector space $V$ and a linear transformation $L \colon V\rightarrow V$, then the matrix for $L$ in the basis $B$ is diagonal if and only if $B$ consists of eigenvectors for $L$.
\end{theorem}

\Videoscriptlink{diagonalization_derivative.mp4}{Non-diagonalizable example}{scripts_diagonalization_derivative}

%\begin{center}\href{\webworkurl ReadingHomework20/2/}{Reading homework: problem 20.2}\end{center}
\Reading{Diagonalization}{1}

Typically, however, we do not begin a problem with a basis of eigenvectors, but rather have to compute these. Hence we need to know how to change from one basis to another:

\section{Change of Basis}\index{Change of basis}

Suppose we have two  ordered bases $S=(v_1, \ldots, v_n )$ and 
$S'=(v'_1, \ldots, v'_n )$ for a vector space $V$. (Here $v_i$ and $v'_i$ are {\it vectors}, not components of vectors in a basis!) 
Then we may write each $v'_k$ uniquely as 
\[
v'_k = \sum_i v_ip^i_k\, ,
\]
this is $v'_k$ as a linear combination of the~$v_i$. 
In  matrix notation
\[ 
\rowvec{v'_1 , v'_2 , \cdots , v'_n} = \rowvec{v_1 , v_2 , \cdots , v_n}
\begin{pmatrix}p^1_1&p^1_2&\cdots &p^1_n\\ p^2_1 & p^2_2 && \\[2mm]
                                              \mc\vdots &&&\mc\vdots\\ p^n_1 &&\cdots & p^n_n\end{pmatrix}\, .
\]
Here, the $p^i_k$ are constants, which we can regard as entries of  a square matrix~$P=(p^i_k)$.  The matrix~$P$ must have an inverse since we can also write each~$v_j$ uniquely as a linear combination of the~$v'_k$;
\[
v_j = \sum_k v_k' q^k_j.
\]

Then we can write
\[
v_j = \sum_k \sum_i v_ip^i_kq^k_j.
\]
But $\sum_k p^i_kq^k_j$ is the $k,j$ entry of the product matrix  $PQ$.  Since the  expression for $v_j$ in the basis $S$ is $v_j$ itself, then $PQ$ maps each  $v_j$ to itself.  As a result, each $v_j$ is an eigenvector for $PQ$ with eigenvalue $1$, so $PQ$ is the identity, {\it i.e.}
$$
PQ=I \Leftrightarrow Q=P^{-1}\, .
$$

\vspace{1mm}
The matrix $P$ is  called a \emph{change of basis} matrix\index{Change of basis matrix}. There is a quick and dirty trick to obtain it; look at the formula above relating the new basis vectors
$v'_1,v'_2,\ldots v'_n$ to the old ones $v_1,v_2,\ldots,v_n$. In particular focus on $v'_1$ for which
$$
v'_1= \begin{pmatrix}v_1 , v_2 , \cdots , v_n\end{pmatrix}
\begin{pmatrix}p^1_1\\p^2_1\\\mc\vdots \\ p^n_1
\end{pmatrix}\, .
$$
This says that the first column of the change of basis matrix $P$ is really just the components of the vector $v'_1$ in the basis $v_1,v_2,\ldots,v_n$. 

\Shabox{1}{\begin{tabular}{c}The columns of the change of basis matrix are the components\\ of the new basis vectors  in terms of the old basis vectors.\end{tabular} }

\begin{example}
Suppose  $S'=(v'_1,v'_2)$ is an ordered  basis for a vector space $V$ and that with respect to some other ordered basis $S=(v_1, v_2)$ for $V$ 
$$
v'_1=
\begin{pmatrix}
\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}
\end{pmatrix} _S
\quad \mbox{and} \quad 
v'_2=
\begin{pmatrix}
\frac{1}{\sqrt{3}}\\-\frac{1}{\sqrt{3}}
\end{pmatrix}_S  \, .
$$
%What is the change of basis matrix $P$ from the old basis $u_1, u_2$ to the new basis $v_1,v_2$?
%
%Before answering note that the above 
This means 
$$
v'_1=\begin{pmatrix}v_1 , v_2 \end{pmatrix}\begin{pmatrix}
\frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}
\end{pmatrix}
=\frac{v_1+v_2}{\sqrt{2}}\quad\mbox{and}\quad
v'_2=\begin{pmatrix}v_1 , v_2 \end{pmatrix}\begin{pmatrix}
\frac{1}{\sqrt{3}}\\-\frac{1}{\sqrt{3}}
\end{pmatrix}
=\frac{v_1-v_2}{\sqrt{3}}\, .
$$
The change of basis matrix has as its columns just the components of $v'_1$ and $v'_2$;
$$
P= \begin{pmatrix}
\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{3}}\\
\frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{3}}
\end{pmatrix}\, .
$$
\end{example}


\vspace{1mm}


Changing basis changes the matrix of a linear transformation. However, as a map between vector spaces, {\bf the linear transformation is the same no matter which basis we use}. Linear transformations are the actual objects of study of this book, not matrices; matrices are merely a convenient way of doing computations.

\Videoscriptlink{diagonalization_basis.mp4}{Change of Basis Example}{scripts_diagonalization_basis}

Lets now calculate how the matrix of a linear transformation changes when changing basis.
To wit, let $L \colon V \longrightarrow W$ with matrix $M=(m^i_j)$ in the ordered input and output bases $S=(v_1, \ldots, v_n )$ and $T=(w_1,\ldots,w_m)$ so
\[
L(v_i) = \sum_k w_km^k_i.
\]
Now, suppose $S'=(v'_1, \ldots, v'_n )$ and $T'=(w'_1,\ldots,w'_m)$ are new  ordered input and out bases with matrix $M'=({m'}_i^k)$. Then
\[
L(v'_i)= \sum_k w_km'^k_i\, .
\]
%where 
%$D$ 
%$$D = (d^i_j)=\begin{pmatrix}
%\lambda_1    \\
%& \lambda_2 &  & \\
%&  & \ddots &  \\
%& & & \lambda_n
%\end{pmatrix}\, ,$$
%is the diagonal matrix whose diagonal entries $d^k_k$ are the eigenvalues~$\lambda_k$. 
Let $P=(p^i_j)$ be the change of basis matrix from input basis $S$ to the basis $S'$ and $Q=(q^j_k)$ be the change of basis matrix from output basis $T$ to the basis $T'$.  Then:
\[
L(v'_j)=L\left(\sum_i v_i p^i_j\right) = \sum_i L(v_i)p^i_j
= \sum_i \sum_k w_k m^k_i p^i_j.
\]
Meanwhile, we have:
\[
L(v'_i) = \sum_kv_km'^k_i = \sum_k \sum_j v_j q^j_km^k_i.
\]
Since the expression for a vector in a basis is unique, then we see that the entries of $MP$ are the same as the entries of $QM'$.  In other words, we see that
\Shabox{1.1}{
$
MP = QM' \qquad \text{or}\qquad M'=Q^{-1}MP.
$}

\begin{example}
Let $V$ be the space of polynomials in $t$ and degree 2 or less and $\sloppy{L:V\to {\mathbb R}^2}$ where
$$
L(1)=\colvec{1\\2}\, \quad L(t)=\colvec{2\\1}\, ,\quad L(t^2)=\colvec{3\\3}\, .
$$
From this information we can immediately read off the matrix~$M$ of $L$ in the bases $S=(1,t,t^2)$ and $T=(e_1,e_2)$, the standard basis for ${\mathbb R}^2$,
because
\begin{eqnarray*}\big(L(1),L(t),L(t^2)\big)&=&(e_1+2 e_2,2e_1+e_2, 3 e_1+3e_2)\\[2mm]&=&(e_1,e_2)\begin{pmatrix}1&2&3\\2&1&3\end{pmatrix}\, \Rightarrow \, M\ =\ 
\begin{pmatrix}1&2&3\\2&1&3\end{pmatrix}\, .\end{eqnarray*}
Now suppose we are more interested in the bases $$S'=(1+t,t+t^2,1+t^2)\, , \quad T'=\left(\colvec{1\\2},\colvec{2\\1}\right)=:(w_1',w_2')\, .$$
To compute the new matrix $M'$ of $L$ we could simply calculate what $L$ does the the new input basis vectors in terms of the new output basis vectors:
\begin{eqnarray*}
\big(L(1+t),L(t+t^2),L(1+t^2))&=&\left(\colvec{1\\2}+\colvec{2\\1},
\colvec{2\\1}+\colvec{3\\3},\colvec{1\\2}+\colvec{3\\3}
\right)\\[2mm]
&=&(w'_1+w'_2,w'_1+2w'_2,2w'_1+w'_2)\\[2mm]&=&(w'_1,w'_2)
\begin{pmatrix}1&1&2\\1&2&1\end{pmatrix}\, \Rightarrow \, 
M'=\begin{pmatrix}1&1&2\\1&2&1\end{pmatrix}\, .
\end{eqnarray*}
Alternatively we could calculate the change of basis matrices $P$ and $Q$ by noting that
$$
(1+t,t+t^2,1+t^2)=(1,t,t^2)\begin{pmatrix}1&0&1\\1&1&0\\0&1&1\end{pmatrix}\, \Rightarrow\, P=\begin{pmatrix}1&0&1\\1&1&0\\0&1&1\end{pmatrix}
$$
and
$$
(w'_1,w'_2)=(e_1+2e_2,2e_1+e_2)=(e_1,e_1)\begin{pmatrix}1&2\\2&1\end{pmatrix}\, \Rightarrow\, Q=\begin{pmatrix}1&2\\2&1\end{pmatrix}\, .
$$
Hence
$$
M'=Q^{-1}MP = -\frac{1}{3}\begin{pmatrix}1&-2\\-2&1\end{pmatrix}\begin{pmatrix}1&2&3\\2&1&3\end{pmatrix}
\begin{pmatrix}1&0&1\\1&1&0\\0&1&1\end{pmatrix}=\begin{pmatrix}1&1&2\\1&2&1\end{pmatrix}\, .
$$
Notice that the change of basis matrices $P$ and $Q$ are both square and invertible. Also, since we really wanted $Q^{-1}$, 
it is more efficient to try and write $(e_1,e_2)$ in terms of $(w'_1,w'_2)$ which would yield directly $Q^{-1}$. Alternatively, one can check that
$MP=QM'$.
\end{example}

\section{Changing to a Basis of Eigenvectors}

If we are changing to a basis of eigenvectors, then there are various simplifications:
\begin{itemize}
\item Since $L:V\to V$, most likely you already know the matrix~$M$ of $L$ using the same input basis as output basis $S=(u_1,\ldots ,u_n)$ (say).
\item In the new basis of eigenvectors $S'(v_1,\ldots,v_n)$, the matrix~$D$ of $L$ is diagonal because $Lv_i=\lambda_i v_i$ and so
$$
\big(L(v_1),L(v_2),\ldots,L(v_n)\big)=(v_1,v_2,\ldots, v_n)
\begin{pmatrix}
\lambda_1&\mc0&\cdots&\mc0\\
\mc0&\lambda_2&&\mc0\\
\mc\vdots&&\ddots&\mc\vdots \\
\mc0&\mc0&\cdots&\lambda_n\end{pmatrix}\, .
$$
\item If $P$ is the change of basis matrix from $S$ to $S'$, the diagonal matrix of eigenvalues~$D$ and the original matrix are related by
\Shabox{1.1}{$D=P^{-1}MP$}
\end{itemize}

This motivates the following definition:
\begin{definition}
A matrix $M$ is {\bf diagonalizable} if there exists an invertible matrix $P$ and a diagonal matrix $D$ such that 
\[
D=P^{-1}MP.
\]
\end{definition}

We can summarize as follows.
\begin{itemize}
\item Change of basis rearranges the components of a vector by the change of basis matrix $P$, to give components in the new basis.
\item To get the matrix of a linear transformation in the new basis, we \emph{conjugate}\index{Conjugation} the matrix of $L$ by the change of basis matrix: $M\mapsto P^{-1}MP$.
\end{itemize}

If for two matrices $N$ and $M$ there exists a matrix $P$ such that $M=P^{-1}NP$, then we say that $M$ and $N$ are {\bf similar}\index{Similar matrices}.  Then the above discussion shows that diagonalizable matrices are similar to diagonal matrices.

\begin{corollary}
A square matrix $M$ is diagonalizable if and only if there exists a basis of eigenvectors for $M$. Moreover, these eigenvectors are the columns of a change of basis matrix \(P\) which diagonalizes \(M\).
\end{corollary}

%\href{\webworkurl ReadingHomework20/3/}{Reading homework: problem 20.3}
\Reading{Diagonalization}{2}

\begin{example}
Let's try to diagonalize the matrix
\[M=\begin{pmatrix}
-14 & -28 & -44 \\
-7 & -14 & -23 \\
9 & 18 & 29 \\
\end{pmatrix}.\]
The eigenvalues of \(M\) are determined by \[\det(M-\lambda I)=-\lambda^3+\lambda^2+2\lambda=0.\]
So the eigenvalues of \(M\) are \(-1,0,\) and \(2\), and associated eigenvectors turn out to be 
$$
v_1=\colvec{-8 \\ -1 \\ 3},~~ v_2=\colvec{-2 \\ 1 \\ 0}, {\rm ~and~~} v_3=\colvec{-1 \\ -1 \\ 1}.
$$ 
In order for \(M\) to be diagonalizable, we need the vectors \(v_1, v_2, v_3\) to be linearly independent. Notice that the matrix
\[P=\rowvec{v_1 & v_2 & v_3}=\begin{pmatrix}
-8 & -2 & -1 \\
-1 & 1 & -1 \\
3 & 0 & 1 \\
\end{pmatrix}\]
is invertible because its determinant is \(-1\). Therefore, the eigenvectors of \(M\) form a basis of \(\Re\), and so \(M\) is diagonalizable. 
Moreover, because the columns of $P$ are the components of eigenvectors, 
$$
MP=\rowvec{Mv_1 &Mv_2& Mv_3}=\rowvec{-1.v_1&0.v_2&2.v_3}=\rowvec{v_1& v_2 & v_3}\begin{pmatrix}
-1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 2 \\
\end{pmatrix}\, .
$$
Hence, the matrix \(P\) of eigenvectors is a change of basis matrix that diagonalizes~\(M\);
\[P^{-1}MP=\begin{pmatrix}
-1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 2 \\
\end{pmatrix}.\]
\end{example}

\Videoscriptlink{diagonalization_example.mp4}{$2\times2$ Example}{scripts_diagonalization_example}

\begin{figure}
\begin{center}
\includegraphics[scale=.33]{\diagPath/eigenbasis.jpg}
\end{center}
\caption{This theorem answers the question: ``What is diagonalization?''}
\end{figure}

%\section*{References}
%Hefferon, Chapter Three, Section V: Change of Basis
%\\
%Beezer, Chapter E, Section SD
%\\
%Beezer, Chapter R, Sections MR-CB
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Change_of_basis}{Change of Basis}
%\item \href{http://en.wikipedia.org/wiki/Diagonalizable_matrix}{Diagonalizable Matrix}
%\item \href{http://en.wikipedia.org/wiki/Similar_matrix}{Similar Matrix}
%\end{itemize}
%

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{Diagonalization}{1}, \hwrref{Diagonalization}{2}\\
No real eigenvalues &  \hwref{Diagonalization}{3}\\
Diagonalization &  \hwref{Diagonalization}{4}, \hwref{Diagonalization}{5},  \hwref{Diagonalization}{6},
 \hwref{Diagonalization}{7}\\
  \hline
\end{tabular}


\input{\diagPath/problems}

\newpage

",lesson
22,Orthonormal Bases,"\chapter{Orthonormal Bases and Complements}\label{orthonormalbases}

You may have noticed that we have only rarely used the dot product. That is because many of the
results we have obtained do not require a preferred notion of lengths of vectors. Once a dot or inner product
is available, lengths of and angles between vectors can be measured--very powerful machinery and results are available
in this case.

\section{Properties of the Standard Basis}

The standard notion of the length of a vector $x=(x_1,x_2,\ldots,x_n)$ $\in$ ${\mathbb R}^n$ is 
$$||x||=\sqrt{x\dotprod x}=\sqrt{(x_1)^2+(x_2)^2+\cdots(x_n)^2}\, .$$
The canonical/standard basis in ${\mathbb R}^n$
\[
e_1=\colvec{1\\0\\ \vdots \\ 0}, \phantom{X}
e_2=\colvec{0\\1\\ \vdots \\ 0}, \phantom{X} \ldots, 
\phantom{X} e_n=\colvec{0\\0\\ \vdots \\ 1}\, ,
\]
has many useful properties with respect to the dot product and lengths.
\begin{itemize}
\item Each of the standard basis vectors has unit length;
\[
\|e_i\|=\sqrt{e_i\dotprod e_i}=\sqrt{e_i^Te_i}=1\, .\]
\item The standard basis vectors are \emph{orthogonal}\index{Orthogonal}\label{orthogonal} (in other words, at right angles or perpendicular); 
\[
e_i\dotprod e_j = e_i^Te_j=0 \text{ when } i\neq j
\]
\end{itemize}
This is summarized by
\[
e_i^Te_j=\delta_{ij}=\left\{ \begin{array}{cc}
1 & \qquad i=j \\
0 & \qquad i\neq j \\
\end{array}\right. ,
\]
where $\delta_{ij}$ is the \emph{Kronecker delta}\index{Kronecker delta}.  Notice that the Kronecker delta gives the entries of the identity matrix.

Given column vectors $v$ and $w$, we have seen that the dot product $v\dotprod w$ is the same as the matrix multiplication $v^Tw$.  This is an \emph{inner product}\index{Inner product} on~$\Re^n$.  We can also form the \emph{outer product}\index{Outer product} $vw^T$, which gives a square matrix.
The outer product on the standard basis vectors is interesting.  Set 
\begin{eqnarray*}
\Pi_1 &=& e_1e_1^T \\
  &=& \colvec{1\\0\\ \vdots \\ 0} \rowvec{1&0& \cdots & 0}\\
  &=& \begin{pmatrix}
      1 & 0 & \cdots & 0 \\
      0 & 0 & \cdots & 0 \\
      \vdots & & & \vdots \\
      0 & 0 &\cdots & 0\\
      \end{pmatrix}\\
&\vdots& \\
\Pi_n &=& e_n e_n^T \\
  &=& \colvec{0\\0\\ \vdots \\ 1} \rowvec{0&0& \cdots & 1}\\
  &=& \begin{pmatrix}
      0 & 0 & \cdots & 0 \\
      0 & 0 & \cdots & 0 \\
      \vdots & & & \vdots \\
      0 & 0 &\cdots & 1\\
      \end{pmatrix}\\
\end{eqnarray*}
In short, $\Pi_i$ is the diagonal square matrix with a $1$ in the $i$th diagonal position and zeros everywhere else\footnote{This is reminiscent of an older notation, where vectors are written in juxtaposition.  This is called a \href{http://en.wikipedia.org/wiki/Dyadic_tensor}{``dyadic tensor''}\index{Dyad}, and is still used in some applications. }.

Notice that 
$\Pi_i\Pi_j=e_ie_i^Te_je_j^T=e_i\delta_{ij}e_j^T$.  Then:
\[
\Pi_i\Pi_j = \left\{ \begin{array}{cc}
\Pi_i & \qquad i=j \\
0 & \qquad i\neq j \\
\end{array}\right. .
\]
Moreover, for a diagonal matrix $D$ with diagonal entries $\lambda_1,\ldots, \lambda_n$, we can write
\[
D= \lambda_1\Pi_1 + \cdots + \lambda_n\Pi_n.
\]

\section{Orthogonal and Orthonormal Bases}

There are many other bases that  behave in the same way as the standard basis.  As such, we will study:
\begin{itemize}
\item \emph{Orthogonal bases}\index{Orthogonal basis} $\{v_1, \ldots, v_n \}$:
\[
v_i\dotprod v_j=0 \text{ if } i\neq j\, .
\]
In other words, all vectors in the basis are perpendicular.

\item \emph{Orthonormal bases}\index{Orthonormal basis} $\{u_1, \ldots, u_n \}$:
\[
u_i\dotprod u_j = \delta_{ij}.
\]
In addition to being orthogonal, each vector has unit length.
\end{itemize}

Suppose $T=\{u_1, \ldots, u_n \}$ is an orthonormal basis for $\Re^n$.  Because $T$ is a basis, we can write any vector $v$ uniquely as a linear combination of the vectors in $T$;
\[
v=c^1u_1+\cdots c^nu_n.
\]
Since $T$ is orthonormal, there is a very easy way to find the coefficients of this linear combination.  By taking the dot product of $v$ with any of the vectors in $T$, we get
\begin{eqnarray*}
v\dotprod u_i &=& c^1u_1\dotprod u_i + \cdots + c^iu_i\dotprod u_i + \cdots + c^nu_n\dotprod u_i \\
& = & c^1\cdot 0 + \cdots + c^i\cdot 1 + \cdots + c^n\cdot 0 \\
& = & c^i, \\[1mm]
\Rightarrow\, c^i &=& v\dotprod u_i \\[1mm]
\Rightarrow\  v &=& (v\dotprod u_1) u_1 + \cdots + (v\dotprod u_n)u_n\\
&=& \sum_i (v\dotprod u_i)u_i\, .
\end{eqnarray*}

This proves the following theorem.
\begin{theorem}
For an orthonormal basis $\{u_1, \ldots, u_n \}$, any vector $v$ can be expressed as
\[
v =\sum_i (v\dotprod u_i)u_i.
\]
\end{theorem}

\Reading{OrthonormalBases}{1}
%\begin{center}\href{\webworkurl ReadingHomework21/1/}{Reading homework: problem \ref{orthonormalbases}.1}\end{center}

\Videoscriptlink{orthonormal_bases_sin_cos.mp4}{All orthonormal bases for $\mathbb{R}^2$}{scripts_orthonormal_bases_sin_cos}

\subsection{Orthonormal Bases and Dot Products}

To calculate lengths of, and angles between vectors in ${\mathbb R}^n$ we most commonly use the dot product:
$$
\colvec{v^1\\\mc \vdots \\ v^n}\cdot \colvec{w^1\\\mc \vdots \\ w^n}:=v^1 w^1 + \cdots + v^n w^n\, .
$$ 
When dealing with more general vector spaces
the dot product makes no sense, and one must instead choose an appropriate inner product. By ``appropriate'', we mean an inner product well-suited to the problem one is trying to solve. If the vector space $V$ under study has an orthonormal basis $O=(u_1,\ldots, u_n)$ meaning 
$$
\langle u_i,u_j\rangle=\delta_{ij}\, ,
$$
where $\langle\cdot,\cdot\rangle$ is the inner product, you might ask whether this can be related to a dot product? The answer to this question is yes and rather easy to understand:

Given an orthonormal basis, the information of two vectors $v$ and $v'$ in~$V$ can be encoded in column vectors
\begin{eqnarray*}
v&=&\, \langle v,u_1\rangle u_1 + \cdots +\langle v,u_n\rangle u_n\, =
\hspace{.3mm}(u_1,\ldots, u_n) \colvec{\langle v,u_1\rangle\\\mc\vdots\\\langle v,u_n\rangle}\hspace{.4mm}=\colvec{\langle v,u_1\rangle\\\mc\vdots\\\langle v,u_n\rangle}_{\! O}\, ,\\[2mm]
v'&=&\langle v',u_1\rangle u_1 + \cdots+ \langle v',u_n\rangle u_n=(u_1,\ldots, u_n)\hspace{-.2mm} \colvec{\langle v',u_1\rangle\\\mc\vdots\\\langle v',u_n\rangle}\hspace{-.2mm}=
\hspace{-.2mm}\colvec{\langle v',u_1\rangle\\\mc\vdots\\\langle v',u_n\rangle}_{\! O}\, .
\end{eqnarray*}
The dot product of these two column vectors is
$$
\colvec{\langle v,u_1\rangle\\\mc\vdots\\\langle v,u_n\rangle}\cdot \colvec{\langle v',u_1\rangle\\\mc\vdots\\\langle v',u_n\rangle}=
\langle v,u_1\rangle\langle v',u_1\rangle+\cdots+\langle v,u_n\rangle\langle v,'u_n\rangle\, .
$$
This agrees exactly with the inner product of $v$ and $v'$ because
\begin{eqnarray*}
\langle v,v'\rangle&=& \big\langle
\langle v,u_1\rangle u_1 + \cdots +\langle v,u_n\rangle u_n,\langle v',u_1\rangle u_1 + \cdots+ \langle v',u_n\rangle u_n
\big\rangle
\\[1mm]
&=&
\langle v,u_1\rangle \langle v',u_1\rangle \langle u_1,u_1\rangle + \langle v,u_2\rangle \langle v',u_1\rangle \langle u_2,u_1\rangle+\cdots \\&&\cdots+
\langle v,u_{n-1}\rangle \langle v',u_n\rangle \langle u_{n-1},u_n\rangle + \langle v,u_n\rangle \langle v',u_n\rangle \langle u_n,u_n\rangle\\[1mm]
&=&
\langle v,u_1\rangle\langle v',u_1\rangle+\cdots+\langle v,u_n\rangle\langle v',u_n\rangle\, .
\end{eqnarray*}
The above computation looks a little daunting, but only the linearity property of inner products and the fact that $\langle u_i,u_j\rangle$ can  equal either zero or one was used.
Because inner products become dot products once one uses an orthonormal basis, we will quite often use the dot product notation in situations where one really should write an inner product. Conversely, dot product computations can always be rewritten in terms of an inner product, if needed.

\begin{example}
Consider the space of polynomials given by $V={\rm span}\{1,x\}$ with inner product $\langle p,p'\rangle=\int_0^1  p(x) p'(x)dx $. An obvious basis to use is $B=(1,x)$ 
but it is not hard to check that this is not orthonormal, instead we take $$O=\Big(1,{2\sqrt{3}}\, \big(x-\frac12\big)\Big)\, .$$
This is an orthonormal basis  since,for example:
$$
\Big\langle{2\sqrt{3}}\big(x-\frac12\big),1\Big\rangle
={2\sqrt{3}}\int_0^1 \big(x-\frac12\big)dx=0\, ,$$
and 
$$
\Big\langle x-\frac12,x-\frac12\Big\rangle=
\int_0^1 \big(x-\frac12\big)^2dx=\frac{1}{12}=\Big(\frac{1}{2\sqrt{3}}\Big)^2\, .
$$
An arbitrary vector $v=a+b x$ is given in terms of the orthonormal basis $O$ by
$$
v=(a+\frac b2).1 + b \big(x-\frac12\big) =\Big(1,{2\sqrt{3}}\, \big(x-\frac12\big)\Big)
\colvec{a+\frac b2\\[2mm] \mc{\frac{b}{2\sqrt{3}}}}=\colvec{a+\frac b2\\[2mm] \mc{\frac{b}{2\sqrt{3}}}}_{\! O}\, .
$$
Hence   we can predict the inner product of $a+bx$ and $a'+b'x$ using the dot product:
$$\colvec{a+\frac b2\\[2mm] \mc{\frac{b}{2\sqrt{3}}}}\cdot \colvec{a'+\frac {b'}2\\[2mm] \mc{\frac{b'}{2\sqrt{3}}}}
=\Big(a+\frac b2\Big) \Big(a'+\frac {b'}2\Big) + \frac{b b'}{12}=aa'+\frac12\, (ab'+a'b) + \frac13\, bb'\, . $$
Indeed
$$
\langle a+bx,a'+b'x\rangle = \int_0^1 (a+bx)(a'+b'x)dx=aa'+\frac12\, (ab'+a'b) + \frac13\, bb'\, .
$$
\end{example}

\section{Relating Orthonormal Bases}

Suppose $T=\{u_1, \ldots, u_n \}$ and $R=\{w_1, \ldots, w_n \}$ are two orthonormal bases for $\Re^n$.  Then\\[3mm]
\begin{eqnarray*}
w_1 &=& (w_1\dotprod u_1) u_1 + \cdots + (w_1\dotprod u_n)u_n\\
 & \vdots & \\
w_n &=& (w_n\dotprod u_1) u_1 + \cdots + (w_n\dotprod u_n)u_n\\
\Rightarrow w_i &=& \sum_j u_j(u_j\dotprod w_i) \\
\end{eqnarray*}
Thus the matrix for the change of basis from $T$ to $R$ is given by 
\[
P = (p^j_i) = (u_j\dotprod w_i).
\]
We would like to calculate the product $PP^T$. For that, we first develop a dirty trick for products of dot products:
$$
(u\dotprod v)(w\dotprod z)=(u^T v) (w^T z) = u^T (v w^T) z\, . 
$$
The object $v w^T$ is the square matrix made from the outer product of $v$ and~$w$. 
Now we are ready to compute the components of the matrix product $PP^T$.
\begin{eqnarray*}
\sum_i(u_j\dotprod w_i)(w_i\dotprod u_k)&=&
 \sum_i(u_j^T w_i) (w_i^T u_k)\\
&=& u_j^T \left[\sum_i (w_i w_i^T) \right] u_k \\
&\stackrel{(*)}=& u_j^T I_n u_k \\\
&=& u_j^T u_k = \delta_{jk}.
\end{eqnarray*}
The equality $(*)$ is explained below.  Assuming $(*)$ holds, we have shown that $PP^T=I_n$, which implies that 
\[
P^T=P^{-1}.
\]

The equality in the line $(*)$ says that $\sum_i w_i w_i^T=I_n$.  To see this, we examine $\left(\sum_i w_i w_i^T\right)v$ for an arbitrary vector $v$.  We can find constants $c^j$ such that $v=\sum_j c^jw_j$, so that
\begin{eqnarray*}
\left(\sum_i w_i w_i^T\right)v &=& \left(\sum_i w_i w_i^T\right)\left(\sum_j c^jw_j\right) \\
&=& \sum_j c^j \sum_i w_i w_i^T w_j \\
&=& \sum_j c^j \sum_i w_i \delta_{ij} \\
&=& \sum_j c^j w_j \text{ since all terms with $i\neq j$ vanish}\\
&=&v.
\end{eqnarray*}
Thus, as a linear transformation, $\sum_i w_i w_i^T=I_n$ fixes every vector, and thus must be the identity $I_n$.

\begin{definition}
A matrix $P$ is {\bf orthogonal}\index{Orthogonal matrix} if $P^{-1}=P^T$.
\end{definition}

Then to summarize,
\begin{theorem}
A change of basis matrix $P$ relating two orthonormal bases is an orthogonal matrix.  \textit{I.e.},
\Shabox{1}{$
P^{-1}=P^T\, .
$}
\end{theorem}

\Reading{OrthonormalBases}{2}
%\begin{center}\href{\webworkurl ReadingHomework21/2/}{Reading homework: problem \ref{orthonormalbases}.2}\end{center}

\begin{example}
Consider $\Re^3$ with the ordered orthonormal basis 
\[
S=\left( u_1,u_2,u_3\right) 
=\left(
\colvec{\frac{2}{\sqrt{6}}\\[1mm] \frac{1}{\sqrt{6}}\\[1mm] \frac{-1}{\sqrt{6}}},
\colvec{0\\[1mm] \frac{1}{\sqrt{2}}\\[1mm] \frac{1}{\sqrt{2}}},
\colvec{\frac{1}{\sqrt{3}}\\[1mm] \frac{-1}{\sqrt{3}}\\[1mm] \frac{1}{\sqrt{3}}}
\right).
\]
Let $E$ be the standard basis $(e_1,e_2,e_3 )$.  Since we are changing from the standard basis to a new basis, then the columns of the change of basis matrix are exactly the  new basis vectors.  Then the change of basis matrix from $E$ to $S$ is given by
\begin{eqnarray*}
P=(P^j_i)=(e_j\cdot u_i)&=&
\begin{pmatrix}
e_1\dotprod u_1 & e_1\dotprod u_2 & e_1\dotprod u_3 \\
e_2\dotprod u_1 & e_2\dotprod u_2 & e_2\dotprod u_3 \\
e_3\dotprod u_1 & e_3\dotprod u_2 & e_3\dotprod u_3 \\
\end{pmatrix} \\
= \begin{pmatrix}
u_1 & u_2 & u_3
\end{pmatrix} 
&=&
\begin{pmatrix}
\frac{2}{\sqrt{6}} & 0 & \frac{1}{\sqrt{3}} \\[1mm]
\frac{1}{\sqrt{6}}& \frac{1}{\sqrt{2}}&\frac{-1}{\sqrt{3}}\\[1mm]
\frac{-1}{\sqrt{6}}& \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{3}}\\[1mm]
\end{pmatrix}. \\
\end{eqnarray*}

From our theorem, we observe that
\begin{eqnarray*}
P^{-1}=P^T &=& \colvec{u_1^T\\u_2^T\\u_3^T} \\
&=& \begin{pmatrix}
\frac{2}{\sqrt{6}}& \frac{1}{\sqrt{6}}& \frac{-1}{\sqrt{6}}\\[1mm]
0 & \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \\[1mm]
\frac{1}{\sqrt{3}}& \frac{-1}{\sqrt{3}}&\frac{1}{\sqrt{3}}\\[1mm]
\end{pmatrix}. \\
\end{eqnarray*}

We can check that $P^TP=I$ by a lengthy computation, or more simply, notice that
\begin{eqnarray*}
(P^TP)%_{ij}
&=& \colvec{u_1^T\\u_2^T\\u_3^T} \rowvec{u_1 & u_2& u_3} \\
&=& \begin{pmatrix}1&0&0\\0&1&0\\0&0&1\end{pmatrix}.
\end{eqnarray*}
Above we are using orthonormality of the $u_i$ and the fact that matrix multiplication amounts to taking dot products between rows and columns.
It is also very \hypertarget{basisorthog}{important to realize that the columns of an {\it orthogonal} matrix are made
from an {\it orthonormal} set of vectors}.
\end{example}

\begin{remark}[Orthonormal Change of Basis and Diagonal Matrices.]
Suppose $D$ is a diagonal matrix  and we are able to use an orthogonal matrix $P$ to change to a new basis.  Then the matrix $M$ of $D$ in the new basis is:
\[
M = PDP^{-1} = PDP^T.
\]
Now we calculate the transpose of $M$.
\begin{eqnarray*}
M^T &=& (PDP^T)^T\\
&=& (P^T)^TD^TP^T \\
&=& PDP^T\\
&=& M
\end{eqnarray*}
The matrix $M=PDP^T$ is symmetric!
\end{remark}


%\section*{References}
%Hefferon, Chapter Three, Section V: Change of Basis
%\\
%Beezer, Chapter V, Section O, Subsection N
%\\
%Beezer, Chapter VS, Section B, Subsection OBC
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Orthogonal_matrix}{Orthogonal Matrix}
%\item \href{http://en.wikipedia.org/wiki/Diagonalizable_matrix}{Diagonalizable Matrix}
%\item \href{http://en.wikipedia.org/wiki/Similar_matrix}{Similar Matrix}
%\end{itemize}

%\section{Review Problems}
%\input{\orthonormPath/problems}



\section{Gram-Schmidt \& Orthogonal Complements}\label{gramschmidt}


Given a vector $v$ and some other vector $u$ not in  $ {\rm span}\, \{v\} $ we can construct the new vector 
\Shabox{1.1}{$
v^\perp:=v-\frac{\textstyle u\cdot v}{\textstyle u\cdot u}\, u\, .
$}
\begin{center}
\hypertarget{projectionpic}{\input{vperp.pdftex_t}}
\end{center}
This new vector $v^\perp$ is orthogonal to $u$ because 
\[
u\dotprod v^\perp = u\dotprod v - \frac{u\cdot v}{u\cdot u}u\dotprod u = 0.
\]
Hence, $\{u, v^\perp\}$ is an orthogonal basis for $\spa \{u,v\}$.  When $v$ is not parallel to $u$, $v^\perp \neq 0$, and normalizing these vectors we obtain $\left\{\frac{u}{|u|}, \frac{v^\perp}{|v^\perp|} \right\}$, an orthonormal basis for the vector space ${\rm span}\, \{u,v\}$.

Sometimes we write $v = v^\perp + v^\parallel$ where:
\begin{eqnarray*}
v^\perp &=& v-\frac{u\cdot v}{u\cdot u}\, u \\[1mm]
v^\parallel &=& \phantom{v-}\frac{u\cdot v}{u\cdot u}\, u.
\end{eqnarray*}
This is called an {\bf orthogonal decomposition}\index{Orthogonal decomposition} because we have decomposed $v$ into a sum of orthogonal vectors.  This decomposition depends on $u$; if we change the direction of $u$ we change $v^\perp$ and $v^\parallel$.

If $u$, $v$ are linearly independent vectors in $\Re^3$, then the set $\{u, v^\perp, u\times v^\perp \}$ would be an orthogonal basis for $\Re^3$.  This set could then be normalized by dividing each vector by its length to obtain an orthonormal basis.

However, it often occurs that we are interested in vector spaces with dimension greater than $3$, and must resort to craftier means than cross products to obtain an orthogonal basis\footnote{Actually, given a set $T$ of $(n-1)$ independent vectors in $n$-space, one can define an analogue of the cross product that will produce a vector orthogonal to the span of~$T$, using a method exactly analogous to the usual computation for calculating the cross product of two vectors in $\Re^3$.  This only gets us the ``\emph{last}'' orthogonal vector, though; the Gram--Schmidt process described in this section gives a way to get a full orthogonal basis.}.
%yeah, and who is going to teach them about the Hodge star in office hours? I say nix this footnote -cherney

Given a third vector $w$, we should first check that $w$ does not lie in the 
$\spa\{ u,v\} $, \textit{i.e.}, check that $u,v$ and $w$ are linearly independent.   If it does not, we then can define
\[
w^\perp := w - \frac{u\dotprod w}{u\dotprod u}\,u - \frac{v^\perp\dotprod w}{v^\perp\dotprod v^\perp}\,v^\perp.
\]
We can check that \(u \dotprod w^\perp\) and \(v^\perp \dotprod w^\perp\) are both zero:
\begin{align*}
u \dotprod w^\perp&=u \dotprod \left(w - \frac{u\dotprod w}{u\dotprod u}\,u - \frac{v^\perp\dotprod w}{v^\perp\dotprod v^\perp}\,v^\perp \right)\\&= u\dotprod w - \frac{u \dotprod w}{u \dotprod u}u \dotprod u - \frac{v^\perp \dotprod w}{v^\perp \dotprod v^\perp} u \dotprod v^\perp \\
&=u\dotprod w-u\dotprod w-\frac{v^\perp \dotprod w}{v^\perp \dotprod v^\perp} u \dotprod v^\perp\ =\ 0
\end{align*}
since \(u\) is orthogonal to \(v^\perp\), and
\begin{align*}
v^\perp \dotprod w^\perp&=v^\perp \dotprod \left(w - \frac{u\dotprod w}{u\dotprod u}\,u - \frac{v^\perp\dotprod w}{v^\perp\dotprod v^\perp}\,v^\perp \right)\\ &=v^\perp\dotprod w - \frac{u \dotprod w}{u \dotprod u}v^\perp \dotprod u - \frac{v^\perp \dotprod w}{v^\perp \dotprod v^\perp} v^\perp \dotprod v^\perp \\
&=v^\perp\dotprod w-\frac{u \dotprod w}{u \dotprod u}v^\perp \dotprod u - v^\perp \dotprod w\ =\ 0
\end{align*}
because \(u\) is orthogonal to \(v^\perp\). Since $w^\perp$ is orthogonal to both $u$ and $v^\perp$, we have that $\{u,v^\perp,w^\perp \}$ is an orthogonal basis for $\spa \{u,v,w\}$.

\subsection{The Gram-Schmidt Procedure}
In fact, given an ordered set $(v_1, v_2, \ldots )$ of linearly independent vectors, we can define an orthogonal basis for $\spa \{v_1,v_2, \ldots \}$ consisting of the  vectors
\begin{eqnarray*}
v_1^\perp&:=&v_1 \\
v_2^\perp &:=& v_2 - \frac{v_1^\perp\cdot v_2}{v_1^\perp\cdot v_1^\perp}\,v_1^\perp \\
v_3^\perp &:=& v_3 - \frac{v_1^\perp\cdot v_3}{v_1^\perp\cdot v_1^\perp}\,v_1^\perp - \frac{v_2^\perp\cdot v_3}{v_2^\perp\cdot v_2^\perp}\,v_2^\perp\\
&\vdots& \\
v_i^\perp%&=&   v_i - \sum_{j<i} \frac{v_j^\perp\cdot v_i}{v_j^\perp\cdot v_j^\perp}\,v_j^\perp \\
 &:=& v_i - \frac{v_1^\perp\cdot v_i}{v_1^\perp\cdot v_1^\perp}\,v_1^\perp   
 - \frac{v_2^\perp\cdot v_i}{v_2^\perp\cdot v_2^\perp}\,v_2^\perp -\cdots
 - \frac{v_{i-1}^\perp\cdot v_i}{v_{i-1}^\perp\cdot v_{i-1}^\perp}\,v_{i-1}^\perp\\
&\vdots& \\
\end{eqnarray*}
Notice that each $v_i^\perp$ here depends on  $v_j^\perp$ for every $j<i$.  This allows us to inductively/algorithmically build up a linearly independent, orthogonal set of vectors 
$\{v_1^\perp,v_2^\perp, \ldots \}$ 
such that 
$\spa \{v_1^\perp,v_2^\perp, \ldots \}=\spa \{v_1, v_2, \ldots \}$. 
That is, an orthogonal basis for the latter vector space. 

Note that the set of vectors you start out with needs to be ordered to uniquely specify the algorithm; changing the order of the vectors will give a different orthogonal basis. You might need to be the one to put an order on the initial set of vectors.

This algorithm is called the {\bf Gram--Schmidt orthogonalization procedure}\index{Gram--Schmidt orthogonalization procedure}\label{GramSchmidt}--Gram worked at a Danish insurance company over one hundred years ago, Schmidt was a student of Hilbert (the famous German mathmatician).

\begin{example}
We'll  obtain an orthogonal basis for $\Re^3$ by appling Gram-Schmidt to the linearly independent set 
$\left\{\colvec{1\\1\\1}, \colvec{1\\1\\0},\colvec{3\\1\\1} \right\}$.\\

Because he Gram-Schmidt algorithm uses the first vector from the ordered set the largest number of times, we will choose the vector with the most zeros to be the first in hopes of simplifying computations; we choose to order the set as
$$( v_1,v_2,v_3)
:= \left( \colvec{1\\1\\0}, \colvec{1\\1\\1},\colvec{3\\1\\1} \right).$$

First, we set $v_1^\perp:=v_1$.  Then
\begin{eqnarray*}
v_2^\perp&:=& \rowvec{1\\1\\1} - \frac{2}{2}\rowvec{1\\1\\0} = \rowvec{0\\0\\1} \\[2mm]
v_3^\perp&:=& \rowvec{3\\1\\1} - \frac{4}{2}\rowvec{1\\1\\0} - \frac{1}{1}\rowvec{0\\0\\1} = \rowvec{1\\-1\\0}. 
\end{eqnarray*}
Then the set
\[
\left\{ \rowvec{1\\1\\0},\rowvec{0\\0\\1},\rowvec{1\\-1\\0}\right\}
\]
is an orthogonal basis for $\Re^3$.  To obtain an ortho{\it normal} basis we simply divide each of these vectors by its length, yielding
\[
\left\{ \rowvec{\frac{1}{\sqrt2}\\[2mm]\frac{1}{\sqrt2}\\[1mm]0},\rowvec{0\\[2mm]0\\[1mm]1},\rowvec{\frac{1}{\sqrt2}\\[2mm]\frac{-1}{\sqrt2}\\[1mm]0}\right\}.
\]
\end{example}

\Videoscriptlink{gram_schimdt_and_orthogonal_complements_4by4_example.mp4}{A $4\times4$ Gram--Schmidt Example} {scripts_gram_schmidt_and_orthogonal_complements_4by4_example}

\section{$QR$ Decomposition}
In Chapter~\ref{Matrices}, Section~\ref{LUdecomp} teaches you how to solve linear systems by decomposing a matrix $M$ into 
a product of lower and upper triangular matrices
$$M=LU\, .$$
The Gram--Schmidt procedure suggests another matrix decomposition,
$$M=QR\, ,$$ 
where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix. So-called QR-decompositions\index{QR decomposition}
are useful for solving linear systems, eigenvalue problems and least squares approximations. You can
easily get the idea behind the $QR$ decomposition by working through a simple example.

\begin{example}
\hypertarget{methodQR}{Find} the $QR$ decomposition of $$M=\begin{pmatrix}2&-1&1\\1&3&-2\\0&1&-2\end{pmatrix}\, .$$
What we will do is to think of the columns of $M$ as three 3-vectors and use Gram--Schmidt to
build an orthonormal basis from these that will become the columns of the orthogonal matrix $Q$.
We will use the matrix $R$ to record the steps of the Gram--Schmidt procedure in such a way
that the product $QR$ equals $M$. 

To begin with we write
$$
M=\begin{pmatrix}2&-\frac75&1\\[1mm]1&\frac{14}5&-2\\[1mm]0&1&-2\end{pmatrix}
\begin{pmatrix}1&\frac15&0\\[1mm]0&1&0\\[1mm]0&0&1\end{pmatrix}\, .
$$
In the first matrix the first two columns are  orthogonal because we simply replaced the second column of $M$ by the vector that the Gram--Schmidt
procedure produces from the first two columns of~$M$, namely
$$
\colvec{-\frac75\\[1mm]\frac{14}5\\[1mm]1}=\colvec{-1\\[1mm]3\\[1mm]1}-\frac15
\colvec{ 2 \\[1mm]1\\[1mm]0}\, .
$$
 The matrix on the right is almost the identity
matrix, save the $+\frac15$ in the second entry of the first row, whose effect upon multiplying the
two matrices precisely undoes what we we did to the second column of the first matrix. 

For the third column of $M$ we use Gram--Schmidt to deduce the third orthogonal vector
$$
\colvec{-\frac16\\[1mm]\frac13\\[1mm]-\frac76}=
\colvec{1\\[1mm]-2\\[1mm]-2}
-0
\colvec{ 2 \\[1mm]1\\[1mm]0}
-\frac{-9}{\frac{54}{5}}\colvec{-\frac75\\[1mm]\frac{14}5\\[1mm]1}\, ,
$$
and therefore, using exactly the same procedure write
$$
M=\begin{pmatrix}2&-\frac75&-\frac16\\[1mm]1&\frac{14}5&\frac13\\[1mm]0&1&-\frac76\end{pmatrix}
\begin{pmatrix}1&\frac15&0\\[1mm]0&1&-\frac56\\[1mm]0&0&1\end{pmatrix}\, .
$$
This is not quite the answer because the first matrix is now made of mutually orthogonal column vectors,
but  a {\it bona fide} orthogonal matrix is comprised of {\it orthonormal} vectors. To achieve that we divide
each column of the first matrix by its length and multiply the corresponding row of the second matrix by the same 
amount:
$$
M=\begin{pmatrix}\frac{2\sqrt{5}}{5}&-\frac{7\sqrt{30}}{90}&-\frac{\sqrt{6}}{18}\\[2mm]
\frac{\sqrt{5}}{5}&\frac{7\sqrt{30}}{45}&\frac{\sqrt{6}}{9}\\[2mm]
0&\frac{\sqrt{30}}{18}&-\frac{7\sqrt{6}}{18}\end{pmatrix}
\begin{pmatrix}\sqrt{5}&\frac{\sqrt{5}}{5}&0\\[2mm]
0&\frac{3\sqrt{30}}{5}&-\frac{\sqrt{30}}{2}\\[2mm]
0&0&\frac{\sqrt{6}}{2}\end{pmatrix}=QR\, .
$$
Geometrically what has happened here is easy to see. We started with three vectors given by the columns of $M$ and rotated them such that the first lies along the $x$-axis, the second in the $xy$-plane and the third in some other generic direction (here it happens to be in the $yz$-plane).

A nice check of the above result is to verify that entry $(i,j)$  of the matrix $R$
equals the dot product of the $i$-th column of $Q$ with the $j$-th column of $M$.
(Some people memorize this fact and use it as a recipe for computing $QR$ decompositions.)
{\it A good test of your own understanding is to work out why this is true!}
\end{example}


\Videoscriptlink{gram_schimdt_and_orthogonal_complements_qr_example.mp4}{Another $QR$ decomposition example}{scripts_gram_schmidt_and_orthogonal_complements_qr_example}

\section{Orthogonal Complements}

Let $U$ and $V$ be subspaces of a vector space $W$.  In \hyperref[UcapV]{Review Exercise}~\ref{UandV}, Chapter~\ref{subspacesspanning}, you are asked to show that $U\cap V$ is a subspace of $W$, and that $U\cup V$ is {\it not} a subspace.  However, $\spa (U\cup V)$ {\it is} certainly a subspace, since the span of \emph{any} subset of a vector space is a subspace.
Notice that all elements of $\spa (U\cup V)$ take the form $u+v$ with $u\in U$ and $v\in V$.  We call the subspace 
\Shabox{1}{$
U+V:=\spa (U\cup V) = \{u+v ~| ~u\in U, v\in V \}$}
the \emph{sum}\index{Sum of vectors spaces} of $U$ and $V$.  Here, we are not adding vectors, but vector spaces to produce a new vector space.
\begin{example}
\[
\spa\left\{  \colvec{1\\1\\0\\0},  \colvec{0\\1\\1\\0} \right \} 
+ \spa\left\{  \colvec{0\\1\\1\\0},  \colvec{0\\0\\1\\1} \right \} 
= \spa\left\{  \colvec{1\\1\\0\\0},  \colvec{0\\1\\1\\0},  \colvec{0\\0\\1\\1} \right \} .
\]
Notice that the addends have elements in common; 
$\colvec{0\\1\\1\\0}$ is in both addends. Even though both of the addends are 2-dimensional their sum is not  4-dimensional.
\end{example}


In the special case that $U$ and $V$ do not have any non-zero vectors in common, their sum is a vector space with dimension $\dim U + \dim V$. 


\begin{definition}
If  $U$ and $V$ are subspaces of a vector space $W$ such that 
$U~\cap~V=~\{0_W\}\, $
then the vector space
\[
U \oplus V := \spa (U\cup V)= \{u+v ~|~ u\in U, v\in V \}
\]
is the {\bf direct sum}\index{Direct sum} of $U$ and $V$.
\end{definition}

\begin{remark}

$\phantom{JUNK HERE}$\\[-8mm]

\begin{itemize}
\item When $U\cap V= \{0_W\}$, $U+V=U\oplus V.$ 
\item When $U\cap V \neq \{0_W\}$, $U+V \neq U\oplus V$.
\end{itemize}
\end{remark}

\noindent This distinction is important because the direct sum has a very nice property:

\begin{theorem}
If $w\in U\oplus V$  then 
%the expression $w=u+v$ is unique.  That is, 
there is only one way to write \(w\) as the sum of a vector in \(U\) and a vector in \(V\).  
\end{theorem}

\begin{proof}
Suppose that $u+v=u'+v'$, with $u,u'\in U$, and $v,v' \in V$.  Then we could express $0=(u-u')+(v-v')$.  Then $(u-u')=-(v-v')$.  Since $U$ and $V$ are subspaces, we have $(u-u')\in U$ and $-(v-v')\in V$.  But since these elements are equal, we also have $(u-u')\in V$.  Since $U\cap V=\{0\}$, then $(u-u')=0$.  Similarly, $(v-v')=0$. Therefore $u=u'$ and  $v=v'$, proving the theorem. 
\end{proof}

\Reading{OrthonormalBases}{3}
%\begin{center}\href{\webworkurl ReadingHomework22/1/}{Reading homework: problem \ref{gramschmidt}.1}\end{center}


\noindent Here is a sophisticated algebra question:
\begin{quote}
Given a subspace $U$ in $W$, what are the solutions to 
$$ U\oplus V =W.$$
That is, how can we write $W$ as the direct sum of $U$ and \emph{something}? 
\end{quote}
There is not a unique answer to this question as can be seen from the following picture of subspaces in $W={\mathbb R}^3$. 
\begin{center}
\includegraphics[scale=.25]{\gramSchmidtPath/direct_sums.jpg}
\end{center}
However, using the inner product, there is a natural candidate $U^\perp$ for this second subspace as shown below.
\begin{center}
\includegraphics[scale=.25]{\gramSchmidtPath/U_perp.jpg}
\end{center}


\begin{definition}
If $U$ is a subspace of the vector space $W$ then the vector space 
$$U^\perp := \big\{w\in W \big| w\dotprod u=0 \text{ for all } u\in U\big\}\, $$
is the {\bf orthogonal complement}\index{Orthogonal complement} of $U$ in $W$. 
\end{definition}


\begin{remark}
The symbols ``$U^\perp$"" are often read as ``$U$-perp''.  This is the set of all vectors in $W$ orthogonal to \emph{every} vector in $U$. 
Notice also that in the above definition we have implicitly assumed that the inner product is the dot product. For a general \hyperlink{inner_product}{inner product}, the  
above definition would read $U^\perp := \big\{w\in W \big|\,  \langle w,u\rangle=0 \ {\rm  for\  all } \ u\in U\big\}\, $.
\end{remark}
Possibly by now you are feeling overwhelmed, it may help to watch this quick overview video.

\Videoscriptlink{gram_schmidt_and_orthogonal_complements_theory.mp4}{Overview}{scripts_gram_schmidt_and_orthogonal_complements_theory}



\begin{example}
Consider any plane $P$ through the origin in $\Re^3$.  Then $P$ is a subspace, and $P^\perp$ is the line through the origin orthogonal to $P$.  For example, if $P$ is the $xy$-plane, then
\[
\Re^3=P\oplus P^\perp=\{(x,y,0)| x,y\in \Re \} \oplus \{(0,0,z)| z\in \Re \}.
\]
\end{example}

\begin{theorem}
Let $U$ be a subspace of a finite-dimensional vector space~$W$.  Then the set $U^\perp$ is a subspace of~$W$, and $W=U\oplus U^\perp$\index{Perp@``Perp''}.
\end{theorem}

\begin{proof}
First, to see that $U^\perp$ is a subspace, we only need to check closure, which requires a simple check:
Suppose $v,w\in U^\perp$, then we know  $$v\dotprod u = 0 = w\dotprod u \quad (\forall u\in U)\, .$$
Hence
$$\Rightarrow u\dotprod(\alpha v+\beta w)= \alpha u\dotprod v + \beta u\dotprod w =0\quad (\forall u\in U)\, ,$$ 
and so $\alpha v+\beta w\in U^\perp$.

Next, to form a direct sum between $U$ and $U^\perp$ we need to show
that $U\cap U^\perp=\{0\}$. This holds because if $u\in U$ and $u\in U^\perp$ it follows that
\[
u\dotprod u = 0 \Leftrightarrow u=0.
\]

Finally, we show that any vector $w\in W$ is in $U\oplus U^\perp$.  (This is where we use the assumption that $W$ is finite-dimensional.)  Let $e_1, \ldots, e_n$ be an orthonormal basis for $U$.  Set: 
\begin{eqnarray*}
u\ &=&(w\dotprod e_1)e_1 + \cdots + (w\dotprod e_n)e_n \in U\, ,\\
u^\perp&=& w-u\, .
\end{eqnarray*}
It is easy to check that $u^\perp \in U^\perp$ (see the Gram-Schmidt procedure).  Then $w=u+u^\perp$, so $w\in U\oplus U^\perp$, and we are done.
\end{proof}

\Reading{OrthonormalBases}{4}
%\begin{center}\href{\webworkurl ReadingHomework22/2/}{Reading homework: problem \ref{gramschmidt}.2}\end{center}

\begin{example}
Consider any line \(L\) through the origin in \(\Re^4\). Then \(L\) is a subspace, and \(L^\perp\) is a \(3\)-dimensional subspace orthogonal to \(L\). For example, let 
$$L=\spa \left\{ \colvec{1\\1\\1\\1} \right\}$$ 
be a line in \(\Re^4.\) Then 
\begin{eqnarray*}
L^\perp&=&\left \{  \colvec{x\\y\\z\\w} \in \Re^4 ~\middle| ~ (x,y,z,w) \dotprod (1,1,1,1)=0 \right\} \\
&=&\left\{ \colvec{x\\y\\z\\w} \in \Re^4 ~\middle| ~ x+y+z+w=0 \right \}.
\end{eqnarray*}

\noindent
Using the Gram-Schmidt procedure one may  find an orthogonal basis for 
\(L^\perp\). The set 
$$
\left\{
\colvec{1\\-1\\0\\0}, \colvec{1\\0\\-1\\0}, \colvec{1\\0\\0\\-1} \right \}\, 
$$ 
forms a basis for \(L^\perp\)  so, first, we order the basis as 
$$
( v_1,v_2,v_2)= 
\left(
\colvec{1\\-1\\0\\0}, \colvec{1\\0\\-1\\0}, \colvec{1\\0\\0\\-1} \right)\,. $$
Next, we set \(v_1^\perp=v_1\). Then
\begin{eqnarray*}
v_2^\perp&=&\colvec{1\\0\\-1\\0}-\frac{1}{2}\colvec{1\\-1\\0\\0}
=\colvec{\frac{1}{2}\\[1mm] \frac{1}{2} \\-1\\ 0 },\\
v_3^\perp&=&\colvec{ 1\\0\\0\\-1} -\frac{1}{2}\colvec{1\\-1\\0\\0}-\frac{1/2}{3/2}
\colvec{ \frac{1}{2}\\[1mm] \frac{1}{2}\\-1\\0} =\colvec{ \frac{1}{3}\\[1mm]\frac{1}{3}\\[1mm]\frac{1}{3}\\-1}.
\end{eqnarray*}
So the set 
\[\left\{ \colvec{ 1\\-1\\0\\0}, \colvec{\frac{1}{2}\\ \frac{1}{2}\\-1\\ 0}, \colvec{\frac{1}{3}\\\frac{1}{3}\\\frac{1}{3}\\-1} \right\} \] is an orthogonal basis for \(L^\perp\).
Dividing each basis vector by its length yields 
\[
\left\{
\colvec{  \frac{1}{\sqrt{2}  }\\[.5mm] -\frac{1}{\sqrt{2}}\\[.5mm]\mc{0}\\[.5mm]\mc{0} },
\colvec{ \frac{1}{\sqrt{6}}\\[.5mm] \frac{1}{\sqrt{6}}\\[.5mm] -\frac{2}{\sqrt{6}}\\[.5mm]\mc{\, 0} },
\colvec{ \frac{\sqrt{3}}{6}\\[.5mm] \frac{\sqrt{3}}{6}\\[.5mm] \frac{\sqrt{3}}{6}\\[.5mm] -\frac{\sqrt{3}}{2} }
\right\},
\]
and orthonormal basis for \(L^\perp\). 
Moreover, we have
\[
\Re^4=L \oplus L^\perp = 
\left\{ \colvec{c\\c\\c\\c} \middle| c \in \Re  \right\} 
\oplus \left\{   \colvec{x\\y\\z\\w} \in \Re^4 \middle| \,  x+y+z+w=0\right\},
\]
a decomposition of $\Re^4$ into a line and its three dimensional orthogonal complement.
\end{example}

Notice that for any subspace $U$, the subspace $(U^\perp)^\perp$ is just $U$ again.  As such, $\perp$ is an {\it involution}\index{Involution} on the set of subspaces of a vector space. (An involution is any mathematical operation which performed twice does nothing.)

%\section*{References}
%Hefferon, Chapter Three, Section VI.2: Gram-Schmidt Orthogonalization
%\\
%Beezer, Chapter V, Section O, Subsection GSP
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Gram_schmidt}{Gram-Schmidt Process}
%\item \href{http://en.wikipedia.org/wiki/QR_decomposition}{QR Decomposition}
%\item \href{http://en.wikipedia.org/wiki/Orthonormal_basis}{Orthonormal Basis}
%\item \href{http://en.wikipedia.org/wiki/Direct_sum}{Direct Sum}
%\end{itemize}
%

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{OrthonormalBases}{1}, 
 \hwrref{OrthonormalBases}{2}, 
 \hwrref{OrthonormalBases}{3}, 
 \hwrref{OrthonormalBases}{4}\\
Gram--Schmidt &  \hwref{OrthonormalBases}{5}\\
Orthogonal eigenbasis &  \hwref{OrthonormalBases}{6}, \hwref{OrthonormalBases}{7}\\
 Orthogonal complement&\hwref{OrthonormalBases}{8}\\
   \hline
\end{tabular}


\input{\gramSchmidtPath/problems}

\newpage


","\chapter{Orthonormal Bases and Complements}\label{orthonormalbases}

You may have noticed that we have only rarely used the dot product. That is because many of the
results we have obtained do not require a preferred notion of lengths of vectors. Once a dot or inner product
is available, lengths of and angles between vectors can be measured--very powerful machinery and results are available
in this case.

\section{Properties of the Standard Basis}

The standard notion of the length of a vector $x=(x_1,x_2,\ldots,x_n)$ $\in$ ${\mathbb R}^n$ is 
$$||x||=\sqrt{x\dotprod x}=\sqrt{(x_1)^2+(x_2)^2+\cdots(x_n)^2}\, .$$
The canonical/standard basis in ${\mathbb R}^n$
\[
e_1=\colvec{1\\0\\ \vdots \\ 0}, \phantom{X}
e_2=\colvec{0\\1\\ \vdots \\ 0}, \phantom{X} \ldots, 
\phantom{X} e_n=\colvec{0\\0\\ \vdots \\ 1}\, ,
\]
has many useful properties with respect to the dot product and lengths.
\begin{itemize}
\item Each of the standard basis vectors has unit length;
\[
\|e_i\|=\sqrt{e_i\dotprod e_i}=\sqrt{e_i^Te_i}=1\, .\]
\item The standard basis vectors are \emph{orthogonal}\index{Orthogonal}\label{orthogonal} (in other words, at right angles or perpendicular); 
\[
e_i\dotprod e_j = e_i^Te_j=0 \text{ when } i\neq j
\]
\end{itemize}
This is summarized by
\[
e_i^Te_j=\delta_{ij}=\left\{ \begin{array}{cc}
1 & \qquad i=j \\
0 & \qquad i\neq j \\
\end{array}\right. ,
\]
where $\delta_{ij}$ is the \emph{Kronecker delta}\index{Kronecker delta}.  Notice that the Kronecker delta gives the entries of the identity matrix.

Given column vectors $v$ and $w$, we have seen that the dot product $v\dotprod w$ is the same as the matrix multiplication $v^Tw$.  This is an \emph{inner product}\index{Inner product} on~$\Re^n$.  We can also form the \emph{outer product}\index{Outer product} $vw^T$, which gives a square matrix.
The outer product on the standard basis vectors is interesting.  Set 
\begin{eqnarray*}
\Pi_1 &=& e_1e_1^T \\
  &=& \colvec{1\\0\\ \vdots \\ 0} \rowvec{1&0& \cdots & 0}\\
  &=& \begin{pmatrix}
      1 & 0 & \cdots & 0 \\
      0 & 0 & \cdots & 0 \\
      \vdots & & & \vdots \\
      0 & 0 &\cdots & 0\\
      \end{pmatrix}\\
&\vdots& \\
\Pi_n &=& e_n e_n^T \\
  &=& \colvec{0\\0\\ \vdots \\ 1} \rowvec{0&0& \cdots & 1}\\
  &=& \begin{pmatrix}
      0 & 0 & \cdots & 0 \\
      0 & 0 & \cdots & 0 \\
      \vdots & & & \vdots \\
      0 & 0 &\cdots & 1\\
      \end{pmatrix}\\
\end{eqnarray*}
In short, $\Pi_i$ is the diagonal square matrix with a $1$ in the $i$th diagonal position and zeros everywhere else\footnote{This is reminiscent of an older notation, where vectors are written in juxtaposition.  This is called a \href{http://en.wikipedia.org/wiki/Dyadic_tensor}{``dyadic tensor''}\index{Dyad}, and is still used in some applications. }.

Notice that 
$\Pi_i\Pi_j=e_ie_i^Te_je_j^T=e_i\delta_{ij}e_j^T$.  Then:
\[
\Pi_i\Pi_j = \left\{ \begin{array}{cc}
\Pi_i & \qquad i=j \\
0 & \qquad i\neq j \\
\end{array}\right. .
\]
Moreover, for a diagonal matrix $D$ with diagonal entries $\lambda_1,\ldots, \lambda_n$, we can write
\[
D= \lambda_1\Pi_1 + \cdots + \lambda_n\Pi_n.
\]

\section{Orthogonal and Orthonormal Bases}

There are many other bases that  behave in the same way as the standard basis.  As such, we will study:
\begin{itemize}
\item \emph{Orthogonal bases}\index{Orthogonal basis} $\{v_1, \ldots, v_n \}$:
\[
v_i\dotprod v_j=0 \text{ if } i\neq j\, .
\]
In other words, all vectors in the basis are perpendicular.

\item \emph{Orthonormal bases}\index{Orthonormal basis} $\{u_1, \ldots, u_n \}$:
\[
u_i\dotprod u_j = \delta_{ij}.
\]
In addition to being orthogonal, each vector has unit length.
\end{itemize}

Suppose $T=\{u_1, \ldots, u_n \}$ is an orthonormal basis for $\Re^n$.  Because $T$ is a basis, we can write any vector $v$ uniquely as a linear combination of the vectors in $T$;
\[
v=c^1u_1+\cdots c^nu_n.
\]
Since $T$ is orthonormal, there is a very easy way to find the coefficients of this linear combination.  By taking the dot product of $v$ with any of the vectors in $T$, we get
\begin{eqnarray*}
v\dotprod u_i &=& c^1u_1\dotprod u_i + \cdots + c^iu_i\dotprod u_i + \cdots + c^nu_n\dotprod u_i \\
& = & c^1\cdot 0 + \cdots + c^i\cdot 1 + \cdots + c^n\cdot 0 \\
& = & c^i, \\[1mm]
\Rightarrow\, c^i &=& v\dotprod u_i \\[1mm]
\Rightarrow\  v &=& (v\dotprod u_1) u_1 + \cdots + (v\dotprod u_n)u_n\\
&=& \sum_i (v\dotprod u_i)u_i\, .
\end{eqnarray*}

This proves the following theorem.
\begin{theorem}
For an orthonormal basis $\{u_1, \ldots, u_n \}$, any vector $v$ can be expressed as
\[
v =\sum_i (v\dotprod u_i)u_i.
\]
\end{theorem}

\Reading{OrthonormalBases}{1}
%\begin{center}\href{\webworkurl ReadingHomework21/1/}{Reading homework: problem \ref{orthonormalbases}.1}\end{center}

\Videoscriptlink{orthonormal_bases_sin_cos.mp4}{All orthonormal bases for $\mathbb{R}^2$}{scripts_orthonormal_bases_sin_cos}

\subsection{Orthonormal Bases and Dot Products}

To calculate lengths of, and angles between vectors in ${\mathbb R}^n$ we most commonly use the dot product:
$$
\colvec{v^1\\\mc \vdots \\ v^n}\cdot \colvec{w^1\\\mc \vdots \\ w^n}:=v^1 w^1 + \cdots + v^n w^n\, .
$$ 
When dealing with more general vector spaces
the dot product makes no sense, and one must instead choose an appropriate inner product. By ``appropriate'', we mean an inner product well-suited to the problem one is trying to solve. If the vector space $V$ under study has an orthonormal basis $O=(u_1,\ldots, u_n)$ meaning 
$$
\langle u_i,u_j\rangle=\delta_{ij}\, ,
$$
where $\langle\cdot,\cdot\rangle$ is the inner product, you might ask whether this can be related to a dot product? The answer to this question is yes and rather easy to understand:

Given an orthonormal basis, the information of two vectors $v$ and $v'$ in~$V$ can be encoded in column vectors
\begin{eqnarray*}
v&=&\, \langle v,u_1\rangle u_1 + \cdots +\langle v,u_n\rangle u_n\, =
\hspace{.3mm}(u_1,\ldots, u_n) \colvec{\langle v,u_1\rangle\\\mc\vdots\\\langle v,u_n\rangle}\hspace{.4mm}=\colvec{\langle v,u_1\rangle\\\mc\vdots\\\langle v,u_n\rangle}_{\! O}\, ,\\[2mm]
v'&=&\langle v',u_1\rangle u_1 + \cdots+ \langle v',u_n\rangle u_n=(u_1,\ldots, u_n)\hspace{-.2mm} \colvec{\langle v',u_1\rangle\\\mc\vdots\\\langle v',u_n\rangle}\hspace{-.2mm}=
\hspace{-.2mm}\colvec{\langle v',u_1\rangle\\\mc\vdots\\\langle v',u_n\rangle}_{\! O}\, .
\end{eqnarray*}
The dot product of these two column vectors is
$$
\colvec{\langle v,u_1\rangle\\\mc\vdots\\\langle v,u_n\rangle}\cdot \colvec{\langle v',u_1\rangle\\\mc\vdots\\\langle v',u_n\rangle}=
\langle v,u_1\rangle\langle v',u_1\rangle+\cdots+\langle v,u_n\rangle\langle v,'u_n\rangle\, .
$$
This agrees exactly with the inner product of $v$ and $v'$ because
\begin{eqnarray*}
\langle v,v'\rangle&=& \big\langle
\langle v,u_1\rangle u_1 + \cdots +\langle v,u_n\rangle u_n,\langle v',u_1\rangle u_1 + \cdots+ \langle v',u_n\rangle u_n
\big\rangle
\\[1mm]
&=&
\langle v,u_1\rangle \langle v',u_1\rangle \langle u_1,u_1\rangle + \langle v,u_2\rangle \langle v',u_1\rangle \langle u_2,u_1\rangle+\cdots \\&&\cdots+
\langle v,u_{n-1}\rangle \langle v',u_n\rangle \langle u_{n-1},u_n\rangle + \langle v,u_n\rangle \langle v',u_n\rangle \langle u_n,u_n\rangle\\[1mm]
&=&
\langle v,u_1\rangle\langle v',u_1\rangle+\cdots+\langle v,u_n\rangle\langle v',u_n\rangle\, .
\end{eqnarray*}
The above computation looks a little daunting, but only the linearity property of inner products and the fact that $\langle u_i,u_j\rangle$ can  equal either zero or one was used.
Because inner products become dot products once one uses an orthonormal basis, we will quite often use the dot product notation in situations where one really should write an inner product. Conversely, dot product computations can always be rewritten in terms of an inner product, if needed.

\begin{example}
Consider the space of polynomials given by $V={\rm span}\{1,x\}$ with inner product $\langle p,p'\rangle=\int_0^1  p(x) p'(x)dx $. An obvious basis to use is $B=(1,x)$ 
but it is not hard to check that this is not orthonormal, instead we take $$O=\Big(1,{2\sqrt{3}}\, \big(x-\frac12\big)\Big)\, .$$
This is an orthonormal basis  since,for example:
$$
\Big\langle{2\sqrt{3}}\big(x-\frac12\big),1\Big\rangle
={2\sqrt{3}}\int_0^1 \big(x-\frac12\big)dx=0\, ,$$
and 
$$
\Big\langle x-\frac12,x-\frac12\Big\rangle=
\int_0^1 \big(x-\frac12\big)^2dx=\frac{1}{12}=\Big(\frac{1}{2\sqrt{3}}\Big)^2\, .
$$
An arbitrary vector $v=a+b x$ is given in terms of the orthonormal basis $O$ by
$$
v=(a+\frac b2).1 + b \big(x-\frac12\big) =\Big(1,{2\sqrt{3}}\, \big(x-\frac12\big)\Big)
\colvec{a+\frac b2\\[2mm] \mc{\frac{b}{2\sqrt{3}}}}=\colvec{a+\frac b2\\[2mm] \mc{\frac{b}{2\sqrt{3}}}}_{\! O}\, .
$$
Hence   we can predict the inner product of $a+bx$ and $a'+b'x$ using the dot product:
$$\colvec{a+\frac b2\\[2mm] \mc{\frac{b}{2\sqrt{3}}}}\cdot \colvec{a'+\frac {b'}2\\[2mm] \mc{\frac{b'}{2\sqrt{3}}}}
=\Big(a+\frac b2\Big) \Big(a'+\frac {b'}2\Big) + \frac{b b'}{12}=aa'+\frac12\, (ab'+a'b) + \frac13\, bb'\, . $$
Indeed
$$
\langle a+bx,a'+b'x\rangle = \int_0^1 (a+bx)(a'+b'x)dx=aa'+\frac12\, (ab'+a'b) + \frac13\, bb'\, .
$$
\end{example}

\section{Relating Orthonormal Bases}

Suppose $T=\{u_1, \ldots, u_n \}$ and $R=\{w_1, \ldots, w_n \}$ are two orthonormal bases for $\Re^n$.  Then\\[3mm]
\begin{eqnarray*}
w_1 &=& (w_1\dotprod u_1) u_1 + \cdots + (w_1\dotprod u_n)u_n\\
 & \vdots & \\
w_n &=& (w_n\dotprod u_1) u_1 + \cdots + (w_n\dotprod u_n)u_n\\
\Rightarrow w_i &=& \sum_j u_j(u_j\dotprod w_i) \\
\end{eqnarray*}
Thus the matrix for the change of basis from $T$ to $R$ is given by 
\[
P = (p^j_i) = (u_j\dotprod w_i).
\]
We would like to calculate the product $PP^T$. For that, we first develop a dirty trick for products of dot products:
$$
(u\dotprod v)(w\dotprod z)=(u^T v) (w^T z) = u^T (v w^T) z\, . 
$$
The object $v w^T$ is the square matrix made from the outer product of $v$ and~$w$. 
Now we are ready to compute the components of the matrix product $PP^T$.
\begin{eqnarray*}
\sum_i(u_j\dotprod w_i)(w_i\dotprod u_k)&=&
 \sum_i(u_j^T w_i) (w_i^T u_k)\\
&=& u_j^T \left[\sum_i (w_i w_i^T) \right] u_k \\
&\stackrel{(*)}=& u_j^T I_n u_k \\\
&=& u_j^T u_k = \delta_{jk}.
\end{eqnarray*}
The equality $(*)$ is explained below.  Assuming $(*)$ holds, we have shown that $PP^T=I_n$, which implies that 
\[
P^T=P^{-1}.
\]

The equality in the line $(*)$ says that $\sum_i w_i w_i^T=I_n$.  To see this, we examine $\left(\sum_i w_i w_i^T\right)v$ for an arbitrary vector $v$.  We can find constants $c^j$ such that $v=\sum_j c^jw_j$, so that
\begin{eqnarray*}
\left(\sum_i w_i w_i^T\right)v &=& \left(\sum_i w_i w_i^T\right)\left(\sum_j c^jw_j\right) \\
&=& \sum_j c^j \sum_i w_i w_i^T w_j \\
&=& \sum_j c^j \sum_i w_i \delta_{ij} \\
&=& \sum_j c^j w_j \text{ since all terms with $i\neq j$ vanish}\\
&=&v.
\end{eqnarray*}
Thus, as a linear transformation, $\sum_i w_i w_i^T=I_n$ fixes every vector, and thus must be the identity $I_n$.

\begin{definition}
A matrix $P$ is {\bf orthogonal}\index{Orthogonal matrix} if $P^{-1}=P^T$.
\end{definition}

Then to summarize,
\begin{theorem}
A change of basis matrix $P$ relating two orthonormal bases is an orthogonal matrix.  \textit{I.e.},
\Shabox{1}{$
P^{-1}=P^T\, .
$}
\end{theorem}

\Reading{OrthonormalBases}{2}
%\begin{center}\href{\webworkurl ReadingHomework21/2/}{Reading homework: problem \ref{orthonormalbases}.2}\end{center}

\begin{example}
Consider $\Re^3$ with the ordered orthonormal basis 
\[
S=\left( u_1,u_2,u_3\right) 
=\left(
\colvec{\frac{2}{\sqrt{6}}\\[1mm] \frac{1}{\sqrt{6}}\\[1mm] \frac{-1}{\sqrt{6}}},
\colvec{0\\[1mm] \frac{1}{\sqrt{2}}\\[1mm] \frac{1}{\sqrt{2}}},
\colvec{\frac{1}{\sqrt{3}}\\[1mm] \frac{-1}{\sqrt{3}}\\[1mm] \frac{1}{\sqrt{3}}}
\right).
\]
Let $E$ be the standard basis $(e_1,e_2,e_3 )$.  Since we are changing from the standard basis to a new basis, then the columns of the change of basis matrix are exactly the  new basis vectors.  Then the change of basis matrix from $E$ to $S$ is given by
\begin{eqnarray*}
P=(P^j_i)=(e_j\cdot u_i)&=&
\begin{pmatrix}
e_1\dotprod u_1 & e_1\dotprod u_2 & e_1\dotprod u_3 \\
e_2\dotprod u_1 & e_2\dotprod u_2 & e_2\dotprod u_3 \\
e_3\dotprod u_1 & e_3\dotprod u_2 & e_3\dotprod u_3 \\
\end{pmatrix} \\
= \begin{pmatrix}
u_1 & u_2 & u_3
\end{pmatrix} 
&=&
\begin{pmatrix}
\frac{2}{\sqrt{6}} & 0 & \frac{1}{\sqrt{3}} \\[1mm]
\frac{1}{\sqrt{6}}& \frac{1}{\sqrt{2}}&\frac{-1}{\sqrt{3}}\\[1mm]
\frac{-1}{\sqrt{6}}& \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{3}}\\[1mm]
\end{pmatrix}. \\
\end{eqnarray*}

From our theorem, we observe that
\begin{eqnarray*}
P^{-1}=P^T &=& \colvec{u_1^T\\u_2^T\\u_3^T} \\
&=& \begin{pmatrix}
\frac{2}{\sqrt{6}}& \frac{1}{\sqrt{6}}& \frac{-1}{\sqrt{6}}\\[1mm]
0 & \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \\[1mm]
\frac{1}{\sqrt{3}}& \frac{-1}{\sqrt{3}}&\frac{1}{\sqrt{3}}\\[1mm]
\end{pmatrix}. \\
\end{eqnarray*}

We can check that $P^TP=I$ by a lengthy computation, or more simply, notice that
\begin{eqnarray*}
(P^TP)%_{ij}
&=& \colvec{u_1^T\\u_2^T\\u_3^T} \rowvec{u_1 & u_2& u_3} \\
&=& \begin{pmatrix}1&0&0\\0&1&0\\0&0&1\end{pmatrix}.
\end{eqnarray*}
Above we are using orthonormality of the $u_i$ and the fact that matrix multiplication amounts to taking dot products between rows and columns.
It is also very \hypertarget{basisorthog}{important to realize that the columns of an {\it orthogonal} matrix are made
from an {\it orthonormal} set of vectors}.
\end{example}

\begin{remark}[Orthonormal Change of Basis and Diagonal Matrices.]
Suppose $D$ is a diagonal matrix  and we are able to use an orthogonal matrix $P$ to change to a new basis.  Then the matrix $M$ of $D$ in the new basis is:
\[
M = PDP^{-1} = PDP^T.
\]
Now we calculate the transpose of $M$.
\begin{eqnarray*}
M^T &=& (PDP^T)^T\\
&=& (P^T)^TD^TP^T \\
&=& PDP^T\\
&=& M
\end{eqnarray*}
The matrix $M=PDP^T$ is symmetric!
\end{remark}


%\section*{References}
%Hefferon, Chapter Three, Section V: Change of Basis
%\\
%Beezer, Chapter V, Section O, Subsection N
%\\
%Beezer, Chapter VS, Section B, Subsection OBC
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Orthogonal_matrix}{Orthogonal Matrix}
%\item \href{http://en.wikipedia.org/wiki/Diagonalizable_matrix}{Diagonalizable Matrix}
%\item \href{http://en.wikipedia.org/wiki/Similar_matrix}{Similar Matrix}
%\end{itemize}

%\section{Review Problems}
%\input{\orthonormPath/problems}



\section{Gram-Schmidt \& Orthogonal Complements}\label{gramschmidt}


Given a vector $v$ and some other vector $u$ not in  $ {\rm span}\, \{v\} $ we can construct the new vector 
\Shabox{1.1}{$
v^\perp:=v-\frac{\textstyle u\cdot v}{\textstyle u\cdot u}\, u\, .
$}
\begin{center}
\hypertarget{projectionpic}{\input{vperp.pdftex_t}}
\end{center}
This new vector $v^\perp$ is orthogonal to $u$ because 
\[
u\dotprod v^\perp = u\dotprod v - \frac{u\cdot v}{u\cdot u}u\dotprod u = 0.
\]
Hence, $\{u, v^\perp\}$ is an orthogonal basis for $\spa \{u,v\}$.  When $v$ is not parallel to $u$, $v^\perp \neq 0$, and normalizing these vectors we obtain $\left\{\frac{u}{|u|}, \frac{v^\perp}{|v^\perp|} \right\}$, an orthonormal basis for the vector space ${\rm span}\, \{u,v\}$.

Sometimes we write $v = v^\perp + v^\parallel$ where:
\begin{eqnarray*}
v^\perp &=& v-\frac{u\cdot v}{u\cdot u}\, u \\[1mm]
v^\parallel &=& \phantom{v-}\frac{u\cdot v}{u\cdot u}\, u.
\end{eqnarray*}
This is called an {\bf orthogonal decomposition}\index{Orthogonal decomposition} because we have decomposed $v$ into a sum of orthogonal vectors.  This decomposition depends on $u$; if we change the direction of $u$ we change $v^\perp$ and $v^\parallel$.

If $u$, $v$ are linearly independent vectors in $\Re^3$, then the set $\{u, v^\perp, u\times v^\perp \}$ would be an orthogonal basis for $\Re^3$.  This set could then be normalized by dividing each vector by its length to obtain an orthonormal basis.

However, it often occurs that we are interested in vector spaces with dimension greater than $3$, and must resort to craftier means than cross products to obtain an orthogonal basis\footnote{Actually, given a set $T$ of $(n-1)$ independent vectors in $n$-space, one can define an analogue of the cross product that will produce a vector orthogonal to the span of~$T$, using a method exactly analogous to the usual computation for calculating the cross product of two vectors in $\Re^3$.  This only gets us the ``\emph{last}'' orthogonal vector, though; the Gram--Schmidt process described in this section gives a way to get a full orthogonal basis.}.
%yeah, and who is going to teach them about the Hodge star in office hours? I say nix this footnote -cherney

Given a third vector $w$, we should first check that $w$ does not lie in the 
$\spa\{ u,v\} $, \textit{i.e.}, check that $u,v$ and $w$ are linearly independent.   If it does not, we then can define
\[
w^\perp := w - \frac{u\dotprod w}{u\dotprod u}\,u - \frac{v^\perp\dotprod w}{v^\perp\dotprod v^\perp}\,v^\perp.
\]
We can check that \(u \dotprod w^\perp\) and \(v^\perp \dotprod w^\perp\) are both zero:
\begin{align*}
u \dotprod w^\perp&=u \dotprod \left(w - \frac{u\dotprod w}{u\dotprod u}\,u - \frac{v^\perp\dotprod w}{v^\perp\dotprod v^\perp}\,v^\perp \right)\\&= u\dotprod w - \frac{u \dotprod w}{u \dotprod u}u \dotprod u - \frac{v^\perp \dotprod w}{v^\perp \dotprod v^\perp} u \dotprod v^\perp \\
&=u\dotprod w-u\dotprod w-\frac{v^\perp \dotprod w}{v^\perp \dotprod v^\perp} u \dotprod v^\perp\ =\ 0
\end{align*}
since \(u\) is orthogonal to \(v^\perp\), and
\begin{align*}
v^\perp \dotprod w^\perp&=v^\perp \dotprod \left(w - \frac{u\dotprod w}{u\dotprod u}\,u - \frac{v^\perp\dotprod w}{v^\perp\dotprod v^\perp}\,v^\perp \right)\\ &=v^\perp\dotprod w - \frac{u \dotprod w}{u \dotprod u}v^\perp \dotprod u - \frac{v^\perp \dotprod w}{v^\perp \dotprod v^\perp} v^\perp \dotprod v^\perp \\
&=v^\perp\dotprod w-\frac{u \dotprod w}{u \dotprod u}v^\perp \dotprod u - v^\perp \dotprod w\ =\ 0
\end{align*}
because \(u\) is orthogonal to \(v^\perp\). Since $w^\perp$ is orthogonal to both $u$ and $v^\perp$, we have that $\{u,v^\perp,w^\perp \}$ is an orthogonal basis for $\spa \{u,v,w\}$.

\subsection{The Gram-Schmidt Procedure}
In fact, given an ordered set $(v_1, v_2, \ldots )$ of linearly independent vectors, we can define an orthogonal basis for $\spa \{v_1,v_2, \ldots \}$ consisting of the  vectors
\begin{eqnarray*}
v_1^\perp&:=&v_1 \\
v_2^\perp &:=& v_2 - \frac{v_1^\perp\cdot v_2}{v_1^\perp\cdot v_1^\perp}\,v_1^\perp \\
v_3^\perp &:=& v_3 - \frac{v_1^\perp\cdot v_3}{v_1^\perp\cdot v_1^\perp}\,v_1^\perp - \frac{v_2^\perp\cdot v_3}{v_2^\perp\cdot v_2^\perp}\,v_2^\perp\\
&\vdots& \\
v_i^\perp%&=&   v_i - \sum_{j<i} \frac{v_j^\perp\cdot v_i}{v_j^\perp\cdot v_j^\perp}\,v_j^\perp \\
 &:=& v_i - \frac{v_1^\perp\cdot v_i}{v_1^\perp\cdot v_1^\perp}\,v_1^\perp   
 - \frac{v_2^\perp\cdot v_i}{v_2^\perp\cdot v_2^\perp}\,v_2^\perp -\cdots
 - \frac{v_{i-1}^\perp\cdot v_i}{v_{i-1}^\perp\cdot v_{i-1}^\perp}\,v_{i-1}^\perp\\
&\vdots& \\
\end{eqnarray*}
Notice that each $v_i^\perp$ here depends on  $v_j^\perp$ for every $j<i$.  This allows us to inductively/algorithmically build up a linearly independent, orthogonal set of vectors 
$\{v_1^\perp,v_2^\perp, \ldots \}$ 
such that 
$\spa \{v_1^\perp,v_2^\perp, \ldots \}=\spa \{v_1, v_2, \ldots \}$. 
That is, an orthogonal basis for the latter vector space. 

Note that the set of vectors you start out with needs to be ordered to uniquely specify the algorithm; changing the order of the vectors will give a different orthogonal basis. You might need to be the one to put an order on the initial set of vectors.

This algorithm is called the {\bf Gram--Schmidt orthogonalization procedure}\index{Gram--Schmidt orthogonalization procedure}\label{GramSchmidt}--Gram worked at a Danish insurance company over one hundred years ago, Schmidt was a student of Hilbert (the famous German mathmatician).

\begin{example}
We'll  obtain an orthogonal basis for $\Re^3$ by appling Gram-Schmidt to the linearly independent set 
$\left\{\colvec{1\\1\\1}, \colvec{1\\1\\0},\colvec{3\\1\\1} \right\}$.\\

Because he Gram-Schmidt algorithm uses the first vector from the ordered set the largest number of times, we will choose the vector with the most zeros to be the first in hopes of simplifying computations; we choose to order the set as
$$( v_1,v_2,v_3)
:= \left( \colvec{1\\1\\0}, \colvec{1\\1\\1},\colvec{3\\1\\1} \right).$$

First, we set $v_1^\perp:=v_1$.  Then
\begin{eqnarray*}
v_2^\perp&:=& \rowvec{1\\1\\1} - \frac{2}{2}\rowvec{1\\1\\0} = \rowvec{0\\0\\1} \\[2mm]
v_3^\perp&:=& \rowvec{3\\1\\1} - \frac{4}{2}\rowvec{1\\1\\0} - \frac{1}{1}\rowvec{0\\0\\1} = \rowvec{1\\-1\\0}. 
\end{eqnarray*}
Then the set
\[
\left\{ \rowvec{1\\1\\0},\rowvec{0\\0\\1},\rowvec{1\\-1\\0}\right\}
\]
is an orthogonal basis for $\Re^3$.  To obtain an ortho{\it normal} basis we simply divide each of these vectors by its length, yielding
\[
\left\{ \rowvec{\frac{1}{\sqrt2}\\[2mm]\frac{1}{\sqrt2}\\[1mm]0},\rowvec{0\\[2mm]0\\[1mm]1},\rowvec{\frac{1}{\sqrt2}\\[2mm]\frac{-1}{\sqrt2}\\[1mm]0}\right\}.
\]
\end{example}

\Videoscriptlink{gram_schimdt_and_orthogonal_complements_4by4_example.mp4}{A $4\times4$ Gram--Schmidt Example} {scripts_gram_schmidt_and_orthogonal_complements_4by4_example}

\section{$QR$ Decomposition}
In Chapter~\ref{Matrices}, Section~\ref{LUdecomp} teaches you how to solve linear systems by decomposing a matrix $M$ into 
a product of lower and upper triangular matrices
$$M=LU\, .$$
The Gram--Schmidt procedure suggests another matrix decomposition,
$$M=QR\, ,$$ 
where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix. So-called QR-decompositions\index{QR decomposition}
are useful for solving linear systems, eigenvalue problems and least squares approximations. You can
easily get the idea behind the $QR$ decomposition by working through a simple example.

\begin{example}
\hypertarget{methodQR}{Find} the $QR$ decomposition of $$M=\begin{pmatrix}2&-1&1\\1&3&-2\\0&1&-2\end{pmatrix}\, .$$
What we will do is to think of the columns of $M$ as three 3-vectors and use Gram--Schmidt to
build an orthonormal basis from these that will become the columns of the orthogonal matrix $Q$.
We will use the matrix $R$ to record the steps of the Gram--Schmidt procedure in such a way
that the product $QR$ equals $M$. 

To begin with we write
$$
M=\begin{pmatrix}2&-\frac75&1\\[1mm]1&\frac{14}5&-2\\[1mm]0&1&-2\end{pmatrix}
\begin{pmatrix}1&\frac15&0\\[1mm]0&1&0\\[1mm]0&0&1\end{pmatrix}\, .
$$
In the first matrix the first two columns are  orthogonal because we simply replaced the second column of $M$ by the vector that the Gram--Schmidt
procedure produces from the first two columns of~$M$, namely
$$
\colvec{-\frac75\\[1mm]\frac{14}5\\[1mm]1}=\colvec{-1\\[1mm]3\\[1mm]1}-\frac15
\colvec{ 2 \\[1mm]1\\[1mm]0}\, .
$$
 The matrix on the right is almost the identity
matrix, save the $+\frac15$ in the second entry of the first row, whose effect upon multiplying the
two matrices precisely undoes what we we did to the second column of the first matrix. 

For the third column of $M$ we use Gram--Schmidt to deduce the third orthogonal vector
$$
\colvec{-\frac16\\[1mm]\frac13\\[1mm]-\frac76}=
\colvec{1\\[1mm]-2\\[1mm]-2}
-0
\colvec{ 2 \\[1mm]1\\[1mm]0}
-\frac{-9}{\frac{54}{5}}\colvec{-\frac75\\[1mm]\frac{14}5\\[1mm]1}\, ,
$$
and therefore, using exactly the same procedure write
$$
M=\begin{pmatrix}2&-\frac75&-\frac16\\[1mm]1&\frac{14}5&\frac13\\[1mm]0&1&-\frac76\end{pmatrix}
\begin{pmatrix}1&\frac15&0\\[1mm]0&1&-\frac56\\[1mm]0&0&1\end{pmatrix}\, .
$$
This is not quite the answer because the first matrix is now made of mutually orthogonal column vectors,
but  a {\it bona fide} orthogonal matrix is comprised of {\it orthonormal} vectors. To achieve that we divide
each column of the first matrix by its length and multiply the corresponding row of the second matrix by the same 
amount:
$$
M=\begin{pmatrix}\frac{2\sqrt{5}}{5}&-\frac{7\sqrt{30}}{90}&-\frac{\sqrt{6}}{18}\\[2mm]
\frac{\sqrt{5}}{5}&\frac{7\sqrt{30}}{45}&\frac{\sqrt{6}}{9}\\[2mm]
0&\frac{\sqrt{30}}{18}&-\frac{7\sqrt{6}}{18}\end{pmatrix}
\begin{pmatrix}\sqrt{5}&\frac{\sqrt{5}}{5}&0\\[2mm]
0&\frac{3\sqrt{30}}{5}&-\frac{\sqrt{30}}{2}\\[2mm]
0&0&\frac{\sqrt{6}}{2}\end{pmatrix}=QR\, .
$$
Geometrically what has happened here is easy to see. We started with three vectors given by the columns of $M$ and rotated them such that the first lies along the $x$-axis, the second in the $xy$-plane and the third in some other generic direction (here it happens to be in the $yz$-plane).

A nice check of the above result is to verify that entry $(i,j)$  of the matrix $R$
equals the dot product of the $i$-th column of $Q$ with the $j$-th column of $M$.
(Some people memorize this fact and use it as a recipe for computing $QR$ decompositions.)
{\it A good test of your own understanding is to work out why this is true!}
\end{example}


\Videoscriptlink{gram_schimdt_and_orthogonal_complements_qr_example.mp4}{Another $QR$ decomposition example}{scripts_gram_schmidt_and_orthogonal_complements_qr_example}

\section{Orthogonal Complements}

Let $U$ and $V$ be subspaces of a vector space $W$.  In \hyperref[UcapV]{Review Exercise}~\ref{UandV}, Chapter~\ref{subspacesspanning}, you are asked to show that $U\cap V$ is a subspace of $W$, and that $U\cup V$ is {\it not} a subspace.  However, $\spa (U\cup V)$ {\it is} certainly a subspace, since the span of \emph{any} subset of a vector space is a subspace.
Notice that all elements of $\spa (U\cup V)$ take the form $u+v$ with $u\in U$ and $v\in V$.  We call the subspace 
\Shabox{1}{$
U+V:=\spa (U\cup V) = \{u+v ~| ~u\in U, v\in V \}$}
the \emph{sum}\index{Sum of vectors spaces} of $U$ and $V$.  Here, we are not adding vectors, but vector spaces to produce a new vector space.
\begin{example}
\[
\spa\left\{  \colvec{1\\1\\0\\0},  \colvec{0\\1\\1\\0} \right \} 
+ \spa\left\{  \colvec{0\\1\\1\\0},  \colvec{0\\0\\1\\1} \right \} 
= \spa\left\{  \colvec{1\\1\\0\\0},  \colvec{0\\1\\1\\0},  \colvec{0\\0\\1\\1} \right \} .
\]
Notice that the addends have elements in common; 
$\colvec{0\\1\\1\\0}$ is in both addends. Even though both of the addends are 2-dimensional their sum is not  4-dimensional.
\end{example}


In the special case that $U$ and $V$ do not have any non-zero vectors in common, their sum is a vector space with dimension $\dim U + \dim V$. 


\begin{definition}
If  $U$ and $V$ are subspaces of a vector space $W$ such that 
$U~\cap~V=~\{0_W\}\, $
then the vector space
\[
U \oplus V := \spa (U\cup V)= \{u+v ~|~ u\in U, v\in V \}
\]
is the {\bf direct sum}\index{Direct sum} of $U$ and $V$.
\end{definition}

\begin{remark}

$\phantom{JUNK HERE}$\\[-8mm]

\begin{itemize}
\item When $U\cap V= \{0_W\}$, $U+V=U\oplus V.$ 
\item When $U\cap V \neq \{0_W\}$, $U+V \neq U\oplus V$.
\end{itemize}
\end{remark}

\noindent This distinction is important because the direct sum has a very nice property:

\begin{theorem}
If $w\in U\oplus V$  then 
%the expression $w=u+v$ is unique.  That is, 
there is only one way to write \(w\) as the sum of a vector in \(U\) and a vector in \(V\).  
\end{theorem}

\begin{proof}
Suppose that $u+v=u'+v'$, with $u,u'\in U$, and $v,v' \in V$.  Then we could express $0=(u-u')+(v-v')$.  Then $(u-u')=-(v-v')$.  Since $U$ and $V$ are subspaces, we have $(u-u')\in U$ and $-(v-v')\in V$.  But since these elements are equal, we also have $(u-u')\in V$.  Since $U\cap V=\{0\}$, then $(u-u')=0$.  Similarly, $(v-v')=0$. Therefore $u=u'$ and  $v=v'$, proving the theorem. 
\end{proof}

\Reading{OrthonormalBases}{3}
%\begin{center}\href{\webworkurl ReadingHomework22/1/}{Reading homework: problem \ref{gramschmidt}.1}\end{center}


\noindent Here is a sophisticated algebra question:
\begin{quote}
Given a subspace $U$ in $W$, what are the solutions to 
$$ U\oplus V =W.$$
That is, how can we write $W$ as the direct sum of $U$ and \emph{something}? 
\end{quote}
There is not a unique answer to this question as can be seen from the following picture of subspaces in $W={\mathbb R}^3$. 
\begin{center}
\includegraphics[scale=.25]{\gramSchmidtPath/direct_sums.jpg}
\end{center}
However, using the inner product, there is a natural candidate $U^\perp$ for this second subspace as shown below.
\begin{center}
\includegraphics[scale=.25]{\gramSchmidtPath/U_perp.jpg}
\end{center}


\begin{definition}
If $U$ is a subspace of the vector space $W$ then the vector space 
$$U^\perp := \big\{w\in W \big| w\dotprod u=0 \text{ for all } u\in U\big\}\, $$
is the {\bf orthogonal complement}\index{Orthogonal complement} of $U$ in $W$. 
\end{definition}


\begin{remark}
The symbols ``$U^\perp$"" are often read as ``$U$-perp''.  This is the set of all vectors in $W$ orthogonal to \emph{every} vector in $U$. 
Notice also that in the above definition we have implicitly assumed that the inner product is the dot product. For a general \hyperlink{inner_product}{inner product}, the  
above definition would read $U^\perp := \big\{w\in W \big|\,  \langle w,u\rangle=0 \ {\rm  for\  all } \ u\in U\big\}\, $.
\end{remark}
Possibly by now you are feeling overwhelmed, it may help to watch this quick overview video.

\Videoscriptlink{gram_schmidt_and_orthogonal_complements_theory.mp4}{Overview}{scripts_gram_schmidt_and_orthogonal_complements_theory}



\begin{example}
Consider any plane $P$ through the origin in $\Re^3$.  Then $P$ is a subspace, and $P^\perp$ is the line through the origin orthogonal to $P$.  For example, if $P$ is the $xy$-plane, then
\[
\Re^3=P\oplus P^\perp=\{(x,y,0)| x,y\in \Re \} \oplus \{(0,0,z)| z\in \Re \}.
\]
\end{example}

\begin{theorem}
Let $U$ be a subspace of a finite-dimensional vector space~$W$.  Then the set $U^\perp$ is a subspace of~$W$, and $W=U\oplus U^\perp$\index{Perp@``Perp''}.
\end{theorem}

\begin{proof}
First, to see that $U^\perp$ is a subspace, we only need to check closure, which requires a simple check:
Suppose $v,w\in U^\perp$, then we know  $$v\dotprod u = 0 = w\dotprod u \quad (\forall u\in U)\, .$$
Hence
$$\Rightarrow u\dotprod(\alpha v+\beta w)= \alpha u\dotprod v + \beta u\dotprod w =0\quad (\forall u\in U)\, ,$$ 
and so $\alpha v+\beta w\in U^\perp$.

Next, to form a direct sum between $U$ and $U^\perp$ we need to show
that $U\cap U^\perp=\{0\}$. This holds because if $u\in U$ and $u\in U^\perp$ it follows that
\[
u\dotprod u = 0 \Leftrightarrow u=0.
\]

Finally, we show that any vector $w\in W$ is in $U\oplus U^\perp$.  (This is where we use the assumption that $W$ is finite-dimensional.)  Let $e_1, \ldots, e_n$ be an orthonormal basis for $U$.  Set: 
\begin{eqnarray*}
u\ &=&(w\dotprod e_1)e_1 + \cdots + (w\dotprod e_n)e_n \in U\, ,\\
u^\perp&=& w-u\, .
\end{eqnarray*}
It is easy to check that $u^\perp \in U^\perp$ (see the Gram-Schmidt procedure).  Then $w=u+u^\perp$, so $w\in U\oplus U^\perp$, and we are done.
\end{proof}

\Reading{OrthonormalBases}{4}
%\begin{center}\href{\webworkurl ReadingHomework22/2/}{Reading homework: problem \ref{gramschmidt}.2}\end{center}

\begin{example}
Consider any line \(L\) through the origin in \(\Re^4\). Then \(L\) is a subspace, and \(L^\perp\) is a \(3\)-dimensional subspace orthogonal to \(L\). For example, let 
$$L=\spa \left\{ \colvec{1\\1\\1\\1} \right\}$$ 
be a line in \(\Re^4.\) Then 
\begin{eqnarray*}
L^\perp&=&\left \{  \colvec{x\\y\\z\\w} \in \Re^4 ~\middle| ~ (x,y,z,w) \dotprod (1,1,1,1)=0 \right\} \\
&=&\left\{ \colvec{x\\y\\z\\w} \in \Re^4 ~\middle| ~ x+y+z+w=0 \right \}.
\end{eqnarray*}

\noindent
Using the Gram-Schmidt procedure one may  find an orthogonal basis for 
\(L^\perp\). The set 
$$
\left\{
\colvec{1\\-1\\0\\0}, \colvec{1\\0\\-1\\0}, \colvec{1\\0\\0\\-1} \right \}\, 
$$ 
forms a basis for \(L^\perp\)  so, first, we order the basis as 
$$
( v_1,v_2,v_2)= 
\left(
\colvec{1\\-1\\0\\0}, \colvec{1\\0\\-1\\0}, \colvec{1\\0\\0\\-1} \right)\,. $$
Next, we set \(v_1^\perp=v_1\). Then
\begin{eqnarray*}
v_2^\perp&=&\colvec{1\\0\\-1\\0}-\frac{1}{2}\colvec{1\\-1\\0\\0}
=\colvec{\frac{1}{2}\\[1mm] \frac{1}{2} \\-1\\ 0 },\\
v_3^\perp&=&\colvec{ 1\\0\\0\\-1} -\frac{1}{2}\colvec{1\\-1\\0\\0}-\frac{1/2}{3/2}
\colvec{ \frac{1}{2}\\[1mm] \frac{1}{2}\\-1\\0} =\colvec{ \frac{1}{3}\\[1mm]\frac{1}{3}\\[1mm]\frac{1}{3}\\-1}.
\end{eqnarray*}
So the set 
\[\left\{ \colvec{ 1\\-1\\0\\0}, \colvec{\frac{1}{2}\\ \frac{1}{2}\\-1\\ 0}, \colvec{\frac{1}{3}\\\frac{1}{3}\\\frac{1}{3}\\-1} \right\} \] is an orthogonal basis for \(L^\perp\).
Dividing each basis vector by its length yields 
\[
\left\{
\colvec{  \frac{1}{\sqrt{2}  }\\[.5mm] -\frac{1}{\sqrt{2}}\\[.5mm]\mc{0}\\[.5mm]\mc{0} },
\colvec{ \frac{1}{\sqrt{6}}\\[.5mm] \frac{1}{\sqrt{6}}\\[.5mm] -\frac{2}{\sqrt{6}}\\[.5mm]\mc{\, 0} },
\colvec{ \frac{\sqrt{3}}{6}\\[.5mm] \frac{\sqrt{3}}{6}\\[.5mm] \frac{\sqrt{3}}{6}\\[.5mm] -\frac{\sqrt{3}}{2} }
\right\},
\]
and orthonormal basis for \(L^\perp\). 
Moreover, we have
\[
\Re^4=L \oplus L^\perp = 
\left\{ \colvec{c\\c\\c\\c} \middle| c \in \Re  \right\} 
\oplus \left\{   \colvec{x\\y\\z\\w} \in \Re^4 \middle| \,  x+y+z+w=0\right\},
\]
a decomposition of $\Re^4$ into a line and its three dimensional orthogonal complement.
\end{example}

Notice that for any subspace $U$, the subspace $(U^\perp)^\perp$ is just $U$ again.  As such, $\perp$ is an {\it involution}\index{Involution} on the set of subspaces of a vector space. (An involution is any mathematical operation which performed twice does nothing.)

%\section*{References}
%Hefferon, Chapter Three, Section VI.2: Gram-Schmidt Orthogonalization
%\\
%Beezer, Chapter V, Section O, Subsection GSP
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Gram_schmidt}{Gram-Schmidt Process}
%\item \href{http://en.wikipedia.org/wiki/QR_decomposition}{QR Decomposition}
%\item \href{http://en.wikipedia.org/wiki/Orthonormal_basis}{Orthonormal Basis}
%\item \href{http://en.wikipedia.org/wiki/Direct_sum}{Direct Sum}
%\end{itemize}
%

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{OrthonormalBases}{1}, 
 \hwrref{OrthonormalBases}{2}, 
 \hwrref{OrthonormalBases}{3}, 
 \hwrref{OrthonormalBases}{4}\\
Gram--Schmidt &  \hwref{OrthonormalBases}{5}\\
Orthogonal eigenbasis &  \hwref{OrthonormalBases}{6}, \hwref{OrthonormalBases}{7}\\
 Orthogonal complement&\hwref{OrthonormalBases}{8}\\
   \hline
\end{tabular}


\input{\gramSchmidtPath/problems}

\newpage


",lesson
23,Gram Schmidt And Orthogonal Complements,"
\chapter{\gramSchmidtTitle}\label{gramschmidt}


Given a vector $v$ and some other vector $u$ not in  $ {\rm span}\, \{v\} $, we can construct a new vector: 
\[
v^\perp:=v-\frac{u\cdot v}{u\cdot u}u.
\]
\begin{center}
\input{vperp.pdftex_t}
\end{center}
This new vector $v^\perp$ is orthogonal to $u$ because 
\[
u\dotprod v^\perp = u\dotprod v - \frac{u\cdot v}{u\cdot u}u\dotprod u = 0.
\]
Hence, $\{u, v^\perp\}$ is an orthogonal basis for $\spa \{u,v\}$.  When $v$ is not parallel to $u$, $v^\perp \neq 0$, and normalizing these vectors we obtain $\left\{\frac{u}{|u|}, \frac{v^\perp}{|v^\perp|} \right\}$, an orthonormal basis for the vector space ${\rm span}\, \{u,v\}$.

Sometimes we write $v = v^\perp + v^\parallel$ where:
\begin{eqnarray*}
v^\perp &=& v-\frac{u\cdot v}{u\cdot u}u \\
v^\parallel &=& \phantom{v-}\frac{u\cdot v}{u\cdot u}u.
\end{eqnarray*}
This is called an \emph{orthogonal decomposition}\index{Orthogonal decomposition} because we have decomposed $v$ into a sum of orthogonal vectors.  This decomposition depends on $u$; if we change the direction of $u$ we change $v^\perp$ and $v^\parallel$.

If $u$, $v$ are linearly independent vectors in $\Re^3$, then the set $\{u, v^\perp, u\times v^\perp \}$ would be an orthogonal basis for $\Re^3$.  This set could then be normalized by dividing each vector by its length to obtain an orthonormal basis.

However, it often occurs that we are interested in vector spaces with dimension greater than $3$, and must resort to craftier means than cross products to obtain an orthogonal basis.
\footnote{Actually, given a set $T$ of $(n-1)$ independent vectors in $n$-space, one can define an analogue of the cross product that will produce a vector orthogonal to the span of $T$, using a method exactly analogous to the usual computation for calculating the cross product of two vectors in $\Re^3$.  This only gets us the \emph{last} orthogonal vector, though; the process in this Section gives a way to get a full orthogonal basis.}
%yeah, and who is going to teach them about the Hodge star in office hours? I say nix this footnote -cherney

Given a third vector $w$, we should first check that $w$ does not lie in the span of $u$ and $v$, \textit{i.e.} check that $u,v$ and $w$ are linearly independent.   We then can define:
\[
w^\perp = w - \frac{u\dotprod w}{u\dotprod u}\,u - \frac{v^\perp\dotprod w}{v^\perp\dotprod v^\perp}\,v^\perp.
\]

We can check that \(u \dotprod w^\perp\) and \(v^\perp \dotprod w^\perp\) are both zero:
\begin{align*}
u \dotprod w^\perp&=u \dotprod \left(w - \frac{u\dotprod w}{u\dotprod u}\,u - \frac{v^\perp\dotprod w}{v^\perp\dotprod v^\perp}\,v^\perp \right)\\&= u\dotprod w - \frac{u \dotprod w}{u \dotprod u}u \dotprod u - \frac{v^\perp \dotprod w}{v^\perp \dotprod v^\perp} u \dotprod v^\perp \\
&=u\dotprod w-u\dotprod w-\frac{v^\perp \dotprod w}{v^\perp \dotprod v^\perp} u \dotprod v^\perp\ =\ 0
\end{align*}
since \(u\) is orthogonal to \(v^\perp\), and
\begin{align*}
v^\perp \dotprod w^\perp&=v^\perp \dotprod \left(w - \frac{u\dotprod w}{u\dotprod u}\,u - \frac{v^\perp\dotprod w}{v^\perp\dotprod v^\perp}\,v^\perp \right)\\ &=v^\perp\dotprod w - \frac{u \dotprod w}{u \dotprod u}v^\perp \dotprod u - \frac{v^\perp \dotprod w}{v^\perp \dotprod v^\perp} v^\perp \dotprod v^\perp \\
&=v^\perp\dotprod w-\frac{u \dotprod w}{u \dotprod u}v^\perp \dotprod u - v^\perp \dotprod w\ =\ 0
\end{align*}
because \(u\) is orthogonal to \(v^\perp\). Since $w^\perp$ is orthogonal to both $u$ and $v^\perp$, we have that $\{u,v^\perp,w^\perp \}$ is an orthogonal basis for $\spa \{u,v,w\}$.

\section{The Gram-Schmidt Procedure}
In fact, given a set $\{v_1, v_2, \ldots \}$ of linearly independent vectors, we can define an orthogonal basis for $\spa \{v_1,v_2, \ldots \}$ consisting of the following vectors:
\begin{eqnarray*}
v_1^\perp&:=&v_1 \\
v_2^\perp &:=& v_2 - \frac{v_1^\perp\cdot v_2}{v_1^\perp\cdot v_1^\perp}\,v_1^\perp \\
v_3^\perp &:=& v_3 - \frac{v_1^\perp\cdot v_3}{v_1^\perp\cdot v_1^\perp}\,v_1^\perp - \frac{v_2^\perp\cdot v_3}{v_2^\perp\cdot v_2^\perp}\,v_2^\perp\\
&\vdots& \\
v_i^\perp%&=&   v_i - \sum_{j<i} \frac{v_j^\perp\cdot v_i}{v_j^\perp\cdot v_j^\perp}\,v_j^\perp \\
 &:=& v_i - \frac{v_1^\perp\cdot v_i}{v_1^\perp\cdot v_1^\perp}\,v_1^\perp -  
 - \frac{v_2^\perp\cdot v_3}{v_2^\perp\cdot v_2^\perp}\,v_2^\perp -\cdots
 - \frac{v_{i-1}^\perp\cdot v_i}{v_{i-1}^\perp\cdot v_{i-1}^\perp}\,v_{i-1}^\perp\\
&\vdots& \\
\end{eqnarray*}
Notice that each $v_i^\perp$ here depends on  $v_j^\perp$ for every $j<i$.  This allows us to inductively/algorithmically build up a linearly independent, orthogonal set of vectors 
$\{v_1^\perp,v_2^\perp, \ldots \}$ 
such that 
$\spa \{v_1^\perp,v_2^\perp, \ldots \}=\spa \{v_1, v_2, \ldots \}$. That is, on orthogonal basis for the latter vector space. This algorithm bears the name \emph{Gram--Schmidt orthogonalization procedure}\index{Gram--Schmidt orthogonalization procedure}\label{GramSchmidt}.

\begin{example}
We'll  obtain an orthogonal basis for $\Re^3$ by appling Gram-Schmidt to the linearly independent set 
$\left\{ v_1=\colvec{1\\1\\0}, v_2=\colvec{1\\1\\1},v_3=\colvec{3\\1\\1} \right\}$.

First, we set $v_1^\perp:=v_1$.  Then:
\begin{eqnarray*}
v_2^\perp&=& \rowvec{1\\1\\1} - \frac{2}{2}\rowvec{1\\1\\0} = \rowvec{0\\0\\1} \\
V_3^\perp&=& \rowvec{3\\1\\1} - \frac{4}{2}\rowvec{1\\1\\0} - \frac{1}{1}\rowvec{0\\0\\1} = \rowvec{1\\-1\\0}. 
\end{eqnarray*}
Then the set
\[
\left\{ \rowvec{1\\1\\0},\rowvec{0\\0\\1},\rowvec{1\\-1\\0}\right\}
\]
is an orthogonal basis for $\Re^3$.  To obtain an orthonormal basis, as always we simply divide each of these vectors by its length, yielding:
\[
\left\{ \rowvec{\frac{1}{\sqrt2}\\\frac{1}{\sqrt2}\\0},\rowvec{0\\0\\1},\rowvec{\frac{1}{\sqrt2}\\\frac{-1}{\sqrt2}\\0}\right\}.
\]
\end{example}

\videoscriptlink{gram_schimdt_and_orthogonal_complements_4by4_example.mp4}{A $4\times4$ Gram Schmidt Example} {scripts_gram_schimdt_and_orthogonal_complements_4by4_example}

\section{$QR$ Decomposition}
In Lecture~\ref{LUdecomp} we learned how to solve linear systems by decomposing a matrix $M$ into 
a product of lower and upper triangular matrices
$$M=LU\, .$$
The Gram--Schmidt procedure suggests another matrix decomposition,
$$M=QR$$ 
where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix. So-called QR-decompositions\index{QR decomposition}
are useful for solving linear systems, eigenvalue problems and least squares approximations. You can
easily get the idea behind $QR$ decomposition by working through a simple example.

\begin{example}
\hypertarget{methodQR}{Find} the $QR$ decomposition of $$M=\begin{pmatrix}2&-1&1\\1&3&-2\\0&1&-2\end{pmatrix}\, .$$
What we will do is to think of the columns of $M$ as three vectors and use Gram--Schmidt to
build an orthonormal basis from these that will become the columns of the orthogonal matrix $Q$.
We will use the matrix $R$ to record the steps of the Gram--Schmidt procedure in such a way
that the product $QR$ equals $M$. 

To begin with we write
$$
M=\begin{pmatrix}2&-\frac75&1\\[1mm]1&\frac{14}5&-2\\[1mm]0&1&-2\end{pmatrix}
\begin{pmatrix}1&\frac15&0\\[1mm]0&1&0\\[1mm]0&0&1\end{pmatrix}\, .
$$
In the first matrix the first two columns are  orthogonal because we simpy replaced the second column of $M$ by the vector that the Gram--Schmidt
procedure produces from the first two columns of~$M$, namely
$$
\colvec{-\frac75\\[1mm]\frac{14}5\\[1mm]1}=\colvec{-1\\[1mm]3\\[1mm]1}-\frac15
\colvec{ 2 \\[1mm]1\\[1mm]0}\, .
$$
 The matrix on the right is almost the identity
matrix, save the $+\frac15$ in the second entry of the first row, whose effect upon multiplying the
two matrices precisely undoes what we we did to the second column of the first matrix. 

For the third column of $M$ we use Gram--Schmidt to deduce the third orthogonal vector
$$
\colvec{-\frac16\\[1mm]\frac13\\[1mm]-\frac76}=
\colvec{1\\[1mm]-2\\[1mm]-2}
-0
\colvec{ 2 \\[1mm]1\\[1mm]0}
-\frac{-9}{\frac{54}{5}}\colvec{-\frac75\\[1mm]\frac{14}5\\[1mm]1}\, ,
$$
and therefore, using exactly the same procedure write
$$
M=\begin{pmatrix}2&-\frac75&-\frac16\\[1mm]1&\frac{14}5&\frac13\\[1mm]0&1&-\frac76\end{pmatrix}
\begin{pmatrix}1&\frac15&0\\[1mm]0&1&-\frac56\\[1mm]0&0&1\end{pmatrix}\, .
$$
This is not quite the answer because the first matrix is now made of mutually orthogonal column vectors,
but  a {\it bona fide} orthogonal matrix is comprised of {\it orthonormal} vectors. To achieve that we divide
each column of the first matrix by its length and multiply the corresponding row of the second matrix by the same 
amount:
$$
M=\begin{pmatrix}\frac{2\sqrt{5}}{5}&-\frac{7\sqrt{30}}{90}&-\frac{\sqrt{6}}{18}\\[2mm]
\frac{\sqrt{5}}{5}&\frac{7\sqrt{30}}{45}&\frac{\sqrt{6}}{9}\\[2mm]
0&\frac{\sqrt{30}}{18}&-\frac{7\sqrt{6}}{18}\end{pmatrix}
\begin{pmatrix}\sqrt{5}&\frac{\sqrt{5}}{5}&0\\[2mm]
0&\frac{3\sqrt{30}}{5}&-\frac{\sqrt{30}}{2}\\[2mm]
0&0&\frac{\sqrt{6}}{2}\end{pmatrix}=QR\, .
$$
A nice check of this result is to verify that entry $(i,j)$  of the matrix $R$
equals the dot product of the $i$-th column of $Q$ with the $j$-th column of $M$.
(Some people memorize this fact and use it as a recipe for computing $QR$ deompositions.)
{\it A good test of your own understanding is to work out why this is true!}
\end{example}


\videoscriptlink{gram_schimdt_and_orthogonal_complements_qr_example.mp4}{Another $QR$ decomposition example}{scripts_gram_schmidt_and_orthogonal_complements_qr_example}

\section{Orthogonal Complements}

Let $U$ and $V$ be subspaces of a vector space $W$.  We saw as a \hyperref[UcapV]{review exercise} that $U\cap V$ is a subspace of $W$, and that $U\cup V$ was not a subspace.  However, $\spa (U\cup V)$ is certainly a subspace, since the span of \emph{any} subset of a vector space is a subspace.
Notice that all elements of $\spa (U\cup V)$ take the form $u+v$ with $u\in U$ and $v\in V$.  We call the subspace 
\[
U+V:=\spa (U\cup V) = \{u+v | u\in U, v\in V \}
\] 
the \emph{sum}\index{Sum of vectors spaces} of $U$ and $V$.  Here, we are not adding vectors, but vector spaces to produce a new vector space!


\begin{definition}
Given two subspaces $U$ and $V$ of a space $W$ such that $U\cap V=\{0_W\}$, the \emph{direct sum}\index{Direct sum} of $U$ and $V$ is defined as:
\[
U \oplus V = \spa (U\cup V)= \{u+v | u\in U, v\in V \}.
\]
\end{definition}
Notice that when $U\cap V= \{0_W\}$, $U+V=U\oplus V$.


The direct sum has a very nice property.

\begin{theorem}
If $w\in U\oplus V$  then 
%the expression $w=u+v$ is unique.  That is, 
there is only one way to write \(w\) as the sum of a vector in \(U\) and a vector in \(V\).  
\end{theorem}

\begin{proof}
Suppose that $u+v=u'+v'$, with $u,u'\in U$, and $v,v' \in V$.  Then we could express $0=(u-u')+(v-v')$.  Then $(u-u')=-(v-v')$.  Since $U$ and $V$ are subspaces, we have $(u-u')\in U$ and $-(v-v')\in V$.  But since these elements are equal, we also have $(u-u')\in V$.  Since $U\cap V=\{0\}$, then $(u-u')=0$.  Similarly, $(v-v')=0$. Therefore $u=u'$ and  $v=v'$, proving the theorem. 
\end{proof}

\reading{22}{1}
%\begin{center}\href{\webworkurl ReadingHomework22/1/}{Reading homework: problem \ref{gramschmidt}.1}\end{center}

Given a subspace $U$ in $W$, how can we write $W$ as the direct sum of $U$ and \emph{something}? There is not a unique answer to this question as can be seen from this picture of subspaces in $W={\mathbb R}^3$: 
\begin{center}
\includegraphics[scale=.25]{\gramSchmidtPath/direct_sums.jpg}
\end{center}
However, using the inner product, there is a natural candidate $U^\perp$ for this second subspace as shown here:
\begin{center}
\includegraphics[scale=.25]{\gramSchmidtPath/U_perp.jpg}
\end{center}

The general definition is as follows:
\begin{definition}
Given a subspace $U$ of a vector space $W$, define:
\[
U^\perp = \{w\in W | w\dotprod u=0 \text{ for all } u\in U\}.
\]
\end{definition}

The set $U^\perp$ (pronounced ``$U$-perp'') is the set of all vectors in $W$ orthogonal to \emph{every} vector in $U$.  This is also often called the \emph{orthogonal complement}\index{Orthogonal complement} of $U$. Probably by now you may be feeling overwhelmed, it may help to watch this quick overview video:

\videoscriptlink{gram_schmidt_and_orthogonal_complements_theory.mp4}{Overview}{scripts_gram_schmidt_and_orthogonal_complements_theory}



\begin{example}
Consider any plane $P$ through the origin in $\Re^3$.  Then $P$ is a subspace, and $P^\perp$ is the line through the origin orthogonal to $P$.  For example, if $P$ is the $xy$-plane, then
\[
\Re^3=P\oplus P^\perp=\{(x,y,0)| x,y\in \Re \} \oplus \{(0,0,z)| z\in \Re \}.
\]
\end{example}

\begin{theorem}
Let $U$ be a subspace of a finite-dimensional vector space $W$.  Then the set $U^\perp$ is a subspace of $W$, and $W=U\oplus U^\perp$\index{Perp@``Perp''}.
\end{theorem}

\begin{proof}
To see that $U^\perp$ is a subspace, we only need to check closure, which requires a simple check.

We have $U\cap U^\perp=\{0\}$, since if $u\in U$ and $u\in U^\perp$, we have:
\[
u\dotprod u = 0 \Leftrightarrow u=0.
\]

Finally, we show that any vector $w\in W$ is in $U\oplus U^\perp$.  (This is where we use the assumption that $W$ is finite-dimensional.)  Let $e_1, \ldots, e_n$ be an orthonormal basis for $W$.  Set: 
\begin{eqnarray*}
u&=&(w\dotprod e_1)e_1 + \cdots + (w\dotprod e_n)e_n \in U\\
u^\perp&=& w-u
\end{eqnarray*}
It is easy to check that $u^\perp \in U^\perp$ (see the Gram-Schmidt procedure).  Then $w=u+u^\perp$, so $w\in U\oplus U^\perp$, and we are done.
\end{proof}

\reading{22}{2}
%\begin{center}\href{\webworkurl ReadingHomework22/2/}{Reading homework: problem \ref{gramschmidt}.2}\end{center}

\begin{example}
Consider any line \(L\) through the origin in \(\Re^4\). Then \(L\) is a subspace, and \(L^\perp\) is a \(3\)-dimensional subspace orthogonal to \(L\). For example, let \(L\) be the line 
$\spa \{ (1,1,1,1)^T\}$ in \(\Re^4.\) Then \(L^\perp\) is given by
\begin{eqnarray*}
L^\perp&=&\{(x,y,z,w)^T \mid x,y,z,w \in \Re \text{ and } (x,y,z,w)^T \dotprod (1,1,1,1)^T=0\} \\
&=&\{(x,y,z,w)^T \mid x,y,z,w \in \Re \text{ and } x,y,z,w=0\}.
\end{eqnarray*}
It is easy to check that 
$$
\left\{
v_1=\colvec{1\\-1\\0\\0}, v_2=\colvec{1\\0\\-1\\0}, v_3=\colvec{1\\0\\0\\-1} \right \}
$$ 
forms a basis for \(L^\perp\). We use Gram-Schmidt to find an orthogonal basis for \(L^\perp\):

First, we set \(v_1^\perp=v_1\). Then:
\begin{eqnarray*}
v_2^\perp&=&\colvec{1\\0\\-1\\0}-\frac{1}{2}\colvec{1,-1,0,0}
=\colvec{\frac{1}{2}\\ \frac{1}{2} \\-1\\ 0 },\\
v_3^\perp&=&\colvec{ 1\\0\\0\\-1} -\frac{1}{2}\colvec{1\\-1\\0\\0}-\frac{1/2}{3/2}
\colvec{ \frac{1}{2}\\\frac{1}{2}\\-1\\0} =\colvec{ \frac{1}{3}\\\frac{1}{3}\\\frac{1}{3}\\-1}.
\end{eqnarray*}
So the set \[\left\{ (1,-1,0,0)^T, \left(\frac{1}{2},\frac{1}{2},-1,0\right)^T, \left(\frac{1}{3},\frac{1}{3},\frac{1}{3},-1\right)^T \right\} \] is an orthogonal basis for \(L^\perp\).
We find an orthonormal basis for \(L^\perp\) by dividing each basis vector by its length:
\[
\left\{
\left( \frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}},0,0 \right)^T,
\left( \frac{1}{\sqrt{6}}, \frac{1}{\sqrt{6}}, -\frac{2}{\sqrt{6}},0 \right)^T,
\left( \frac{\sqrt{3}}{6}, \frac{\sqrt{3}}{6}, \frac{\sqrt{3}}{6}, -\frac{\sqrt{3}}{2} \right)^T
\right\}.
\]
Moreover, we have
\[
\Re^4=L \oplus L^\perp = \{(c,c,c,c)^T \mid c \in \Re\} \oplus \{(x,y,z,w)^T \mid x,y,z,w \in \Re \text{ and } x+y+z+w=0\}.
\]
\end{example}

Notice that for any subspace $U$, the subspace $(U^\perp)^\perp$ is just $U$ again.  As such, $\perp$ is an involution on the set of subspaces of a vector space.

%\section*{References}
%Hefferon, Chapter Three, Section VI.2: Gram-Schmidt Orthogonalization
%\\
%Beezer, Chapter V, Section O, Subsection GSP
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Gram_schmidt}{Gram-Schmidt Process}
%\item \href{http://en.wikipedia.org/wiki/QR_decomposition}{QR Decomposition}
%\item \href{http://en.wikipedia.org/wiki/Orthonormal_basis}{Orthonormal Basis}
%\item \href{http://en.wikipedia.org/wiki/Direct_sum}{Direct Sum}
%\end{itemize}
%

\section{Review Problems}
\input{\gramSchmidtPath/problems}

\newpage

","
\chapter{\gramSchmidtTitle}\label{gramschmidt}


Given a vector $v$ and some other vector $u$ not in  $ {\rm span}\, \{v\} $, we can construct a new vector: 
\[
v^\perp:=v-\frac{u\cdot v}{u\cdot u}u.
\]
\begin{center}
\input{vperp.pdftex_t}
\end{center}
This new vector $v^\perp$ is orthogonal to $u$ because 
\[
u\dotprod v^\perp = u\dotprod v - \frac{u\cdot v}{u\cdot u}u\dotprod u = 0.
\]
Hence, $\{u, v^\perp\}$ is an orthogonal basis for $\spa \{u,v\}$.  When $v$ is not parallel to $u$, $v^\perp \neq 0$, and normalizing these vectors we obtain $\left\{\frac{u}{|u|}, \frac{v^\perp}{|v^\perp|} \right\}$, an orthonormal basis for the vector space ${\rm span}\, \{u,v\}$.

Sometimes we write $v = v^\perp + v^\parallel$ where:
\begin{eqnarray*}
v^\perp &=& v-\frac{u\cdot v}{u\cdot u}u \\
v^\parallel &=& \phantom{v-}\frac{u\cdot v}{u\cdot u}u.
\end{eqnarray*}
This is called an \emph{orthogonal decomposition}\index{Orthogonal decomposition} because we have decomposed $v$ into a sum of orthogonal vectors.  This decomposition depends on $u$; if we change the direction of $u$ we change $v^\perp$ and $v^\parallel$.

If $u$, $v$ are linearly independent vectors in $\Re^3$, then the set $\{u, v^\perp, u\times v^\perp \}$ would be an orthogonal basis for $\Re^3$.  This set could then be normalized by dividing each vector by its length to obtain an orthonormal basis.

However, it often occurs that we are interested in vector spaces with dimension greater than $3$, and must resort to craftier means than cross products to obtain an orthogonal basis.
\footnote{Actually, given a set $T$ of $(n-1)$ independent vectors in $n$-space, one can define an analogue of the cross product that will produce a vector orthogonal to the span of $T$, using a method exactly analogous to the usual computation for calculating the cross product of two vectors in $\Re^3$.  This only gets us the \emph{last} orthogonal vector, though; the process in this Section gives a way to get a full orthogonal basis.}
%yeah, and who is going to teach them about the Hodge star in office hours? I say nix this footnote -cherney

Given a third vector $w$, we should first check that $w$ does not lie in the span of $u$ and $v$, \textit{i.e.} check that $u,v$ and $w$ are linearly independent.   We then can define:
\[
w^\perp = w - \frac{u\dotprod w}{u\dotprod u}\,u - \frac{v^\perp\dotprod w}{v^\perp\dotprod v^\perp}\,v^\perp.
\]

We can check that \(u \dotprod w^\perp\) and \(v^\perp \dotprod w^\perp\) are both zero:
\begin{align*}
u \dotprod w^\perp&=u \dotprod \left(w - \frac{u\dotprod w}{u\dotprod u}\,u - \frac{v^\perp\dotprod w}{v^\perp\dotprod v^\perp}\,v^\perp \right)\\&= u\dotprod w - \frac{u \dotprod w}{u \dotprod u}u \dotprod u - \frac{v^\perp \dotprod w}{v^\perp \dotprod v^\perp} u \dotprod v^\perp \\
&=u\dotprod w-u\dotprod w-\frac{v^\perp \dotprod w}{v^\perp \dotprod v^\perp} u \dotprod v^\perp\ =\ 0
\end{align*}
since \(u\) is orthogonal to \(v^\perp\), and
\begin{align*}
v^\perp \dotprod w^\perp&=v^\perp \dotprod \left(w - \frac{u\dotprod w}{u\dotprod u}\,u - \frac{v^\perp\dotprod w}{v^\perp\dotprod v^\perp}\,v^\perp \right)\\ &=v^\perp\dotprod w - \frac{u \dotprod w}{u \dotprod u}v^\perp \dotprod u - \frac{v^\perp \dotprod w}{v^\perp \dotprod v^\perp} v^\perp \dotprod v^\perp \\
&=v^\perp\dotprod w-\frac{u \dotprod w}{u \dotprod u}v^\perp \dotprod u - v^\perp \dotprod w\ =\ 0
\end{align*}
because \(u\) is orthogonal to \(v^\perp\). Since $w^\perp$ is orthogonal to both $u$ and $v^\perp$, we have that $\{u,v^\perp,w^\perp \}$ is an orthogonal basis for $\spa \{u,v,w\}$.

\section{The Gram-Schmidt Procedure}
In fact, given a set $\{v_1, v_2, \ldots \}$ of linearly independent vectors, we can define an orthogonal basis for $\spa \{v_1,v_2, \ldots \}$ consisting of the following vectors:
\begin{eqnarray*}
v_1^\perp&:=&v_1 \\
v_2^\perp &:=& v_2 - \frac{v_1^\perp\cdot v_2}{v_1^\perp\cdot v_1^\perp}\,v_1^\perp \\
v_3^\perp &:=& v_3 - \frac{v_1^\perp\cdot v_3}{v_1^\perp\cdot v_1^\perp}\,v_1^\perp - \frac{v_2^\perp\cdot v_3}{v_2^\perp\cdot v_2^\perp}\,v_2^\perp\\
&\vdots& \\
v_i^\perp%&=&   v_i - \sum_{j<i} \frac{v_j^\perp\cdot v_i}{v_j^\perp\cdot v_j^\perp}\,v_j^\perp \\
 &:=& v_i - \frac{v_1^\perp\cdot v_i}{v_1^\perp\cdot v_1^\perp}\,v_1^\perp -  
 - \frac{v_2^\perp\cdot v_3}{v_2^\perp\cdot v_2^\perp}\,v_2^\perp -\cdots
 - \frac{v_{i-1}^\perp\cdot v_i}{v_{i-1}^\perp\cdot v_{i-1}^\perp}\,v_{i-1}^\perp\\
&\vdots& \\
\end{eqnarray*}
Notice that each $v_i^\perp$ here depends on  $v_j^\perp$ for every $j<i$.  This allows us to inductively/algorithmically build up a linearly independent, orthogonal set of vectors 
$\{v_1^\perp,v_2^\perp, \ldots \}$ 
such that 
$\spa \{v_1^\perp,v_2^\perp, \ldots \}=\spa \{v_1, v_2, \ldots \}$. That is, on orthogonal basis for the latter vector space. This algorithm bears the name \emph{Gram--Schmidt orthogonalization procedure}\index{Gram--Schmidt orthogonalization procedure}\label{GramSchmidt}.

\begin{example}
We'll  obtain an orthogonal basis for $\Re^3$ by appling Gram-Schmidt to the linearly independent set 
$\left\{ v_1=\colvec{1\\1\\0}, v_2=\colvec{1\\1\\1},v_3=\colvec{3\\1\\1} \right\}$.

First, we set $v_1^\perp:=v_1$.  Then:
\begin{eqnarray*}
v_2^\perp&=& \rowvec{1\\1\\1} - \frac{2}{2}\rowvec{1\\1\\0} = \rowvec{0\\0\\1} \\
V_3^\perp&=& \rowvec{3\\1\\1} - \frac{4}{2}\rowvec{1\\1\\0} - \frac{1}{1}\rowvec{0\\0\\1} = \rowvec{1\\-1\\0}. 
\end{eqnarray*}
Then the set
\[
\left\{ \rowvec{1\\1\\0},\rowvec{0\\0\\1},\rowvec{1\\-1\\0}\right\}
\]
is an orthogonal basis for $\Re^3$.  To obtain an orthonormal basis, as always we simply divide each of these vectors by its length, yielding:
\[
\left\{ \rowvec{\frac{1}{\sqrt2}\\\frac{1}{\sqrt2}\\0},\rowvec{0\\0\\1},\rowvec{\frac{1}{\sqrt2}\\\frac{-1}{\sqrt2}\\0}\right\}.
\]
\end{example}

\videoscriptlink{gram_schimdt_and_orthogonal_complements_4by4_example.mp4}{A $4\times4$ Gram Schmidt Example} {scripts_gram_schimdt_and_orthogonal_complements_4by4_example}

\section{$QR$ Decomposition}
In Lecture~\ref{LUdecomp} we learned how to solve linear systems by decomposing a matrix $M$ into 
a product of lower and upper triangular matrices
$$M=LU\, .$$
The Gram--Schmidt procedure suggests another matrix decomposition,
$$M=QR$$ 
where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix. So-called QR-decompositions\index{QR decomposition}
are useful for solving linear systems, eigenvalue problems and least squares approximations. You can
easily get the idea behind $QR$ decomposition by working through a simple example.

\begin{example}
\hypertarget{methodQR}{Find} the $QR$ decomposition of $$M=\begin{pmatrix}2&-1&1\\1&3&-2\\0&1&-2\end{pmatrix}\, .$$
What we will do is to think of the columns of $M$ as three vectors and use Gram--Schmidt to
build an orthonormal basis from these that will become the columns of the orthogonal matrix $Q$.
We will use the matrix $R$ to record the steps of the Gram--Schmidt procedure in such a way
that the product $QR$ equals $M$. 

To begin with we write
$$
M=\begin{pmatrix}2&-\frac75&1\\[1mm]1&\frac{14}5&-2\\[1mm]0&1&-2\end{pmatrix}
\begin{pmatrix}1&\frac15&0\\[1mm]0&1&0\\[1mm]0&0&1\end{pmatrix}\, .
$$
In the first matrix the first two columns are  orthogonal because we simpy replaced the second column of $M$ by the vector that the Gram--Schmidt
procedure produces from the first two columns of~$M$, namely
$$
\colvec{-\frac75\\[1mm]\frac{14}5\\[1mm]1}=\colvec{-1\\[1mm]3\\[1mm]1}-\frac15
\colvec{ 2 \\[1mm]1\\[1mm]0}\, .
$$
 The matrix on the right is almost the identity
matrix, save the $+\frac15$ in the second entry of the first row, whose effect upon multiplying the
two matrices precisely undoes what we we did to the second column of the first matrix. 

For the third column of $M$ we use Gram--Schmidt to deduce the third orthogonal vector
$$
\colvec{-\frac16\\[1mm]\frac13\\[1mm]-\frac76}=
\colvec{1\\[1mm]-2\\[1mm]-2}
-0
\colvec{ 2 \\[1mm]1\\[1mm]0}
-\frac{-9}{\frac{54}{5}}\colvec{-\frac75\\[1mm]\frac{14}5\\[1mm]1}\, ,
$$
and therefore, using exactly the same procedure write
$$
M=\begin{pmatrix}2&-\frac75&-\frac16\\[1mm]1&\frac{14}5&\frac13\\[1mm]0&1&-\frac76\end{pmatrix}
\begin{pmatrix}1&\frac15&0\\[1mm]0&1&-\frac56\\[1mm]0&0&1\end{pmatrix}\, .
$$
This is not quite the answer because the first matrix is now made of mutually orthogonal column vectors,
but  a {\it bona fide} orthogonal matrix is comprised of {\it orthonormal} vectors. To achieve that we divide
each column of the first matrix by its length and multiply the corresponding row of the second matrix by the same 
amount:
$$
M=\begin{pmatrix}\frac{2\sqrt{5}}{5}&-\frac{7\sqrt{30}}{90}&-\frac{\sqrt{6}}{18}\\[2mm]
\frac{\sqrt{5}}{5}&\frac{7\sqrt{30}}{45}&\frac{\sqrt{6}}{9}\\[2mm]
0&\frac{\sqrt{30}}{18}&-\frac{7\sqrt{6}}{18}\end{pmatrix}
\begin{pmatrix}\sqrt{5}&\frac{\sqrt{5}}{5}&0\\[2mm]
0&\frac{3\sqrt{30}}{5}&-\frac{\sqrt{30}}{2}\\[2mm]
0&0&\frac{\sqrt{6}}{2}\end{pmatrix}=QR\, .
$$
A nice check of this result is to verify that entry $(i,j)$  of the matrix $R$
equals the dot product of the $i$-th column of $Q$ with the $j$-th column of $M$.
(Some people memorize this fact and use it as a recipe for computing $QR$ deompositions.)
{\it A good test of your own understanding is to work out why this is true!}
\end{example}


\videoscriptlink{gram_schimdt_and_orthogonal_complements_qr_example.mp4}{Another $QR$ decomposition example}{scripts_gram_schmidt_and_orthogonal_complements_qr_example}

\section{Orthogonal Complements}

Let $U$ and $V$ be subspaces of a vector space $W$.  We saw as a \hyperref[UcapV]{review exercise} that $U\cap V$ is a subspace of $W$, and that $U\cup V$ was not a subspace.  However, $\spa (U\cup V)$ is certainly a subspace, since the span of \emph{any} subset of a vector space is a subspace.
Notice that all elements of $\spa (U\cup V)$ take the form $u+v$ with $u\in U$ and $v\in V$.  We call the subspace 
\[
U+V:=\spa (U\cup V) = \{u+v | u\in U, v\in V \}
\] 
the \emph{sum}\index{Sum of vectors spaces} of $U$ and $V$.  Here, we are not adding vectors, but vector spaces to produce a new vector space!


\begin{definition}
Given two subspaces $U$ and $V$ of a space $W$ such that $U\cap V=\{0_W\}$, the \emph{direct sum}\index{Direct sum} of $U$ and $V$ is defined as:
\[
U \oplus V = \spa (U\cup V)= \{u+v | u\in U, v\in V \}.
\]
\end{definition}
Notice that when $U\cap V= \{0_W\}$, $U+V=U\oplus V$.


The direct sum has a very nice property.

\begin{theorem}
If $w\in U\oplus V$  then 
%the expression $w=u+v$ is unique.  That is, 
there is only one way to write \(w\) as the sum of a vector in \(U\) and a vector in \(V\).  
\end{theorem}

\begin{proof}
Suppose that $u+v=u'+v'$, with $u,u'\in U$, and $v,v' \in V$.  Then we could express $0=(u-u')+(v-v')$.  Then $(u-u')=-(v-v')$.  Since $U$ and $V$ are subspaces, we have $(u-u')\in U$ and $-(v-v')\in V$.  But since these elements are equal, we also have $(u-u')\in V$.  Since $U\cap V=\{0\}$, then $(u-u')=0$.  Similarly, $(v-v')=0$. Therefore $u=u'$ and  $v=v'$, proving the theorem. 
\end{proof}

\reading{22}{1}
%\begin{center}\href{\webworkurl ReadingHomework22/1/}{Reading homework: problem \ref{gramschmidt}.1}\end{center}

Given a subspace $U$ in $W$, how can we write $W$ as the direct sum of $U$ and \emph{something}? There is not a unique answer to this question as can be seen from this picture of subspaces in $W={\mathbb R}^3$: 
\begin{center}
\includegraphics[scale=.25]{\gramSchmidtPath/direct_sums.jpg}
\end{center}
However, using the inner product, there is a natural candidate $U^\perp$ for this second subspace as shown here:
\begin{center}
\includegraphics[scale=.25]{\gramSchmidtPath/U_perp.jpg}
\end{center}

The general definition is as follows:
\begin{definition}
Given a subspace $U$ of a vector space $W$, define:
\[
U^\perp = \{w\in W | w\dotprod u=0 \text{ for all } u\in U\}.
\]
\end{definition}

The set $U^\perp$ (pronounced ``$U$-perp'') is the set of all vectors in $W$ orthogonal to \emph{every} vector in $U$.  This is also often called the \emph{orthogonal complement}\index{Orthogonal complement} of $U$. Probably by now you may be feeling overwhelmed, it may help to watch this quick overview video:

\videoscriptlink{gram_schmidt_and_orthogonal_complements_theory.mp4}{Overview}{scripts_gram_schmidt_and_orthogonal_complements_theory}



\begin{example}
Consider any plane $P$ through the origin in $\Re^3$.  Then $P$ is a subspace, and $P^\perp$ is the line through the origin orthogonal to $P$.  For example, if $P$ is the $xy$-plane, then
\[
\Re^3=P\oplus P^\perp=\{(x,y,0)| x,y\in \Re \} \oplus \{(0,0,z)| z\in \Re \}.
\]
\end{example}

\begin{theorem}
Let $U$ be a subspace of a finite-dimensional vector space $W$.  Then the set $U^\perp$ is a subspace of $W$, and $W=U\oplus U^\perp$\index{Perp@``Perp''}.
\end{theorem}

\begin{proof}
To see that $U^\perp$ is a subspace, we only need to check closure, which requires a simple check.

We have $U\cap U^\perp=\{0\}$, since if $u\in U$ and $u\in U^\perp$, we have:
\[
u\dotprod u = 0 \Leftrightarrow u=0.
\]

Finally, we show that any vector $w\in W$ is in $U\oplus U^\perp$.  (This is where we use the assumption that $W$ is finite-dimensional.)  Let $e_1, \ldots, e_n$ be an orthonormal basis for $W$.  Set: 
\begin{eqnarray*}
u&=&(w\dotprod e_1)e_1 + \cdots + (w\dotprod e_n)e_n \in U\\
u^\perp&=& w-u
\end{eqnarray*}
It is easy to check that $u^\perp \in U^\perp$ (see the Gram-Schmidt procedure).  Then $w=u+u^\perp$, so $w\in U\oplus U^\perp$, and we are done.
\end{proof}

\reading{22}{2}
%\begin{center}\href{\webworkurl ReadingHomework22/2/}{Reading homework: problem \ref{gramschmidt}.2}\end{center}

\begin{example}
Consider any line \(L\) through the origin in \(\Re^4\). Then \(L\) is a subspace, and \(L^\perp\) is a \(3\)-dimensional subspace orthogonal to \(L\). For example, let \(L\) be the line 
$\spa \{ (1,1,1,1)^T\}$ in \(\Re^4.\) Then \(L^\perp\) is given by
\begin{eqnarray*}
L^\perp&=&\{(x,y,z,w)^T \mid x,y,z,w \in \Re \text{ and } (x,y,z,w)^T \dotprod (1,1,1,1)^T=0\} \\
&=&\{(x,y,z,w)^T \mid x,y,z,w \in \Re \text{ and } x,y,z,w=0\}.
\end{eqnarray*}
It is easy to check that 
$$
\left\{
v_1=\colvec{1\\-1\\0\\0}, v_2=\colvec{1\\0\\-1\\0}, v_3=\colvec{1\\0\\0\\-1} \right \}
$$ 
forms a basis for \(L^\perp\). We use Gram-Schmidt to find an orthogonal basis for \(L^\perp\):

First, we set \(v_1^\perp=v_1\). Then:
\begin{eqnarray*}
v_2^\perp&=&\colvec{1\\0\\-1\\0}-\frac{1}{2}\colvec{1,-1,0,0}
=\colvec{\frac{1}{2}\\ \frac{1}{2} \\-1\\ 0 },\\
v_3^\perp&=&\colvec{ 1\\0\\0\\-1} -\frac{1}{2}\colvec{1\\-1\\0\\0}-\frac{1/2}{3/2}
\colvec{ \frac{1}{2}\\\frac{1}{2}\\-1\\0} =\colvec{ \frac{1}{3}\\\frac{1}{3}\\\frac{1}{3}\\-1}.
\end{eqnarray*}
So the set \[\left\{ (1,-1,0,0)^T, \left(\frac{1}{2},\frac{1}{2},-1,0\right)^T, \left(\frac{1}{3},\frac{1}{3},\frac{1}{3},-1\right)^T \right\} \] is an orthogonal basis for \(L^\perp\).
We find an orthonormal basis for \(L^\perp\) by dividing each basis vector by its length:
\[
\left\{
\left( \frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}},0,0 \right)^T,
\left( \frac{1}{\sqrt{6}}, \frac{1}{\sqrt{6}}, -\frac{2}{\sqrt{6}},0 \right)^T,
\left( \frac{\sqrt{3}}{6}, \frac{\sqrt{3}}{6}, \frac{\sqrt{3}}{6}, -\frac{\sqrt{3}}{2} \right)^T
\right\}.
\]
Moreover, we have
\[
\Re^4=L \oplus L^\perp = \{(c,c,c,c)^T \mid c \in \Re\} \oplus \{(x,y,z,w)^T \mid x,y,z,w \in \Re \text{ and } x+y+z+w=0\}.
\]
\end{example}

Notice that for any subspace $U$, the subspace $(U^\perp)^\perp$ is just $U$ again.  As such, $\perp$ is an involution on the set of subspaces of a vector space.

%\section*{References}
%Hefferon, Chapter Three, Section VI.2: Gram-Schmidt Orthogonalization
%\\
%Beezer, Chapter V, Section O, Subsection GSP
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Gram_schmidt}{Gram-Schmidt Process}
%\item \href{http://en.wikipedia.org/wiki/QR_decomposition}{QR Decomposition}
%\item \href{http://en.wikipedia.org/wiki/Orthonormal_basis}{Orthonormal Basis}
%\item \href{http://en.wikipedia.org/wiki/Direct_sum}{Direct Sum}
%\end{itemize}
%

\section{Review Problems}
\input{\gramSchmidtPath/problems}

\newpage

",lesson
24,Diagonalizing Symmetric Matrices,"
\chapter{\diagSymMatTitle}\label{symmetricmatrices}

Symmetric matrices have many applications.  For example, if we consider the shortest distance between pairs of important cities, we might get a table like the following.
\[
\begin{array}{c|ccc}
 & \text{Davis} & \text{Seattle} 
& \text{San Francisco} \\ \hline
\text{Davis} & 0 & 2000 & 80 \\
\text{Seattle} & 2000 & 0 & 2010 \\
\text{San Francisco} & 80 & 2010 & 0
\end{array}
\]
Encoded as a matrix, we obtain
\[
M=\begin{pmatrix}
\mc{0} & \mc{2000} & \mc{80} \\
\mc{2000} & \mc{0} & \mc{2010} \\
\mc{80} & \mc{2010} & \mc{0}
\end{pmatrix}=M^T.
\]

\begin{definition}
A matrix $M$ is {\bf symmetric}\index{Symmetric matrix} if  $M^T=M.$
\end{definition}

One very nice property of symmetric matrices is that they always have real eigenvalues.  Review exercise~\ref{prob_real_eigenvalues} guides you through the general proof, but below is an example for $2\times 2$ matrices.

\begin{example}
For a general symmetric $2\times 2$ matrix, we have:
\begin{eqnarray*}
P_\lambda \begin{pmatrix} a & b \\ b& d \end{pmatrix}
 &=&
\det\begin{pmatrix}\lambda-a&\mc{-b}\\\mc{-b}&\lambda-d \end{pmatrix}\\[1mm]
&=& (\lambda-a)(\lambda-d)-b^2 \\[2mm]
&=& \lambda^2-(a+d)\lambda-b^2+ad\\[1mm]
\Rightarrow \lambda &=& \frac{a+d}{2}\pm \sqrt{b^2+\left(\frac{a-d}{2}\right)^2}.
\end{eqnarray*}
Notice that the discriminant $4b^2+(a-d)^2$ is always positive, so that the eigenvalues must be real.
\end{example}

Now, suppose a symmetric matrix $M$ has two distinct eigenvalues $\lambda \neq \mu$ and eigenvectors $x$ and $y$;
\[
Mx=\lambda x, \qquad My=\mu y.
\] 
Consider the dot product $x\dotprod y = x^Ty = y^Tx$ and calculate
\begin{eqnarray*}
x^TM y &=& x^T\mu y = \mu x\dotprod y, \text{ and }\\[3mm]
x^TM y &=& (y^TMx)^T \text{ (by transposing a $1\times 1$ matrix)}\\[1mm]
       &=& (y^T\lambda x)^T \\
       &=& (\lambda x\dotprod y)^T \\
             &=& \lambda x\dotprod y.
\end{eqnarray*}
Subtracting these two results tells us that:
\begin{eqnarray*}
0 &=& x^TMy-x^TMy=(\mu-\lambda)\,x\dotprod y.
\end{eqnarray*}
Since $\mu$ and $\lambda$ were assumed to be distinct eigenvalues, $\lambda-\mu$ is non-zero, and so $x\dotprod y=0$.  We have proved the following theorem.

\begin{theorem}
Eigenvectors of a symmetric matrix with distinct eigenvalues are orthogonal.
\end{theorem}

%\begin{center}\href{\webworkurl ReadingHomework23/1/}{Reading homework: problem \ref{symmetricmatrices}.1}\end{center}
\Reading{DiagonalizingSymmetricMatrices}{1}

\begin{example}
The matrix $M=\begin{pmatrix}2&1\\1&2\end{pmatrix}$
has eigenvalues determined by
\[
\det(M-\lambda I)=(2-\lambda)^2-1=0.
\] 
So the eigenvalues of $M$ are $3$ and $1$, and the associated eigenvectors turn out to be 
$\colvec{1\\1}$ and $\colvec{1\\-1}$.  It is easily seen that these eigenvectors are \hyperref[orthogonal]{orthogonal}; 
\[
\colvec{1\\1} \dotprod \colvec{1\\-1}=0.
\]
\end{example}

In \hyperlink{basisorthog}{chapter~\ref{orthonormalbases}} we saw that the matrix $P$ built from any orthonormal basis  $(v_1,\ldots, v_n )$
for ${\mathbb R}^n$ as its columns,
\[
P=\rowvec{v_1 & \cdots & v_n}\, ,
\]
was an orthogonal matrix. This means that 
\[
P^{-1}=P^T, \text{ or } PP^T=I=P^TP.
\]
Moreover, given any (unit) vector $x_1$, one can always find vectors $x_2, \ldots, x_n$ such that $(x_1,\ldots, x_n)$ is an orthonormal basis.  (Such a basis can be obtained using the~\hyperref[GramSchmidt]{Gram-Schmidt procedure}.)

Now suppose $M$ is a symmetric $n\times n$ matrix and $\lambda_1$ is an eigenvalue with eigenvector $x_1$ (this is always the case because every matrix has at least one eigenvalue--see Review Problem~\ref{atleastone}).  
Let $P$ be the square matrix of orthonormal column vectors 
\[
P=\rowvec{x_1 & x_2 & \cdots & x_n},
\]
While $x_1$ is an eigenvector for $M$, the others are not necessarily eigenvectors for $M$.  
Then
\[
MP=\rowvec{\lambda_1 x_1 & Mx_2 & \cdots & Mx_n}.
\]
But $P$ is an orthogonal matrix, so $P^{-1}=P^T$.  Then:
\begin{eqnarray*}
P^{-1}=P^T &=& \ccolvec{x_1^T\\ \vdots \\ x_n^T} \\[1mm]
\Rightarrow P^TMP &=& \begin{pmatrix}
  x_1^T\lambda_1x_1  & * & \cdots & *\\
  x_2^T\lambda_1x_1  & * & \cdots & *\\
  \mc\vdots             &   & & \mc\vdots\\
   x_n^T\lambda_1x_1 & * & \cdots & *\\
  \end{pmatrix}\\[2mm]
&=& \begin{pmatrix}
  \lambda_1  & * & \cdots & *\\
  \mc 0  & * & \cdots & *\\
 \mc \vdots             & *  & & \mc\vdots\\
  \mc 0 & * & \cdots & *\\
  \end{pmatrix}\\[2mm]
&=& \begin{pmatrix}
  \lambda_1  & 0 & \cdots & 0\\
  \mc 0          & & & \\
  \mc\vdots     & & \hat{M} & \\
  \mc0          & & & \\
  \end{pmatrix}\, .\\
\end{eqnarray*}
The last equality follows since $P^TMP$ is symmetric.  The asterisks in the matrix are where ``stuff'' happens; this extra information is denoted by $\hat{M}$ in the final expression.  We know nothing about $\hat{M}$ except that it is an $(n-1)\times (n-1)$ matrix and that it is symmetric.  But then, by finding an (unit) eigenvector for $\hat{M}$, we could repeat this procedure successively.  The end result would be a diagonal matrix with eigenvalues of $M$ on the diagonal. Again, we have proved a theorem: %we also need that every matrix has an eigenvector.

\begin{theorem}
Every symmetric matrix is similar to a diagonal matrix of its eigenvalues.  In other words,
\[
M=M^T \Leftrightarrow M=PDP^T
\]
where $P$ is an orthogonal matrix and $D$ is a diagonal matrix whose entries are the eigenvalues of $M$.
\end{theorem}

%\begin{center}\href{\webworkurl ReadingHomework23/2/}{Reading homework: problem \ref{symmetricmatrices}.2}
\Reading{DiagonalizingSymmetricMatrices}{2}
%\end{center}

To diagonalize a real symmetric matrix, begin by building an orthogonal matrix from an orthonormal basis of eigenvectors, as in the example below. 

\begin{example}
The symmetric matrix 
$$M=\begin{pmatrix}2&1\\1&2\end{pmatrix}\,  ,$$ has eigenvalues $3$ and $1$ with eigenvectors $\colvec{1\\1}$ and $\colvec{1\\-1}$ respectively.  After normalizing these eigenvectors, we  build the orthogonal matrix:
\[
P = \begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\[2mm]
\frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{2}}
\end{pmatrix}\, .
\]
Notice that $P^TP=I$.  Then:
\[
MP = \begin{pmatrix}
\frac{3}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\[2mm]
\frac{3}{\sqrt{2}} & \frac{-1}{\sqrt{2}}
\end{pmatrix} = 
\begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\[2mm]
\frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{2}}
\end{pmatrix} \begin{pmatrix}
3 & 0 \\[2mm]
0 & 1
\end{pmatrix}.
\]
In short, $MP=PD$, so $D=P^TMP$.  Then $D$ is the diagonalized form of $M$ and $P$ the associated change-of-basis matrix from the standard basis to the basis of eigenvectors.
\end{example}

\Videoscriptlink{diagonalizing_symmetric_matrices_3by3_example.mp4}{ $3\times 3$ Example}{scripts_diagonalizing_symmetric_matrices_3by3_example}












%\section*{References}
%Hefferon, Chapter Three, Section V: Change of Basis
%\\
%Beezer, Chapter E, Section PEE, Subsection EHM
%\\
%Beezer, Chapter E, Section SD, Subsection D
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Symmetric_matrix}{Symmetric Matrix}
%\item \href{http://en.wikipedia.org/wiki/Diagonalizable_matrix}{Diagonalizable Matrix}
%\item \href{http://en.wikipedia.org/wiki/Similar_matrix}{Similar Matrix}
%\end{itemize}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{DiagonalizingSymmetricMatrices}{1}, 
 \hwrref{DiagonalizingSymmetricMatrices}{2}, 
 \\
Diagonalizing a symmetric matrix &  \hwref{DiagonalizingSymmetricMatrices}{3}, \hwref{DiagonalizingSymmetricMatrices}{4}\\
   \hline
\end{tabular}




\input{\diagSymMatPath/problems}

%\newpage
","
\chapter{\diagSymMatTitle}\label{symmetricmatrices}

Symmetric matrices have many applications.  For example, if we consider the shortest distance between pairs of important cities, we might get a table like the following.
\[
\begin{array}{c|ccc}
 & \text{Davis} & \text{Seattle} 
& \text{San Francisco} \\ \hline
\text{Davis} & 0 & 2000 & 80 \\
\text{Seattle} & 2000 & 0 & 2010 \\
\text{San Francisco} & 80 & 2010 & 0
\end{array}
\]
Encoded as a matrix, we obtain
\[
M=\begin{pmatrix}
\mc{0} & \mc{2000} & \mc{80} \\
\mc{2000} & \mc{0} & \mc{2010} \\
\mc{80} & \mc{2010} & \mc{0}
\end{pmatrix}=M^T.
\]

\begin{definition}
A matrix $M$ is {\bf symmetric}\index{Symmetric matrix} if  $M^T=M.$
\end{definition}

One very nice property of symmetric matrices is that they always have real eigenvalues.  Review exercise~\ref{prob_real_eigenvalues} guides you through the general proof, but below is an example for $2\times 2$ matrices.

\begin{example}
For a general symmetric $2\times 2$ matrix, we have:
\begin{eqnarray*}
P_\lambda \begin{pmatrix} a & b \\ b& d \end{pmatrix}
 &=&
\det\begin{pmatrix}\lambda-a&\mc{-b}\\\mc{-b}&\lambda-d \end{pmatrix}\\[1mm]
&=& (\lambda-a)(\lambda-d)-b^2 \\[2mm]
&=& \lambda^2-(a+d)\lambda-b^2+ad\\[1mm]
\Rightarrow \lambda &=& \frac{a+d}{2}\pm \sqrt{b^2+\left(\frac{a-d}{2}\right)^2}.
\end{eqnarray*}
Notice that the discriminant $4b^2+(a-d)^2$ is always positive, so that the eigenvalues must be real.
\end{example}

Now, suppose a symmetric matrix $M$ has two distinct eigenvalues $\lambda \neq \mu$ and eigenvectors $x$ and $y$;
\[
Mx=\lambda x, \qquad My=\mu y.
\] 
Consider the dot product $x\dotprod y = x^Ty = y^Tx$ and calculate
\begin{eqnarray*}
x^TM y &=& x^T\mu y = \mu x\dotprod y, \text{ and }\\[3mm]
x^TM y &=& (y^TMx)^T \text{ (by transposing a $1\times 1$ matrix)}\\[1mm]
       &=& (y^T\lambda x)^T \\
       &=& (\lambda x\dotprod y)^T \\
             &=& \lambda x\dotprod y.
\end{eqnarray*}
Subtracting these two results tells us that:
\begin{eqnarray*}
0 &=& x^TMy-x^TMy=(\mu-\lambda)\,x\dotprod y.
\end{eqnarray*}
Since $\mu$ and $\lambda$ were assumed to be distinct eigenvalues, $\lambda-\mu$ is non-zero, and so $x\dotprod y=0$.  We have proved the following theorem.

\begin{theorem}
Eigenvectors of a symmetric matrix with distinct eigenvalues are orthogonal.
\end{theorem}

%\begin{center}\href{\webworkurl ReadingHomework23/1/}{Reading homework: problem \ref{symmetricmatrices}.1}\end{center}
\Reading{DiagonalizingSymmetricMatrices}{1}

\begin{example}
The matrix $M=\begin{pmatrix}2&1\\1&2\end{pmatrix}$
has eigenvalues determined by
\[
\det(M-\lambda I)=(2-\lambda)^2-1=0.
\] 
So the eigenvalues of $M$ are $3$ and $1$, and the associated eigenvectors turn out to be 
$\colvec{1\\1}$ and $\colvec{1\\-1}$.  It is easily seen that these eigenvectors are \hyperref[orthogonal]{orthogonal}; 
\[
\colvec{1\\1} \dotprod \colvec{1\\-1}=0.
\]
\end{example}

In \hyperlink{basisorthog}{chapter~\ref{orthonormalbases}} we saw that the matrix $P$ built from any orthonormal basis  $(v_1,\ldots, v_n )$
for ${\mathbb R}^n$ as its columns,
\[
P=\rowvec{v_1 & \cdots & v_n}\, ,
\]
was an orthogonal matrix. This means that 
\[
P^{-1}=P^T, \text{ or } PP^T=I=P^TP.
\]
Moreover, given any (unit) vector $x_1$, one can always find vectors $x_2, \ldots, x_n$ such that $(x_1,\ldots, x_n)$ is an orthonormal basis.  (Such a basis can be obtained using the~\hyperref[GramSchmidt]{Gram-Schmidt procedure}.)

Now suppose $M$ is a symmetric $n\times n$ matrix and $\lambda_1$ is an eigenvalue with eigenvector $x_1$ (this is always the case because every matrix has at least one eigenvalue--see Review Problem~\ref{atleastone}).  
Let $P$ be the square matrix of orthonormal column vectors 
\[
P=\rowvec{x_1 & x_2 & \cdots & x_n},
\]
While $x_1$ is an eigenvector for $M$, the others are not necessarily eigenvectors for $M$.  
Then
\[
MP=\rowvec{\lambda_1 x_1 & Mx_2 & \cdots & Mx_n}.
\]
But $P$ is an orthogonal matrix, so $P^{-1}=P^T$.  Then:
\begin{eqnarray*}
P^{-1}=P^T &=& \ccolvec{x_1^T\\ \vdots \\ x_n^T} \\[1mm]
\Rightarrow P^TMP &=& \begin{pmatrix}
  x_1^T\lambda_1x_1  & * & \cdots & *\\
  x_2^T\lambda_1x_1  & * & \cdots & *\\
  \mc\vdots             &   & & \mc\vdots\\
   x_n^T\lambda_1x_1 & * & \cdots & *\\
  \end{pmatrix}\\[2mm]
&=& \begin{pmatrix}
  \lambda_1  & * & \cdots & *\\
  \mc 0  & * & \cdots & *\\
 \mc \vdots             & *  & & \mc\vdots\\
  \mc 0 & * & \cdots & *\\
  \end{pmatrix}\\[2mm]
&=& \begin{pmatrix}
  \lambda_1  & 0 & \cdots & 0\\
  \mc 0          & & & \\
  \mc\vdots     & & \hat{M} & \\
  \mc0          & & & \\
  \end{pmatrix}\, .\\
\end{eqnarray*}
The last equality follows since $P^TMP$ is symmetric.  The asterisks in the matrix are where ``stuff'' happens; this extra information is denoted by $\hat{M}$ in the final expression.  We know nothing about $\hat{M}$ except that it is an $(n-1)\times (n-1)$ matrix and that it is symmetric.  But then, by finding an (unit) eigenvector for $\hat{M}$, we could repeat this procedure successively.  The end result would be a diagonal matrix with eigenvalues of $M$ on the diagonal. Again, we have proved a theorem: %we also need that every matrix has an eigenvector.

\begin{theorem}
Every symmetric matrix is similar to a diagonal matrix of its eigenvalues.  In other words,
\[
M=M^T \Leftrightarrow M=PDP^T
\]
where $P$ is an orthogonal matrix and $D$ is a diagonal matrix whose entries are the eigenvalues of $M$.
\end{theorem}

%\begin{center}\href{\webworkurl ReadingHomework23/2/}{Reading homework: problem \ref{symmetricmatrices}.2}
\Reading{DiagonalizingSymmetricMatrices}{2}
%\end{center}

To diagonalize a real symmetric matrix, begin by building an orthogonal matrix from an orthonormal basis of eigenvectors, as in the example below. 

\begin{example}
The symmetric matrix 
$$M=\begin{pmatrix}2&1\\1&2\end{pmatrix}\,  ,$$ has eigenvalues $3$ and $1$ with eigenvectors $\colvec{1\\1}$ and $\colvec{1\\-1}$ respectively.  After normalizing these eigenvectors, we  build the orthogonal matrix:
\[
P = \begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\[2mm]
\frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{2}}
\end{pmatrix}\, .
\]
Notice that $P^TP=I$.  Then:
\[
MP = \begin{pmatrix}
\frac{3}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\[2mm]
\frac{3}{\sqrt{2}} & \frac{-1}{\sqrt{2}}
\end{pmatrix} = 
\begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\[2mm]
\frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{2}}
\end{pmatrix} \begin{pmatrix}
3 & 0 \\[2mm]
0 & 1
\end{pmatrix}.
\]
In short, $MP=PD$, so $D=P^TMP$.  Then $D$ is the diagonalized form of $M$ and $P$ the associated change-of-basis matrix from the standard basis to the basis of eigenvectors.
\end{example}

\Videoscriptlink{diagonalizing_symmetric_matrices_3by3_example.mp4}{ $3\times 3$ Example}{scripts_diagonalizing_symmetric_matrices_3by3_example}












%\section*{References}
%Hefferon, Chapter Three, Section V: Change of Basis
%\\
%Beezer, Chapter E, Section PEE, Subsection EHM
%\\
%Beezer, Chapter E, Section SD, Subsection D
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Symmetric_matrix}{Symmetric Matrix}
%\item \href{http://en.wikipedia.org/wiki/Diagonalizable_matrix}{Diagonalizable Matrix}
%\item \href{http://en.wikipedia.org/wiki/Similar_matrix}{Similar Matrix}
%\end{itemize}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{DiagonalizingSymmetricMatrices}{1}, 
 \hwrref{DiagonalizingSymmetricMatrices}{2}, 
 \\
Diagonalizing a symmetric matrix &  \hwref{DiagonalizingSymmetricMatrices}{3}, \hwref{DiagonalizingSymmetricMatrices}{4}\\
   \hline
\end{tabular}




\input{\diagSymMatPath/problems}

%\newpage
",lesson
25,Kernel Range Nullity Rank,"\chapter{\kernelTitle}\label{kernelrank}

Given a linear transformation $$L \colon V \to W\, ,$$ we often want  to know if it has an inverse, {\it i.e.}, if there exists a linear transformation $$M \colon W \to V$$ such that for any vector \(v \in V\), we have $$MLv=v\, ,$$ and for any vector \(w \in W\), we have $$LMw=w\, .$$ A linear transformation is a special kind of function from one vector space to another. So before we discuss which linear transformations have inverses, let us first discuss inverses of arbitrary functions. When we later specialize to linear transformations, we'll also find some nice ways of creating subspaces.

Let \(f \colon S \to T\) be a function from a set \(S\) to a set \(T\). \href{\webworkurl Homework0-Background/3/}{Recall} that \(S\) is called the {\it domain} of \(f\), \(T\) is called the {\it codomain} or {\it target} of \(f\).  
We now formally introduce a term that should be familar to you from many previous courses.

\section{Range}

\begin{definition} 
The {\bf range}  of a function $f:S\to T$ is  the set
\[
{\rm ran}(f):=\left\{ f(s) | s\in S \right\}\subset T\, .
\]
%is called\(f\). 
\end{definition} 
It is the subset of the codomain consisting of elements  to which the function \(f\) maps, {\it i.e.}, the things in \(T\) which you can get to by starting in \(S\) and applying \(f\). 

The range of a matrix is very easy to find; the range of a matrix is the span of its columns. 
Thus, calculation of the range of a matrix is very easy until the last step: simplification. One aught to end by the calculation by writing the vector space as the span of a linearly independent set. 

\begin{example}{ of calculating the range of a matrix.}
$${\rm ran} 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
:= 
\left\{ 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
\colvec{x\\y\\z\\w } | \colvec{x\\y\\z\\w} \in \R^4 \right\}
$$
$$=
\left\{   
x\colvec{1\\1\\0 } +y \colvec{ 2\\2\\0 } +z \colvec{0\\1\\1} + w\colvec{1\\2\\1} \middle| x,y,z,w \in \R \right\}
.$$
That is 
$$
{\rm ran} 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
= 
\spa \left\{   
\colvec{1\\1\\0 } , \colvec{ 2\\2\\0 } , \colvec{0\\1\\1}, \colvec{1\\2\\1} \right\}
$$
but since 
$${\rm RREF} \begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
=\begin{pmatrix} 
1&2&0&1\\
0&0&1&1\\
0&0&0&0
\end{pmatrix} 
$$
the second and fourth columns (which are the non-pivot columns), can be expressed as linear combinations of columns to their left. 
They can then be removed from the set in the span to obtain
$${\rm ran} 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
= \spa \left\{   
\colvec{1\\1\\0 } , \colvec{0\\1\\1} \right\} .
$$
\end{example}

It might occur to you that the range of the $3\times 4$ matrix from the last example can be expressed as the range of a $3\times 2$ matrix;
$$
{\rm ran} 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
={\rm ran} 
\begin{pmatrix} 
1&0\\
1&1\\
0&1
\end{pmatrix}.$$
Indeed, because the span of a set of vectors does not change when we replace the vectors with another set through an invertible process, we can calculate ranges through strings of equalities of ranges of matrices that differer by Elementary Column Operations, ECOs, ending with the range of a matrix in Column Reduced Echelon Form, CREF, with its zero columns deleted.

\begin{example} Calculating a range with ECOs\\
$$
{\rm ran} 
\begin{pmatrix} 
0&1&1\\
1&3&1\\
1&2&0
\end{pmatrix} 
\stackrel{c_1 \leftrightarrow c_3}{=}
{\rm ran} 
\begin{pmatrix} 
1&1&0\\
1&3&1\\
0&2&1
\end{pmatrix} 
\stackrel{c_2'= c_2-c_1}{=}
{\rm ran} 
\begin{pmatrix} 
1&0&0\\
1&2&1\\
0&2&1
\end{pmatrix} 
\stackrel{c_2'= \frac12 c_2}{=}
{\rm ran} 
\begin{pmatrix} 
1&0&0\\
1&1&1\\
0&1&1
\end{pmatrix} 
$$
$$\stackrel{c_3'= c_3-c_2}{=}
{\rm ran} 
\begin{pmatrix} 
1&0&0\\
1&1&0\\
0&1&0
\end{pmatrix} 
={\rm ran} 
\begin{pmatrix} 
1&0\\
1&1\\
0&1
\end{pmatrix}. 
$$


\end{example}

\noindent
This is an efficient way to compute and encode the range of a matrix.
%We think this is the most sophisticated and efficient way to calculate the range of a matrix, and encourage students to use this line of thinking.

\section{Image} 

\begin{definition}
For any subset $U$ of the domain $S$ of a function $f:S\to T$  the {\bf image} of $U$ is 
$$f(U)={\rm Im} \,U:= \left\{  f(x) | x\in U \right\} .$$
\end{definition}

\begin{example} 
The image of the  cube
$$U= \left\{  a \colvec{1\\0\\0} +b \colvec{0\\1\\0}+c\colvec{0\\0\\1} \middle| a,b,c \in [0,1] \right\} $$ 
under multiplication by the matrix 
$$ M=
\begin{pmatrix}
1&0&0\\
1&1&1\\
0&0&1
\end{pmatrix}
$$
is the parallelepiped 
$$
{\rm Img}\,U= 
 \left\{  a \colvec{1\\1\\0} +b \colvec{0\\1\\0}+c\colvec{0\\1\\1} \middle| a,b,c \in [0,1] \right\} .
$$
\end{example}

Note that for most subsets $U$ of the domain $S$  of a function  $f$ the image of $U$ is not a vector space. 
The range of a function is the particular case of the image where the subset of the domain is the entire domain; ${\rm ran} f = {\rm Img} S$. 
For this reason, the range of $f$ is also sometimes called the {image} of $f$ and is sometimes denoted ${ \rm im}(f)$ or  $f(S).$  We have seen that the range of a matrix is always a span of vectors, and hence a vector space. 

Note that we prefer the phrase ``range of $f$"" to the phrase ``image of f""  
because we wish to avoid confusion between homophones;
 the word 
``image"" is also used to describe a single element of the codomain assigned to a single element of the domain. 
For example, 
one might say of 
the function $A:\mathbb{R}\to\mathbb{R}$ with rule of correspondence $A(x=)=2x-1$ for all $x$ in $\mathbb{R}$ that the image of $2$ is $3$ with this second meaning of the word ``image"" in mind. 
By contrast, one would never say that the range of $2$ is $3$ since the former is not a function and the latter is not a set.


For thinking about inverses of function we want to think in the oposite direction in a sense. 
\begin{definition} 
The {\bf pre-image}\index{Pre-image} of any subset $U \subset T$ is 
\[
f^{-1}(U):=\{ s\in S | f(s)\in U \}\subset S.
\]
\end{definition}
The pre-image of a set \(U\) is the set of all elements of \(S\) which map to \(U\). 
\begin{example}
The pre-image of the set $U= \left\{ a\colvec{2\\1\\1 } \middle| a\in [0,1]\right\}$ (a line segment) under the matrix 
$$ 
M=\begin{pmatrix}
1&0&1\\
0&1&1\\
0&1&1
\end{pmatrix}: \R^3\to \R^3
$$
is the set 
\begin{eqnarray*}
M^{-1} U&=& \left\{  x \middle| Mx=v {\rm ~for ~some~ } v\in U \right\} \\[3mm]
&=& \left\{  \colvec{x\\y\\z} \middle| 
\begin{pmatrix}
1&0&1\\
0&1&1\\
0&1&1
\end{pmatrix} \colvec{x\\y\\z} =a\colvec{2\\1\\1}  {\rm ~for ~some~ } a\in [0,1] \right\} .
\end{eqnarray*}
Since
$$
{\rm RREF }
\begin{amatrix}{3}
1&0&1&2a\\
0&1&1&a\\
0&1&1&a
\end{amatrix} 
= 
\begin{amatrix}{3}
1&0&1&2a\\
0&1&1&a\\
0&0&0&0
\end{amatrix} 
$$
we have 
$$ M^{-1} U = \left\{   a \colvec{2\\1\\0} +b \colvec{-1\\-1\\ 1}\,  \middle|\,  a\in [0,1] ,b\in \R\right\},$$
a strip from a plane in $\R^3$.
\end{example}


\begin{figure}
\begin{center}
\includegraphics[scale=.25]{functions.jpg}
\end{center}
\caption{For the function $f:S\to T$, $S$ is the domain\index{Domain}, $T$ is the target/codomain\index{Codomain}\index{Target|see{Codomain}}, $f(S)$ is the range and $f^{-1}(U)$ is the
pre-image of $U\subset T$.}
\end{figure}

\subsection{ One-to-one and Onto}
The function \(f\) is {\bf one-to-one} (sometimes denoted 1:1) if different elements in \(S\) always map to different elements in \(T\). That is, \(f\) is one-to-one if for any elements \(x \neq y \in S,\) we have that \(f(x) \neq f(y)\), as pictured below.
\begin{center}
\includegraphics[scale=.25]{121.jpg}
\end{center} 
One-to-one functions are also called {\bf injective} functions (and sometimes called monomorphisms.) Notice that injectivity is a condition on the pre-images of \(f\).

The function \(f\) is {\bf onto} if every element of \(T\) is mapped to by some element of \(S\). That is, \(f\) is onto if for any \(t \in T\), there exists some \(s \in S\) such that \(f(s)=t\). Onto functions are also called {\bf surjective} functions (and sometimes epimorphisms.) Notice that surjectivity is a condition on the range of \(f\).
\begin{center}
\includegraphics[scale=.25]{onto.jpg}
\end{center} 

If \(f\) is both injective and surjective, it is {\bf bijective} (or an isomorphism.)
\begin{center}
\includegraphics[scale=.25]{biject.jpg}
\end{center} 
\begin{theorem}
A function \(f \colon S \to T\) has an inverse function \(g \colon T \to S\) if and only if $f$ is bijective.
\end{theorem}
\begin{proof}
This is an ``if and only if'' statement  so the proof has two parts.
\begin{enumerate}
\item {\it (Existence of an inverse $\Rightarrow$ bijective.)}

Suppose that \(f\) has an inverse function \(g\). We need to show $f$ is bijective, which
we break down into injective and surjective.
\begin{itemize}
\item The function \(f\) is injective: 
Suppose that we have \(s,s' \in S\) such that \(f(s)=f(s')\). We must have that \(g(f(s))=s\) for any \(s \in S\), so in particular \(g(f(s))=s\) and \(g(f(s'))=s'\). But since \(f(s)=f(s'),\) we have \(g(f(s))=g(f(s'))\) so \(s=s'\). Therefore, \(f\) is injective.

\item The function \(f\) is surjective:
Let \(t\) be any element of \(T\). We must have that \(f(g(t))=t\). Thus, \(g(t)\) is an element of \(S\) which maps to \(t\). So \(f\) is surjective.
\end{itemize}

\item {\it (Bijectivity $\Rightarrow$ existence of an inverse.)}
Suppose that \(f\) is bijective. Hence \(f\) is surjective, so every element \(t \in T\) has at least one pre-image. Being bijective, \(f\) is also injective, so every \(t\) has no more than one pre-image. Therefore, to construct an inverse function \(g\), we simply define \(g(t)\) to be the unique pre-image \(f^{-1}(t)\) of \(t\).
\end{enumerate}
\end{proof}

Now let us specialize to functions \(f\) that are linear maps between two vector spaces. Everything we said above for arbitrary functions is exactly the same for linear functions. However, the structure of vector spaces lets us say much more about one-to-one and onto functions whose domains are vector spaces than we can say about functions on general sets.  For example, we know that a linear function always sends $0_V$ to $0_W$, {\it i.e.},
\Shabox{1.1}{$f(0_V)=0_W$}  
In Review Exercise~\ref{injectivekernalprob}, you will show that a linear transformation is one-to-one if and only if $0_V$ is the only vector that is sent to $0_W$. Linear functions are unlike  arbitrary functions between sets in that, by looking at {\it just one} (very special) vector, we can figure out whether $f$ is one-to-one!  
\subsection{Kernel}
Let \(L \colon V \to W\) be a linear transformation. Suppose \(L\) is \emph{not} injective.  Then we can find $v_1 \neq v_2$ such that $Lv_1=Lv_2$.  So $v_1-v_2\neq 0$, but
\[
L(v_1-v_2)=0.
\]

\begin{definition}
If $L \colon V\rightarrow W$ is a linear function  then the set 
\index{Kernel}
\[
\ker L = \{v\in V ~|~ Lv=0_W \}\subset V 
\]
is called the {\bf kernel of $L$}.
\end{definition}


Notice that if $L$ has matrix $M$ in some basis, then finding the kernel of~$L$ is equivalent to solving the homogeneous system 
\[
MX=0.
\]

\begin{example}
Let $L(x,y)=(x+y,x+2y,y)$.  Is $L$ one-to-one?

To find out, we can solve the linear system:
\[
\begin{amatrix}{2}
1 & 1 & 0 \\
1 & 2 & 0 \\
0 & 1 & 0 \\
\end{amatrix} \sim
\begin{amatrix}{2}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0 \\
\end{amatrix}.
\]
Then all solutions of $MX=0$ are of the form $x=y=0$.  In other words, $\ker L=\{0\}$, and so $L$ is injective.
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework24/1/}{Reading homework: problem \ref{kernelrank}.1}\end{center}
\Reading{KernelRangeNullityRank}{1}

Notice that in the above example we found 
$$\ker 
\begin{pmatrix}
1 & 1  \\
1 & 2  \\
0 & 1 \\
\end{pmatrix} =
\ker
\begin{pmatrix}
1 & 0 \\
0 & 1  \\
0 & 0  \\
\end{pmatrix}.
$$
In general, an efficient way to get the  kernel of a matrix is to write a string of equalities between kernels of matrices which differ by row operations and, once RREF is reached, note that the linear relationships between the columns for a basis for the nullspace.

\begin{example} of calculating the kernel of a matrix.\\
$$
\ker 
\begin{pmatrix}
1&2&0&1 \\
1&2&1&2 \\
0&0&1&1
\end{pmatrix}
=
\ker 
\begin{pmatrix}
1&2&0&1 \\
0&0&1&1 \\
0&0&1&1
\end{pmatrix}
=
\ker 
\begin{pmatrix}
1&2&0&1 \\
0&0&1&1 \\
0&0&0&0
\end{pmatrix}
$$
$$
=\spa \left\{ 
\colvec{-2\\1 \\0\\0}, \colvec{-1\\0\\-1 \\1 } 
 \right\} .
$$
The two column vectors in this last line describe linear relations between the columns $c_1,c_2,c_3,c_4$. 
In particular $-2c_1+1c_2=0$ and $-c_1-c_3 +c_4=0$. 
\end{example}
In general, a description of the kernel of a matrix should be of the form 
$\spa \{ v_1,v_2,\dots,v_n\}$ with one vector $v_i$ for each non-pivot column. 
To agree with the standard procedure, think about how to describe each non-pivot column in terms of columns to its left; this will yield an expression of the form wherein each vector  has a 1 as its last non-zero entry. (Think of Column Reduced Echelon Form, CREF.) 

Thinking again of augmented matrices, 
if a matrix has more than one element in its kernel then it is not invertible since the existence of multiple solutions to $Mx=0$ implies that ${\rm RREF} \,M\neq I$. 
However just because the kernel of a linear function is trivial does not mean that the function is invertible. 

\begin{example}
$\ker 
\begin{pmatrix}
1&0\\
1&1\\
0&1
\end{pmatrix} =\left\{  \colvec{0\\0} \right\} $
since the matrix has no non-pivot columns. However, 
$\begin{pmatrix}
1&0\\
1&1\\
0&1
\end{pmatrix}:  \R^2 \to \R^3$
is not invertible because there are many things in its codomain that are not in its range, such as  $\colvec{1\\0\\0}$. 
\end{example}

A trivial kernel only gives us half of what is needed for invertibility.




\begin{theorem}
A linear transformation $L\colon V\rightarrow W$ is injective iff $${\rm ker} L=\{0_V\}\, .$$
\end{theorem}

\begin{proof}
The proof of this theorem is Review Exercise~\ref{injectivekernalprob}.
\end{proof}



\begin{theorem}
If $L \colon V\stackrel{\rm linear}{-\!\!\!-\!\!\!-\!\!\!\rightarrow} W$  then $\ker L$ is a subspace of $V$.
\end{theorem}

\begin{proof}
Notice that if $L(v)=0$ and $L(u)=0$, then for any constants $c,d$, $L(cu+dv)=0$.  Then by the \hyperref[subspacetheorem]{subspace theorem}, the kernel of $L$ is a subspace of $V$.
\end{proof}

\begin{example}
Let \(L \colon \Re^3 \to \Re\) be the linear transformation defined by \(L(x,y,z)=(x+y+z)\). Then \(\ker L\) consists of all vectors \((x,y,z) \in \Re^3\) such that \(x+y+z=0\). Therefore, the set
\[
V=\{(x,y,z) \in \Re^3 \mid x+y+z=0\}
\]
is a subspace of \(\Re^3\).
\end{example}

When $L:V\to V$, the above theorem has an interpretation in terms of the eigenspaces of $L$. Suppose $L$ has a zero eigenvalue.  Then the associated eigenspace consists of all vectors $v$ such that $Lv=0v=0$; the $0$-eigenspace of $L$ is exactly the kernel of $L$.  



In the example where $L(x,y)=(x+y,x+2y,y)$, the map $L$ is clearly not surjective, since $L$ maps $\Re^2$ to a plane through the origin in $\Re^3$. But any plane through the origin is a subspace. In general notice 
that if $w=L(v)$ and $w'=L(v')$, then for any constants $c,d$, linearity of $L$ ensures that $$cw+dw' = L(cv+dv')\, .$$  Now the subspace theorem strikes again, and we have the following theorem:

\begin{theorem}
If $L \colon V\rightarrow W$ is linear then the range $L(V)$ is a subspace of~$W$.
\end{theorem}

\begin{example}
Let $L(x,y)=(x+y,x+2y,y)$. The range of $L$ is a plane through the origin and thus a subspace of ${\mathbb R}^3$.
Indeed the matrix of $L$ in the standard basis is 
$$
\begin{pmatrix}1&1\\1&2\\0&1\end{pmatrix}\, .
$$
The columns of this matrix encode the possible outputs of the function $L$ because
$$
L(x,y)=\begin{pmatrix}1&1\\1&2\\0&1\end{pmatrix}\colvec{x\\ y}=x \colvec{1\\1\\0}+y\colvec{1\\2\\1}\, . 
$$
Thus 
$$
L({\mathbb R}^2)=\spa \left\{\colvec{1\\1\\0},\colvec{1\\2\\1}\right\}
$$
Hence, when  bases and a linear transformation is are given, people often refer to its range as the {\it column space}\index{Column space}
of the corresponding matrix.
\end{example}

To find a basis of the range of $L$, we can start with a basis $S=\{v_1, \ldots, v_n\} $ for $V$. Then
the most general input for $L$ is of the form  $\alpha^1 v_1 + \cdots + \alpha^n v_n$. In turn, its most general output looks like
$$
L\big(\alpha^1 v_1 + \cdots + \alpha^n v_n\big)=\alpha^1 Lv_1 + \cdots + \alpha^n Lv_n\in \spa\{Lv_1,\ldots\,Lv_n\}\, .
$$
Thus
\[
L(V)=\spa L(S) = \spa \{Lv_1, \ldots, Lv_n\}\, .
\]
However, the set $\{Lv_1, \ldots, Lv_n\}$ may not be linearly independent; we must solve 
\[
c^1Lv_1+ \cdots + c^nLv_n=0\, ,
\]
to determine whether it is.
By finding relations amongst the elements of $L(S)=\{Lv_1,\ldots ,L v_n\}$, we can discard vectors until a basis is arrived at.  The size of this basis is the dimension of the range of $L$, which is known as the \emph{rank}\index{Rank} of $L$.


\begin{definition}
The {\bf rank} of a linear transformation $L$ is the dimension of its range.
The {\bf nullity}\index{Nullity} of a linear transformation is the dimension of the kernel.
\end{definition}
The notation for these numbers is 
 \Shabox{1}{$\nul L:=\dim \ker L$,}  
 \Shabox{1}{$\rank L:=\dim L(V) = \dim\, \text{ran}\, L$.} 

\begin{theorem}[Dimension Formula]\index{Dimension formula}\label{dimension_formula}
Let $L \colon V\rightarrow W$ be a linear transformation, with $V$ a finite-dimensional vector space\footnote{The formula still makes sense for infinite dimensional vector spaces, such as the space of all polynomials, but the notion of a basis for an infinite dimensional space is more sticky than in the finite-dimensional case.  Furthermore, the dimension formula for infinite dimensional vector spaces isn't useful for computing the rank of a linear transformation, since an equation like $\infty=\infty+x$ cannot be solved for $x$. As such, the proof presented assumes a finite basis for $V$.}.  Then:
\begin{eqnarray*}
\dim V &=& \dim \ker V + \dim L(V)\\
 &=& \nul L + \rank L.
\end{eqnarray*}
\end{theorem}



\begin{proof}
Pick a basis for $V$:
\[
\{ v_1,\ldots,v_p,u_1,\ldots, u_q \},
\]
where $v_1,\ldots,v_p$ is also a basis for $\ker L$.  This can always be done, for example, by finding a basis for the kernel of $L$ and then extending to a basis for $V$.  Then $p=\nul L$ and $p+q=\dim V$.  Then we need to show that $q=\rank L$.  To accomplish this, we show that 
$\{L(u_1),\ldots,L(u_q)\}$ is a basis for $L(V)$.

To see that $\{L(u_1),\ldots,L(u_q)\}$ spans $L(V)$, consider any vector $w$ in $L(V)$.  Then we can find constants $c^i, d^j$ such that:
\begin{eqnarray*}
w &=& L(c^1v_1 + \cdots + c^pv_p+d^1u_1 + \cdots + d^qu_q)\\
  &=& c^1L(v_1) + \cdots + c^pL(v_p)+d^1L(u_1)+\cdots+d^qL(u_q)\\
  &=& d^1L(u_1)+\cdots+d^qL(u_q) \text{ since $L(v_i)=0$,}\\
\Rightarrow L(V) &=& \spa \{L(u_1), \ldots, L(u_q)  \}.
\end{eqnarray*}

Now we show that $\{L(u_1),\ldots,L(u_q)\}$ is linearly independent.  We argue by contradiction. Suppose there exist constants $d^j$ (not all zero) such that
\begin{eqnarray*}
0 &=& d^1L(u_1)+\cdots+d^qL(u_q)\\
  &=& L(d^1u_1+\cdots+d^qu_q).\\
\end{eqnarray*}
But since the $u^j$ are linearly independent, then $d^1u_1+\cdots+d^qu_q\neq 0$, and so $d^1u_1+\cdots+d^qu_q$ is in the kernel of $L$.  But then $d^1u_1+\cdots+d^qu_q$ must be in the span of $\{v_1,\ldots, v_p\}$, since this was a basis for the kernel.  This contradicts the assumption that $\{ v_1,\ldots,v_p,u_1,\ldots, u_q \}$ was a basis for $V$, so we are done.
\end{proof}

%\begin{center}\href{\webworkurl ReadingHomework24/2/}{Reading homework: problem \ref{kernelrank}.2}\end{center}
\Reading{KernelRangeNullityRank}{2}

\begin{example} (Row rank equals column rank)\\
Suppose $M$ is an $m\times n$ matrix. The matrix  $M$ itself is a linear transformation $M:{\mathbb R}^n \rightarrow {\mathbb R}^m$ but it  must also  be the matrix of some linear transformation
$$
L:V\stackrel{\rm linear}\longrightarrow W\, .
$$
Here we only know that $\dim V =n$ and $\dim W =m$. The rank of the map $L$ is the dimension of its image and also the number of linearly independent columns of $M$. Hence, this is sometimes called the \index{Rank!column rank}{\it column rank} of $M$. The dimension formula predicts  the dimension of the kernel, {\it i.e.} the nullity:  $ {\rm null}\, L= {\rm dim}V-{\rm rank}L=n-r$. 

To compute the kernel we would study the linear system $$Mx=0\, ,$$ which gives $m$ equations for the $n$-vector~$x$. The \index{Rank!row rank}{\it row rank} of a matrix is the number of linearly independent rows (viewed as vectors).
Each linearly independent row of $M$ gives an independent equation satisfied by the $n$-vector $x$. Every independent equation on $x$ reduces the size of the kernel by one, so if the row rank is $s$, then ${\rm null}\, L+ s = n$.  Thus we have two equations: 
$$
{\rm null}\, L+s=n \mbox{ and } {\rm null } \, L = n-r\, .
$$
From these we conclude the $r=s$. In other words, the row rank of $M$ equals its column rank.
 \end{example}



\section{Summary}\label{thelist}
We have seen that a linear transformation has an inverse if and only if it is bijective ({\it i.e.}, one-to-one and onto). We also know that linear transformations can be represented by matrices, and we have seen many ways to tell whether a matrix is invertible. Here is a list of them:
\begin{theorem}[Invertibility]
\label{theorem:invertibility}
Let $V$  be an $n$-dimensional vector space  
and suppose  $L:V\to V$ is a linear transformation with matrix $M$ in some basis.
Then \(M\) is an \(n \times n\) matrix, and 
%let $$L \colon \Re^n \to \Re^n$$ be the linear transformation defined by \(L(v)=Mv\). Then 
the following statements are equivalent:
\newpage
\begin{enumerate}
\item If $v$ is any vector in \(\Re^n\), then the system \(Mx=v\) has exactly one solution.
\item The matrix \(M\) is row-equivalent to the identity matrix.
\item If \(v\) is any vector in \(V\), then \(L(x)=v\) has exactly one solution.
\item The matrix \(M\) is invertible.
\item The homogeneous system \(Mx=0\) has no non-zero solutions.
\item The determinant of \(M\) is not equal to \(0\).
%\item The matrix \(M\) is a product of elementary matrices of the form \(E_j^i, R^i(\lambda), S_j^i(\gamma)\) with \(\lambda \neq 0\).
\item The transpose matrix \(M^T\) is invertible.
\item The matrix \(M\) does not have \(0\) as an eigenvalue.
\item The linear transformation \(L\) does not have \(0\) as an eigenvalue.
\item The characteristic polynomial \(\det(\lambda I-M)\) does not have \(0\) as a root.
\item The columns (or rows) of \(M\) span \(\Re^n\).
\item The columns (or rows) of \(M\) are linearly independent.
\item The columns (or rows) of \(M\) are a basis for \(\Re^n\).
\item The linear transformation \(L\) is injective.
\item The linear transformation \(L\) is surjective.
\item The linear transformation \(L\) is bijective.
\end{enumerate}
\end{theorem}
Note: it is important that \(M\) be an \(n \times n\) matrix! If \(M\) is not square, then it can't be invertible, and many of the statements above are no longer equivalent to each other.
%I hate it when books do this. ""Here is our proof: you prove it or figure out where it has been proven. "" I think it is dishonest to write proof: and then not actually give a proof.
%Yes, but some books make proving the above list there raison d'�tre which I really don't like...plus we have a video handling this!!!!
\begin{proof}
Many of these equivalences were proved earlier in other chapters. Some were left as review questions or sample final questions. The rest are left as exercises for the reader.
\end{proof}

\Videoscriptlink{kernel_range_nullity_rank_inv_cond.mp4}{Invertibility Conditions}{scripts_kernel_range_nullity_rank_inv_cond}

%\section*{References}
%Hefferon, Chapter Three, Section II.2: Rangespace and Nullspace (Recall that ``homomorphism'' is is used instead of ``linear transformation'' in Hefferon.)
%\\
%Beezer, Chapter LT, Sections ILT-IVLT
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Rank_(linear_algebra)}{Rank}
%\item \href{http://en.wikipedia.org/wiki/Dimension_theorem}{Dimension Theorem}
%\item \href{http://en.wikipedia.org/wiki/Kernel_(linear_operator)}{Kernel of a Linear Operator}
%\end{itemize}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{KernelRangeRankNullity}{1}, 
 \hwrref{KernelRangeRankNullity}{2}, 
 \\
Elements of kernel &  \hwref{KernelRangeRankNullity}{3}\\
Basis for column space &\hwref{KernelRangeRankNullity}{4}\\
Basis for kernel & \hwref{KernelRangeRankNullity}{5}\\
Basis for kernel and range& \hwref{KernelRangeRankNullity}{6}\\
Orthonomal range basis&\hwref{KernelRangeRankNullity}{7}\\
Orthonomal kernel basis&\hwref{KernelRangeRankNullity}{8}\\
Orthonomal kernel and range bases&\hwref{KernelRangeRankNullity}{9}\\
Orthonomal kernel,  range and row space bases&\hwref{KernelRangeRankNullity}{10}\\
Rank&\hwref{KernelRangeRankNullity}{11}\\
   \hline
\end{tabular}


\input{\kernelPath/problems}

\newpage
","\chapter{\kernelTitle}\label{kernelrank}

Given a linear transformation $$L \colon V \to W\, ,$$ we often want  to know if it has an inverse, {\it i.e.}, if there exists a linear transformation $$M \colon W \to V$$ such that for any vector \(v \in V\), we have $$MLv=v\, ,$$ and for any vector \(w \in W\), we have $$LMw=w\, .$$ A linear transformation is a special kind of function from one vector space to another. So before we discuss which linear transformations have inverses, let us first discuss inverses of arbitrary functions. When we later specialize to linear transformations, we'll also find some nice ways of creating subspaces.

Let \(f \colon S \to T\) be a function from a set \(S\) to a set \(T\). \href{\webworkurl Homework0-Background/3/}{Recall} that \(S\) is called the {\it domain} of \(f\), \(T\) is called the {\it codomain} or {\it target} of \(f\).  
We now formally introduce a term that should be familar to you from many previous courses.

\section{Range}

\begin{definition} 
The {\bf range}  of a function $f:S\to T$ is  the set
\[
{\rm ran}(f):=\left\{ f(s) | s\in S \right\}\subset T\, .
\]
%is called\(f\). 
\end{definition} 
It is the subset of the codomain consisting of elements  to which the function \(f\) maps, {\it i.e.}, the things in \(T\) which you can get to by starting in \(S\) and applying \(f\). 

The range of a matrix is very easy to find; the range of a matrix is the span of its columns. 
Thus, calculation of the range of a matrix is very easy until the last step: simplification. One aught to end by the calculation by writing the vector space as the span of a linearly independent set. 

\begin{example}{ of calculating the range of a matrix.}
$${\rm ran} 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
:= 
\left\{ 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
\colvec{x\\y\\z\\w } | \colvec{x\\y\\z\\w} \in \R^4 \right\}
$$
$$=
\left\{   
x\colvec{1\\1\\0 } +y \colvec{ 2\\2\\0 } +z \colvec{0\\1\\1} + w\colvec{1\\2\\1} \middle| x,y,z,w \in \R \right\}
.$$
That is 
$$
{\rm ran} 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
= 
\spa \left\{   
\colvec{1\\1\\0 } , \colvec{ 2\\2\\0 } , \colvec{0\\1\\1}, \colvec{1\\2\\1} \right\}
$$
but since 
$${\rm RREF} \begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
=\begin{pmatrix} 
1&2&0&1\\
0&0&1&1\\
0&0&0&0
\end{pmatrix} 
$$
the second and fourth columns (which are the non-pivot columns), can be expressed as linear combinations of columns to their left. 
They can then be removed from the set in the span to obtain
$${\rm ran} 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
= \spa \left\{   
\colvec{1\\1\\0 } , \colvec{0\\1\\1} \right\} .
$$
\end{example}

It might occur to you that the range of the $3\times 4$ matrix from the last example can be expressed as the range of a $3\times 2$ matrix;
$$
{\rm ran} 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
={\rm ran} 
\begin{pmatrix} 
1&0\\
1&1\\
0&1
\end{pmatrix}.$$
Indeed, because the span of a set of vectors does not change when we replace the vectors with another set through an invertible process, we can calculate ranges through strings of equalities of ranges of matrices that differer by Elementary Column Operations, ECOs, ending with the range of a matrix in Column Reduced Echelon Form, CREF, with its zero columns deleted.

\begin{example} Calculating a range with ECOs\\
$$
{\rm ran} 
\begin{pmatrix} 
0&1&1\\
1&3&1\\
1&2&0
\end{pmatrix} 
\stackrel{c_1 \leftrightarrow c_3}{=}
{\rm ran} 
\begin{pmatrix} 
1&1&0\\
1&3&1\\
0&2&1
\end{pmatrix} 
\stackrel{c_2'= c_2-c_1}{=}
{\rm ran} 
\begin{pmatrix} 
1&0&0\\
1&2&1\\
0&2&1
\end{pmatrix} 
\stackrel{c_2'= \frac12 c_2}{=}
{\rm ran} 
\begin{pmatrix} 
1&0&0\\
1&1&1\\
0&1&1
\end{pmatrix} 
$$
$$\stackrel{c_3'= c_3-c_2}{=}
{\rm ran} 
\begin{pmatrix} 
1&0&0\\
1&1&0\\
0&1&0
\end{pmatrix} 
={\rm ran} 
\begin{pmatrix} 
1&0\\
1&1\\
0&1
\end{pmatrix}. 
$$


\end{example}

\noindent
This is an efficient way to compute and encode the range of a matrix.
%We think this is the most sophisticated and efficient way to calculate the range of a matrix, and encourage students to use this line of thinking.

\section{Image} 

\begin{definition}
For any subset $U$ of the domain $S$ of a function $f:S\to T$  the {\bf image} of $U$ is 
$$f(U)={\rm Im} \,U:= \left\{  f(x) | x\in U \right\} .$$
\end{definition}

\begin{example} 
The image of the  cube
$$U= \left\{  a \colvec{1\\0\\0} +b \colvec{0\\1\\0}+c\colvec{0\\0\\1} \middle| a,b,c \in [0,1] \right\} $$ 
under multiplication by the matrix 
$$ M=
\begin{pmatrix}
1&0&0\\
1&1&1\\
0&0&1
\end{pmatrix}
$$
is the parallelepiped 
$$
{\rm Img}\,U= 
 \left\{  a \colvec{1\\1\\0} +b \colvec{0\\1\\0}+c\colvec{0\\1\\1} \middle| a,b,c \in [0,1] \right\} .
$$
\end{example}

Note that for most subsets $U$ of the domain $S$  of a function  $f$ the image of $U$ is not a vector space. 
The range of a function is the particular case of the image where the subset of the domain is the entire domain; ${\rm ran} f = {\rm Img} S$. 
For this reason, the range of $f$ is also sometimes called the {image} of $f$ and is sometimes denoted ${ \rm im}(f)$ or  $f(S).$  We have seen that the range of a matrix is always a span of vectors, and hence a vector space. 

Note that we prefer the phrase ``range of $f$"" to the phrase ``image of f""  
because we wish to avoid confusion between homophones;
 the word 
``image"" is also used to describe a single element of the codomain assigned to a single element of the domain. 
For example, 
one might say of 
the function $A:\mathbb{R}\to\mathbb{R}$ with rule of correspondence $A(x=)=2x-1$ for all $x$ in $\mathbb{R}$ that the image of $2$ is $3$ with this second meaning of the word ``image"" in mind. 
By contrast, one would never say that the range of $2$ is $3$ since the former is not a function and the latter is not a set.


For thinking about inverses of function we want to think in the oposite direction in a sense. 
\begin{definition} 
The {\bf pre-image}\index{Pre-image} of any subset $U \subset T$ is 
\[
f^{-1}(U):=\{ s\in S | f(s)\in U \}\subset S.
\]
\end{definition}
The pre-image of a set \(U\) is the set of all elements of \(S\) which map to \(U\). 
\begin{example}
The pre-image of the set $U= \left\{ a\colvec{2\\1\\1 } \middle| a\in [0,1]\right\}$ (a line segment) under the matrix 
$$ 
M=\begin{pmatrix}
1&0&1\\
0&1&1\\
0&1&1
\end{pmatrix}: \R^3\to \R^3
$$
is the set 
\begin{eqnarray*}
M^{-1} U&=& \left\{  x \middle| Mx=v {\rm ~for ~some~ } v\in U \right\} \\[3mm]
&=& \left\{  \colvec{x\\y\\z} \middle| 
\begin{pmatrix}
1&0&1\\
0&1&1\\
0&1&1
\end{pmatrix} \colvec{x\\y\\z} =a\colvec{2\\1\\1}  {\rm ~for ~some~ } a\in [0,1] \right\} .
\end{eqnarray*}
Since
$$
{\rm RREF }
\begin{amatrix}{3}
1&0&1&2a\\
0&1&1&a\\
0&1&1&a
\end{amatrix} 
= 
\begin{amatrix}{3}
1&0&1&2a\\
0&1&1&a\\
0&0&0&0
\end{amatrix} 
$$
we have 
$$ M^{-1} U = \left\{   a \colvec{2\\1\\0} +b \colvec{-1\\-1\\ 1}\,  \middle|\,  a\in [0,1] ,b\in \R\right\},$$
a strip from a plane in $\R^3$.
\end{example}


\begin{figure}
\begin{center}
\includegraphics[scale=.25]{functions.jpg}
\end{center}
\caption{For the function $f:S\to T$, $S$ is the domain\index{Domain}, $T$ is the target/codomain\index{Codomain}\index{Target|see{Codomain}}, $f(S)$ is the range and $f^{-1}(U)$ is the
pre-image of $U\subset T$.}
\end{figure}

\subsection{ One-to-one and Onto}
The function \(f\) is {\bf one-to-one} (sometimes denoted 1:1) if different elements in \(S\) always map to different elements in \(T\). That is, \(f\) is one-to-one if for any elements \(x \neq y \in S,\) we have that \(f(x) \neq f(y)\), as pictured below.
\begin{center}
\includegraphics[scale=.25]{121.jpg}
\end{center} 
One-to-one functions are also called {\bf injective} functions (and sometimes called monomorphisms.) Notice that injectivity is a condition on the pre-images of \(f\).

The function \(f\) is {\bf onto} if every element of \(T\) is mapped to by some element of \(S\). That is, \(f\) is onto if for any \(t \in T\), there exists some \(s \in S\) such that \(f(s)=t\). Onto functions are also called {\bf surjective} functions (and sometimes epimorphisms.) Notice that surjectivity is a condition on the range of \(f\).
\begin{center}
\includegraphics[scale=.25]{onto.jpg}
\end{center} 

If \(f\) is both injective and surjective, it is {\bf bijective} (or an isomorphism.)
\begin{center}
\includegraphics[scale=.25]{biject.jpg}
\end{center} 
\begin{theorem}
A function \(f \colon S \to T\) has an inverse function \(g \colon T \to S\) if and only if $f$ is bijective.
\end{theorem}
\begin{proof}
This is an ``if and only if'' statement  so the proof has two parts.
\begin{enumerate}
\item {\it (Existence of an inverse $\Rightarrow$ bijective.)}

Suppose that \(f\) has an inverse function \(g\). We need to show $f$ is bijective, which
we break down into injective and surjective.
\begin{itemize}
\item The function \(f\) is injective: 
Suppose that we have \(s,s' \in S\) such that \(f(s)=f(s')\). We must have that \(g(f(s))=s\) for any \(s \in S\), so in particular \(g(f(s))=s\) and \(g(f(s'))=s'\). But since \(f(s)=f(s'),\) we have \(g(f(s))=g(f(s'))\) so \(s=s'\). Therefore, \(f\) is injective.

\item The function \(f\) is surjective:
Let \(t\) be any element of \(T\). We must have that \(f(g(t))=t\). Thus, \(g(t)\) is an element of \(S\) which maps to \(t\). So \(f\) is surjective.
\end{itemize}

\item {\it (Bijectivity $\Rightarrow$ existence of an inverse.)}
Suppose that \(f\) is bijective. Hence \(f\) is surjective, so every element \(t \in T\) has at least one pre-image. Being bijective, \(f\) is also injective, so every \(t\) has no more than one pre-image. Therefore, to construct an inverse function \(g\), we simply define \(g(t)\) to be the unique pre-image \(f^{-1}(t)\) of \(t\).
\end{enumerate}
\end{proof}

Now let us specialize to functions \(f\) that are linear maps between two vector spaces. Everything we said above for arbitrary functions is exactly the same for linear functions. However, the structure of vector spaces lets us say much more about one-to-one and onto functions whose domains are vector spaces than we can say about functions on general sets.  For example, we know that a linear function always sends $0_V$ to $0_W$, {\it i.e.},
\Shabox{1.1}{$f(0_V)=0_W$}  
In Review Exercise~\ref{injectivekernalprob}, you will show that a linear transformation is one-to-one if and only if $0_V$ is the only vector that is sent to $0_W$. Linear functions are unlike  arbitrary functions between sets in that, by looking at {\it just one} (very special) vector, we can figure out whether $f$ is one-to-one!  
\subsection{Kernel}
Let \(L \colon V \to W\) be a linear transformation. Suppose \(L\) is \emph{not} injective.  Then we can find $v_1 \neq v_2$ such that $Lv_1=Lv_2$.  So $v_1-v_2\neq 0$, but
\[
L(v_1-v_2)=0.
\]

\begin{definition}
If $L \colon V\rightarrow W$ is a linear function  then the set 
\index{Kernel}
\[
\ker L = \{v\in V ~|~ Lv=0_W \}\subset V 
\]
is called the {\bf kernel of $L$}.
\end{definition}


Notice that if $L$ has matrix $M$ in some basis, then finding the kernel of~$L$ is equivalent to solving the homogeneous system 
\[
MX=0.
\]

\begin{example}
Let $L(x,y)=(x+y,x+2y,y)$.  Is $L$ one-to-one?

To find out, we can solve the linear system:
\[
\begin{amatrix}{2}
1 & 1 & 0 \\
1 & 2 & 0 \\
0 & 1 & 0 \\
\end{amatrix} \sim
\begin{amatrix}{2}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0 \\
\end{amatrix}.
\]
Then all solutions of $MX=0$ are of the form $x=y=0$.  In other words, $\ker L=\{0\}$, and so $L$ is injective.
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework24/1/}{Reading homework: problem \ref{kernelrank}.1}\end{center}
\Reading{KernelRangeNullityRank}{1}

Notice that in the above example we found 
$$\ker 
\begin{pmatrix}
1 & 1  \\
1 & 2  \\
0 & 1 \\
\end{pmatrix} =
\ker
\begin{pmatrix}
1 & 0 \\
0 & 1  \\
0 & 0  \\
\end{pmatrix}.
$$
In general, an efficient way to get the  kernel of a matrix is to write a string of equalities between kernels of matrices which differ by row operations and, once RREF is reached, note that the linear relationships between the columns for a basis for the nullspace.

\begin{example} of calculating the kernel of a matrix.\\
$$
\ker 
\begin{pmatrix}
1&2&0&1 \\
1&2&1&2 \\
0&0&1&1
\end{pmatrix}
=
\ker 
\begin{pmatrix}
1&2&0&1 \\
0&0&1&1 \\
0&0&1&1
\end{pmatrix}
=
\ker 
\begin{pmatrix}
1&2&0&1 \\
0&0&1&1 \\
0&0&0&0
\end{pmatrix}
$$
$$
=\spa \left\{ 
\colvec{-2\\1 \\0\\0}, \colvec{-1\\0\\-1 \\1 } 
 \right\} .
$$
The two column vectors in this last line describe linear relations between the columns $c_1,c_2,c_3,c_4$. 
In particular $-2c_1+1c_2=0$ and $-c_1-c_3 +c_4=0$. 
\end{example}
In general, a description of the kernel of a matrix should be of the form 
$\spa \{ v_1,v_2,\dots,v_n\}$ with one vector $v_i$ for each non-pivot column. 
To agree with the standard procedure, think about how to describe each non-pivot column in terms of columns to its left; this will yield an expression of the form wherein each vector  has a 1 as its last non-zero entry. (Think of Column Reduced Echelon Form, CREF.) 

Thinking again of augmented matrices, 
if a matrix has more than one element in its kernel then it is not invertible since the existence of multiple solutions to $Mx=0$ implies that ${\rm RREF} \,M\neq I$. 
However just because the kernel of a linear function is trivial does not mean that the function is invertible. 

\begin{example}
$\ker 
\begin{pmatrix}
1&0\\
1&1\\
0&1
\end{pmatrix} =\left\{  \colvec{0\\0} \right\} $
since the matrix has no non-pivot columns. However, 
$\begin{pmatrix}
1&0\\
1&1\\
0&1
\end{pmatrix}:  \R^2 \to \R^3$
is not invertible because there are many things in its codomain that are not in its range, such as  $\colvec{1\\0\\0}$. 
\end{example}

A trivial kernel only gives us half of what is needed for invertibility.




\begin{theorem}
A linear transformation $L\colon V\rightarrow W$ is injective iff $${\rm ker} L=\{0_V\}\, .$$
\end{theorem}

\begin{proof}
The proof of this theorem is Review Exercise~\ref{injectivekernalprob}.
\end{proof}



\begin{theorem}
If $L \colon V\stackrel{\rm linear}{-\!\!\!-\!\!\!-\!\!\!\rightarrow} W$  then $\ker L$ is a subspace of $V$.
\end{theorem}

\begin{proof}
Notice that if $L(v)=0$ and $L(u)=0$, then for any constants $c,d$, $L(cu+dv)=0$.  Then by the \hyperref[subspacetheorem]{subspace theorem}, the kernel of $L$ is a subspace of $V$.
\end{proof}

\begin{example}
Let \(L \colon \Re^3 \to \Re\) be the linear transformation defined by \(L(x,y,z)=(x+y+z)\). Then \(\ker L\) consists of all vectors \((x,y,z) \in \Re^3\) such that \(x+y+z=0\). Therefore, the set
\[
V=\{(x,y,z) \in \Re^3 \mid x+y+z=0\}
\]
is a subspace of \(\Re^3\).
\end{example}

When $L:V\to V$, the above theorem has an interpretation in terms of the eigenspaces of $L$. Suppose $L$ has a zero eigenvalue.  Then the associated eigenspace consists of all vectors $v$ such that $Lv=0v=0$; the $0$-eigenspace of $L$ is exactly the kernel of $L$.  



In the example where $L(x,y)=(x+y,x+2y,y)$, the map $L$ is clearly not surjective, since $L$ maps $\Re^2$ to a plane through the origin in $\Re^3$. But any plane through the origin is a subspace. In general notice 
that if $w=L(v)$ and $w'=L(v')$, then for any constants $c,d$, linearity of $L$ ensures that $$cw+dw' = L(cv+dv')\, .$$  Now the subspace theorem strikes again, and we have the following theorem:

\begin{theorem}
If $L \colon V\rightarrow W$ is linear then the range $L(V)$ is a subspace of~$W$.
\end{theorem}

\begin{example}
Let $L(x,y)=(x+y,x+2y,y)$. The range of $L$ is a plane through the origin and thus a subspace of ${\mathbb R}^3$.
Indeed the matrix of $L$ in the standard basis is 
$$
\begin{pmatrix}1&1\\1&2\\0&1\end{pmatrix}\, .
$$
The columns of this matrix encode the possible outputs of the function $L$ because
$$
L(x,y)=\begin{pmatrix}1&1\\1&2\\0&1\end{pmatrix}\colvec{x\\ y}=x \colvec{1\\1\\0}+y\colvec{1\\2\\1}\, . 
$$
Thus 
$$
L({\mathbb R}^2)=\spa \left\{\colvec{1\\1\\0},\colvec{1\\2\\1}\right\}
$$
Hence, when  bases and a linear transformation is are given, people often refer to its range as the {\it column space}\index{Column space}
of the corresponding matrix.
\end{example}

To find a basis of the range of $L$, we can start with a basis $S=\{v_1, \ldots, v_n\} $ for $V$. Then
the most general input for $L$ is of the form  $\alpha^1 v_1 + \cdots + \alpha^n v_n$. In turn, its most general output looks like
$$
L\big(\alpha^1 v_1 + \cdots + \alpha^n v_n\big)=\alpha^1 Lv_1 + \cdots + \alpha^n Lv_n\in \spa\{Lv_1,\ldots\,Lv_n\}\, .
$$
Thus
\[
L(V)=\spa L(S) = \spa \{Lv_1, \ldots, Lv_n\}\, .
\]
However, the set $\{Lv_1, \ldots, Lv_n\}$ may not be linearly independent; we must solve 
\[
c^1Lv_1+ \cdots + c^nLv_n=0\, ,
\]
to determine whether it is.
By finding relations amongst the elements of $L(S)=\{Lv_1,\ldots ,L v_n\}$, we can discard vectors until a basis is arrived at.  The size of this basis is the dimension of the range of $L$, which is known as the \emph{rank}\index{Rank} of $L$.


\begin{definition}
The {\bf rank} of a linear transformation $L$ is the dimension of its range.
The {\bf nullity}\index{Nullity} of a linear transformation is the dimension of the kernel.
\end{definition}
The notation for these numbers is 
 \Shabox{1}{$\nul L:=\dim \ker L$,}  
 \Shabox{1}{$\rank L:=\dim L(V) = \dim\, \text{ran}\, L$.} 

\begin{theorem}[Dimension Formula]\index{Dimension formula}\label{dimension_formula}
Let $L \colon V\rightarrow W$ be a linear transformation, with $V$ a finite-dimensional vector space\footnote{The formula still makes sense for infinite dimensional vector spaces, such as the space of all polynomials, but the notion of a basis for an infinite dimensional space is more sticky than in the finite-dimensional case.  Furthermore, the dimension formula for infinite dimensional vector spaces isn't useful for computing the rank of a linear transformation, since an equation like $\infty=\infty+x$ cannot be solved for $x$. As such, the proof presented assumes a finite basis for $V$.}.  Then:
\begin{eqnarray*}
\dim V &=& \dim \ker V + \dim L(V)\\
 &=& \nul L + \rank L.
\end{eqnarray*}
\end{theorem}



\begin{proof}
Pick a basis for $V$:
\[
\{ v_1,\ldots,v_p,u_1,\ldots, u_q \},
\]
where $v_1,\ldots,v_p$ is also a basis for $\ker L$.  This can always be done, for example, by finding a basis for the kernel of $L$ and then extending to a basis for $V$.  Then $p=\nul L$ and $p+q=\dim V$.  Then we need to show that $q=\rank L$.  To accomplish this, we show that 
$\{L(u_1),\ldots,L(u_q)\}$ is a basis for $L(V)$.

To see that $\{L(u_1),\ldots,L(u_q)\}$ spans $L(V)$, consider any vector $w$ in $L(V)$.  Then we can find constants $c^i, d^j$ such that:
\begin{eqnarray*}
w &=& L(c^1v_1 + \cdots + c^pv_p+d^1u_1 + \cdots + d^qu_q)\\
  &=& c^1L(v_1) + \cdots + c^pL(v_p)+d^1L(u_1)+\cdots+d^qL(u_q)\\
  &=& d^1L(u_1)+\cdots+d^qL(u_q) \text{ since $L(v_i)=0$,}\\
\Rightarrow L(V) &=& \spa \{L(u_1), \ldots, L(u_q)  \}.
\end{eqnarray*}

Now we show that $\{L(u_1),\ldots,L(u_q)\}$ is linearly independent.  We argue by contradiction. Suppose there exist constants $d^j$ (not all zero) such that
\begin{eqnarray*}
0 &=& d^1L(u_1)+\cdots+d^qL(u_q)\\
  &=& L(d^1u_1+\cdots+d^qu_q).\\
\end{eqnarray*}
But since the $u^j$ are linearly independent, then $d^1u_1+\cdots+d^qu_q\neq 0$, and so $d^1u_1+\cdots+d^qu_q$ is in the kernel of $L$.  But then $d^1u_1+\cdots+d^qu_q$ must be in the span of $\{v_1,\ldots, v_p\}$, since this was a basis for the kernel.  This contradicts the assumption that $\{ v_1,\ldots,v_p,u_1,\ldots, u_q \}$ was a basis for $V$, so we are done.
\end{proof}

%\begin{center}\href{\webworkurl ReadingHomework24/2/}{Reading homework: problem \ref{kernelrank}.2}\end{center}
\Reading{KernelRangeNullityRank}{2}

\begin{example} (Row rank equals column rank)\\
Suppose $M$ is an $m\times n$ matrix. The matrix  $M$ itself is a linear transformation $M:{\mathbb R}^n \rightarrow {\mathbb R}^m$ but it  must also  be the matrix of some linear transformation
$$
L:V\stackrel{\rm linear}\longrightarrow W\, .
$$
Here we only know that $\dim V =n$ and $\dim W =m$. The rank of the map $L$ is the dimension of its image and also the number of linearly independent columns of $M$. Hence, this is sometimes called the \index{Rank!column rank}{\it column rank} of $M$. The dimension formula predicts  the dimension of the kernel, {\it i.e.} the nullity:  $ {\rm null}\, L= {\rm dim}V-{\rm rank}L=n-r$. 

To compute the kernel we would study the linear system $$Mx=0\, ,$$ which gives $m$ equations for the $n$-vector~$x$. The \index{Rank!row rank}{\it row rank} of a matrix is the number of linearly independent rows (viewed as vectors).
Each linearly independent row of $M$ gives an independent equation satisfied by the $n$-vector $x$. Every independent equation on $x$ reduces the size of the kernel by one, so if the row rank is $s$, then ${\rm null}\, L+ s = n$.  Thus we have two equations: 
$$
{\rm null}\, L+s=n \mbox{ and } {\rm null } \, L = n-r\, .
$$
From these we conclude the $r=s$. In other words, the row rank of $M$ equals its column rank.
 \end{example}



\section{Summary}\label{thelist}
We have seen that a linear transformation has an inverse if and only if it is bijective ({\it i.e.}, one-to-one and onto). We also know that linear transformations can be represented by matrices, and we have seen many ways to tell whether a matrix is invertible. Here is a list of them:
\begin{theorem}[Invertibility]
\label{theorem:invertibility}
Let $V$  be an $n$-dimensional vector space  
and suppose  $L:V\to V$ is a linear transformation with matrix $M$ in some basis.
Then \(M\) is an \(n \times n\) matrix, and 
%let $$L \colon \Re^n \to \Re^n$$ be the linear transformation defined by \(L(v)=Mv\). Then 
the following statements are equivalent:
\newpage
\begin{enumerate}
\item If $v$ is any vector in \(\Re^n\), then the system \(Mx=v\) has exactly one solution.
\item The matrix \(M\) is row-equivalent to the identity matrix.
\item If \(v\) is any vector in \(V\), then \(L(x)=v\) has exactly one solution.
\item The matrix \(M\) is invertible.
\item The homogeneous system \(Mx=0\) has no non-zero solutions.
\item The determinant of \(M\) is not equal to \(0\).
%\item The matrix \(M\) is a product of elementary matrices of the form \(E_j^i, R^i(\lambda), S_j^i(\gamma)\) with \(\lambda \neq 0\).
\item The transpose matrix \(M^T\) is invertible.
\item The matrix \(M\) does not have \(0\) as an eigenvalue.
\item The linear transformation \(L\) does not have \(0\) as an eigenvalue.
\item The characteristic polynomial \(\det(\lambda I-M)\) does not have \(0\) as a root.
\item The columns (or rows) of \(M\) span \(\Re^n\).
\item The columns (or rows) of \(M\) are linearly independent.
\item The columns (or rows) of \(M\) are a basis for \(\Re^n\).
\item The linear transformation \(L\) is injective.
\item The linear transformation \(L\) is surjective.
\item The linear transformation \(L\) is bijective.
\end{enumerate}
\end{theorem}
Note: it is important that \(M\) be an \(n \times n\) matrix! If \(M\) is not square, then it can't be invertible, and many of the statements above are no longer equivalent to each other.
%I hate it when books do this. ""Here is our proof: you prove it or figure out where it has been proven. "" I think it is dishonest to write proof: and then not actually give a proof.
%Yes, but some books make proving the above list there raison d'�tre which I really don't like...plus we have a video handling this!!!!
\begin{proof}
Many of these equivalences were proved earlier in other chapters. Some were left as review questions or sample final questions. The rest are left as exercises for the reader.
\end{proof}

\Videoscriptlink{kernel_range_nullity_rank_inv_cond.mp4}{Invertibility Conditions}{scripts_kernel_range_nullity_rank_inv_cond}

%\section*{References}
%Hefferon, Chapter Three, Section II.2: Rangespace and Nullspace (Recall that ``homomorphism'' is is used instead of ``linear transformation'' in Hefferon.)
%\\
%Beezer, Chapter LT, Sections ILT-IVLT
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Rank_(linear_algebra)}{Rank}
%\item \href{http://en.wikipedia.org/wiki/Dimension_theorem}{Dimension Theorem}
%\item \href{http://en.wikipedia.org/wiki/Kernel_(linear_operator)}{Kernel of a Linear Operator}
%\end{itemize}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{KernelRangeRankNullity}{1}, 
 \hwrref{KernelRangeRankNullity}{2}, 
 \\
Elements of kernel &  \hwref{KernelRangeRankNullity}{3}\\
Basis for column space &\hwref{KernelRangeRankNullity}{4}\\
Basis for kernel & \hwref{KernelRangeRankNullity}{5}\\
Basis for kernel and range& \hwref{KernelRangeRankNullity}{6}\\
Orthonomal range basis&\hwref{KernelRangeRankNullity}{7}\\
Orthonomal kernel basis&\hwref{KernelRangeRankNullity}{8}\\
Orthonomal kernel and range bases&\hwref{KernelRangeRankNullity}{9}\\
Orthonomal kernel,  range and row space bases&\hwref{KernelRangeRankNullity}{10}\\
Rank&\hwref{KernelRangeRankNullity}{11}\\
   \hline
\end{tabular}


\input{\kernelPath/problems}

\newpage
",lesson
26,Least Squares,"
\chapter{Least squares and Singular Values}
\label{sec:leastsquaresSVD}
\index{Least squares}

Consider the linear algebraic equation $L(x)=v$, where $L \colon U\stackrel{\text{linear}}{-\!\!\!-\!\!\!\longrightarrow}W$ and $v\in W$ are known while $x$ is unknown. As we have seen, this system may have 
one solution, no solutions, or infinitely many solutions.  
But if $v$ is not in the range of $L$ there will {\it never} be any solutions for $L(x)=v$.
\vspace{-.1cm}
\begin{center}
\includegraphics[scale=.24]{notinimage.jpg}
\end{center} 
\vspace{-1.8cm}
However, for many applications we do not need an exact solution of the system; instead, we may only need the best approximation possible.  

\begin{quote}
``My work always tried to unite the Truth with the Beautiful, but when I had to choose one or the other, I usually chose the Beautiful.'' 

\vspace{-2mm}
\hspace{7cm}-- Hermann Weyl.
\end{quote}

If the vector space $W$ has a notion of lengths of vectors, we can try to find $x$ that minimizes $||L(x)-v||$.
\begin{center}
\includegraphics[scale=.24]{minimize.jpg}
\end{center} 
This method has many applications, such as when trying to fit a (perhaps linear) function to a ``noisy'' set of observations.  For example, suppose we measured the position of a bicycle on a racetrack once every five seconds.  Our observations won't be exact, but so long as the observations are right on average, we can figure out a best-possible linear function of position of the bicycle in terms of time.

Suppose $M$ is the matrix for the linear function $L:U \to W$ in some bases for $U$ and $W$. The vectors~$v$ and~$x$ are represented by column vectors $V$ and $X$ in these bases.  Then we need to approximate
\[
MX-V\approx 0\, .
\]

Note that if $\dim U=n$ and $\dim W=m$ then $M$ can be represented by an $m\times n$ matrix and $x$ and $v$ as vectors in $\Re^n$ and $\Re^m$, respectively. Thus, we can write $W=L(U)\oplus L(U)^\perp$.  Then we can uniquely write $v=v^\parallel + v^\perp$, with $v^\parallel \in L(U)$ and $v^\perp \in L(U)^\perp$.  



Thus we should solve $L(u)=v^\parallel$.  In components, $v^\perp$ is just $V-MX$, and is the part we will eventually wish to minimize.  

In terms of $M$, recall that $L(V)$ is spanned by the columns of $M$.  (In the standard basis, the columns of $M$ are $Me_1$, 
$\ldots$, $Me_n$.)  Then $v^\perp$ must be perpendicular to the columns of $M$.  \textit{i.e.}, $M^T(V-MX)=0$, or
\[
M^TMX = M^TV.
\]
Solutions of $M^TMX = M^TV$ for $X$ are called \emph{least squares}\index{Least squares!solutions} solutions to $MX=V$.  
Notice that any solution $X$ to $MX=V$ is a least squares solution.  However, the converse is often false.  In fact, the equation $MX=V$ may have no solutions at all, but still have least squares solutions to $M^TMX = M^TV$.

Observe that since $M$ is an $m\times n$ matrix, then $M^T$ is an $n\times m$ matrix.  Then $M^TM$ is an $n\times n$ matrix, and is symmetric, since $(M^TM)^T=M^TM$.  Then, for any vector $X$, we can evaluate $X^TM^TMX$ to obtain a number.  This is a very nice number, though!  It is just the length $|MX|^2 = (MX)^T(MX)=X^TM^TMX$.

%\href{\webworkurl ReadingHomework25/1/}{Reading homework: problem 25.1}
\Reading{LeastSquares}{1}

Now suppose that $\ker L=\{0\}$, so that the only solution to $MX=0$ is $X=0$. (This need not mean that $M$ is invertible because $M$ is an $n\times m $ matrix, so not necessarily square.) 
However the square matrix $M^TM$ {\it is} invertible. To see this, suppose there was a vector $X$ such that 
$M^T M X=0$. Then it would follow that $X^T M^T M X = |M X|^2=0$. In other words the vector $MX$ would have zero length, so could only be the zero vector. But we are assuming that $\ker L=\{0\}$ so $MX=0$ implies $X=0$. Thus the kernel of $M^TM$ is $\{0\}$ so this matrix is invertible.
So, in this case, the least squares solution (the $X$ that solves $M^TMX=MV$) is unique, and is equal to 
\[
X = (M^TM)^{-1}M^TV.
\]
In a nutshell, this is the least squares method:

\begin{itemize}
\item Compute $M^TM$ and $M^TV$.
\item Solve $(M^TM)X=M^TV$ by Gaussian elimination.
\end{itemize}


\begin{example}
Captain Conundrum\index{Captain Conundrum} falls off of the leaning tower of Pisa and makes three (rather shaky) measurements of his velocity at three different times.

\begin{center}
\begin{tabular}{c|c}
$t$ s & $v $ m/s \\ \hline
$1$ & $11$ \\
$2$ & $19$ \\
$3$ & $31$
\end{tabular}
\end{center}

Having taken some calculus\footnote{In fact, he is a \emph{Calculus Superhero}\index{Calculus Superhero}.}, he believes that his data are best approximated by a straight line
\[
v = at+b.
\]
Then he should find $a$ and $b$ to best fit the data.
\begin{eqnarray*}
11 &=& a\cdot 1 + b \\
19 &=& a\cdot 2 + b \\
31 &=& a\cdot 3 + b.
\end{eqnarray*}
As a system of linear equations, this becomes:

\[
\begin{pmatrix}
1 & 1 \\
2 & 1 \\
3 & 1 \\
\end{pmatrix}
\colvec{a\\b} \stackrel{?}{=}
\colvec{11\\19\\31}.
\]
There is likely no actual straight line solution, so instead solve $M^TMX=M^TV$.

\[
\begin{pmatrix}
1 & 2 & 3 \\
1 & 1 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1 & 1 \\
2 & 1 \\
3 & 1 \\
\end{pmatrix} \colvec{a\\b}
= 
\begin{pmatrix}
1 & 2 & 3 \\
1 & 1 & 1 \\
\end{pmatrix}
\colvec{11\\19\\31}.
\]
This simplifies to 

\[
\begin{amatrix}{2}
14 & 6 & 142 \\
6 & 3 & 61
\end{amatrix}
\sim
\begin{amatrix}{2}
1 & 0 & 10 \\
0 & 1 & \frac{1}{3}
\end{amatrix}.
\]
Thus, the least-squares fit is the line

\[
v = 10\ t + \frac{1}{3}\, .
\]
Notice that this equation implies that Captain Conundrum accelerates towards Italian soil at 10 m/s$^2$ (which is an excellent
approximation to reality) and that he started at a downward velocity of $\frac13$ m/s (perhaps somebody gave him a shove...)!

\end{example}

\section{Projection Matrices}
We have seen that even if $MX=V$ has no solutions $M^TMX=M^T V$ does have solutions. One way to think about this is, since the codomain of $M$ is the direct sum 
$$ \text{codom M}=\text{ran} M \oplus \ker M^T$$ 
there is a unique way to write  $V=V_r+V_k$ with $V_k\in \ker M^T$ and $V_r\in \text{ran }\, M$, and it is clear that $Mx=V$ only has a solution of 
$V\in \text{ran}\, M \Leftrightarrow V_k=0$. If not, then the closest thing to a solution of $MX=V$ is a solution to $MX=V_r$. We learned to find solutions to this in the previous subsection of this book. 

But here is another question, how can we determine what $V_r$ is given $M$ and $V$? The answer is simple; suppose $X$ is a solution to $MX=V_r$. Then
$$  MX=V_r 
\implies M^TMx=M^T V_r 
\implies M^TMx=M^T (V_r + 0) $$ 
$$
\implies M^TMx=M^T (V_r+V_k)
\implies M^TMx=M^T V 
\implies X=(M^TM)^{-1} M^T V 
$$
if indeed $M^TM$ is invertible. Since, by assumption, $X$ is a solution \\
\begin{center}
\shabox{ $M(M^TM)^{-1} M^T\, V =V_r. $}
\end{center}
That is, the matrix which projects $V$ onto its $\text{ran} \, M$ part is $M(M^TM)^{-1} M^T$. 

\begin{example} To project $\colvec{1\\1\\1}$ onto $\spa \left\{    \colvec{ 1\\1\\0}, \colvec{1\\-1\\0 }  \right\} = \text{ran} 
\begin{pmatrix}
 1& 1  \\
1 & -1  \\
0 & 0 
\end{pmatrix}
 $ 
  multiply by the matrix 
 $$
\begin{pmatrix}
 1& 1  \\
1 & -1  \\
0 & 0 
\end{pmatrix}
\left [ 
 \begin{pmatrix}
 1& 1 &0 \\
1 & -1 &0 
\end{pmatrix}
\begin{pmatrix}
 1& 1  \\
1 & -1  \\
0 & 0 
\end{pmatrix}
 \right]^{-1}
 \begin{pmatrix}
 1& 1 &0 \\
1 & -1 &0 
\end{pmatrix}
 $$
 $$
=\begin{pmatrix}
 1& 1  \\
1 & -1  \\
0 & 0 
\end{pmatrix}
 \begin{pmatrix}
 2& 0  \\
0 & 2  
\end{pmatrix}^{-1}
 \begin{pmatrix}
 1& 1 &0 \\
1 & -1 &0 
\end{pmatrix} 
$$
$$
=\frac12 \begin{pmatrix}
 1& 1  \\
1 & -1  \\
0 & 0 
\end{pmatrix}
 \begin{pmatrix}
 1& 1 &0 \\
1 & -1 &0 
\end{pmatrix} 
=
\frac12 \begin{pmatrix}
 2 & 0 &0 \\
0 & 2  &0\\
0 & 0 &0
\end{pmatrix}. 
$$

This gives 
$$\frac12 \begin{pmatrix}
 2 & 0 &0 \\
0 & 2  &0\\
0 & 0 &0
\end{pmatrix}
\colvec{1\\1\\1 } = \colvec{1\\1\\0} .$$
\end{example}



\section{Singular Value Decomposition}

Suppose 
$$
L:V\tolinear W\, .
$$
It is unlikely that $\dim V=:n=m:=\dim W$ so a $m\times n$ matrix $M$ of $L$ in bases for $V$ and $W$ will not be square.
Therefore there is no eigenvalue problem  we can use to uncover a preferred basis. However, if the vector spaces $V$ and 
$W$ both have inner products, there does exist an analog of the eigenvalue problem, namely the singular values of $L$.

Before giving the details of the powerful technique known as the singular value decomposition, we note that it is an 
excellent example of what Eugene Wigner called the ``Unreasonable Effectiveness of Mathematics'':
\begin{quote}{\scriptsize
There is a story about two friends who were classmates in high school, talking about their jobs. One of them became a statistician
and was working on population trends. He showed a reprint to his former classmate.
The reprint started, as usual with the Gaussian distribution and the statistician explained
to his former classmate the meaning of the symbols for the actual population and so on. His classmate
was a bit incredulous and was not quite sure whether the statistician was pulling his leg. ``How can you 
know that?'' was his query. ``And what is this symbol here?'' ``Oh,'' said the statistician, this is ``$\pi$.''
``And what is that?'' ``The ratio of the circumference of the circle to its diameter.'' ``Well, now
you are pushing your joke too far,'' said the classmate, ``surely the population has nothing to do with the 
circumference of the circle.''


Eugene Wigner, Commun. Pure and Appl. Math. {\bf XIII}, 1 (1960).
}
\end{quote}
Whenever we mathematically model a system, any ``canonical quantities'' 
(those that  %on which we can all agree and 
do not
depend on any choices we make for calculating them) will correspond to important features of the system. For examples, the eigenvalues
of the eigenvector equation you found in review question~\ref{stringval}, chapter~\ref{eigenvalseigenvects} encode the notes and harmonics that a guitar string can play! 

Singular values appear in many linear algebra applications, especially those involving very large data sets such as statistics and signal processing. 

Let us focus on the $m\times n$ matrix $M$ of a linear transformation $L:V\to W$ written in orthonormal bases for the input and outputs of $L$ (notice, the existence of these othonormal bases is predicated on having inner products for $V$ and $W$).
Even though the matrix $M$ is not square, both the matrices $M M^T$ and $M^T M$ are square and symmetric! 
In terms of linear transformations $M^T$ is the matrix of a linear transformation 
$$
L^*:W\tolinear V\, .
$$
Thus $LL^*:W\to W$ and $L^*L:V\to V$ and both have eigenvalue problems.
Moreover,  as is shown  in Chapter~\ref{symmetricmatrices},  both $L^*L$ and $LL^*$ have orthonormal bases of eigenvectors, and
 both $MM^T$ and $M^TM$ can be diagonalized. 
 
Next, let us make a simplifying assumption, namely $\ker L=\{0\}$. This is not necessary, but will make some of our computations simpler.
Now suppose we have found an orthonormal basis $(u_1,\ldots , u_n)$ for $V$ composed of eigenvectors for $L^*L$. That is 
$$
L^*L u_i= \lambda_i u_i\, .
$$
Then multiplying by $L$ gives 
$$
L L^* L u_i = \lambda_i L u_i\, .
$$
{\it I.e.}, $L u_i$ is an eigenvector of $L L^*$.
The vectors $(Lu_1,\ldots, Lu_n)$ are linearly independent, because $\ker L=\{0\}$ (this is where we use our simplifying assumption, but you can 
try and extend our analysis to the case where it no longer holds). 

Lets compute the angles between and lengths of these vectors. 
For that we express the vectors $u_i$ in the bases used to compute the matrix $M$ of $L$. Denoting these column vectors by $U_i$ we then compute
$$
(MU_i)\cdot (MU_j)=U_i^T M^T M U_j = \lambda_j \, U_i^T U_j=\lambda_j \, U_i\cdot U_j = \lambda_j \delta_{ij}\, .
$$
We see that  vectors $(Lu_1,\ldots, Lu_n)$ are orthogonal but not orthonormal. Moreover, the length of $Lu_i$ is $\sqrt{\lambda_i}$.
Normalizing gives the orthonormal and linearly independent ordered set
$$
\left(\frac{Lu_1}{\sqrt{\lambda_1}},\ldots,\frac{Lu_n}{\sqrt{\lambda_n}}\right).
$$

In general, this cannot be a basis for $W$ 
since $\ker L=\{0\},~\dim L(V)=\dim V,$
and in turn $\dim V\leq \dim W$, so $n\leq m$. 

However,  it is a subset of the eigenvectors of $LL^*$ so there is an orthonormal basis of eigenvectors of $LL^*$ of the form 
$$
O'=\left(\frac{Lu_1}{\sqrt{\lambda_1}},\ldots,\frac{Lu_n}{\sqrt{\lambda_n}},v_{n+1},\ldots,v_{m}\right)=:(v_1,\ldots,v_m)\, .
$$
Now lets compute the matrix of $L$ with respect to the orthonormal basis $O=(u_1,\ldots,u_n)$ for $V$ and the orthonormal basis~$O'=(v_1,\ldots,v_m)$ for~$W$. As usual, our starting point is the computation of $L$ acting on the input basis vectors;
\begin{eqnarray*}
LO=\big(Lu_1,\ldots, Lu_n\big)&=&
\big(\sqrt{\lambda_1}\,  v_1,\ldots,\sqrt{\lambda_n}\,  v_n\big)\\[2mm]&=&\big(v_1,\ldots,v_m\big)
%\begin{pmatrix}
%\sqrt{\lambda_1}&\mc0&\cdots&\mc0&0&\cdots&0\\[1mm]
%\mc0&\sqrt{\lambda_2}&\cdots&\mc0&0&\cdots&0\\
%\mc{\vdots}&\mc\vdots&\ddots&\mc\vdots&\mc\vdots&&\mc\vdots\\[1mm]
%\mc0&\mc0&\cdots&\sqrt{\lambda_n}&0&\cdots&0\\
%\end{pmatrix}
\begin{pmatrix}
\sqrt{\lambda_1}&\mc0&\cdots&\mc0\\[1mm]
\mc0&\sqrt{\lambda_2}&\cdots&\mc0\\
\mc{\vdots}&\mc\vdots&\ddots&\mc\vdots\\[1mm]
\mc0&\mc0&\cdots&\sqrt{\lambda_n}\\[1mm]
\mc 0 & \mc 0& \cdots &\mc 0\\
\mc{\vdots}&\mc\vdots&&\mc\vdots\\
\mc 0 & \mc 0& \cdots &\mc 0
\end{pmatrix}\, .
\end{eqnarray*}
The result is very close to diagonalization; the numbers $\sqrt{\lambda_i}$ along the leading diagonal are called the singular values of $L$.

\begin{example} Let the matrix of a linear transformation be
$$
M=\begin{pmatrix}
\frac12&\frac12\\[1mm]-1&1\\[1mm]-\frac12&-\frac12
\end{pmatrix}\, .
$$
Clearly $\ker M=\{0\}$ while
$$
M^TM=\begin{pmatrix}\frac32&-\frac12\\[2mm]-\frac12&\frac32\end{pmatrix}
$$
which has eigenvalues and eigenvectors
$$
 \lambda=1\, ,\,  u_1:=\colvec{\frac{1}{\sqrt2}\\[2mm]\frac{1}{\sqrt2}}; \qquad
\lambda=2\, ,\,  u_2:=\colvec{\frac{1}{\sqrt2}\\[2mm]-\frac{1}{\sqrt2}}\,\, .
$$
so our orthonormal input basis is $$O=\left(\colvec{\frac{1}{\sqrt2}\\[2mm]\frac{1}{\sqrt2}},\colvec{\frac{1}{\sqrt2}\\[2mm]-\frac{1}{\sqrt2}}\right)\, .
$$
These are called the {\it right singular vectors}\index{Right singular vector} of $M$.
The vectors 
$$
M u_1= \colvec{\frac1{\sqrt{2}}\\[1mm]\mc{\ \ 0}\\-\frac1{\sqrt{2}}}\mbox{ and }
M u_2=\ccolvec{0\\[1mm]-\sqrt{2}\\[1mm]0}
$$
are eigenvectors of 
$$M M^T=\begin{pmatrix}\frac12&\ 0&\!-\frac12\\0&2&0\\-\frac12&0&\frac12\end{pmatrix}$$ 
with eigenvalues $1$ and $2$, respectively. The third eigenvector (with eigenvalue~$0$) of $MM^T$ is 
$$v_3=\colvec{\frac1{\sqrt{2}}\\[1mm]\mc{\ \ 0}\\ \frac1{\sqrt{2}}}\, .$$
The eigenvectors $Mu_1$ and $Mu_2$ are necessarily orthogonal, dividing them by their lengths we obtain the {\it left singular vectors}\index{Left singular vectors} and in turn  our orthonormal output basis
$$
O'=\left(\colvec{\frac1{\sqrt{2}}\\[1mm]\mc{\ \ 0}\\-\frac1{\sqrt{2}}},\ccolvec{0\\[1mm]-1\\[1mm]0},\colvec{\frac1{\sqrt{2}}\\[1mm]\mc{\ \ 0}\\\frac1{\sqrt{2}}}\right)\, .
$$
The new matrix~$M'$ of the linear transformation given by $M$ with respect to the bases $O$ and $O'$ is
$$
M'=\begin{pmatrix}
1&0\\0&\sqrt{2}\\0&0
\end{pmatrix}\, ,
$$
so the singular values are $1,\sqrt{2}$. 

Finally note that arranging the column vectors of $O$ and $O'$ into change of basis matrices
$$
P=\begin{pmatrix}
\frac1{\sqrt{2}}&\frac1{\sqrt{2}}\\[2mm]
\frac1{\sqrt{2}}&-\frac1{\sqrt{2}}
\end{pmatrix}\, ,\qquad
Q=
\begin{pmatrix}
\frac1{\sqrt{2}}&0&\frac1{\sqrt{2}}\\[2mm]
\mc {\ \ 0}&-1&\mc 0\\[2mm]
\!-\frac1{\sqrt{2}}&0&\frac1{\sqrt{2}}
\end{pmatrix}\, ,
$$
we have, as usual,
$$
M'=Q^{-1}MP\, .
$$
\end{example}

Singular vectors and values have a very nice geometric interpretation; they provide an orthonormal bases for the domain and range of $L$
and give the factors by which $L$ stretches the orthonormal input basis vectors. This is depicted below for the example we just computed.
\begin{center}
\includegraphics[scale=.27]{singval.jpg}
\end{center} 



%{\it Congratulations, you have reached the end of these notes! You can test your skills
%on the \hyperref[sample3]{sample final exam}.}
\begin{center}
\shabox{
{\bf \hyperref[sample3]{\begin{tabular}{c}Congratulations, you have reached the end of the book! \\[2mm]
\includegraphics[scale=.15]{final.jpg}\\
Now test your skills on the \hyperref[sample3]{sample final exam}.
%You are now ready to 
%apply for membership in\\
% be a minion of Captain Conundrum's nemesis, 
%The League of Ninjas of Numbers. 
%Now test your skills
%on the sample final exam. 
\end{tabular}
}}}
\end{center}













%\section*{References}
%Hefferon, Chapter Three, Section VI.2: Gram-Schmidt Orthogonalization \\
%Beezer, Part A, Section CF, Subsection DF \\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Linear_least_squares}{Linear Least Squares}
%\item \href{http://en.wikipedia.org/wiki/Least_squares}{Least Squares}
%\end{itemize}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problem & 
 \hwrref{LeastSquares}{1}, 
\\
   \hline
\end{tabular}


\input{\leastSquaresPath/problems}


\newpage
","
\chapter{Least squares and Singular Values}
\label{sec:leastsquaresSVD}
\index{Least squares}

Consider the linear algebraic equation $L(x)=v$, where $L \colon U\stackrel{\text{linear}}{-\!\!\!-\!\!\!\longrightarrow}W$ and $v\in W$ are known while $x$ is unknown. As we have seen, this system may have 
one solution, no solutions, or infinitely many solutions.  
But if $v$ is not in the range of $L$ there will {\it never} be any solutions for $L(x)=v$.
\vspace{-.1cm}
\begin{center}
\includegraphics[scale=.24]{notinimage.jpg}
\end{center} 
\vspace{-1.8cm}
However, for many applications we do not need an exact solution of the system; instead, we may only need the best approximation possible.  

\begin{quote}
``My work always tried to unite the Truth with the Beautiful, but when I had to choose one or the other, I usually chose the Beautiful.'' 

\vspace{-2mm}
\hspace{7cm}-- Hermann Weyl.
\end{quote}

If the vector space $W$ has a notion of lengths of vectors, we can try to find $x$ that minimizes $||L(x)-v||$.
\begin{center}
\includegraphics[scale=.24]{minimize.jpg}
\end{center} 
This method has many applications, such as when trying to fit a (perhaps linear) function to a ``noisy'' set of observations.  For example, suppose we measured the position of a bicycle on a racetrack once every five seconds.  Our observations won't be exact, but so long as the observations are right on average, we can figure out a best-possible linear function of position of the bicycle in terms of time.

Suppose $M$ is the matrix for the linear function $L:U \to W$ in some bases for $U$ and $W$. The vectors~$v$ and~$x$ are represented by column vectors $V$ and $X$ in these bases.  Then we need to approximate
\[
MX-V\approx 0\, .
\]

Note that if $\dim U=n$ and $\dim W=m$ then $M$ can be represented by an $m\times n$ matrix and $x$ and $v$ as vectors in $\Re^n$ and $\Re^m$, respectively. Thus, we can write $W=L(U)\oplus L(U)^\perp$.  Then we can uniquely write $v=v^\parallel + v^\perp$, with $v^\parallel \in L(U)$ and $v^\perp \in L(U)^\perp$.  



Thus we should solve $L(u)=v^\parallel$.  In components, $v^\perp$ is just $V-MX$, and is the part we will eventually wish to minimize.  

In terms of $M$, recall that $L(V)$ is spanned by the columns of $M$.  (In the standard basis, the columns of $M$ are $Me_1$, 
$\ldots$, $Me_n$.)  Then $v^\perp$ must be perpendicular to the columns of $M$.  \textit{i.e.}, $M^T(V-MX)=0$, or
\[
M^TMX = M^TV.
\]
Solutions of $M^TMX = M^TV$ for $X$ are called \emph{least squares}\index{Least squares!solutions} solutions to $MX=V$.  
Notice that any solution $X$ to $MX=V$ is a least squares solution.  However, the converse is often false.  In fact, the equation $MX=V$ may have no solutions at all, but still have least squares solutions to $M^TMX = M^TV$.

Observe that since $M$ is an $m\times n$ matrix, then $M^T$ is an $n\times m$ matrix.  Then $M^TM$ is an $n\times n$ matrix, and is symmetric, since $(M^TM)^T=M^TM$.  Then, for any vector $X$, we can evaluate $X^TM^TMX$ to obtain a number.  This is a very nice number, though!  It is just the length $|MX|^2 = (MX)^T(MX)=X^TM^TMX$.

%\href{\webworkurl ReadingHomework25/1/}{Reading homework: problem 25.1}
\Reading{LeastSquares}{1}

Now suppose that $\ker L=\{0\}$, so that the only solution to $MX=0$ is $X=0$. (This need not mean that $M$ is invertible because $M$ is an $n\times m $ matrix, so not necessarily square.) 
However the square matrix $M^TM$ {\it is} invertible. To see this, suppose there was a vector $X$ such that 
$M^T M X=0$. Then it would follow that $X^T M^T M X = |M X|^2=0$. In other words the vector $MX$ would have zero length, so could only be the zero vector. But we are assuming that $\ker L=\{0\}$ so $MX=0$ implies $X=0$. Thus the kernel of $M^TM$ is $\{0\}$ so this matrix is invertible.
So, in this case, the least squares solution (the $X$ that solves $M^TMX=MV$) is unique, and is equal to 
\[
X = (M^TM)^{-1}M^TV.
\]
In a nutshell, this is the least squares method:

\begin{itemize}
\item Compute $M^TM$ and $M^TV$.
\item Solve $(M^TM)X=M^TV$ by Gaussian elimination.
\end{itemize}


\begin{example}
Captain Conundrum\index{Captain Conundrum} falls off of the leaning tower of Pisa and makes three (rather shaky) measurements of his velocity at three different times.

\begin{center}
\begin{tabular}{c|c}
$t$ s & $v $ m/s \\ \hline
$1$ & $11$ \\
$2$ & $19$ \\
$3$ & $31$
\end{tabular}
\end{center}

Having taken some calculus\footnote{In fact, he is a \emph{Calculus Superhero}\index{Calculus Superhero}.}, he believes that his data are best approximated by a straight line
\[
v = at+b.
\]
Then he should find $a$ and $b$ to best fit the data.
\begin{eqnarray*}
11 &=& a\cdot 1 + b \\
19 &=& a\cdot 2 + b \\
31 &=& a\cdot 3 + b.
\end{eqnarray*}
As a system of linear equations, this becomes:

\[
\begin{pmatrix}
1 & 1 \\
2 & 1 \\
3 & 1 \\
\end{pmatrix}
\colvec{a\\b} \stackrel{?}{=}
\colvec{11\\19\\31}.
\]
There is likely no actual straight line solution, so instead solve $M^TMX=M^TV$.

\[
\begin{pmatrix}
1 & 2 & 3 \\
1 & 1 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1 & 1 \\
2 & 1 \\
3 & 1 \\
\end{pmatrix} \colvec{a\\b}
= 
\begin{pmatrix}
1 & 2 & 3 \\
1 & 1 & 1 \\
\end{pmatrix}
\colvec{11\\19\\31}.
\]
This simplifies to 

\[
\begin{amatrix}{2}
14 & 6 & 142 \\
6 & 3 & 61
\end{amatrix}
\sim
\begin{amatrix}{2}
1 & 0 & 10 \\
0 & 1 & \frac{1}{3}
\end{amatrix}.
\]
Thus, the least-squares fit is the line

\[
v = 10\ t + \frac{1}{3}\, .
\]
Notice that this equation implies that Captain Conundrum accelerates towards Italian soil at 10 m/s$^2$ (which is an excellent
approximation to reality) and that he started at a downward velocity of $\frac13$ m/s (perhaps somebody gave him a shove...)!

\end{example}

\section{Projection Matrices}
We have seen that even if $MX=V$ has no solutions $M^TMX=M^T V$ does have solutions. One way to think about this is, since the codomain of $M$ is the direct sum 
$$ \text{codom M}=\text{ran} M \oplus \ker M^T$$ 
there is a unique way to write  $V=V_r+V_k$ with $V_k\in \ker M^T$ and $V_r\in \text{ran }\, M$, and it is clear that $Mx=V$ only has a solution of 
$V\in \text{ran}\, M \Leftrightarrow V_k=0$. If not, then the closest thing to a solution of $MX=V$ is a solution to $MX=V_r$. We learned to find solutions to this in the previous subsection of this book. 

But here is another question, how can we determine what $V_r$ is given $M$ and $V$? The answer is simple; suppose $X$ is a solution to $MX=V_r$. Then
$$  MX=V_r 
\implies M^TMx=M^T V_r 
\implies M^TMx=M^T (V_r + 0) $$ 
$$
\implies M^TMx=M^T (V_r+V_k)
\implies M^TMx=M^T V 
\implies X=(M^TM)^{-1} M^T V 
$$
if indeed $M^TM$ is invertible. Since, by assumption, $X$ is a solution \\
\begin{center}
\shabox{ $M(M^TM)^{-1} M^T\, V =V_r. $}
\end{center}
That is, the matrix which projects $V$ onto its $\text{ran} \, M$ part is $M(M^TM)^{-1} M^T$. 

\begin{example} To project $\colvec{1\\1\\1}$ onto $\spa \left\{    \colvec{ 1\\1\\0}, \colvec{1\\-1\\0 }  \right\} = \text{ran} 
\begin{pmatrix}
 1& 1  \\
1 & -1  \\
0 & 0 
\end{pmatrix}
 $ 
  multiply by the matrix 
 $$
\begin{pmatrix}
 1& 1  \\
1 & -1  \\
0 & 0 
\end{pmatrix}
\left [ 
 \begin{pmatrix}
 1& 1 &0 \\
1 & -1 &0 
\end{pmatrix}
\begin{pmatrix}
 1& 1  \\
1 & -1  \\
0 & 0 
\end{pmatrix}
 \right]^{-1}
 \begin{pmatrix}
 1& 1 &0 \\
1 & -1 &0 
\end{pmatrix}
 $$
 $$
=\begin{pmatrix}
 1& 1  \\
1 & -1  \\
0 & 0 
\end{pmatrix}
 \begin{pmatrix}
 2& 0  \\
0 & 2  
\end{pmatrix}^{-1}
 \begin{pmatrix}
 1& 1 &0 \\
1 & -1 &0 
\end{pmatrix} 
$$
$$
=\frac12 \begin{pmatrix}
 1& 1  \\
1 & -1  \\
0 & 0 
\end{pmatrix}
 \begin{pmatrix}
 1& 1 &0 \\
1 & -1 &0 
\end{pmatrix} 
=
\frac12 \begin{pmatrix}
 2 & 0 &0 \\
0 & 2  &0\\
0 & 0 &0
\end{pmatrix}. 
$$

This gives 
$$\frac12 \begin{pmatrix}
 2 & 0 &0 \\
0 & 2  &0\\
0 & 0 &0
\end{pmatrix}
\colvec{1\\1\\1 } = \colvec{1\\1\\0} .$$
\end{example}



\section{Singular Value Decomposition}

Suppose 
$$
L:V\tolinear W\, .
$$
It is unlikely that $\dim V=:n=m:=\dim W$ so a $m\times n$ matrix $M$ of $L$ in bases for $V$ and $W$ will not be square.
Therefore there is no eigenvalue problem  we can use to uncover a preferred basis. However, if the vector spaces $V$ and 
$W$ both have inner products, there does exist an analog of the eigenvalue problem, namely the singular values of $L$.

Before giving the details of the powerful technique known as the singular value decomposition, we note that it is an 
excellent example of what Eugene Wigner called the ``Unreasonable Effectiveness of Mathematics'':
\begin{quote}{\scriptsize
There is a story about two friends who were classmates in high school, talking about their jobs. One of them became a statistician
and was working on population trends. He showed a reprint to his former classmate.
The reprint started, as usual with the Gaussian distribution and the statistician explained
to his former classmate the meaning of the symbols for the actual population and so on. His classmate
was a bit incredulous and was not quite sure whether the statistician was pulling his leg. ``How can you 
know that?'' was his query. ``And what is this symbol here?'' ``Oh,'' said the statistician, this is ``$\pi$.''
``And what is that?'' ``The ratio of the circumference of the circle to its diameter.'' ``Well, now
you are pushing your joke too far,'' said the classmate, ``surely the population has nothing to do with the 
circumference of the circle.''


Eugene Wigner, Commun. Pure and Appl. Math. {\bf XIII}, 1 (1960).
}
\end{quote}
Whenever we mathematically model a system, any ``canonical quantities'' 
(those that  %on which we can all agree and 
do not
depend on any choices we make for calculating them) will correspond to important features of the system. For examples, the eigenvalues
of the eigenvector equation you found in review question~\ref{stringval}, chapter~\ref{eigenvalseigenvects} encode the notes and harmonics that a guitar string can play! 

Singular values appear in many linear algebra applications, especially those involving very large data sets such as statistics and signal processing. 

Let us focus on the $m\times n$ matrix $M$ of a linear transformation $L:V\to W$ written in orthonormal bases for the input and outputs of $L$ (notice, the existence of these othonormal bases is predicated on having inner products for $V$ and $W$).
Even though the matrix $M$ is not square, both the matrices $M M^T$ and $M^T M$ are square and symmetric! 
In terms of linear transformations $M^T$ is the matrix of a linear transformation 
$$
L^*:W\tolinear V\, .
$$
Thus $LL^*:W\to W$ and $L^*L:V\to V$ and both have eigenvalue problems.
Moreover,  as is shown  in Chapter~\ref{symmetricmatrices},  both $L^*L$ and $LL^*$ have orthonormal bases of eigenvectors, and
 both $MM^T$ and $M^TM$ can be diagonalized. 
 
Next, let us make a simplifying assumption, namely $\ker L=\{0\}$. This is not necessary, but will make some of our computations simpler.
Now suppose we have found an orthonormal basis $(u_1,\ldots , u_n)$ for $V$ composed of eigenvectors for $L^*L$. That is 
$$
L^*L u_i= \lambda_i u_i\, .
$$
Then multiplying by $L$ gives 
$$
L L^* L u_i = \lambda_i L u_i\, .
$$
{\it I.e.}, $L u_i$ is an eigenvector of $L L^*$.
The vectors $(Lu_1,\ldots, Lu_n)$ are linearly independent, because $\ker L=\{0\}$ (this is where we use our simplifying assumption, but you can 
try and extend our analysis to the case where it no longer holds). 

Lets compute the angles between and lengths of these vectors. 
For that we express the vectors $u_i$ in the bases used to compute the matrix $M$ of $L$. Denoting these column vectors by $U_i$ we then compute
$$
(MU_i)\cdot (MU_j)=U_i^T M^T M U_j = \lambda_j \, U_i^T U_j=\lambda_j \, U_i\cdot U_j = \lambda_j \delta_{ij}\, .
$$
We see that  vectors $(Lu_1,\ldots, Lu_n)$ are orthogonal but not orthonormal. Moreover, the length of $Lu_i$ is $\sqrt{\lambda_i}$.
Normalizing gives the orthonormal and linearly independent ordered set
$$
\left(\frac{Lu_1}{\sqrt{\lambda_1}},\ldots,\frac{Lu_n}{\sqrt{\lambda_n}}\right).
$$

In general, this cannot be a basis for $W$ 
since $\ker L=\{0\},~\dim L(V)=\dim V,$
and in turn $\dim V\leq \dim W$, so $n\leq m$. 

However,  it is a subset of the eigenvectors of $LL^*$ so there is an orthonormal basis of eigenvectors of $LL^*$ of the form 
$$
O'=\left(\frac{Lu_1}{\sqrt{\lambda_1}},\ldots,\frac{Lu_n}{\sqrt{\lambda_n}},v_{n+1},\ldots,v_{m}\right)=:(v_1,\ldots,v_m)\, .
$$
Now lets compute the matrix of $L$ with respect to the orthonormal basis $O=(u_1,\ldots,u_n)$ for $V$ and the orthonormal basis~$O'=(v_1,\ldots,v_m)$ for~$W$. As usual, our starting point is the computation of $L$ acting on the input basis vectors;
\begin{eqnarray*}
LO=\big(Lu_1,\ldots, Lu_n\big)&=&
\big(\sqrt{\lambda_1}\,  v_1,\ldots,\sqrt{\lambda_n}\,  v_n\big)\\[2mm]&=&\big(v_1,\ldots,v_m\big)
%\begin{pmatrix}
%\sqrt{\lambda_1}&\mc0&\cdots&\mc0&0&\cdots&0\\[1mm]
%\mc0&\sqrt{\lambda_2}&\cdots&\mc0&0&\cdots&0\\
%\mc{\vdots}&\mc\vdots&\ddots&\mc\vdots&\mc\vdots&&\mc\vdots\\[1mm]
%\mc0&\mc0&\cdots&\sqrt{\lambda_n}&0&\cdots&0\\
%\end{pmatrix}
\begin{pmatrix}
\sqrt{\lambda_1}&\mc0&\cdots&\mc0\\[1mm]
\mc0&\sqrt{\lambda_2}&\cdots&\mc0\\
\mc{\vdots}&\mc\vdots&\ddots&\mc\vdots\\[1mm]
\mc0&\mc0&\cdots&\sqrt{\lambda_n}\\[1mm]
\mc 0 & \mc 0& \cdots &\mc 0\\
\mc{\vdots}&\mc\vdots&&\mc\vdots\\
\mc 0 & \mc 0& \cdots &\mc 0
\end{pmatrix}\, .
\end{eqnarray*}
The result is very close to diagonalization; the numbers $\sqrt{\lambda_i}$ along the leading diagonal are called the singular values of $L$.

\begin{example} Let the matrix of a linear transformation be
$$
M=\begin{pmatrix}
\frac12&\frac12\\[1mm]-1&1\\[1mm]-\frac12&-\frac12
\end{pmatrix}\, .
$$
Clearly $\ker M=\{0\}$ while
$$
M^TM=\begin{pmatrix}\frac32&-\frac12\\[2mm]-\frac12&\frac32\end{pmatrix}
$$
which has eigenvalues and eigenvectors
$$
 \lambda=1\, ,\,  u_1:=\colvec{\frac{1}{\sqrt2}\\[2mm]\frac{1}{\sqrt2}}; \qquad
\lambda=2\, ,\,  u_2:=\colvec{\frac{1}{\sqrt2}\\[2mm]-\frac{1}{\sqrt2}}\,\, .
$$
so our orthonormal input basis is $$O=\left(\colvec{\frac{1}{\sqrt2}\\[2mm]\frac{1}{\sqrt2}},\colvec{\frac{1}{\sqrt2}\\[2mm]-\frac{1}{\sqrt2}}\right)\, .
$$
These are called the {\it right singular vectors}\index{Right singular vector} of $M$.
The vectors 
$$
M u_1= \colvec{\frac1{\sqrt{2}}\\[1mm]\mc{\ \ 0}\\-\frac1{\sqrt{2}}}\mbox{ and }
M u_2=\ccolvec{0\\[1mm]-\sqrt{2}\\[1mm]0}
$$
are eigenvectors of 
$$M M^T=\begin{pmatrix}\frac12&\ 0&\!-\frac12\\0&2&0\\-\frac12&0&\frac12\end{pmatrix}$$ 
with eigenvalues $1$ and $2$, respectively. The third eigenvector (with eigenvalue~$0$) of $MM^T$ is 
$$v_3=\colvec{\frac1{\sqrt{2}}\\[1mm]\mc{\ \ 0}\\ \frac1{\sqrt{2}}}\, .$$
The eigenvectors $Mu_1$ and $Mu_2$ are necessarily orthogonal, dividing them by their lengths we obtain the {\it left singular vectors}\index{Left singular vectors} and in turn  our orthonormal output basis
$$
O'=\left(\colvec{\frac1{\sqrt{2}}\\[1mm]\mc{\ \ 0}\\-\frac1{\sqrt{2}}},\ccolvec{0\\[1mm]-1\\[1mm]0},\colvec{\frac1{\sqrt{2}}\\[1mm]\mc{\ \ 0}\\\frac1{\sqrt{2}}}\right)\, .
$$
The new matrix~$M'$ of the linear transformation given by $M$ with respect to the bases $O$ and $O'$ is
$$
M'=\begin{pmatrix}
1&0\\0&\sqrt{2}\\0&0
\end{pmatrix}\, ,
$$
so the singular values are $1,\sqrt{2}$. 

Finally note that arranging the column vectors of $O$ and $O'$ into change of basis matrices
$$
P=\begin{pmatrix}
\frac1{\sqrt{2}}&\frac1{\sqrt{2}}\\[2mm]
\frac1{\sqrt{2}}&-\frac1{\sqrt{2}}
\end{pmatrix}\, ,\qquad
Q=
\begin{pmatrix}
\frac1{\sqrt{2}}&0&\frac1{\sqrt{2}}\\[2mm]
\mc {\ \ 0}&-1&\mc 0\\[2mm]
\!-\frac1{\sqrt{2}}&0&\frac1{\sqrt{2}}
\end{pmatrix}\, ,
$$
we have, as usual,
$$
M'=Q^{-1}MP\, .
$$
\end{example}

Singular vectors and values have a very nice geometric interpretation; they provide an orthonormal bases for the domain and range of $L$
and give the factors by which $L$ stretches the orthonormal input basis vectors. This is depicted below for the example we just computed.
\begin{center}
\includegraphics[scale=.27]{singval.jpg}
\end{center} 



%{\it Congratulations, you have reached the end of these notes! You can test your skills
%on the \hyperref[sample3]{sample final exam}.}
\begin{center}
\shabox{
{\bf \hyperref[sample3]{\begin{tabular}{c}Congratulations, you have reached the end of the book! \\[2mm]
\includegraphics[scale=.15]{final.jpg}\\
Now test your skills on the \hyperref[sample3]{sample final exam}.
%You are now ready to 
%apply for membership in\\
% be a minion of Captain Conundrum's nemesis, 
%The League of Ninjas of Numbers. 
%Now test your skills
%on the sample final exam. 
\end{tabular}
}}}
\end{center}













%\section*{References}
%Hefferon, Chapter Three, Section VI.2: Gram-Schmidt Orthogonalization \\
%Beezer, Part A, Section CF, Subsection DF \\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Linear_least_squares}{Linear Least Squares}
%\item \href{http://en.wikipedia.org/wiki/Least_squares}{Least Squares}
%\end{itemize}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problem & 
 \hwrref{LeastSquares}{1}, 
\\
   \hline
\end{tabular}


\input{\leastSquaresPath/problems}


\newpage
",lesson
